"""
é é¢çµæ§‹åˆ†æå™¨
è¨ªå•æ¸¬è©¦é é¢ï¼Œä¿å­˜HTMLå¿«ç…§ï¼Œåˆ†æDOMçµæ§‹ä»¥ä¿®å¾©é¸æ“‡å™¨
"""

import asyncio
import json
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Optional, List
from bs4 import BeautifulSoup
import re

import sys
sys.path.append(str(Path(__file__).parent))

from playwright.async_api import async_playwright
from common.config import get_auth_file_path

# æ¸¬è©¦ç›®æ¨™
TEST_URL = "https://www.threads.net/@threads/post/DMxtXaggxsL"

class PageAnalyzer:
    """é é¢çµæ§‹åˆ†æå™¨"""
    
    def __init__(self):
        self.auth_file_path = get_auth_file_path()
    
    async def analyze_page_structure(self):
        """åˆ†æé é¢çµæ§‹ä¸¦ä¿å­˜HTMLå¿«ç…§"""
        print("ğŸ” åˆ†æé é¢çµæ§‹...")
        print(f"ğŸ¯ ç›®æ¨™é é¢: {TEST_URL}")
        
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=False)
            context = await browser.new_context(
                storage_state=str(self.auth_file_path),
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
                viewport={"width": 1920, "height": 1080}
            )
            
            page = await context.new_page()
            
            try:
                # å°èˆªåˆ°é é¢
                print("   ğŸŒ å°èˆªåˆ°æ¸¬è©¦é é¢...")
                await page.goto(TEST_URL, wait_until="networkidle", timeout=60000)
                await asyncio.sleep(5)  # ç­‰å¾…å®Œå…¨è¼‰å…¥
                
                # ä¿å­˜é é¢å¿«ç…§
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                
                # 1. ä¿å­˜å®Œæ•´HTML
                html_content = await page.content()
                html_file = Path(f"page_snapshot_{timestamp}.html")
                with open(html_file, 'w', encoding='utf-8') as f:
                    f.write(html_content)
                print(f"   ğŸ“„ HTMLå¿«ç…§å·²ä¿å­˜: {html_file}")
                
                # 2. ä¿å­˜æˆªåœ–
                screenshot_file = Path(f"page_screenshot_{timestamp}.png")
                await page.screenshot(path=str(screenshot_file), full_page=True)
                print(f"   ğŸ“¸ æˆªåœ–å·²ä¿å­˜: {screenshot_file}")
                
                # 3. åˆ†æDOMçµæ§‹
                print("   ğŸ” åˆ†æDOMçµæ§‹...")
                analysis_result = await self._analyze_dom_structure(page, html_content)
                
                # 4. ä¿å­˜åˆ†æçµæœ
                analysis_file = Path(f"dom_analysis_{timestamp}.json")
                with open(analysis_file, 'w', encoding='utf-8') as f:
                    json.dump(analysis_result, f, indent=2, ensure_ascii=False)
                print(f"   ğŸ“Š åˆ†æçµæœå·²ä¿å­˜: {analysis_file}")
                
                # 5. ç”Ÿæˆä¿®å¾©å»ºè­°
                suggestions = self._generate_fix_suggestions(analysis_result)
                suggestions_file = Path(f"fix_suggestions_{timestamp}.md")
                with open(suggestions_file, 'w', encoding='utf-8') as f:
                    f.write(suggestions)
                print(f"   ğŸ’¡ ä¿®å¾©å»ºè­°å·²ä¿å­˜: {suggestions_file}")
                
                print(f"\nâœ… é é¢åˆ†æå®Œæˆï¼")
                print(f"   ğŸ“ ç”Ÿæˆäº† 4 å€‹æ–‡ä»¶ï¼šHTMLå¿«ç…§ã€æˆªåœ–ã€åˆ†æçµæœã€ä¿®å¾©å»ºè­°")
                
                return analysis_result
                
            except Exception as e:
                print(f"   âŒ é é¢åˆ†æå¤±æ•—: {e}")
                return None
            finally:
                await browser.close()
    
    async def _analyze_dom_structure(self, page, html_content: str) -> Dict[str, Any]:
        """åˆ†æDOMçµæ§‹ï¼Œå°‹æ‰¾äº’å‹•æ•¸æ“šå…ƒç´ """
        analysis = {
            "page_info": {
                "url": TEST_URL,
                "title": await page.title(),
                "analyzed_at": datetime.now().isoformat()
            },
            "potential_selectors": {
                "likes": [],
                "comments": [],
                "reposts": [],
                "shares": [],
                "content": []
            },
            "found_numbers": [],
            "button_elements": [],
            "aria_labels": []
        }
        
        # ä½¿ç”¨BeautifulSoupè§£æHTML
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # 1. æ‰¾åˆ°æ‰€æœ‰åŒ…å«æ•¸å­—çš„å…ƒç´ 
        print("      ğŸ”¢ æœç´¢åŒ…å«æ•¸å­—çš„å…ƒç´ ...")
        all_elements = soup.find_all(string=re.compile(r'\d+'))
        for element in all_elements[:50]:  # é™åˆ¶æ•¸é‡é¿å…å¤ªå¤š
            parent = element.parent
            if parent:
                text = element.strip()
                if re.search(r'\d+', text):
                    analysis["found_numbers"].append({
                        "text": text,
                        "tag": parent.name,
                        "classes": parent.get('class', []),
                        "aria_label": parent.get('aria-label', ''),
                        "parent_tag": parent.parent.name if parent.parent else None
                    })
        
        # 2. æ‰¾åˆ°æ‰€æœ‰æŒ‰éˆ•å…ƒç´ 
        print("      ğŸ”˜ åˆ†ææŒ‰éˆ•å…ƒç´ ...")
        buttons = soup.find_all(['button', 'div'], attrs={'role': 'button'})
        for button in buttons[:30]:  # é™åˆ¶æ•¸é‡
            button_info = {
                "tag": button.name,
                "aria_label": button.get('aria-label', ''),
                "classes": button.get('class', []),
                "text": button.get_text().strip()[:100],  # é™åˆ¶é•·åº¦
                "has_svg": bool(button.find('svg')),
                "data_attributes": {k: v for k, v in button.attrs.items() if k.startswith('data-')}
            }
            analysis["button_elements"].append(button_info)
        
        # 3. æ”¶é›†æ‰€æœ‰aria-label
        print("      ğŸ·ï¸ æ”¶é›†aria-labelå±¬æ€§...")
        aria_elements = soup.find_all(attrs={'aria-label': True})
        for element in aria_elements[:50]:
            aria_label = element.get('aria-label', '')
            if aria_label:
                analysis["aria_labels"].append({
                    "aria_label": aria_label,
                    "tag": element.name,
                    "classes": element.get('class', []),
                    "text": element.get_text().strip()[:50]
                })
        
        # 4. åŸºæ–¼å•Ÿç™¼å¼æ–¹æ³•æ¨è–¦é¸æ“‡å™¨
        print("      ğŸ¯ ç”Ÿæˆé¸æ“‡å™¨å»ºè­°...")
        
        # æŒ‰è®šç›¸é—œ
        like_keywords = ['like', 'likes', 'å–œæ­¡', 'æŒ‰è®š', 'ğŸ‘', 'â¤ï¸']
        for item in analysis["aria_labels"] + analysis["button_elements"]:
            text = (item.get('aria_label', '') + ' ' + item.get('text', '')).lower()
            if any(keyword in text for keyword in like_keywords):
                selector = self._generate_selector_from_element(item)
                if selector:
                    analysis["potential_selectors"]["likes"].append(selector)
        
        # ç•™è¨€ç›¸é—œ
        comment_keywords = ['comment', 'comments', 'ç•™è¨€', 'è©•è«–', 'ğŸ’¬']
        for item in analysis["aria_labels"] + analysis["button_elements"]:
            text = (item.get('aria_label', '') + ' ' + item.get('text', '')).lower()
            if any(keyword in text for keyword in comment_keywords):
                selector = self._generate_selector_from_element(item)
                if selector:
                    analysis["potential_selectors"]["comments"].append(selector)
        
        # è½‰ç™¼ç›¸é—œ
        repost_keywords = ['repost', 'reposts', 'è½‰ç™¼', 'è½‰è²¼', 'ğŸ”„']
        for item in analysis["aria_labels"] + analysis["button_elements"]:
            text = (item.get('aria_label', '') + ' ' + item.get('text', '')).lower()
            if any(keyword in text for keyword in repost_keywords):
                selector = self._generate_selector_from_element(item)
                if selector:
                    analysis["potential_selectors"]["reposts"].append(selector)
        
        # åˆ†äº«ç›¸é—œ
        share_keywords = ['share', 'shares', 'åˆ†äº«', 'ğŸ“¤']
        for item in analysis["aria_labels"] + analysis["button_elements"]:
            text = (item.get('aria_label', '') + ' ' + item.get('text', '')).lower()
            if any(keyword in text for keyword in share_keywords):
                selector = self._generate_selector_from_element(item)
                if selector:
                    analysis["potential_selectors"]["shares"].append(selector)
        
        return analysis
    
    def _generate_selector_from_element(self, element_info: Dict) -> Optional[str]:
        """å¾å…ƒç´ ä¿¡æ¯ç”ŸæˆCSSé¸æ“‡å™¨"""
        tag = element_info.get('tag', '')
        classes = element_info.get('classes', [])
        aria_label = element_info.get('aria_label', '')
        
        selectors = []
        
        # åŸºæ–¼aria-labelçš„é¸æ“‡å™¨
        if aria_label:
            # å®Œæ•´åŒ¹é…
            selectors.append(f'{tag}[aria-label="{aria_label}"]')
            # éƒ¨åˆ†åŒ¹é…
            selectors.append(f'{tag}[aria-label*="{aria_label[:20]}"]')
        
        # åŸºæ–¼classçš„é¸æ“‡å™¨
        if classes:
            class_selector = '.'.join(classes[:3])  # æœ€å¤šç”¨å‰3å€‹class
            selectors.append(f'{tag}.{class_selector}')
        
        # è¿”å›ç¬¬ä¸€å€‹æœ‰æ•ˆçš„é¸æ“‡å™¨
        return selectors[0] if selectors else None
    
    def _generate_fix_suggestions(self, analysis: Dict[str, Any]) -> str:
        """ç”Ÿæˆä¿®å¾©å»ºè­°æ–‡æª”"""
        suggestions = f"""# Threads DOMçµæ§‹åˆ†æ - ä¿®å¾©å»ºè­°

## åˆ†ææ™‚é–“
{analysis['page_info']['analyzed_at']}

## é é¢ä¿¡æ¯
- URL: {analysis['page_info']['url']}
- æ¨™é¡Œ: {analysis['page_info']['title']}

## ğŸ”§ å»ºè­°çš„æ–°é¸æ“‡å™¨

### æŒ‰è®šæ•¸æå–
"""
        
        # æ·»åŠ æŒ‰è®šé¸æ“‡å™¨å»ºè­°
        likes_selectors = analysis['potential_selectors']['likes']
        if likes_selectors:
            suggestions += "```python\nlikes_selectors = [\n"
            for selector in likes_selectors[:5]:
                suggestions += f'    "{selector}",\n'
            suggestions += "]\n```\n\n"
        else:
            suggestions += "âŒ æœªæ‰¾åˆ°æ˜é¡¯çš„æŒ‰è®šç›¸é—œå…ƒç´ \n\n"
        
        # æ·»åŠ ç•™è¨€é¸æ“‡å™¨å»ºè­°
        suggestions += "### ç•™è¨€æ•¸æå–\n"
        comments_selectors = analysis['potential_selectors']['comments']
        if comments_selectors:
            suggestions += "```python\ncomments_selectors = [\n"
            for selector in comments_selectors[:5]:
                suggestions += f'    "{selector}",\n'
            suggestions += "]\n```\n\n"
        else:
            suggestions += "âŒ æœªæ‰¾åˆ°æ˜é¡¯çš„ç•™è¨€ç›¸é—œå…ƒç´ \n\n"
        
        # æ·»åŠ æ•¸å­—å…ƒç´ åˆ†æ
        suggestions += "## ğŸ“Š ç™¼ç¾çš„æ•¸å­—å…ƒç´ \n\n"
        found_numbers = analysis['found_numbers'][:10]
        for i, num_info in enumerate(found_numbers):
            suggestions += f"**å…ƒç´  {i+1}:**\n"
            suggestions += f"- æ–‡å­—: `{num_info['text']}`\n"
            suggestions += f"- æ¨™ç±¤: `{num_info['tag']}`\n"
            suggestions += f"- é¡åˆ¥: `{num_info['classes']}`\n"
            if num_info['aria_label']:
                suggestions += f"- Aria-label: `{num_info['aria_label']}`\n"
            suggestions += "\n"
        
        # æ·»åŠ å¯¦æ–½å»ºè­°
        suggestions += """## ğŸ› ï¸ å¯¦æ–½å»ºè­°

1. **æ›´æ–° details_extractor.py**
   - å°‡æ–°çš„é¸æ“‡å™¨æ·»åŠ åˆ° `count_selectors` å­—å…¸ä¸­
   - æŒ‰å„ªå…ˆç´šæ’åºï¼ˆæœ€å¯èƒ½æˆåŠŸçš„æ”¾åœ¨å‰é¢ï¼‰

2. **æ¸¬è©¦å»ºè­°**
   - å…ˆæ¸¬è©¦å–®å€‹é¸æ“‡å™¨
   - ç¢ºèªæå–åˆ°çš„æ–‡å­—æ ¼å¼
   - é©—è­‰æ•¸å­—è§£æé‚è¼¯

3. **å¾Œå‚™ç­–ç•¥**
   - å¦‚æœç‰¹å®šé¸æ“‡å™¨å¤±æ•—ï¼Œå˜—è©¦é€šç”¨æ•¸å­—æå–
   - ä½¿ç”¨æ™ºèƒ½æ•¸å­—åˆ†é…é‚è¼¯

## ğŸ“ å¯¦æ–½ä»£ç¢¼ç¯„ä¾‹

```python
# åœ¨ details_extractor.py ä¸­æ›´æ–°é¸æ“‡å™¨
count_selectors = {
    "likes": [
        # æ–°çš„é¸æ“‡å™¨ï¼ˆå¾åˆ†æçµæœä¸­ç²å¾—ï¼‰
"""
        
        # æ·»åŠ å¯¦éš›çš„é¸æ“‡å™¨
        for selector in likes_selectors[:3]:
            suggestions += f'        "{selector}",\n'
        
        suggestions += """    ],
    "comments": [
        # æ–°çš„ç•™è¨€é¸æ“‡å™¨
"""
        
        for selector in comments_selectors[:3]:
            suggestions += f'        "{selector}",\n'
        
        suggestions += """    ]
}
```
"""
        
        return suggestions

async def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸ” Threadsé é¢çµæ§‹åˆ†æå™¨")
    
    analyzer = PageAnalyzer()
    
    # æª¢æŸ¥èªè­‰æ–‡ä»¶
    if not analyzer.auth_file_path.exists():
        print(f"âŒ èªè­‰æª”æ¡ˆ {analyzer.auth_file_path} ä¸å­˜åœ¨")
        print("ğŸ’¡ è«‹å…ˆé‹è¡Œ save_auth.py ç”Ÿæˆèªè­‰æª”æ¡ˆ")
        return
    
    # åˆ†æé é¢çµæ§‹
    result = await analyzer.analyze_page_structure()
    
    if result:
        print(f"\nğŸ‰ åˆ†æå®Œæˆï¼")
        print(f"ğŸ“Š æ‰¾åˆ° {len(result['found_numbers'])} å€‹æ•¸å­—å…ƒç´ ")
        print(f"ğŸ”˜ æ‰¾åˆ° {len(result['button_elements'])} å€‹æŒ‰éˆ•å…ƒç´ ")
        print(f"ğŸ·ï¸ æ‰¾åˆ° {len(result['aria_labels'])} å€‹aria-label")
        
        # é¡¯ç¤ºå»ºè­°çš„é¸æ“‡å™¨æ•¸é‡
        selectors_count = sum(len(selectors) for selectors in result['potential_selectors'].values())
        print(f"ğŸ¯ ç”Ÿæˆ {selectors_count} å€‹å€™é¸é¸æ“‡å™¨")
        
        print(f"\nğŸ’¡ æ¥ä¸‹ä¾†å¯ä»¥:")
        print(f"   1. æŸ¥çœ‹ç”Ÿæˆçš„HTMLå¿«ç…§å’Œæˆªåœ–")
        print(f"   2. é–±è®€ä¿®å¾©å»ºè­°æ–‡ä»¶")
        print(f"   3. å°‡æ–°é¸æ“‡å™¨æ•´åˆåˆ°playwright_crawlerä¸­")
    else:
        print(f"\nğŸ˜ åˆ†æå¤±æ•—")

if __name__ == "__main__":
    asyncio.run(main())