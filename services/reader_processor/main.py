#!/usr/bin/env python3
"""
Reader Processor Service - æ‰¹é‡è™•ç†Readerè«‹æ±‚

è² è²¬ï¼š
1. æ¥æ”¶URLåˆ—è¡¨æ‰¹é‡è™•ç†è«‹æ±‚
2. ä¸¦è¡Œèª¿ç”¨Readeræœå‹™é›†ç¾¤
3. èšåˆçµæœä¸¦æ›´æ–°æ•¸æ“šåº«ç‹€æ…‹
4. æä¾›é‡è©¦å’ŒéŒ¯èª¤è™•ç†æ©Ÿåˆ¶
"""

import asyncio
import aiohttp
import async_timeout
import uuid
import json
import logging
from datetime import datetime
from typing import List, Dict, Any, Optional
from fastapi import FastAPI, HTTPException, BackgroundTasks
from pydantic import BaseModel, Field
import sys
import os

# æ·»åŠ å°ˆæ¡ˆæ ¹ç›®éŒ„åˆ° Python è·¯å¾‘
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))

from common.models import PostMetrics
from common.history import CrawlHistoryDAO
from common.settings import get_settings

app = FastAPI(
    title="Reader Processor Service",
    version="1.0.0",
    description="æ‰¹é‡è™•ç†Readerè«‹æ±‚çš„å”èª¿æœå‹™"
)

# é…ç½®
READER_LB_URL = "http://reader-lb:80"
MAX_CONCURRENT_REQUESTS = 10  # æœ€å¤§ä¸¦ç™¼è«‹æ±‚æ•¸
DEFAULT_TIMEOUT = 60  # é»˜èªè¶…æ™‚æ™‚é–“ï¼ˆç§’ï¼‰

class ReaderRequest(BaseModel):
    """Readerè™•ç†è«‹æ±‚"""
    urls: List[str] = Field(..., description="è¦è™•ç†çš„URLåˆ—è¡¨")
    username: str = Field(..., description="ç›®æ¨™ç”¨æˆ¶å")
    task_id: Optional[str] = Field(None, description="ä»»å‹™ID")
    timeout: int = Field(DEFAULT_TIMEOUT, description="è¶…æ™‚æ™‚é–“ï¼ˆç§’ï¼‰")
    return_format: str = Field("text", description="è¿”å›æ ¼å¼: text/markdown/json")

class ReaderResult(BaseModel):
    """å–®å€‹Readerè™•ç†çµæœ"""
    url: str
    status: str  # success/failed/timeout
    content: Optional[str] = None
    title: Optional[str] = None
    error: Optional[str] = None
    processing_time: Optional[float] = None
    processed_at: datetime

class ReaderBatchResponse(BaseModel):
    """æ‰¹é‡Readerè™•ç†éŸ¿æ‡‰"""
    task_id: str
    username: str
    total_urls: int
    successful: int
    failed: int
    results: List[ReaderResult]
    total_time: float
    completed_at: datetime

class ReaderProcessor:
    """Readerè™•ç†å™¨æ ¸å¿ƒé‚è¼¯"""
    
    def __init__(self):
        self.history_dao = CrawlHistoryDAO()
        self.semaphore = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)
    
    async def process_url(self, session: aiohttp.ClientSession, url: str, timeout: int, return_format: str) -> ReaderResult:
        """è™•ç†å–®å€‹URL"""
        start_time = asyncio.get_event_loop().time()
        
        async with self.semaphore:
            try:
                # æ§‹å»ºReaderè«‹æ±‚URL
                reader_url = f"{READER_LB_URL}/{url}"
                
                headers = {
                    "X-Return-Format": return_format,
                    "User-Agent": "Social-Media-Content-Generator/1.0"
                }
                
                async with async_timeout.timeout(timeout):
                    async with session.get(reader_url, headers=headers) as response:
                        processing_time = asyncio.get_event_loop().time() - start_time
                        
                        if response.status == 200:
                            content = await response.text()
                            
                            # å˜—è©¦è§£ææ¨™é¡Œï¼ˆå¦‚æœæ˜¯JSONæ ¼å¼ï¼‰
                            title = None
                            if return_format == "json":
                                try:
                                    json_data = json.loads(content)
                                    title = json_data.get("title")
                                    content = json_data.get("content", content)
                                except json.JSONDecodeError:
                                    pass
                            
                            return ReaderResult(
                                url=url,
                                status="success",
                                content=content,
                                title=title,
                                processing_time=processing_time,
                                processed_at=datetime.utcnow()
                            )
                        else:
                            error_text = await response.text()
                            return ReaderResult(
                                url=url,
                                status="failed",
                                error=f"HTTP {response.status}: {error_text}",
                                processing_time=processing_time,
                                processed_at=datetime.utcnow()
                            )
                            
            except asyncio.TimeoutError:
                processing_time = asyncio.get_event_loop().time() - start_time
                return ReaderResult(
                    url=url,
                    status="timeout",
                    error=f"Request timeout after {timeout}s",
                    processing_time=processing_time,
                    processed_at=datetime.utcnow()
                )
            except Exception as e:
                processing_time = asyncio.get_event_loop().time() - start_time
                return ReaderResult(
                    url=url,
                    status="failed",
                    error=str(e),
                    processing_time=processing_time,
                    processed_at=datetime.utcnow()
                )
    
    async def process_batch(self, request: ReaderRequest) -> ReaderBatchResponse:
        """æ‰¹é‡è™•ç†URLs"""
        task_id = request.task_id or str(uuid.uuid4())
        start_time = asyncio.get_event_loop().time()
        
        logging.info(f"ğŸš€ [Task: {task_id}] é–‹å§‹æ‰¹é‡Readerè™•ç†: {len(request.urls)} å€‹URLs")
        
        # å‰µå»ºHTTPæœƒè©±
        connector = aiohttp.TCPConnector(
            limit=MAX_CONCURRENT_REQUESTS * 2,
            ttl_dns_cache=300,
            use_dns_cache=True
        )
        
        async with aiohttp.ClientSession(connector=connector) as session:
            # ä¸¦è¡Œè™•ç†æ‰€æœ‰URLs
            tasks = [
                self.process_url(session, url, request.timeout, request.return_format)
                for url in request.urls
            ]
            
            results = await asyncio.gather(*tasks, return_exceptions=False)
        
        # çµ±è¨ˆçµæœ
        successful = sum(1 for r in results if r.status == "success")
        failed = len(results) - successful
        total_time = asyncio.get_event_loop().time() - start_time
        
        logging.info(f"âœ… [Task: {task_id}] Readerè™•ç†å®Œæˆ: {successful}/{len(results)} æˆåŠŸ")
        
        return ReaderBatchResponse(
            task_id=task_id,
            username=request.username,
            total_urls=len(request.urls),
            successful=successful,
            failed=failed,
            results=results,
            total_time=total_time,
            completed_at=datetime.utcnow()
        )
    
    async def update_database_status(self, results: List[ReaderResult], username: str):
        """æ ¹æ“šReaderçµæœæ›´æ–°æ•¸æ“šåº«ç‹€æ…‹"""
        
        post_updates = []
        
        for result in results:
            if result.status == "success" and result.content:
                # æå–post_id
                post_id = result.url.split('/')[-1]
                full_post_id = f"{username}_{post_id}"
                
                # å‰µå»ºPostMetricså°è±¡ç”¨æ–¼æ›´æ–°
                post_metrics = PostMetrics(
                    post_id=full_post_id,
                    username=username,
                    url=result.url,
                    content=result.content,
                    created_at=datetime.utcnow(),
                    fetched_at=datetime.utcnow(),
                    source="reader",
                    processing_stage="reader_completed",
                    is_complete=False,  # Readeråªæä¾›å…§å®¹ï¼Œä¸ç®—å®Œæ•´
                    reader_status="success",
                    reader_processed_at=result.processed_at
                )
                
                post_updates.append(post_metrics)
            
            elif result.status in ["failed", "timeout"]:
                # æ¨™è¨˜Readerè™•ç†å¤±æ•—
                post_id = result.url.split('/')[-1]
                full_post_id = f"{username}_{post_id}"
                
                post_metrics = PostMetrics(
                    post_id=full_post_id,
                    username=username,
                    url=result.url,
                    created_at=datetime.utcnow(),
                    fetched_at=datetime.utcnow(),
                    source="reader",
                    processing_stage="reader_failed",
                    is_complete=False,
                    reader_status="failed",
                    reader_processed_at=result.processed_at
                )
                
                post_updates.append(post_metrics)
        
        # æ‰¹é‡æ›´æ–°æ•¸æ“šåº«
        if post_updates:
            saved_count = await self.history_dao.upsert_posts(post_updates)
            logging.info(f"ğŸ“Š Readerç‹€æ…‹æ›´æ–°å®Œæˆ: {saved_count}/{len(post_updates)} ç­†è¨˜éŒ„")
            return saved_count
        
        return 0

# å…¨å±€è™•ç†å™¨å¯¦ä¾‹
processor = ReaderProcessor()

@app.get("/health")
async def health_check():
    """å¥åº·æª¢æŸ¥"""
    try:
        # æª¢æŸ¥Reader LBé€£é€šæ€§
        async with aiohttp.ClientSession() as session:
            async with async_timeout.timeout(5):
                async with session.get(f"{READER_LB_URL}/health") as response:
                    if response.status == 200:
                        return {
                            "status": "healthy",
                            "service": "Reader Processor",
                            "reader_lb": "connected"
                        }
                    else:
                        return {
                            "status": "unhealthy",
                            "service": "Reader Processor",
                            "reader_lb": f"HTTP {response.status}"
                        }
    except Exception as e:
        return {
            "status": "unhealthy",
            "service": "Reader Processor",
            "reader_lb": f"error: {str(e)}"
        }

@app.post("/process", response_model=ReaderBatchResponse)
async def process_reader_batch(request: ReaderRequest, background_tasks: BackgroundTasks):
    """
    æ‰¹é‡è™•ç†Readerè«‹æ±‚
    
    è™•ç†æµç¨‹ï¼š
    1. ä¸¦è¡Œèª¿ç”¨Readeræœå‹™è™•ç†æ‰€æœ‰URLs
    2. èšåˆçµæœä¸¦è¿”å›
    3. èƒŒæ™¯æ›´æ–°æ•¸æ“šåº«ç‹€æ…‹
    """
    try:
        # ä¸»è¦è™•ç†
        response = await processor.process_batch(request)
        
        # èƒŒæ™¯ä»»å‹™ï¼šæ›´æ–°æ•¸æ“šåº«ç‹€æ…‹
        background_tasks.add_task(
            processor.update_database_status,
            response.results,
            request.username
        )
        
        return response
        
    except Exception as e:
        logging.error(f"âŒ Readeræ‰¹é‡è™•ç†å¤±æ•—: {e}")
        raise HTTPException(status_code=500, detail=f"Readerè™•ç†å¤±æ•—: {str(e)}")

@app.post("/process-sync", response_model=ReaderBatchResponse)
async def process_reader_batch_sync(request: ReaderRequest):
    """
    æ‰¹é‡è™•ç†Readerè«‹æ±‚ï¼ˆåŒæ­¥æ›´æ–°æ•¸æ“šåº«ï¼‰
    
    èˆ‡ /process çš„å€åˆ¥ï¼šæœƒç­‰å¾…æ•¸æ“šåº«æ›´æ–°å®Œæˆæ‰è¿”å›
    """
    try:
        # ä¸»è¦è™•ç†
        response = await processor.process_batch(request)
        
        # åŒæ­¥æ›´æ–°æ•¸æ“šåº«ç‹€æ…‹
        await processor.update_database_status(response.results, request.username)
        
        return response
        
    except Exception as e:
        logging.error(f"âŒ Readeræ‰¹é‡è™•ç†ï¼ˆåŒæ­¥ï¼‰å¤±æ•—: {e}")
        raise HTTPException(status_code=500, detail=f"Readerè™•ç†å¤±æ•—: {str(e)}")

@app.get("/status/{task_id}")
async def get_task_status(task_id: str):
    """
    ç²å–ä»»å‹™ç‹€æ…‹ï¼ˆæœªä¾†å¯æ“´å±•ï¼‰
    ç›®å‰è¿”å›åŸºæœ¬ä¿¡æ¯
    """
    return {
        "task_id": task_id,
        "message": "Task status tracking not implemented yet"
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8009)