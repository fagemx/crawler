#!/usr/bin/env python3
"""
Orchestration Service - Plan F

è² è²¬å”èª¿å¤šå€‹ Agentï¼Œå®Œæˆä¸€å€‹å®Œæ•´çš„è³‡æ–™è™•ç†ç®¡é“ã€‚
"""

import httpx
import asyncio
from typing import Dict, Any
import json # Added for json.load
from datetime import datetime
from pathlib import Path

from common.models import PostMetricsBatch
from common.config import get_auth_file_path # å‡è¨­æˆ‘å€‘æœ‰ä¸€å€‹å…±ç”¨çš„è¨­å®šæª”
from common.db_client import DatabaseClient
from common.redis_client import get_redis_client

# Agent çš„ API ç«¯é»è¨­å®š
# åœ¨çœŸå¯¦ç’°å¢ƒä¸­ï¼Œé€™äº›æ‡‰è©²ä¾†è‡ªè¨­å®šæª” (e.g., a settings module)
PLAYWRIGHT_AGENT_URL = "http://localhost:8006/v1/playwright/crawl"
JINA_AGENT_URL = "http://localhost:8004/v1/jina/enrich" # ä¿®æ­£ç«¯å£ç‚º 8004
# VISION_AGENT_URL = "http://localhost:8005/v1/vision/fill" # æœªä¾†æ“´å±•

class PipelineService:
    """
    å”èª¿æœå‹™ï¼Œç”¨æ–¼åŸ·è¡Œå¤šéšæ®µçš„è³‡æ–™è™•ç†ç®¡é“ã€‚
    """
    def __init__(self, client: httpx.AsyncClient):
        self.client = client
        self.db_client = DatabaseClient()
        self.redis_client = get_redis_client()

    async def run_crawling_pipeline(self, username: str, max_posts: int) -> PostMetricsBatch:
        """
        åŸ·è¡Œæ¨™æº–çš„çˆ¬èŸ²èˆ‡è³‡æ–™è±å¯ŒåŒ–ç®¡é“ã€‚
        æµç¨‹: Playwright -> Jina
        """
        print("ğŸš€ [Pipeline] Stage 1: Calling Playwright Agent...")
        
        # 1. æº–å‚™ Playwright Agent çš„è«‹æ±‚
        # ç”± Orchestrator è² è²¬è®€å–èªè­‰æª”æ¡ˆä¸¦å‚³é
        try:
            auth_file = get_auth_file_path(from_project_root=True)
            with open(auth_file, 'r', encoding='utf-8') as f:
                auth_content = json.load(f)
        except FileNotFoundError:
            print(f"âŒ [Pipeline] Authentication file not found at {get_auth_file_path(from_project_root=True)}")
            raise
        
        playwright_payload = {
            "username": username, 
            "max_posts": max_posts,
            "auth_json_content": auth_content
        }

        try:
            response_p = await self.client.post(PLAYWRIGHT_AGENT_URL, json=playwright_payload, timeout=300)
            response_p.raise_for_status()
            batch_v1_data = response_p.json()
            batch_v1 = PostMetricsBatch(**batch_v1_data)
            print(f"âœ… [Pipeline] Playwright Agent completed. Got {len(batch_v1.posts)} posts.")
        except httpx.HTTPStatusError as e:
            print(f"âŒ [Pipeline] Playwright Agent failed with status {e.response.status_code}: {e.response.text}")
            raise
        except Exception as e:
            print(f"âŒ [Pipeline] Failed to call or parse Playwright Agent response: {e}")
            raise

        print("\nğŸš€ [Pipeline] Stage 2: Calling Jina Agent for enrichment...")

        # 2. å‘¼å« Jina Agent
        try:
            # Pydantic v2 çš„ model_dump() é è¨­æœƒå°‡ datetime è½‰ç‚ºå­—ä¸²ï¼Œä½†æˆ‘å€‘éœ€è¦ç¢ºä¿æ ¼å¼æ­£ç¢º
            # æœ€ç©©å¥çš„æ–¹å¼æ˜¯ä½¿ç”¨ model_dump_json()ï¼Œå®ƒæœƒè™•ç†å¥½æ‰€æœ‰åºåˆ—åŒ–
            batch_v1_json_str = batch_v1.model_dump_json()

            response_j = await self.client.post(
                JINA_AGENT_URL, 
                content=batch_v1_json_str,
                headers={"Content-Type": "application/json"},
                timeout=900  # å¢åŠ åˆ° 15 åˆ†é˜ï¼Œé©æ‡‰å„ªåŒ–å¾Œçš„é€Ÿç‡æ§åˆ¶è™•ç†æ™‚é–“
            )
            response_j.raise_for_status()
            batch_v2_data = response_j.json()
            batch_v2 = PostMetricsBatch(**batch_v2_data)
            
            # ç°¡å–®æ¯”è¼ƒä¸€ä¸‹è±å¯ŒåŒ–å‰å¾Œçš„è®ŠåŒ–
            enriched_count = 0
            if batch_v1.posts and batch_v2.posts:
                # ç¢ºä¿ç´¢å¼•åœ¨ç¯„åœå…§
                if len(batch_v1.posts) > 0 and len(batch_v2.posts) > 0:
                    if batch_v2.posts[0].views_count is not None and batch_v1.posts[0].views_count is None:
                        enriched_count = sum(1 for post in batch_v2.posts if post.views_count is not None)
            
            print(f"âœ… [Pipeline] Jina Agent completed. Enriched {enriched_count} posts with views.")
            
            # ğŸ—„ï¸ å­˜å„²æœ€çµ‚çµæœåˆ°è³‡æ–™åº«ã€Redis å’Œ JSON æª”æ¡ˆ
            print("\nğŸ’¾ [Pipeline] Stage 3: Saving results to database, cache, and JSON file...")
            await self._save_batch_results(batch_v2)
            json_file_path = self._save_to_json(batch_v2)
            print(f"âœ… [Pipeline] Results saved successfully.")
            print(f"ğŸ“„ [Pipeline] JSON file saved to: {json_file_path}")
            
        except httpx.HTTPStatusError as e:
            print(f"âŒ [Pipeline] Jina Agent failed with status {e.response.status_code}: {e.response.text}")
            # å³ä½¿ Jina å¤±æ•—ï¼Œæˆ‘å€‘ä»ç„¶å¯ä»¥å­˜å„²å’Œå›å‚³ Playwright çš„çµæœ
            json_file_path = self._save_to_json(batch_v1)
            print(f"ğŸ“„ [Pipeline] Playwright-only results saved to: {json_file_path}")
            return batch_v1
        except Exception as e:
            print(f"âŒ [Pipeline] Failed to call or parse Jina Agent response: {e}")
            print(f"ğŸ’¡ [Pipeline] éŒ¯èª¤è©³æƒ…: {type(e).__name__}")
            # åŒæ¨£ï¼Œå­˜å„²å’Œå›å‚³ v1 çš„çµæœ
            json_file_path = self._save_to_json(batch_v1)
            print(f"ğŸ“„ [Pipeline] Playwright-only results saved to: {json_file_path}")
            return batch_v1

        return batch_v2

    async def _save_batch_results(self, batch: PostMetricsBatch) -> None:
        """
        å°‡æ‰¹æ¬¡çµæœå­˜å„²åˆ°è³‡æ–™åº«å’Œ Redis
        """
        try:
            # åˆå§‹åŒ–è³‡æ–™åº«é€£æ¥æ± 
            await self.db_client.init_pool()
            
            # æº–å‚™æ‰¹æ¬¡æ•¸æ“š
            posts_data = []
            metrics_data = []
            
            for post in batch.posts:
                # è²¼æ–‡åŸºæœ¬è³‡æ–™
                posts_data.append({
                    "url": post.url,
                    "author": post.username,
                    "markdown": post.content,
                    "media_urls": post.media_urls or []
                })
                
                # æŒ‡æ¨™è³‡æ–™
                metrics_data.append({
                    "url": post.url,
                    "views": post.views_count,
                    "likes": post.likes_count,
                    "comments": post.comments_count,
                    "reposts": post.reposts_count,
                    "shares": post.shares_count
                })
                
                # å­˜åˆ° Redis å¿«å–
                redis_metrics = {
                    "views": post.views_count or 0,
                    "likes": post.likes_count or 0,
                    "comments": post.comments_count or 0,
                    "reposts": post.reposts_count or 0,
                    "shares": post.shares_count or 0
                }
                self.redis_client.set_post_metrics(post.url, redis_metrics)
            
            # æ‰¹æ¬¡å­˜å…¥ PostgreSQL
            posts_saved = await self.db_client.batch_upsert_posts(posts_data)
            metrics_saved = await self.db_client.batch_upsert_metrics(metrics_data)
            
            print(f"ğŸ’¾ [Pipeline] Saved {posts_saved} posts and {metrics_saved} metrics to database")
            print(f"âš¡ [Pipeline] Cached {len(batch.posts)} posts to Redis")
            
        except Exception as e:
            print(f"âŒ [Pipeline] Failed to save results: {e}")
            # ä¸æ‹‹å‡ºç•°å¸¸ï¼Œå› ç‚ºè™•ç†å·²ç¶“å®Œæˆï¼Œåªæ˜¯å­˜å„²å¤±æ•—

    def _save_to_json(self, batch: PostMetricsBatch) -> str:
        """
        å°‡æ‰¹æ¬¡çµæœå­˜å„²ç‚º JSON æª”æ¡ˆï¼Œæ–¹ä¾¿å³æ™‚æŸ¥çœ‹
        """
        try:
            # å‰µå»ºè¼¸å‡ºç›®éŒ„
            output_dir = Path("pipeline_results")
            output_dir.mkdir(exist_ok=True)
            
            # ç”Ÿæˆæª”æ¡ˆåï¼ˆåŒ…å«æ™‚é–“æˆ³å’Œç”¨æˆ¶åï¼‰
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"pipeline_results_{batch.username}_{timestamp}.json"
            file_path = output_dir / filename
            
            # æº–å‚™ JSON æ•¸æ“š
            result_data = {
                "batch_info": {
                    "batch_id": batch.batch_id,
                    "username": batch.username,
                    "total_posts": len(batch.posts),
                    "processing_stage": batch.processing_stage,
                    "timestamp": timestamp
                },
                "posts": []
            }
            
            # æ·»åŠ æ¯å€‹è²¼æ–‡çš„è©³ç´°è³‡æ–™
            for post in batch.posts:
                post_data = {
                    "url": post.url,
                    "post_id": post.post_id,
                    "username": post.username,
                    "metrics": {
                        "views_count": post.views_count,
                        "likes_count": post.likes_count,
                        "comments_count": post.comments_count,
                        "reposts_count": post.reposts_count,
                        "shares_count": post.shares_count,
                        "calculated_score": post.calculate_score()
                    },
                    "content": {
                        "text": post.content[:200] + "..." if post.content and len(post.content) > 200 else post.content,
                        "media_urls": post.media_urls or []
                    },
                    "metadata": {
                        "source": post.source,
                        "processing_stage": post.processing_stage,
                        "is_complete": post.is_complete,
                        "last_updated": post.last_updated.isoformat() if post.last_updated else None,
                        "created_at": post.created_at.isoformat() if post.created_at else None
                    }
                }
                result_data["posts"].append(post_data)
            
            # æ’åºè²¼æ–‡ï¼ˆæŒ‰åˆ†æ•¸é™åºï¼‰
            result_data["posts"].sort(key=lambda x: x["metrics"]["calculated_score"], reverse=True)
            
            # å¯«å…¥ JSON æª”æ¡ˆ
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(result_data, f, ensure_ascii=False, indent=2)
            
            return str(file_path.absolute())
            
        except Exception as e:
            print(f"âŒ [Pipeline] Failed to save JSON file: {e}")
            return "JSON save failed"

# --- æ¸¬è©¦ç”¨çš„ä¸»å‡½æ•¸ ---
async def main():
    """
    ä¸€å€‹ç°¡å–®çš„æ¸¬è©¦å‡½æ•¸ï¼Œç”¨æ–¼æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ PipelineServiceã€‚
    """
    print("===== Running Pipeline Test =====")
    test_username = "starettoday"  # ğŸ”„ ä¿®æ”¹é€™è£¡: ä¾‹å¦‚ "zuck", "elonmusk", "instagram" ç­‰
    test_max_posts = 100       # ğŸ”„ ä¿®æ”¹é€™è£¡: ä¾‹å¦‚ 20, 50, 100 ç­‰ (å»ºè­°ä¸è¶…é 50)

    async with httpx.AsyncClient() as client:
        pipeline = PipelineService(client)
        try:
            final_batch = await pipeline.run_crawling_pipeline(test_username, test_max_posts)
            
            print("\nâœ…âœ…âœ… Pipeline Completed Successfully! âœ…âœ…âœ…")
            print("-" * 50)
            print("Final Batch Summary:")
            print(f"  - Batch ID: {final_batch.batch_id}")
            print(f"  - Username: {final_batch.username}")
            print(f"  - Total Posts: {len(final_batch.posts)}")
            print(f"  - Final Stage: {final_batch.processing_stage}")
            
            if final_batch.posts:
                print("\n--- Sample of Final Enriched Post ---")
                sample_post = final_batch.posts[0]
                print(f"  URL: {sample_post.url}")
                print(f"  Post ID: {sample_post.post_id}")
                print(f"  Likes: {sample_post.likes_count}")
                print(f"  Comments: {sample_post.comments_count}")
                print(f"  Views (from Jina): {sample_post.views_count}")
                print(f"  Content Preview: {sample_post.content[:80] if sample_post.content else 'N/A'}...")
            print("-" * 50)

        except Exception as e:
            print(f"\nâŒâŒâŒ Pipeline Failed: {e} âŒâŒâŒ")

if __name__ == "__main__":
    # ç¢ºä¿ common.models åœ¨ sys.path ä¸­
    import sys
    from pathlib import Path
    project_root = Path(__file__).resolve().parent.parent
    sys.path.insert(0, str(project_root))
    
    asyncio.run(main()) 