"""
RustFS å®¢æˆ¶ç«¯æœå‹™

ç”¨æ–¼è™•ç†åª’é«”æª”æ¡ˆçš„ä¸Šå‚³ã€ä¸‹è¼‰å’Œç®¡ç†
"""

import os
import socket
from urllib.parse import urlparse, urlunparse
import hashlib
import mimetypes
from typing import Optional, Dict, Any, List
import uuid
from datetime import datetime
from pathlib import Path
import httpx
import asyncio
import boto3
from botocore.client import Config as BotoConfig
from botocore.exceptions import ClientError
import json

from common.settings import get_settings
from common.db_client import get_db_client
from common.config import get_auth_file_path


class RustFSClient:
    """RustFS å®¢æˆ¶ç«¯"""
    
    def __init__(self):
        self.settings = get_settings()
        # å¾ç’°å¢ƒè®€å–è¨­å®šï¼ˆæ”¯æ´æœ¬æ©Ÿèˆ‡å®¹å™¨å…§ç¶²ï¼‰
        self.base_url = os.getenv("RUSTFS_ENDPOINT", "http://rustfs:9000").rstrip("/")
        self.access_key = os.getenv("RUSTFS_ACCESS_KEY", "rustfsadmin")
        self.secret_key = os.getenv("RUSTFS_SECRET_KEY", "rustfsadmin")
        self.bucket_name = os.getenv("RUSTFS_BUCKET", "social-media-content")
        self.region = os.getenv("RUSTFS_REGION", "us-east-1")
        # è‡ªå‹•åµæ¸¬ï¼šè‹¥æœ¬æ©Ÿç„¡æ³•è§£æ rustfsï¼Œæ”¹ç”¨ localhost
        self._auto_select_endpoint()
        self._s3_client = None
        self._cookie_header: Optional[str] = None

    def _auto_select_endpoint(self):
        try:
            parsed = urlparse(self.base_url)
            host = parsed.hostname or ""
            if host.lower() == "rustfs":
                try:
                    socket.gethostbyname(host)
                except OSError:
                    # ç„¡æ³•è§£æå®¹å™¨åŸŸåï¼Œå›é€€æœ¬æ©Ÿç«¯å£
                    new_netloc = f"localhost:{parsed.port or 9000}"
                    self.base_url = urlunparse((parsed.scheme, new_netloc, "", "", "", ""))
        except Exception:
            # éœé»˜å¤±æ•—ï¼Œç¶­æŒåŸç«¯é»
            pass
        
    def _create_s3_client(self):
        """å»ºç«‹çµ±ä¸€è¨­å®šçš„ S3 å®¢æˆ¶ç«¯ï¼ˆpath-style ä½å€ï¼Œæ–¹ä¾¿æœ¬æ©Ÿ/MinIOï¼‰ã€‚"""
        return boto3.client(
            's3', endpoint_url=self.base_url,
            aws_access_key_id=self.access_key,
            aws_secret_access_key=self.secret_key,
            config=BotoConfig(signature_version='s3v4', s3={'addressing_style': 'path'}),
            region_name=self.region
        )

    async def initialize(self):
        """åˆå§‹åŒ–ï¼šä½¿ç”¨ S3 æª¢æŸ¥/å»ºç«‹ bucket"""
        try:
            s3 = self._create_s3_client()
            def _head():
                try:
                    s3.head_bucket(Bucket=self.bucket_name)
                    return True
                except ClientError as e:
                    code = e.response.get('Error', {}).get('Code')
                    if code in ('404', 'NoSuchBucket'):
                        return False
                    if code in ('403', 'AccessDenied'):
                        return True
                    raise
            exists = await asyncio.to_thread(_head)
            if not exists:
                def _create():
                    try:
                        s3.create_bucket(Bucket=self.bucket_name)
                        return True
                    except ClientError as e:
                        if e.response.get('Error', {}).get('Code') in ('BucketAlreadyOwnedByYou', 'BucketAlreadyExists'):
                            return True
                        raise
                await asyncio.to_thread(_create)
        except Exception as e:
            print(f"âŒ Failed to initialize RustFS: {e}")
            raise

    def health_check(self) -> Dict[str, Any]:
        """RustFS å¥æª¢ï¼ˆS3 HeadBucketï¼‰"""
        try:
            s3 = self._create_s3_client()
            try:
                s3.head_bucket(Bucket=self.bucket_name)
                return {"status": "healthy", "endpoint": self.base_url, "bucket": self.bucket_name}
            except ClientError as e:
                code = e.response.get('Error', {}).get('Code')
                if code in ('404', 'NoSuchBucket', '403', 'AccessDenied'):
                    return {"status": "healthy", "endpoint": self.base_url, "bucket": self.bucket_name, "note": code}
                return {"status": "unhealthy", "endpoint": self.base_url, "bucket": self.bucket_name, "error": code}
        except Exception as e:
            return {"status": "unhealthy", "endpoint": self.base_url, "bucket": self.bucket_name, "error": str(e)}
    
    def _generate_key(self, post_url: str, original_url: str, media_type: str) -> str:
        """ç”Ÿæˆ RustFS å­˜å„²éµå€¼"""
        # ä½¿ç”¨ post_url å’Œ original_url çš„ hash ç”Ÿæˆå”¯ä¸€éµå€¼
        url_hash = hashlib.md5(f"{post_url}:{original_url}".encode()).hexdigest()
        
        # å¾åŸå§‹ URL ç²å–æª”æ¡ˆæ“´å±•å
        parsed_url = urlparse(original_url)
        path = Path(parsed_url.path)
        extension = path.suffix.lower()
        
        if not extension:
            # æ ¹æ“šåª’é«”é¡å‹æ¨æ¸¬æ“´å±•å
            if media_type == 'image':
                extension = '.jpg'
            elif media_type == 'video':
                extension = '.mp4'
            elif media_type == 'audio':
                extension = '.mp3'
            else:
                extension = '.bin'
        
        return f"{media_type}/{url_hash[:2]}/{url_hash[2:4]}/{url_hash}{extension}"
    
    async def download_and_store_media(
        self, 
        post_url: str, 
        media_urls: List[str],
        max_concurrent: int = 3
    ) -> List[Dict[str, Any]]:
        """
        æ‰¹æ¬¡ä¸‹è¼‰ä¸¦å­˜å„²åª’é«”æª”æ¡ˆåˆ° RustFS
        
        Args:
            post_url: è²¼æ–‡ URL
            media_urls: åª’é«” URL åˆ—è¡¨
            max_concurrent: æœ€å¤§ä¸¦ç™¼ä¸‹è¼‰æ•¸
            
        Returns:
            List[Dict]: ä¸‹è¼‰çµæœåˆ—è¡¨
        """
        if not media_urls:
            return []
        
        # é™åˆ¶ä¸¦ç™¼æ•¸é‡
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def download_single_media(media_url: str) -> Dict[str, Any]:
            async with semaphore:
                return await self._download_single_media(post_url, media_url)
        
        # ä¸¦ç™¼ä¸‹è¼‰æ‰€æœ‰åª’é«”æª”æ¡ˆ
        # ä¿è­‰æ‰€æœ‰å­ä»»å‹™åœ¨åŒä¸€äº‹ä»¶è¿´åœˆä¸­å»ºç«‹èˆ‡ç­‰å¾…ï¼Œé¿å…è·¨è¿´åœˆ Future
        loop = asyncio.get_running_loop()
        tasks = [loop.create_task(download_single_media(url)) for url in media_urls]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # è™•ç†çµæœï¼Œéæ¿¾ç•°å¸¸
        processed_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                print(f"âŒ Failed to download {media_urls[i]}: {result}")
                processed_results.append({
                    "original_url": media_urls[i],
                    "status": "failed",
                    "error": str(result)
                })
            else:
                processed_results.append(result)
        
        return processed_results
    
    async def _download_single_media(self, post_url: str, media_url: str) -> Dict[str, Any]:
        """ä¸‹è¼‰å–®å€‹åª’é«”æª”æ¡ˆ"""
        try:
            # æª¢æ¸¬åª’é«”é¡å‹
            media_type = self._detect_media_type(media_url)
            rustfs_key = self._generate_key(post_url, media_url, media_type)
            
            # è¨˜éŒ„åˆ°è³‡æ–™åº«ï¼ˆpending ç‹€æ…‹ï¼‰
            # æ¯æ¬¡åœ¨ç•¶å‰äº‹ä»¶è¿´åœˆä¸­ç²å–/å»ºç«‹é€£ç·šæ± ï¼Œé¿å…è·¨è¿´åœˆä½¿ç”¨
            db_client = await get_db_client()
            # ç›´æ¥ä»¥å®‰å…¨æ–¹å¼ç¢ºä¿ posts(url) å­˜åœ¨ï¼Œé¿å… upsert_post å‡½æ•¸é€ æˆ created_at NOT NULL å•é¡Œ
            await self._ensure_post_url(db_client, post_url)
            media_id = await self._record_media_file(
                db_client, post_url, media_url, media_type, rustfs_key, "pending"
            )
            # è‹¥æ­·å²ç’°å¢ƒç¼º created_at é è¨­ï¼Œé¿å… NULL å°è‡´å¾ŒçºŒé•å not-null
            try:
                async with db_client.get_connection() as conn:
                    await conn.execute("UPDATE media_files SET created_at = NOW() WHERE id = $1 AND created_at IS NULL", media_id)
            except Exception:
                pass
            
            # ä¸‹è¼‰æª”æ¡ˆ
            # å¼·åŒ–ä¸‹è¼‰å±¤ï¼šé‡å°ä¸åŒåª’é«”é¡å‹ä½¿ç”¨ä¸åŒçš„ headers
            if media_type == 'video' and 'instagram' in media_url.lower():
                # Instagram å½±ç‰‡éœ€è¦ç‰¹æ®Šè™•ç†
                headers = {
                    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0 Safari/537.36",
                    "Referer": "https://www.instagram.com/",
                    "Origin": "https://www.instagram.com",
                    "Accept": "video/webm,video/ogg,video/*;q=0.9,application/ogg;q=0.7,audio/*;q=0.6,*/*;q=0.5",
                    "Accept-Language": "en-US,en;q=0.5",
                    "Accept-Encoding": "gzip, deflate, br",
                    "DNT": "1",
                    "Connection": "keep-alive",
                    "Sec-Fetch-Dest": "video",
                    "Sec-Fetch-Mode": "cors",
                    "Sec-Fetch-Site": "same-site",
                }
            else:
                # åœ–ç‰‡å’Œå…¶ä»–åª’é«”
                headers = {
                    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0 Safari/537.36",
                    "Referer": "https://www.threads.net/",
                    "Accept": "*/*",
                }
            # å˜—è©¦é™„å¸¶ Threads cookiesï¼ˆå¾ auth.json è¼‰å…¥ï¼‰ï¼Œæå‡æˆåŠŸç‡
            cookie_header = self._get_cookie_header()
            if cookie_header:
                headers["Cookie"] = cookie_header
            
            # Instagram å½±ç‰‡ç‰¹æ®Šè™•ç†ï¼šå˜—è©¦æ›´å¤šèªè­‰ç›¸é—œçš„ headers
            if media_type == 'video' and 'instagram' in media_url.lower():
                # å¢åŠ  Instagram å¸¸ç”¨çš„å®‰å…¨ headers
                headers.update({
                    "X-Requested-With": "XMLHttpRequest",
                    "X-IG-App-ID": "936619743392459",
                    "X-Instagram-Ajax": "1",
                    "X-CSRFToken": "missing",  # å¦‚æœæ²’æœ‰çœŸå¯¦ token å°±ç”¨ä½”ä½ç¬¦
                })
            async with httpx.AsyncClient(timeout=120.0, headers=headers) as client:
                print(f"ğŸ“¥ Downloading {media_type}: {media_url}")
                
                # ä¸‹è¼‰åŸå§‹æª”æ¡ˆï¼ˆå½±ç‰‡å¯èƒ½éœ€è¦æ›´é•·æ™‚é–“ï¼‰
                max_retries = 3 if media_type == 'video' else 1
                last_exception = None
                
                for attempt in range(max_retries):
                    try:
                        # Instagram å½±ç‰‡ï¼šå˜—è©¦ HEAD è«‹æ±‚å…ˆç¢ºèªå¯ç”¨æ€§
                        if media_type == 'video' and 'instagram' in media_url.lower() and attempt == 0:
                            try:
                                head_response = await client.head(media_url, follow_redirects=True)
                                if head_response.status_code != 200:
                                    print(f"âš ï¸ HEAD check failed with {head_response.status_code}, trying direct download...")
                            except Exception:
                                print(f"âš ï¸ HEAD check failed, trying direct download...")
                        
                        response = await client.get(media_url, follow_redirects=True)
                        response.raise_for_status()
                        break
                    except httpx.HTTPStatusError as e:
                        if e.response.status_code == 403:
                            print(f"âŒ 403 Forbidden (attempt {attempt + 1}): {e.response.headers.get('X-FB-Debug', 'No debug info')}")
                            if attempt < max_retries - 1:
                                print(f"ğŸ”„ Retrying with different approach in 3s...")
                                await asyncio.sleep(3)
                                # å˜—è©¦ç§»é™¤ä¸€äº›å¯èƒ½å°è‡´å•é¡Œçš„ headers
                                if "X-Instagram-Ajax" in headers:
                                    del headers["X-Instagram-Ajax"]
                                if "X-CSRFToken" in headers:
                                    del headers["X-CSRFToken"]
                                last_exception = e
                                continue
                        raise
                    except Exception as e:
                        if attempt < max_retries - 1:
                            print(f"âš ï¸ Download failed (attempt {attempt + 1}): {e}, retrying...")
                            await asyncio.sleep(2)
                            last_exception = e
                            continue
                        raise
                else:
                    # All retries failed
                    raise last_exception or Exception("Max retries exceeded")
                
                file_content = response.content
                file_size = len(file_content)
                content_type = response.headers.get('content-type', '')
                
                # ç²å–æª”æ¡ˆæ“´å±•å
                file_extension = self._get_file_extension(media_url, content_type)
                
                # ä¸Šå‚³åˆ° RustFS
                try:
                    rustfs_url = await self._upload_to_rustfs(rustfs_key, file_content, content_type)
                except ClientError as s3e:
                    # è‹¥ç‰©ä»¶å·²å­˜åœ¨ï¼ˆé‡è©¦/ä¸¦ç™¼é‡è¤‡ï¼‰ï¼Œæ”¹ç‚ºå–å·²å­˜åœ¨çš„ URL
                    err_code = s3e.response.get('Error', {}).get('Code') if hasattr(s3e, 'response') else None
                    if err_code in ('EntityAlreadyExists', 'BucketAlreadyOwnedByYou'):
                        rustfs_url = f"{self.base_url}/{self.bucket_name}/{rustfs_key}"
                    else:
                        raise
                
                # ç²å–åª’é«”æª”æ¡ˆçš„å…ƒæ•¸æ“šï¼ˆå¯¬åº¦ã€é«˜åº¦ã€æ™‚é•·ç­‰ï¼‰
                metadata = await self._extract_media_metadata(file_content, media_type)
                
                # æ›´æ–°è³‡æ–™åº«è¨˜éŒ„
                await self._update_media_file(
                    db_client, media_id, rustfs_url, file_size, 
                    file_extension, metadata, "completed"
                )
                
                print(f"âœ… Successfully stored: {media_url} -> {rustfs_key}")
                
                return {
                    "original_url": media_url,
                    "rustfs_key": rustfs_key,
                    "rustfs_url": rustfs_url,
                    "media_type": media_type,
                    "file_size": file_size,
                    "status": "completed",
                    "metadata": metadata
                }
                
        except Exception as e:
            print(f"âŒ Failed to download {media_url}: {e}")
            
            # æ›´æ–°è³‡æ–™åº«è¨˜éŒ„ç‚ºå¤±æ•—ç‹€æ…‹
            try:
                if 'media_id' in locals():
                    await self._update_media_file(
                        db_client, media_id, None, None, None, {}, "failed", str(e)
                    )
            except:
                pass
            
            return {
                "original_url": media_url,
                "status": "failed",
                "error": str(e)
            }

    async def _ensure_post_url(self, db_client, post_url: str) -> None:
        """æœ€å°åŒ–ä¿è­‰ posts(url) å­˜åœ¨ï¼Œé¿å… FK é˜»æ“‹ã€‚"""
        try:
            async with db_client.get_connection() as conn:
                # æŸ¥è©¢ posts æ¬„ä½
                col_rows = await conn.fetch(
                    """
                    SELECT column_name FROM information_schema.columns 
                    WHERE table_schema = 'public' AND table_name = 'posts'
                    """
                )
                cols = {r["column_name"] for r in col_rows}

                # æ§‹å»º INSERT æ¬„ä½/å€¼
                fields = ["url", "author"]
                values = ["$1", "$2"]
                params = [post_url, "playwright"]

                if "markdown" in cols:
                    fields.append("markdown"); values.append("$3"); params.append(None)
                if "media_urls" in cols:
                    # å„²å­˜ç‚ºç©ºé™£åˆ—
                    fields.append("media_urls"); values.append("'[]'::jsonb")
                if "created_at" in cols:
                    fields.append("created_at"); values.append("NOW()")
                if "last_seen" in cols:
                    fields.append("last_seen"); values.append("NOW()")

                insert_sql = f"INSERT INTO posts ({', '.join(fields)}) VALUES ({', '.join(values)}) "
                # è¡çªæ™‚æ›´æ–° last_seen
                update_parts = []
                if "last_seen" in cols:
                    update_parts.append("last_seen = NOW()")
                if "media_urls" in cols:
                    update_parts.append("media_urls = COALESCE(posts.media_urls, EXCLUDED.media_urls)")
                if "markdown" in cols:
                    update_parts.append("markdown = COALESCE(posts.markdown, EXCLUDED.markdown)")
                if update_parts:
                    insert_sql += "ON CONFLICT (url) DO UPDATE SET " + ", ".join(update_parts)
                else:
                    insert_sql += "ON CONFLICT (url) DO NOTHING"

                await conn.execute(insert_sql, *params)
        except Exception:
            # éœé»˜å¤±æ•—ï¼Œä¸é˜»æ–·ä¸»æµç¨‹
            pass

    def _get_cookie_header(self) -> Optional[str]:
        """
        å¾ Playwright çš„ auth.json è¼‰å…¥ cookiesï¼Œåˆä½µç‚º Cookie æ¨™é ­å­—ä¸²ã€‚
        æ³¨æ„ï¼šæ­¤ç‚ºæœ€ä½³åŠªåŠ›ï¼Œå¯¦éš›ç«™é»å¯èƒ½éœ€è¦ç‰¹å®šåŸŸå/è·¯å¾‘ï¼›é€™è£¡çµ±ä¸€é™„ä¸Šå¸¸ç”¨ cookiesã€‚
        """
        if self._cookie_header is not None:
            return self._cookie_header
        try:
            auth_path = get_auth_file_path()
            if not auth_path.exists():
                self._cookie_header = ""
                return self._cookie_header
            with open(auth_path, "r", encoding="utf-8") as f:
                data = json.load(f)
            cookies = data.get("cookies") or data.get("orig_cookies") or []
            parts = []
            for c in cookies:
                name = c.get("name")
                value = c.get("value")
                if not name or value is None:
                    continue
                # éæ¿¾æ˜é¡¯ç„¡æ•ˆ/æ•æ„Ÿåå¯è‡ªè¡Œæ“´å……
                parts.append(f"{name}={value}")
            self._cookie_header = "; ".join(parts) if parts else ""
            return self._cookie_header
        except Exception:
            self._cookie_header = ""
            return self._cookie_header
    
    def _detect_media_type(self, url: str) -> str:
        """æª¢æ¸¬åª’é«”é¡å‹"""
        url_lower = url.lower()
        
        # åœ–ç‰‡æ ¼å¼
        if any(ext in url_lower for ext in ['.jpg', '.jpeg', '.png', '.gif', '.webp', '.bmp', '.svg']):
            return 'image'
        
        # å½±ç‰‡æ ¼å¼
        if any(ext in url_lower for ext in ['.mp4', '.avi', '.mov', '.wmv', '.flv', '.webm', '.mkv']):
            return 'video'
        
        # éŸ³é »æ ¼å¼
        if any(ext in url_lower for ext in ['.mp3', '.wav', '.flac', '.aac', '.ogg', '.m4a']):
            return 'audio'
        
        # é è¨­ç‚ºåœ–ç‰‡ï¼ˆç¤¾äº¤åª’é«”å¤§å¤šæ˜¯åœ–ç‰‡ï¼‰
        return 'image'
    
    def _get_file_extension(self, url: str, content_type: str) -> str:
        """ç²å–æª”æ¡ˆæ“´å±•å"""
        # å…ˆå¾ URL ç²å–
        parsed_url = urlparse(url)
        path = Path(parsed_url.path)
        extension = path.suffix.lower()
        
        if extension:
            return extension
        
        # å¾ content-type æ¨æ¸¬
        if content_type:
            extension = mimetypes.guess_extension(content_type)
            if extension:
                return extension
        
        # é è¨­
        return '.bin'
    
    async def _upload_to_rustfs(self, key: str, content: bytes, content_type: str) -> str:
        """ä¸Šå‚³æª”æ¡ˆåˆ° RustFS"""
        s3 = self._create_s3_client()
        def _put():
            s3.put_object(Bucket=self.bucket_name, Key=key, Body=content, ContentType=content_type)
            return f"{self.base_url}/{self.bucket_name}/{key}"
        return await asyncio.to_thread(_put)

    async def upload_user_media(self, filename: str, content: bytes, content_type: str) -> Dict[str, str]:
        """
        ä¸Šå‚³ä½¿ç”¨è€…åœ¨æ’°å¯«å™¨åŒ¯å…¥çš„åª’é«”åˆ° RustFSï¼Œè¿”å› key èˆ‡å¯ç€è¦½ URLã€‚
        è·¯å¾‘æ ¼å¼ï¼šuser-media/YYYYMMDD/uuid_filename
        """
        safe_name = ''.join(ch for ch in filename if ch.isalnum() or ch in ('-', '_', '.', ' ')).strip().replace(' ', '_')
        day = datetime.utcnow().strftime('%Y%m%d')
        unique = uuid.uuid4().hex[:12]
        key = f"user-media/{day}/{unique}_{safe_name}"
        rustfs_url = await self._upload_to_rustfs(key, content, content_type or 'application/octet-stream')
        browse_url = self.get_public_or_presigned_url(key, prefer_presigned=True)
        return {"key": key, "rustfs_url": rustfs_url, "url": browse_url, "content_type": content_type or ""}

    def make_object_url(self, key: str) -> str:
        """å›å‚³æœªç°½åçš„ç‰©ä»¶ URLï¼ˆé©ç”¨æ–¼å·²è¨­å…¬é–‹è®€å–çš„ bucketï¼‰ã€‚"""
        return f"{self.base_url}/{self.bucket_name}/{key}"

    def generate_presigned_url(self, key: str, expires_in: int = 3600) -> Optional[str]:
        """ç”¢ç”Ÿ GET ç‰©ä»¶çš„ç°½å URLï¼ˆé è¨­ä¸€å°æ™‚ï¼‰ã€‚"""
        try:
            s3 = self._create_s3_client()
            return s3.generate_presigned_url(
                ClientMethod='get_object',
                Params={'Bucket': self.bucket_name, 'Key': key},
                ExpiresIn=expires_in
            )
        except Exception as e:
            print(f"âš ï¸ Failed to generate presigned url for {key}: {e}")
            return None

    def get_public_or_presigned_url(self, key: str, prefer_presigned: bool = True, expires_in: int = 3600) -> str:
        """å–å¾—å¯ç”¨æ–¼ç€è¦½çš„ URLã€‚
        - è‹¥ prefer_presigned ç‚º Trueï¼Œå„ªå…ˆå›å‚³ç°½å URLï¼›å¤±æ•—å‰‡é€€å›æœªç°½å URLã€‚
        - è‹¥ bucket æœªè¨­å…¬é–‹è®€å–ï¼Œæœªç°½å URL æœƒ 403ã€‚
        """
        if prefer_presigned:
            url = self.generate_presigned_url(key, expires_in=expires_in)
            if url:
                return url
        return self.make_object_url(key)
    
    async def _extract_media_metadata(self, content: bytes, media_type: str) -> Dict[str, Any]:
        """æå–åª’é«”æª”æ¡ˆå…ƒæ•¸æ“š"""
        metadata = {}
        
        try:
            if media_type == 'image':
                # ä½¿ç”¨ PIL ç²å–åœ–ç‰‡å°ºå¯¸
                try:
                    from PIL import Image
                    import io
                    
                    image = Image.open(io.BytesIO(content))
                    metadata['width'] = image.width
                    metadata['height'] = image.height
                    metadata['format'] = image.format
                except ImportError:
                    print("âš ï¸ PIL not available, skipping image metadata extraction")
                except Exception as e:
                    print(f"âš ï¸ Failed to extract image metadata: {e}")
            
            elif media_type == 'video':
                # å°æ–¼å½±ç‰‡ï¼Œå¯ä»¥ä½¿ç”¨ ffprobe æˆ–å…¶ä»–å·¥å…·
                # é€™è£¡å…ˆè·³éï¼Œå› ç‚ºéœ€è¦é¡å¤–çš„ä¾è³´
                pass
                
        except Exception as e:
            print(f"âš ï¸ Failed to extract metadata: {e}")
        
        return metadata
    
    async def _record_media_file(
        self, 
        db_client, 
        post_url: str, 
        original_url: str, 
        media_type: str, 
        rustfs_key: str, 
        status: str
    ) -> int:
        """è¨˜éŒ„åª’é«”æª”æ¡ˆåˆ°è³‡æ–™åº«"""
        async with db_client.get_connection() as conn:
            media_id = await conn.fetchval(
                """
                INSERT INTO media_files (
                    post_url, original_url, media_type, rustfs_key, download_status
                )
                VALUES ($1, $2, $3, $4, $5)
                ON CONFLICT (rustfs_key) DO UPDATE SET
                    original_url = EXCLUDED.original_url,
                    media_type = EXCLUDED.media_type
                RETURNING media_files.id
                """,
                post_url, original_url, media_type, rustfs_key, status,
            )
            return media_id
    
    async def _update_media_file(
        self, 
        db_client, 
        media_id: int, 
        rustfs_url: Optional[str], 
        file_size: Optional[int],
        file_extension: Optional[str],
        metadata: Dict[str, Any], 
        status: str,
        error: Optional[str] = None
    ):
        """æ›´æ–°åª’é«”æª”æ¡ˆè¨˜éŒ„"""
        async with db_client.get_connection() as conn:
            await conn.execute("""
                UPDATE media_files 
                SET 
                    rustfs_url = $2,
                    file_size = $3,
                    file_extension = $4,
                    width = $5,
                    height = $6,
                    duration = $7,
                    download_status = $8::text,
                    download_error = $9,
                    downloaded_at = CASE WHEN $8::text = 'completed' THEN now() ELSE downloaded_at END,
                    metadata = $10::jsonb
                WHERE id = $1
            """, 
            media_id, rustfs_url, file_size, file_extension,
            metadata.get('width'), metadata.get('height'), metadata.get('duration'),
            status, error, json.dumps(metadata) if metadata else '{}')
    
    async def get_media_files(self, post_url: str) -> List[Dict[str, Any]]:
        """ç²å–è²¼æ–‡çš„åª’é«”æª”æ¡ˆ"""
        db_client = await get_db_client()
        
        async with db_client.get_connection() as conn:
            rows = await conn.fetch("""
                SELECT 
                    id, original_url, media_type, file_extension, rustfs_key, rustfs_url,
                    file_size, width, height, duration, download_status, download_error,
                    created_at, downloaded_at, metadata
                FROM media_files 
                WHERE post_url = $1
                ORDER BY created_at
            """, post_url)
            
            return [dict(row) for row in rows]
    
    async def cleanup_failed_downloads(self, max_age_hours: int = 24):
        """æ¸…ç†å¤±æ•—çš„ä¸‹è¼‰è¨˜éŒ„"""
        db_client = await get_db_client()
        
        async with db_client.get_connection() as conn:
            deleted_count = await conn.fetchval("""
                DELETE FROM media_files 
                WHERE download_status = 'failed' 
                AND created_at < now() - interval '%s hours'
                RETURNING COUNT(*)
            """, max_age_hours)
            
            print(f"ğŸ§¹ Cleaned up {deleted_count} failed download records")
            return deleted_count


# å…¨åŸŸ RustFS å®¢æˆ¶ç«¯å¯¦ä¾‹
rustfs_client = RustFSClient()


async def get_rustfs_client() -> RustFSClient:
    """ç²å– RustFS å®¢æˆ¶ç«¯å¯¦ä¾‹"""
    return rustfs_client


async def download_media_for_post(post_url: str, media_urls: List[str]) -> List[Dict[str, Any]]:
    """ç‚ºè²¼æ–‡ä¸‹è¼‰åª’é«”æª”æ¡ˆçš„ä¾¿åˆ©å‡½æ•¸"""
    client = await get_rustfs_client()
    return await client.download_and_store_media(post_url, media_urls)


if __name__ == "__main__":
    # æ¸¬è©¦ç”¨ä¾‹
    async def test_rustfs():
        client = RustFSClient()
        await client.initialize()
        
        # æ¸¬è©¦ä¸‹è¼‰
        test_urls = [
            "https://example.com/image1.jpg",
            "https://example.com/image2.png"
        ]
        
        results = await client.download_and_store_media(
            "https://example.com/post/123", 
            test_urls
        )
        
        print("Download results:", results)
    
    asyncio.run(test_rustfs())

