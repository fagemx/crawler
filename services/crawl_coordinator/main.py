#!/usr/bin/env python3
"""
Crawl Coordinator Service - çµ±ä¸€çˆ¬èŸ²å”èª¿å™¨

å¯¦ç¾ä½ è¦æ±‚çš„ã€Œå…ˆå¿«å¾Œå…¨ã€é›™è»Œçˆ¬èŸ²æµç¨‹ï¼š
1. fast: åªä½¿ç”¨Readerå¿«é€Ÿæå–
2. full: åªä½¿ç”¨Playwrightå®Œæ•´çˆ¬å–  
3. hybrid: å…ˆReaderç«‹å³è¿”å›ï¼ŒèƒŒæ™¯Playwrightè£œå®Œ

æ”¯æŒçš„å·¥ä½œæµç¨‹ï¼š
- ç”¨æˆ¶é¸æ“‡å¸³è™Ÿå’Œæ¨¡å¼
- å³æ™‚è¿”å›å¯ç”¨æ•¸æ“š
- èƒŒæ™¯è£œå®Œè©³ç´°æ•¸æ“š
- å‰ç«¯å¯¦æ™‚æ›´æ–°ç‹€æ…‹
"""

import asyncio
import aiohttp
import uuid
import logging
from datetime import datetime
from typing import List, Dict, Any, Optional
from fastapi import FastAPI, HTTPException, BackgroundTasks
from pydantic import BaseModel, Field
import sys
import os

# æ·»åŠ å°ˆæ¡ˆæ ¹ç›®éŒ„åˆ° Python è·¯å¾‘
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))

from common.models import PostMetrics, PostMetricsBatch
from common.history import CrawlHistoryDAO
from common.settings import get_settings

app = FastAPI(
    title="Crawl Coordinator Service", 
    version="1.0.0",
    description="çµ±ä¸€çˆ¬èŸ²å”èª¿å™¨ - æ”¯æ´å¿«é€Ÿ/å®Œæ•´/æ··åˆæ¨¡å¼"
)

# æœå‹™ç«¯é»é…ç½®
READER_PROCESSOR_URL = "http://reader-processor:8009"
PLAYWRIGHT_CRAWLER_URL = "http://playwright-crawler-agent:8006"

class CrawlRequest(BaseModel):
    """çµ±ä¸€çˆ¬èŸ²è«‹æ±‚"""
    username: str = Field(..., description="è¦çˆ¬å–çš„ç”¨æˆ¶å")
    max_posts: int = Field(default=20, description="æœ€å¤§è²¼æ–‡æ•¸é‡")
    mode: str = Field(default="hybrid", description="çˆ¬å–æ¨¡å¼: fast/full/hybrid")
    also_slow: bool = Field(default=True, description="hybridæ¨¡å¼æ˜¯å¦å•Ÿç”¨èƒŒæ™¯å®Œæ•´çˆ¬å–")
    auth_json_content: Optional[Dict[str, Any]] = Field(None, description="èªè­‰ä¿¡æ¯")
    task_id: Optional[str] = Field(None, description="ä»»å‹™ID")

class CrawlResponse(BaseModel):
    """çµ±ä¸€çˆ¬èŸ²éŸ¿æ‡‰"""
    task_id: str
    username: str
    mode: str
    status: str  # immediate/processing/completed
    message: str
    posts: List[Dict[str, Any]]
    summary: Dict[str, Any]
    estimated_completion: Optional[datetime] = None

class CrawlCoordinator:
    """çˆ¬èŸ²å”èª¿å™¨æ ¸å¿ƒé‚è¼¯"""
    
    def __init__(self):
        self.history_dao = CrawlHistoryDAO()
    
    async def get_urls_for_processing(self, username: str, max_posts: int) -> List[str]:
        """ç²å–éœ€è¦è™•ç†çš„URLs"""
        try:
            # æ–¹æ³•1: å¾æ•¸æ“šåº«ç²å–å·²çŸ¥URLs
            existing_status = await self.history_dao.get_posts_status(username)
            existing_urls = [item['url'] for item in existing_status[:max_posts]]
            
            if existing_urls:
                logging.info(f"ğŸ“‹ å¾æ•¸æ“šåº«ç²å– {username} çš„ {len(existing_urls)} å€‹URLs")
                return existing_urls
            
            # æ–¹æ³•2: èª¿ç”¨Playwright Crawlerçš„URLç«¯é»ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            async with aiohttp.ClientSession() as session:
                url_endpoint = f"{PLAYWRIGHT_CRAWLER_URL}/urls/{username}?max_posts={max_posts}"
                async with session.get(url_endpoint) as response:
                    if response.status == 200:
                        data = await response.json()
                        urls = [item['url'] for item in data['urls']]
                        logging.info(f"ğŸ”— å¾Playwrightç²å– {username} çš„ {len(urls)} å€‹URLs")
                        return urls
            
            logging.warning(f"âš ï¸ ç„¡æ³•ç²å– {username} çš„URLs")
            return []
            
        except Exception as e:
            logging.error(f"âŒ ç²å–URLså¤±æ•—: {e}")
            return []
    
    async def process_fast_mode(self, request: CrawlRequest) -> CrawlResponse:
        """å¿«é€Ÿæ¨¡å¼ï¼šåªä½¿ç”¨Reader"""
        task_id = request.task_id or str(uuid.uuid4())
        
        # ç²å–URLs
        urls = await self.get_urls_for_processing(request.username, request.max_posts)
        if not urls:
            raise HTTPException(status_code=404, detail=f"æ‰¾ä¸åˆ°ç”¨æˆ¶ {request.username} çš„è²¼æ–‡URLs")
        
        # æª¢æŸ¥å“ªäº›éœ€è¦Readerè™•ç†
        existing_status = await self.history_dao.get_posts_status(request.username)
        status_map = {item['url']: item for item in existing_status}
        
        need_reader_urls = [
            url for url in urls 
            if url not in status_map or status_map[url]['reader_status'] != 'success'
        ]
        
        if not need_reader_urls:
            # æ‰€æœ‰URLséƒ½å·²ç¶“æœ‰Readerçµæœ
            posts = [
                {
                    "url": url,
                    "post_id": status_map[url]['post_id'],
                    "reader_status": status_map[url]['reader_status'],
                    "dom_status": status_map[url]['dom_status'],
                    "content": "å·²å¿«å–" if status_map[url]['has_content'] else None
                }
                for url in urls if url in status_map
            ]
            
            return CrawlResponse(
                task_id=task_id,
                username=request.username,
                mode="fast",
                status="completed",
                message=f"æ‰€æœ‰ {len(posts)} ç¯‡è²¼æ–‡å·²æœ‰Readerçµæœ",
                posts=posts,
                summary={"from_cache": len(posts), "processed": 0}
            )
        
        # èª¿ç”¨Reader Processor
        async with aiohttp.ClientSession() as session:
            reader_request = {
                "urls": need_reader_urls,
                "username": request.username,
                "task_id": task_id,
                "return_format": "text"
            }
            
            async with session.post(f"{READER_PROCESSOR_URL}/process", json=reader_request) as response:
                if response.status == 200:
                    reader_result = await response.json()
                    
                    # æ§‹å»ºéŸ¿æ‡‰
                    posts = []
                    for result in reader_result['results']:
                        posts.append({
                            "url": result['url'],
                            "post_id": result['url'].split('/')[-1],
                            "reader_status": result['status'],
                            "dom_status": "pending",
                            "content": result.get('content'),
                            "processing_time": result.get('processing_time')
                        })
                    
                    return CrawlResponse(
                        task_id=task_id,
                        username=request.username,
                        mode="fast",
                        status="completed",
                        message=f"Readerè™•ç†å®Œæˆ: {reader_result['successful']}/{reader_result['total_urls']} æˆåŠŸ",
                        posts=posts,
                        summary={
                            "successful": reader_result['successful'],
                            "failed": reader_result['failed'],
                            "total_time": reader_result['total_time']
                        }
                    )
                else:
                    error_text = await response.text()
                    raise HTTPException(status_code=500, detail=f"Readerè™•ç†å¤±æ•—: {error_text}")
    
    async def process_full_mode(self, request: CrawlRequest) -> CrawlResponse:
        """å®Œæ•´æ¨¡å¼ï¼šåªä½¿ç”¨Playwright Crawler"""
        task_id = request.task_id or str(uuid.uuid4())
        
        if not request.auth_json_content:
            raise HTTPException(status_code=400, detail="å®Œæ•´æ¨¡å¼éœ€è¦æä¾›èªè­‰ä¿¡æ¯")
        
        # èª¿ç”¨Playwright Crawler
        async with aiohttp.ClientSession() as session:
            playwright_request = {
                "username": request.username,
                "max_posts": request.max_posts,
                "auth_json_content": request.auth_json_content,
                "task_id": task_id
            }
            
            async with session.post(f"{PLAYWRIGHT_CRAWLER_URL}/v1/playwright/crawl", json=playwright_request) as response:
                if response.status == 200:
                    crawler_result = await response.json()
                    
                    # è½‰æ›æ ¼å¼
                    posts = []
                    for post in crawler_result['posts']:
                        posts.append({
                            "url": post['url'],
                            "post_id": post['post_id'],
                            "reader_status": post.get('reader_status', 'success'),
                            "dom_status": post.get('dom_status', 'success'),
                            "content": post.get('content'),
                            "likes_count": post.get('likes_count'),
                            "views_count": post.get('views_count'),
                            "images": post.get('images', []),
                            "videos": post.get('videos', [])
                        })
                    
                    return CrawlResponse(
                        task_id=task_id,
                        username=request.username,
                        mode="full",
                        status="completed",
                        message=f"å®Œæ•´çˆ¬å–å®Œæˆ: {len(posts)} ç¯‡è²¼æ–‡",
                        posts=posts,
                        summary={
                            "total_count": crawler_result['total_count'],
                            "processing_stage": crawler_result.get('processing_stage')
                        }
                    )
                else:
                    error_text = await response.text()
                    raise HTTPException(status_code=500, detail=f"Playwrightçˆ¬å–å¤±æ•—: {error_text}")
    
    async def process_hybrid_mode(self, request: CrawlRequest, background_tasks: BackgroundTasks) -> CrawlResponse:
        """æ··åˆæ¨¡å¼ï¼šå…ˆå¿«å¾Œå…¨"""
        task_id = request.task_id or str(uuid.uuid4())
        
        # ç¬¬1éšæ®µï¼šå¿«é€ŸReaderè™•ç†
        fast_request = CrawlRequest(
            username=request.username,
            max_posts=request.max_posts,
            mode="fast",
            task_id=f"{task_id}_fast"
        )
        
        fast_response = await self.process_fast_mode(fast_request)
        
        # ç¬¬2éšæ®µï¼šèƒŒæ™¯Playwrightè£œå®Œï¼ˆå¦‚æœæœ‰èªè­‰ä¿¡æ¯ä¸”å•Ÿç”¨also_slowï¼‰
        if request.also_slow and request.auth_json_content:
            background_tasks.add_task(
                self.background_full_crawl,
                request.username,
                request.max_posts,
                request.auth_json_content,
                f"{task_id}_full"
            )
            
            return CrawlResponse(
                task_id=task_id,
                username=request.username,
                mode="hybrid",
                status="processing",
                message=f"å¿«é€Ÿçµæœå·²è¿”å›ï¼ŒèƒŒæ™¯è£œå®Œé€²è¡Œä¸­",
                posts=fast_response.posts,
                summary={
                    **fast_response.summary,
                    "background_processing": True
                },
                estimated_completion=datetime.utcnow()
            )
        else:
            return CrawlResponse(
                task_id=task_id,
                username=request.username,
                mode="hybrid",
                status="completed",
                message="åªæœ‰å¿«é€Ÿçµæœï¼ˆæœªå•Ÿç”¨èƒŒæ™¯è£œå®Œï¼‰",
                posts=fast_response.posts,
                summary=fast_response.summary
            )
    
    async def background_full_crawl(self, username: str, max_posts: int, auth_json_content: Dict[str, Any], task_id: str):
        """èƒŒæ™¯åŸ·è¡Œå®Œæ•´çˆ¬å–"""
        try:
            logging.info(f"ğŸ”„ [Task: {task_id}] é–‹å§‹èƒŒæ™¯å®Œæ•´çˆ¬å–: {username}")
            
            async with aiohttp.ClientSession() as session:
                playwright_request = {
                    "username": username,
                    "max_posts": max_posts,
                    "auth_json_content": auth_json_content,
                    "task_id": task_id
                }
                
                async with session.post(f"{PLAYWRIGHT_CRAWLER_URL}/v1/playwright/crawl", json=playwright_request) as response:
                    if response.status == 200:
                        result = await response.json()
                        logging.info(f"âœ… [Task: {task_id}] èƒŒæ™¯çˆ¬å–å®Œæˆ: {result['total_count']} ç¯‡è²¼æ–‡")
                    else:
                        error_text = await response.text()
                        logging.error(f"âŒ [Task: {task_id}] èƒŒæ™¯çˆ¬å–å¤±æ•—: {error_text}")
                        
        except Exception as e:
            logging.error(f"âŒ [Task: {task_id}] èƒŒæ™¯çˆ¬å–ç•°å¸¸: {e}")

# å…¨å±€å”èª¿å™¨å¯¦ä¾‹
coordinator = CrawlCoordinator()

@app.get("/health")
async def health_check():
    """å¥åº·æª¢æŸ¥"""
    return {
        "status": "healthy",
        "service": "Crawl Coordinator",
        "supported_modes": ["fast", "full", "hybrid"]
    }

@app.post("/crawl", response_model=CrawlResponse)
async def unified_crawl(request: CrawlRequest, background_tasks: BackgroundTasks):
    """
    çµ±ä¸€çˆ¬èŸ²ç«¯é»
    
    æ”¯æŒæ¨¡å¼ï¼š
    - fast: åªä½¿ç”¨Readerå¿«é€Ÿæå–å…§å®¹
    - full: åªä½¿ç”¨Playwrightå®Œæ•´çˆ¬å–ï¼ˆéœ€è¦èªè­‰ï¼‰
    - hybrid: å…ˆReaderå¿«é€Ÿè¿”å›ï¼ŒèƒŒæ™¯Playwrightè£œå®Œ
    """
    try:
        if request.mode == "fast":
            return await coordinator.process_fast_mode(request)
        elif request.mode == "full":
            return await coordinator.process_full_mode(request)
        elif request.mode == "hybrid":
            return await coordinator.process_hybrid_mode(request, background_tasks)
        else:
            raise HTTPException(status_code=400, detail=f"ä¸æ”¯æŒçš„æ¨¡å¼: {request.mode}")
            
    except Exception as e:
        logging.error(f"âŒ çˆ¬èŸ²å”èª¿å¤±æ•—: {e}")
        raise HTTPException(status_code=500, detail=f"çˆ¬èŸ²è™•ç†å¤±æ•—: {str(e)}")

@app.get("/status/{username}")
async def get_user_status(username: str):
    """ç²å–ç”¨æˆ¶è²¼æ–‡è™•ç†ç‹€æ…‹æ‘˜è¦"""
    try:
        summary = await coordinator.history_dao.get_processing_needs(username)
        posts_status = await coordinator.history_dao.get_posts_status(username)
        
        return {
            "username": username,
            "summary": summary,
            "recent_posts": posts_status[:10]  # æœ€è¿‘10ç¯‡
        }
    except Exception as e:
        logging.error(f"âŒ ç²å–ç‹€æ…‹å¤±æ•—: {e}")
        raise HTTPException(status_code=500, detail=f"ç‹€æ…‹æŸ¥è©¢å¤±æ•—: {str(e)}")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8008)