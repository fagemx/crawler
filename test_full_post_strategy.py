"""
å¯¦ç¾å®Œæ•´çš„ã€Œè¨ˆæ•¸ + å…§å®¹ã€é›™æŸ¥è©¢ç­–ç•¥
åŸºæ–¼ç”¨æˆ¶æä¾›çš„å°ˆæ¥­è§£æ±ºæ–¹æ¡ˆ
"""

import sys
import asyncio
import json
import re
import httpx
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Optional

# æ·»åŠ  zstd æ”¯æŒ
try:
    import zstandard
    HAS_ZSTD = True
except ImportError:
    print("âš ï¸ å»ºè­°å®‰è£ zstandard: pip install zstandard")
    HAS_ZSTD = False

# Windows asyncio ä¿®å¾©
if sys.platform.startswith("win"):
    asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())

from playwright.async_api import async_playwright, Page

# å°å…¥å¿…è¦çš„è§£æå‡½æ•¸
sys.path.append(str(Path(__file__).parent))
from common.config import get_auth_file_path

# æ¸¬è©¦è²¼æ–‡
TEST_URL = "https://www.threads.com/@star_shining0828/post/DMyvZJRz5Cz"
TARGET_CODE = "DMyvZJRz5Cz"

# é æœŸæ•¸æ“š
EXPECTED_DATA = {
    "likes": 233,
    "comments": 66,
    "reposts": 6,
    "shares": 34
}

# å¸¸æ•¸
APP_ID = "238260118697367"
# å˜—è©¦å¤šå€‹å¯èƒ½çš„ manifest URL
MANIFEST_URLS = [
    "https://www.threads.com/data/manifest.json",
    "https://www.threads.com/static/bundles/metro/barcelona_web-QueryMap.json",
    "https://static.cdninstagram.com/rsrc.php/v4/manifest.json",
    "https://www.threads.com/qp/batch_fetch_web/",
]
COUNTS_DOC_ID = "6637585034415426"  # å›ºå®šçš„ counts doc_id

class ThreadsFullPostFetcher:
    def __init__(self, auth_file_path: Path):
        """åˆå§‹åŒ– Threads å®Œæ•´è²¼æ–‡æŠ“å–å™¨"""
        # è®€å–èªè­‰
        auth_data = json.loads(auth_file_path.read_text())
        self.cookies = {c["name"]: c["value"] for c in auth_data["cookies"]}
        
        # è¨­ç½® HTTP å®¢æˆ¶ç«¯
        self.headers = {
            "x-ig-app-id": APP_ID,
            "user-agent": "Mozilla/5.0 (iPhone; CPU iPhone OS 15_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.0 Mobile/15E148 Safari/604.1",
            "accept": "*/*",
            "accept-language": "zh-TW,zh;q=0.9,en;q=0.8",
            "content-type": "application/x-www-form-urlencoded",
            "origin": "https://www.threads.com",
            "referer": "https://www.threads.com/",
            "x-requested-with": "XMLHttpRequest",
            "x-csrftoken": self.cookies.get("csrftoken", ""),
            "x-asbd-id": "129477",
            "sec-fetch-dest": "empty",
            "sec-fetch-mode": "cors",
            "sec-fetch-site": "same-origin",
        }
        
        # å¦‚æœæœ‰ authorization tokenï¼Œæ·»åŠ å®ƒ
        if "authorization" in [c["name"].lower() for c in auth_data["cookies"]]:
            auth_cookie = next((c for c in auth_data["cookies"] if c["name"].lower() == "authorization"), None)
            if auth_cookie:
                self.headers["authorization"] = auth_cookie["value"]
        
        self.http_client = httpx.AsyncClient(
            cookies=self.cookies,
            headers=self.headers,
            timeout=30.0,
            http2=True
        )
        
        self.content_doc_id = None
        self.lsd_token = None
    
    async def _load_manifest(self) -> Optional[str]:
        """è¼‰å…¥ä¸¦è§£å£“ç¸® manifest.json"""
        try:
            print("      ğŸ”„ è¼‰å…¥ manifest.json...")
            response = await self.http_client.get("https://www.threads.com/data/manifest.json")
            if response.status_code != 200:
                print(f"         âŒ HTTP {response.status_code}")
                return None
            
            raw_content = response.content
            
            # æª¢æŸ¥ Content-Encoding header
            content_encoding = response.headers.get("content-encoding", "").lower()
            print(f"         ğŸ“‹ Content-Encoding: {content_encoding or 'none'}")
            
            # å˜—è©¦ä¸åŒçš„è§£å£“ç¸®æ–¹æ³•
            manifest_text = None
            
            # æ–¹æ³•1: ç›´æ¥ç•¶ä½œ UTF-8 æ–‡æœ¬
            try:
                manifest_text = raw_content.decode('utf-8')
                print(f"         âœ… ç›´æ¥è§£æç‚º UTF-8 æˆåŠŸ ({len(manifest_text):,} å­—ç¬¦)")
                return manifest_text
            except UnicodeDecodeError:
                print(f"         âš ï¸ ä¸æ˜¯ç´” UTF-8ï¼Œå˜—è©¦è§£å£“ç¸®...")
            
            # æ–¹æ³•2: zstd è§£å£“ç¸®
            if content_encoding == "zstd" or raw_content.startswith(b'\x28\xb5\x2f\xfd'):
                if HAS_ZSTD:
                    try:
                        print(f"         ğŸ”„ å˜—è©¦ zstd è§£å£“ç¸®...")
                        decompressed = zstandard.ZstdDecompressor().decompress(raw_content)
                        manifest_text = decompressed.decode('utf-8')
                        print(f"         âœ… zstd è§£å£“ç¸®æˆåŠŸ ({len(manifest_text):,} å­—ç¬¦)")
                        return manifest_text
                    except Exception as e:
                        print(f"         âŒ zstd è§£å£“ç¸®å¤±æ•—: {e}")
                else:
                    print(f"         âŒ éœ€è¦ zstandard åº«ä¾†è§£å£“ç¸®")
            
            # æ–¹æ³•3: gzip è§£å£“ç¸®
            if content_encoding == "gzip" or raw_content.startswith(b'\x1f\x8b'):
                try:
                    import gzip
                    print(f"         ğŸ”„ å˜—è©¦ gzip è§£å£“ç¸®...")
                    decompressed = gzip.decompress(raw_content)
                    manifest_text = decompressed.decode('utf-8')
                    print(f"         âœ… gzip è§£å£“ç¸®æˆåŠŸ ({len(manifest_text):,} å­—ç¬¦)")
                    return manifest_text
                except Exception as e:
                    print(f"         âŒ gzip è§£å£“ç¸®å¤±æ•—: {e}")
            
            # æ–¹æ³•4: ä¿å­˜åŸå§‹å…§å®¹ç”¨æ–¼èª¿è©¦
            debug_file = Path(f"debug_manifest_raw_{datetime.now().strftime('%H%M%S')}.bin")
            with open(debug_file, 'wb') as f:
                f.write(raw_content)
            print(f"         ğŸ“ å·²ä¿å­˜åŸå§‹ manifest åˆ°: {debug_file}")
            print(f"         ğŸ“‹ åŸå§‹æ•¸æ“šå‰20å­—ç¯€: {raw_content[:20]}")
            
            return None
            
        except Exception as e:
            print(f"         âŒ è¼‰å…¥ manifest å¤±æ•—: {e}")
            return None

    async def get_content_doc_id(self) -> Optional[str]:
        """å¾ manifest.json ç²å–å…§å®¹æŸ¥è©¢çš„ doc_id"""
        if self.content_doc_id:
            return self.content_doc_id
        
        print("   ğŸ”„ å¾ manifest.json ç²å–æœ€æ–° doc_id...")
        
        manifest_text = await self._load_manifest()
        if not manifest_text:
            print("   âŒ ç„¡æ³•è¼‰å…¥ manifest")
            return None
        
        # æ–°çš„æœç´¢æ¨¡å¼ï¼šæ‰¾ã€Œå« PostPage ä¸”æœ‰ media/__typename çš„æŸ¥è©¢ã€
        patterns = [
            # ä¸»è¦æ¨¡å¼ - ä»»ä½•åŒ…å« PostPage çš„æŸ¥è©¢
            r'"([A-Za-z0-9]+PostPage[^"]*)":\s*\{\s*"id":\s*"(\d{15,19})"',
            # å‚™ç”¨æ¨¡å¼
            r'"([A-Za-z0-9]*Thread[^"]*)":\s*\{\s*"id":\s*"(\d{15,19})"',
            r'"([A-Za-z0-9]*Media[^"]*)":\s*\{\s*"id":\s*"(\d{15,19})"',
        ]
        
        for i, pattern in enumerate(patterns):
            matches = re.findall(pattern, manifest_text)
            if matches:
                # å–ç¬¬ä¸€å€‹åŒ¹é…é …
                query_name, doc_id = matches[0]
                self.content_doc_id = doc_id
                print(f"      âœ… æ‰¾åˆ°å…§å®¹æŸ¥è©¢: {query_name} -> doc_id: {doc_id}")
                return self.content_doc_id
        
        print(f"      âŒ åœ¨ manifest ä¸­æœªæ‰¾åˆ°åŒ¹é…çš„ doc_id")
        
        # ä¿å­˜ manifest ç”¨æ–¼èª¿è©¦
        debug_file = Path(f"debug_manifest_parsed_{datetime.now().strftime('%H%M%S')}.txt")
        with open(debug_file, 'w', encoding='utf-8') as f:
            f.write(manifest_text)
        print(f"      ğŸ“ å·²ä¿å­˜è§£æå¾Œçš„ manifest åˆ°: {debug_file}")
        
        return None
    
    async def get_lsd_token(self) -> Optional[str]:
        """ç²å– LSD tokenï¼ˆå·²å¾ GraphQL response ä¸­ç²å–ï¼‰"""
        if self.lsd_token:
            return self.lsd_token
        
        print("   âš ï¸ å°šæœªå¾ GraphQL éŸ¿æ‡‰ä¸­ç²å– LSD token")
        return None
    
    async def get_counts_from_pk(self, pk: str) -> Optional[Dict]:
        """ä½¿ç”¨ PK ç²å–è¨ˆæ•¸æ•¸æ“š"""
        print(f"   ğŸ“Š ä½¿ç”¨ PK {pk} ç²å–è¨ˆæ•¸æ•¸æ“š...")
        try:
            # ç¢ºä¿æœ‰ LSD token
            lsd_token = await self.get_lsd_token()
            if not lsd_token:
                print(f"   âŒ æ²’æœ‰ LSD tokenï¼Œç„¡æ³•è«‹æ±‚è¨ˆæ•¸")
                return None
            
            variables = json.dumps({"postIDs": [pk]})
            
            # ğŸ”§ ä½¿ç”¨æ–°çš„ payload æ ¼å¼
            data = f"lsd={lsd_token}&doc_id={COUNTS_DOC_ID}&variables={variables}"
            
            # headers
            headers = {
                "x-fb-lsd": lsd_token
            }
            
            response = await self.http_client.post(
                "https://www.threads.com/graphql/query",
                data=data,
                headers=headers
            )
            response.raise_for_status()
            
            result = response.json()
            if "data" in result and "data" in result["data"] and "posts" in result["data"]["data"]:
                posts = result["data"]["data"]["posts"]
                if posts and len(posts) > 0:
                    post = posts[0]
                    print(f"   âœ… æˆåŠŸç²å–è¨ˆæ•¸æ•¸æ“š")
                    return post
            
            print(f"   âŒ è¨ˆæ•¸æ•¸æ“šæ ¼å¼ç•°å¸¸")
            return None
            
        except Exception as e:
            print(f"   âŒ ç²å–è¨ˆæ•¸æ•¸æ“šå¤±æ•—: {e}")
            return None
    
    async def get_content_from_pk(self, pk: str) -> Optional[Dict]:
        """ä½¿ç”¨ PK ç²å–å®Œæ•´å…§å®¹"""
        doc_id = await self.get_content_doc_id()
        if not doc_id:
            return None
        
        print(f"   ğŸ“ ä½¿ç”¨ PK {pk} å’Œ doc_id {doc_id} ç²å–å®Œæ•´å…§å®¹...")
        try:
            # ç¢ºä¿æœ‰ LSD token
            lsd_token = await self.get_lsd_token()
            if not lsd_token:
                print(f"   âŒ æ²’æœ‰ LSD tokenï¼Œç„¡æ³•è«‹æ±‚å…§å®¹")
                return None
            
            # ğŸ”§ ä½¿ç”¨æ–°çš„è®Šæ•¸æ ¼å¼
            variables = json.dumps({
                "postID_pk": pk,               # æ–°æ ¼å¼ï¼špostID â†’ postID_pk
                "withShallowTree": False,      # å¿…é ˆç‚º False æ‰èƒ½ç²å¾—å®Œæ•´å…§å®¹
                "includePromotedPosts": False
            })
            
            # ğŸ”§ ä½¿ç”¨æ–°çš„ payload æ ¼å¼
            data = f"lsd={lsd_token}&doc_id={doc_id}&variables={variables}"
            
            # headers
            headers = {
                "x-fb-lsd": lsd_token
            }
            
            response = await self.http_client.post(
                "https://www.threads.com/graphql/query",
                data=data,
                headers=headers
            )
            response.raise_for_status()
            
            result = response.json()
            if "data" in result and "media" in result["data"]:
                media = result["data"]["media"]
                print(f"   âœ… æˆåŠŸç²å–å®Œæ•´å…§å®¹")
                return media
            
            print(f"   âŒ å…§å®¹æ•¸æ“šæ ¼å¼ç•°å¸¸")
            # ä¿å­˜éŸ¿æ‡‰ç”¨æ–¼èª¿è©¦
            debug_file = Path(f"debug_content_response_{datetime.now().strftime('%H%M%S')}.json")
            with open(debug_file, 'w', encoding='utf-8') as f:
                json.dump(result, f, indent=2, ensure_ascii=False)
            print(f"   ğŸ“ å·²ä¿å­˜å…§å®¹éŸ¿æ‡‰åˆ°: {debug_file}")
            
            return None
            
        except Exception as e:
            print(f"   âŒ ç²å–å®Œæ•´å…§å®¹å¤±æ•—: {e}")
            return None
    
    async def extract_pk_from_gate_page(self, post_url: str) -> tuple[Optional[str], list]:
        """å¾ Gate é é¢æ””æˆªç²å– PKï¼Œè¿”å› (pk, captured_pks_list)"""
        print(f"   ğŸšª å¾ Gate é é¢æ””æˆª PK...")
        
        captured_pks = []
        
        async with async_playwright() as p:
            browser = await p.chromium.launch(
                headless=True,
                args=["--no-sandbox", "--disable-dev-shm-usage", "--disable-blink-features=AutomationControlled"]
            )
            
            context = await browser.new_context(
                storage_state=str(get_auth_file_path()),
                user_agent=self.headers["user-agent"],
                viewport={"width": 375, "height": 812},  # iPhone å°ºå¯¸
                locale="zh-TW",
                bypass_csp=True
            )
            
            await context.add_init_script(
                "Object.defineProperty(navigator,'webdriver',{get:()=>undefined})"
            )
            
            page = await context.new_page()
            
            # æ””æˆªè¨ˆæ•¸æŸ¥è©¢
            async def response_handler(response):
                url = response.url.lower()
                qname = response.request.headers.get("x-fb-friendly-name", "")
                
                # ğŸ”‘ å¾ç¬¬ä¸€å€‹ GraphQL éŸ¿æ‡‰ä¸­æå– LSD token
                if "/graphql" in url and not self.lsd_token:
                    lsd = (response.request.headers.get("x-fb-lsd") or 
                           response.headers.get("x-fb-lsd"))
                    if lsd:
                        self.lsd_token = lsd
                        print(f"      ğŸ”‘ å¾ GraphQL éŸ¿æ‡‰ç²å– LSD token: {lsd[:10]}...")
                
                if ("/graphql" in url and 
                    "DynamicPostCountsSubscriptionQuery" in qname and 
                    response.status == 200):
                    
                    try:
                        data = await response.json()
                        if ("data" in data and "data" in data["data"] and 
                            "posts" in data["data"]["data"]):
                            
                            posts = data["data"]["data"]["posts"]
                            for post in posts:
                                if isinstance(post, dict) and "pk" in post:
                                    pk = post["pk"]
                                    like_count = post.get("like_count", 0)
                                    captured_pks.append({
                                        "pk": pk,
                                        "like_count": like_count,
                                        "post": post
                                    })
                                    print(f"      ğŸ“ æ””æˆªåˆ° PK: {pk}, è®šæ•¸: {like_count}")
                    except:
                        pass
            
            page.on("response", response_handler)
            
            # å°èˆªåˆ°é é¢
            await page.goto(post_url, wait_until="networkidle", timeout=60000)
            await asyncio.sleep(3)
            
            # è¼•å¾®æ»¾å‹•è§¸ç™¼æ›´å¤šè«‹æ±‚
            await page.evaluate("window.scrollTo(0, 100)")
            await asyncio.sleep(2)
            
            await browser.close()
        
        # åˆ†ææ””æˆªåˆ°çš„ PK
        if captured_pks:
            print(f"   ğŸ“Š æ””æˆªåˆ° {len(captured_pks)} å€‹ PK")
            
            # é¸æ“‡æœ€æœ‰å¯èƒ½çš„ PKï¼ˆè®šæ•¸æœ€é«˜çš„ï¼‰
            best_pk = max(captured_pks, key=lambda x: x["like_count"])
            selected_pk = best_pk["pk"]
            
            print(f"   ğŸ¯ é¸æ“‡ PK: {selected_pk} (è®šæ•¸: {best_pk['like_count']})")
            return selected_pk, captured_pks
        else:
            print(f"   âŒ æœªæ””æˆªåˆ°ä»»ä½• PK")
            return None, []
    
    async def fetch_full_post(self, post_url: str) -> Optional[Dict]:
        """ç²å–å®Œæ•´è²¼æ–‡ï¼ˆè¨ˆæ•¸ + å…§å®¹ï¼‰"""
        print(f"\nğŸ” ç²å–å®Œæ•´è²¼æ–‡: {post_url}")
        
        # æ­¥é©Ÿ1: å¾ Gate é é¢æ””æˆª PK
        pk, captured_pks = await self.extract_pk_from_gate_page(post_url)
        if not pk:
            print(f"âŒ ç„¡æ³•ç²å– PK")
            return None
        
        # æ­¥é©Ÿ2: ä½¿ç”¨ PK ç²å–è¨ˆæ•¸å’Œå…§å®¹
        # å„ªå…ˆå˜—è©¦å¾å·²æ””æˆªçš„æ•¸æ“šä¸­æ‰¾åˆ°åŒ¹é…çš„è¨ˆæ•¸
        counts = None
        for captured in captured_pks:
            if captured["pk"] == pk:
                counts = captured["post"]
                print(f"   âœ… ä½¿ç”¨å·²æ””æˆªçš„è¨ˆæ•¸æ•¸æ“š")
                break
        
        # å¦‚æœæ²’æœ‰æ‰¾åˆ°ï¼Œå˜—è©¦ API ç²å–
        if not counts:
            counts = await self.get_counts_from_pk(pk)
        
        # ç²å–å…§å®¹
        content = await self.get_content_from_pk(pk)
        
        # æª¢æŸ¥çµæœ
        if isinstance(counts, Exception):
            print(f"âŒ ç²å–è¨ˆæ•¸å¤±æ•—: {counts}")
            counts = None
        
        if isinstance(content, Exception):
            print(f"âŒ ç²å–å…§å®¹å¤±æ•—: {content}")
            content = None
        
        if not counts:
            print(f"âŒ ç„¡æ³•ç²å–è¨ˆæ•¸æ•¸æ“š")
            return None
        
        # æ­¥é©Ÿ3: åˆä½µæ•¸æ“š
        print(f"   ğŸ”„ åˆä½µè¨ˆæ•¸å’Œå…§å®¹æ•¸æ“š...")
        
        text_info = counts.get("text_post_app_info", {})
        merged_post = {
            "post_id": pk,
            "code": TARGET_CODE,  # å¾ URL æå–
            "author": post_url.split("/@")[1].split("/")[0] if "/@" in post_url else "unknown",
            
            # è¨ˆæ•¸æ•¸æ“šï¼ˆä¾†è‡ªè¨ˆæ•¸ APIï¼‰
            "like_count": counts.get("like_count", 0),
            "comment_count": text_info.get("direct_reply_count", 0),
            "repost_count": text_info.get("repost_count", 0),
            "share_count": text_info.get("reshare_count", 0),
            
            # å…§å®¹æ•¸æ“šï¼ˆä¾†è‡ªå…§å®¹ APIï¼‰
            "content": "",
            "images": [],
            "videos": [],
            
            # å…ƒæ•¸æ“š
            "data_source": "dual_api",
            "has_content_api": content is not None,
            "raw_counts": counts,
            "raw_content": content
        }
        
        # å¦‚æœæœ‰å…§å®¹æ•¸æ“šï¼Œå¡«å……å…§å®¹å­—æ®µ
        if content:
            # æ–‡å­—å…§å®¹
            if "caption" in content and content["caption"]:
                merged_post["content"] = content["caption"].get("text", "")
            
            # åœ–ç‰‡
            if "image_versions2" in content and content["image_versions2"]:
                candidates = content["image_versions2"].get("candidates", [])
                merged_post["images"] = [c["url"] for c in candidates if "url" in c]
            
            # å½±ç‰‡
            if "video_versions" in content and content["video_versions"]:
                merged_post["videos"] = [v["url"] for v in content["video_versions"] if "url" in v]
        
        print(f"   âœ… æˆåŠŸåˆä½µæ•¸æ“š")
        print(f"      è®šæ•¸: {merged_post['like_count']}")
        print(f"      ç•™è¨€æ•¸: {merged_post['comment_count']}")
        print(f"      è½‰ç™¼æ•¸: {merged_post['repost_count']}")
        print(f"      åˆ†äº«æ•¸: {merged_post['share_count']}")
        print(f"      å…§å®¹é•·åº¦: {len(merged_post['content'])}")
        print(f"      åœ–ç‰‡æ•¸: {len(merged_post['images'])}")
        print(f"      å½±ç‰‡æ•¸: {len(merged_post['videos'])}")
        
        return merged_post
    
    async def close(self):
        """é—œé–‰ HTTP å®¢æˆ¶ç«¯"""
        await self.http_client.aclose()

async def test_full_post_strategy():
    """æ¸¬è©¦å®Œæ•´è²¼æ–‡ç­–ç•¥"""
    print("ğŸš€ æ¸¬è©¦å®Œæ•´è²¼æ–‡ç­–ç•¥ï¼ˆè¨ˆæ•¸ + å…§å®¹é›™æŸ¥è©¢ï¼‰...")
    
    auth_file_path = get_auth_file_path()
    if not auth_file_path.exists():
        print(f"âŒ èªè­‰æª”æ¡ˆä¸å­˜åœ¨: {auth_file_path}")
        return
    
    fetcher = ThreadsFullPostFetcher(auth_file_path)
    
    try:
        result = await fetcher.fetch_full_post(TEST_URL)
        
        if result:
            print(f"\nğŸ“Š å®Œæ•´è²¼æ–‡æ•¸æ“š:")
            print(f"   ID: {result['post_id']}")
            print(f"   ä»£ç¢¼: {result['code']}")
            print(f"   ä½œè€…: {result['author']}")
            print(f"   è®šæ•¸: {result['like_count']:,}")
            print(f"   ç•™è¨€æ•¸: {result['comment_count']:,}")
            print(f"   è½‰ç™¼æ•¸: {result['repost_count']:,}")
            print(f"   åˆ†äº«æ•¸: {result['share_count']:,}")
            print(f"   å…§å®¹: {result['content'][:100]}...")
            print(f"   åœ–ç‰‡: {len(result['images'])} å¼µ")
            print(f"   å½±ç‰‡: {len(result['videos'])} å€‹")
            
            # é©—è­‰æº–ç¢ºæ€§
            print(f"\nğŸ¯ æº–ç¢ºæ€§é©—è­‰:")
            accuracy_checks = []
            
            if abs(result['like_count'] - EXPECTED_DATA['likes']) <= 5:
                accuracy_checks.append("âœ… è®šæ•¸æº–ç¢º")
            else:
                accuracy_checks.append(f"âŒ è®šæ•¸åå·® ({result['like_count']} vs {EXPECTED_DATA['likes']})")
            
            if result['comment_count'] == EXPECTED_DATA['comments']:
                accuracy_checks.append("âœ… ç•™è¨€æ•¸æº–ç¢º")
            else:
                accuracy_checks.append(f"âŒ ç•™è¨€æ•¸åå·® ({result['comment_count']} vs {EXPECTED_DATA['comments']})")
            
            if result['repost_count'] == EXPECTED_DATA['reposts']:
                accuracy_checks.append("âœ… è½‰ç™¼æ•¸æº–ç¢º")
            else:
                accuracy_checks.append(f"âŒ è½‰ç™¼æ•¸åå·® ({result['repost_count']} vs {EXPECTED_DATA['reposts']})")
            
            if result['share_count'] == EXPECTED_DATA['shares']:
                accuracy_checks.append("âœ… åˆ†äº«æ•¸æº–ç¢º")
            else:
                accuracy_checks.append(f"âŒ åˆ†äº«æ•¸åå·® ({result['share_count']} vs {EXPECTED_DATA['shares']})")
            
            for check in accuracy_checks:
                print(f"   {check}")
            
            # è¨ˆç®—æº–ç¢ºç‡
            correct_count = len([c for c in accuracy_checks if c.startswith("âœ…")])
            accuracy_rate = (correct_count / len(accuracy_checks)) * 100
            
            print(f"\nğŸ† æ•´é«”æº–ç¢ºç‡: {accuracy_rate:.1f}% ({correct_count}/{len(accuracy_checks)})")
            
            if accuracy_rate >= 75:
                print("ğŸ‰ ç­–ç•¥æˆåŠŸï¼æ•¸æ“šé«˜åº¦æº–ç¢ºï¼")
            else:
                print("âš ï¸ ç­–ç•¥éœ€è¦é€²ä¸€æ­¥èª¿æ•´")
            
            # ä¿å­˜çµæœ
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            result_file = Path(f"full_post_result_{timestamp}.json")
            with open(result_file, 'w', encoding='utf-8') as f:
                json.dump(result, f, indent=2, ensure_ascii=False, default=str)
            print(f"\nğŸ“ å®Œæ•´çµæœå·²ä¿å­˜è‡³: {result_file}")
            
        else:
            print(f"âŒ ç²å–å®Œæ•´è²¼æ–‡å¤±æ•—")
            
    except Exception as e:
        print(f"âŒ æ¸¬è©¦å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        await fetcher.close()

async def main():
    """ä¸»å‡½æ•¸"""
    await test_full_post_strategy()

if __name__ == "__main__":
    asyncio.run(main())