"""
åˆ†ææ‰€æœ‰ GraphQL æŸ¥è©¢ï¼Œæ‰¾åˆ°çœŸæ­£çš„ä¸»è²¼æ–‡å…§å®¹æŸ¥è©¢
"""

import asyncio
import json
import urllib.parse
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Optional, List

import sys
sys.path.append(str(Path(__file__).parent))

from playwright.async_api import async_playwright
from common.config import get_auth_file_path

# æ¸¬è©¦è²¼æ–‡
TEST_POST_URL = "https://www.threads.com/@star_shining0828/post/DMyvZJRz5Cz"
TARGET_PK = "3689219480905289907"

async def analyze_all_graphql_queries():
    """åˆ†ææ‰€æœ‰ GraphQL æŸ¥è©¢ä¸¦æ‰¾åˆ°åŒ…å«ä¸»è²¼æ–‡å…§å®¹çš„æŸ¥è©¢"""
    print("ğŸ” åˆ†ææ‰€æœ‰ GraphQL æŸ¥è©¢...")
    
    auth_file_path = get_auth_file_path()
    all_queries = []
    
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=False)
        context = await browser.new_context(
            storage_state=str(auth_file_path),
            user_agent="Mozilla/5.0 (iPhone; CPU iPhone OS 15_0 like Mac OS X) AppleWebKit/605.1.15",
            viewport={"width": 375, "height": 812},
            locale="zh-TW"
        )
        
        page = await context.new_page()
        
        async def response_handler(response):
            url = response.url.lower()
            if "/graphql" in url and response.status == 200:
                friendly_name = response.request.headers.get("x-fb-friendly-name", "Unknown")
                root_field = response.request.headers.get("x-root-field-name", "")
                
                print(f"   ğŸ“¡ {friendly_name}")
                if root_field:
                    print(f"      ğŸ” Root field: {root_field}")
                
                try:
                    data = await response.json()
                    
                    # åˆ†æéŸ¿æ‡‰çµæ§‹
                    content_indicators = []
                    target_post_found = False
                    
                    if "data" in data and data["data"]:
                        # æª¢æŸ¥æ˜¯å¦åŒ…å«ç›®æ¨™è²¼æ–‡
                        data_str = json.dumps(data, ensure_ascii=False)
                        if TARGET_PK in data_str:
                            target_post_found = True
                            content_indicators.append("HAS_TARGET_POST")
                        
                        # æª¢æŸ¥å…§å®¹æŒ‡æ¨™
                        if "media" in data["data"]:
                            content_indicators.append("has_media")
                        if "caption" in data_str:
                            content_indicators.append("has_caption")
                        if "image_versions" in data_str:
                            content_indicators.append("has_images")
                        if "video_versions" in data_str:
                            content_indicators.append("has_videos")
                        if "like_count" in data_str:
                            content_indicators.append("has_likes")
                        if "text_post_app_info" in data_str:
                            content_indicators.append("has_text_info")
                        if len(data_str) > 10000:
                            content_indicators.append("large_response")
                    
                    indicators_text = ", ".join(content_indicators) if content_indicators else "no_content"
                    print(f"      ğŸ“Š æŒ‡æ¨™: {indicators_text}")
                    
                    # è¨˜éŒ„æŸ¥è©¢ä¿¡æ¯
                    query_info = {
                        "friendly_name": friendly_name,
                        "root_field": root_field,
                        "url": response.url,
                        "has_target_post": target_post_found,
                        "content_indicators": content_indicators,
                        "request_headers": dict(response.request.headers),
                        "request_data": response.request.post_data,
                        "response_size": len(json.dumps(data)) if data else 0,
                        "timestamp": datetime.now().isoformat()
                    }
                    
                    all_queries.append(query_info)
                    
                    # å¦‚æœæ‰¾åˆ°ç›®æ¨™è²¼æ–‡ï¼Œè©³ç´°åˆ†æ
                    if target_post_found:
                        print(f"      ğŸ¯ æ‰¾åˆ°ç›®æ¨™è²¼æ–‡ï¼")
                        
                        # ä¿å­˜å®Œæ•´éŸ¿æ‡‰
                        detail_file = Path(f"target_post_found_{friendly_name}_{datetime.now().strftime('%H%M%S')}.json")
                        with open(detail_file, 'w', encoding='utf-8') as f:
                            json.dump({
                                "query_info": query_info,
                                "full_response": data
                            }, f, indent=2, ensure_ascii=False)
                        print(f"      ğŸ“ è©³ç´°æ•¸æ“šå·²ä¿å­˜: {detail_file}")
                
                except Exception as e:
                    print(f"      âŒ è§£æå¤±æ•—: {e}")
        
        page.on("response", response_handler)
        
        # å°èˆªåˆ°é é¢
        print(f"   ğŸŒ å°èˆªåˆ°: {TEST_POST_URL}")
        await page.goto(TEST_POST_URL, wait_until="networkidle", timeout=60000)
        
        # ç­‰å¾…åˆå§‹è¼‰å…¥
        await asyncio.sleep(5)
        
        # å˜—è©¦ä¸€äº›æ“ä½œä¾†è§¸ç™¼æ›´å¤šæŸ¥è©¢
        print(f"   ğŸ–±ï¸ å˜—è©¦ç”¨æˆ¶æ“ä½œ...")
        
        # æ»¾å‹•é é¢
        await page.evaluate("window.scrollTo(0, 300)")
        await asyncio.sleep(2)
        
        # å˜—è©¦é»æ“Šè²¼æ–‡å€åŸŸ
        try:
            # é»æ“Šä¸»è²¼æ–‡å…§å®¹å€åŸŸ
            await page.click('article', timeout=5000)
            await asyncio.sleep(2)
        except:
            pass
        
        # å˜—è©¦åˆ·æ–°é é¢
        print(f"   ğŸ”„ åˆ·æ–°é é¢...")
        await page.reload(wait_until="networkidle")
        await asyncio.sleep(5)
        
        # å†æ¬¡æ»¾å‹•
        await page.evaluate("window.scrollTo(0, 600)")
        await asyncio.sleep(2)
        
        await browser.close()
    
    # åˆ†æçµæœ
    print(f"\nğŸ“Š åˆ†æçµæœ:")
    print(f"   ç¸½æŸ¥è©¢æ•¸: {len(all_queries)}")
    
    # æ‰¾åˆ°åŒ…å«ç›®æ¨™è²¼æ–‡çš„æŸ¥è©¢
    target_queries = [q for q in all_queries if q["has_target_post"]]
    print(f"   åŒ…å«ç›®æ¨™è²¼æ–‡çš„æŸ¥è©¢: {len(target_queries)}")
    
    if target_queries:
        print(f"\nğŸ¯ åŒ…å«ç›®æ¨™è²¼æ–‡çš„æŸ¥è©¢:")
        for i, query in enumerate(target_queries):
            print(f"   {i+1}. {query['friendly_name']}")
            print(f"      Root field: {query['root_field']}")
            print(f"      æŒ‡æ¨™: {', '.join(query['content_indicators'])}")
            print(f"      éŸ¿æ‡‰å¤§å°: {query['response_size']:,} å­—ç¬¦")
    
    # æŒ‰æŸ¥è©¢åç¨±åˆ†çµ„çµ±è¨ˆ
    query_stats = {}
    for query in all_queries:
        name = query["friendly_name"]
        if name not in query_stats:
            query_stats[name] = {"count": 0, "has_target": 0, "avg_size": 0}
        query_stats[name]["count"] += 1
        if query["has_target_post"]:
            query_stats[name]["has_target"] += 1
        query_stats[name]["avg_size"] += query["response_size"]
    
    for name, stats in query_stats.items():
        stats["avg_size"] = stats["avg_size"] // stats["count"] if stats["count"] > 0 else 0
    
    print(f"\nğŸ“‹ æŸ¥è©¢çµ±è¨ˆ:")
    for name, stats in sorted(query_stats.items(), key=lambda x: x[1]["has_target"], reverse=True):
        print(f"   {name}:")
        print(f"      æ¬¡æ•¸: {stats['count']}, åŒ…å«ç›®æ¨™: {stats['has_target']}, å¹³å‡å¤§å°: {stats['avg_size']:,}")
    
    # ä¿å­˜å®Œæ•´åˆ†æçµæœ
    analysis_file = Path(f"graphql_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
    with open(analysis_file, 'w', encoding='utf-8') as f:
        json.dump({
            "test_url": TEST_POST_URL,
            "target_pk": TARGET_PK,
            "all_queries": all_queries,
            "summary": {
                "total_queries": len(all_queries),
                "target_queries_count": len(target_queries),
                "query_stats": query_stats
            }
        }, f, indent=2, ensure_ascii=False, default=str)
    
    print(f"\nğŸ“ å®Œæ•´åˆ†æå·²ä¿å­˜: {analysis_file}")
    
    # æ¨è–¦æœ€ä½³æŸ¥è©¢
    if target_queries:
        # é¸æ“‡åŒ…å«æœ€å¤šå…§å®¹æŒ‡æ¨™çš„æŸ¥è©¢
        best_query = max(target_queries, key=lambda q: len(q["content_indicators"]))
        print(f"\nğŸ’¡ æ¨è–¦æŸ¥è©¢: {best_query['friendly_name']}")
        print(f"   Root field: {best_query['root_field']}")
        print(f"   å…§å®¹æŒ‡æ¨™: {', '.join(best_query['content_indicators'])}")
        print(f"   éŸ¿æ‡‰å¤§å°: {best_query['response_size']:,} å­—ç¬¦")
        
        return best_query
    else:
        print(f"\nğŸ˜ æœªæ‰¾åˆ°åŒ…å«ç›®æ¨™è²¼æ–‡çš„æŸ¥è©¢")
        print(f"ğŸ’¡ å¯èƒ½éœ€è¦:")
        print(f"   1. æª¢æŸ¥è²¼æ–‡ URL æ˜¯å¦æ­£ç¢º")
        print(f"   2. å˜—è©¦ä¸åŒçš„ç”¨æˆ¶æ“ä½œ")
        print(f"   3. æª¢æŸ¥èªè­‰ç‹€æ…‹")
        
        return None

async def main():
    """ä¸»å‡½æ•¸"""
    auth_file = get_auth_file_path()
    if not auth_file.exists():
        print(f"âŒ èªè­‰æª”æ¡ˆ {auth_file} ä¸å­˜åœ¨ã€‚è«‹å…ˆåŸ·è¡Œ save_auth.pyã€‚")
        return

    best_query = await analyze_all_graphql_queries()
    
    if best_query:
        print(f"\nğŸ‰ åˆ†æå®Œæˆï¼æ‰¾åˆ°æœ€ä½³æŸ¥è©¢")
        print(f"ğŸ’¡ è«‹ä½¿ç”¨é€™å€‹æŸ¥è©¢ä¾†ç²å–ä¸»è²¼æ–‡å…§å®¹")
    else:
        print(f"\nğŸ˜ åˆ†æå®Œæˆï¼Œä½†æœªæ‰¾åˆ°åˆé©çš„æŸ¥è©¢")

if __name__ == "__main__":
    asyncio.run(main())