"""
ä½¿ç”¨æ­£ç¢ºçš„æ ¼å¼æ¸¬è©¦å…§å®¹æŸ¥è©¢ï¼ˆåŸºæ–¼ç”¨æˆ¶æŒ‡å°ï¼‰
"""

import asyncio
import json
import httpx
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Optional

import sys
sys.path.append(str(Path(__file__).parent))

from playwright.async_api import async_playwright
from common.config import get_auth_file_path

# æ¸¬è©¦è²¼æ–‡
TEST_POST_URL = "https://www.threads.com/@star_shining0828/post/DMyvZJRz5Cz"
TARGET_PK = "3689219480905289907"

async def get_real_lsd_token():
    """ç²å–çœŸå¯¦çš„ LSD token"""
    auth_file_path = get_auth_file_path()
    
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context(
            storage_state=str(auth_file_path),
            user_agent="Mozilla/5.0 (iPhone; CPU iPhone OS 15_0 like Mac OS X) AppleWebKit/605.1.15",
            viewport={"width": 375, "height": 812}
        )
        
        page = await context.new_page()
        lsd_token = None
        
        async def response_handler(response):
            nonlocal lsd_token
            if "/graphql" in response.url.lower() and response.status == 200:
                try:
                    post_data = response.request.post_data
                    if post_data and "lsd=" in post_data:
                        import urllib.parse
                        for part in post_data.split('&'):
                            if part.startswith('lsd='):
                                lsd_token = urllib.parse.unquote(part.split('=', 1)[1])
                                break
                except:
                    pass
        
        page.on("response", response_handler)
        await page.goto("https://www.threads.com/@threads", wait_until="networkidle")
        await asyncio.sleep(3)
        await browser.close()
    
    return lsd_token

async def test_with_known_doc_ids():
    """ä½¿ç”¨å·²çŸ¥çš„ doc_id æ¸¬è©¦"""
    print("ğŸ§ª ä½¿ç”¨å·²çŸ¥ doc_id æ¸¬è©¦...")
    
    # ç²å–èªè­‰
    auth_file_path = get_auth_file_path()
    auth_data = json.loads(auth_file_path.read_text())
    cookies = {cookie['name']: cookie['value'] for cookie in auth_data.get('cookies', [])}
    
    lsd_token = await get_real_lsd_token()
    if not lsd_token:
        print("âŒ ç„¡æ³•ç²å– LSD token")
        return False
    
    print(f"âœ… ç²å–åˆ° LSD token: {lsd_token[:10]}...")
    
    # å˜—è©¦ä¸åŒçš„ doc_idï¼ˆå¾å¯¦éš›æ””æˆªä¸­ç²å¾—ï¼‰
    doc_ids_to_try = [
        ("24061215210199287", "BarcelonaPostPageRefetchableDirectQuery"),  # æ””æˆªåˆ°çš„
        ("25073444793714143", "BarcelonaPostPageContentQuery"),  # ç”¨æˆ¶æåˆ°çš„
        ("7428920450586442", "èˆŠç‰ˆæœ¬æ¸¬è©¦"),
    ]
    
    for doc_id, description in doc_ids_to_try:
        print(f"\nğŸ” æ¸¬è©¦ {description}: {doc_id}")
        
        success = await test_content_query_with_doc_id(doc_id, lsd_token, cookies)
        if success:
            print(f"ğŸ‰ æˆåŠŸä½¿ç”¨ {description}!")
            return True
        
        await asyncio.sleep(1)
    
    return False

async def test_content_query_with_doc_id(doc_id: str, lsd_token: str, cookies: dict):
    """ä½¿ç”¨ç‰¹å®š doc_id æ¸¬è©¦å…§å®¹æŸ¥è©¢"""
    
    # æŒ‰ç…§ç”¨æˆ¶æŒ‡å°çš„æ­£ç¢ºæ ¼å¼
    variables = {
        "postID_pk": TARGET_PK,  # é—œéµï¼šä½¿ç”¨ postID_pk è€Œä¸æ˜¯ postID
        "withShallowTree": False,  # é—œéµï¼šå¿…éœ€åƒæ•¸
        "includePromotedPosts": False
    }
    
    # æ§‹å»º payloadï¼ˆç°¡åŒ–ç‰ˆæœ¬ï¼‰
    payload = f"lsd={lsd_token}&doc_id={doc_id}&variables={json.dumps(variables)}"
    
    headers = {
        "User-Agent": "Mozilla/5.0 (iPhone; CPU iPhone OS 15_0 like Mac OS X) AppleWebKit/605.1.15",
        "Content-Type": "application/x-www-form-urlencoded",
        "X-FB-LSD": lsd_token,  # é—œéµï¼šLSD token æ”¾åœ¨ header
        "X-IG-App-ID": "238260118697367",  # æŒ‰ç…§ç”¨æˆ¶æŒ‡å°æ·»åŠ 
        "Origin": "https://www.threads.com",
        "Referer": TEST_POST_URL,
    }
    
    # ä½¿ç”¨æ­£ç¢ºçš„ endpoint
    endpoint = "https://www.threads.com/graphql/query"  # é—œéµï¼šä¸æ˜¯ /api/graphql
    
    async with httpx.AsyncClient(cookies=cookies, timeout=30.0, http2=True) as client:
        try:
            response = await client.post(endpoint, data=payload, headers=headers)
            
            print(f"   ğŸ“¡ HTTP {response.status_code}")
            
            if response.status_code == 200:
                try:
                    result = response.json()
                    
                    if "errors" in result:
                        errors = result["errors"]
                        print(f"   âŒ éŒ¯èª¤: {errors[:1]}")
                        return False
                    
                    if "data" in result:
                        data = result["data"]
                        
                        if data and "media" in data and data["media"]:
                            # æŒ‰ç…§ç”¨æˆ¶æŒ‡å°çš„çµæ§‹è§£æ
                            media = data["media"]
                            print(f"   âœ… æˆåŠŸç²å–åª’é«”æ•¸æ“šï¼")
                            
                            # æå–åŸºæœ¬ä¿¡æ¯
                            pk = media.get("pk", "unknown")
                            typename = media.get("__typename", "unknown")
                            
                            # æå–å…§å®¹
                            caption = media.get("caption", {})
                            content_text = caption.get("text", "") if caption else ""
                            
                            # æå–è¨ˆæ•¸
                            like_count = media.get("like_count", 0)
                            text_info = media.get("text_post_app_info", {})
                            comment_count = text_info.get("direct_reply_count", 0)
                            repost_count = text_info.get("repost_count", 0)
                            share_count = text_info.get("reshare_count", 0)
                            
                            # æå–åª’é«”
                            images = []
                            videos = []
                            
                            if "image_versions2" in media:
                                candidates = media["image_versions2"].get("candidates", [])
                                if candidates:
                                    # å–æœ€é«˜è§£æåº¦çš„åœ–ç‰‡
                                    best_image = max(candidates, key=lambda c: c.get("width", 0))
                                    images.append(best_image.get("url", ""))
                            
                            if "video_versions" in media and media["video_versions"]:
                                # å–ç¬¬ä¸€å€‹å½±ç‰‡ç‰ˆæœ¬
                                videos.append(media["video_versions"][0].get("url", ""))
                            
                            # æª¢æŸ¥è¼ªæ’­åª’é«”
                            if "carousel_media" in media:
                                for carousel_item in media["carousel_media"] or []:
                                    if "image_versions2" in carousel_item:
                                        candidates = carousel_item["image_versions2"].get("candidates", [])
                                        if candidates:
                                            best_image = max(candidates, key=lambda c: c.get("width", 0))
                                            images.append(best_image.get("url", ""))
                                    if "video_versions" in carousel_item and carousel_item["video_versions"]:
                                        videos.append(carousel_item["video_versions"][0].get("url", ""))
                            
                            print(f"   ğŸ“„ PK: {pk}")
                            print(f"   ğŸ·ï¸ é¡å‹: {typename}")
                            print(f"   ğŸ“ å…§å®¹: {content_text[:100]}...")
                            print(f"   ğŸ‘ è®šæ•¸: {like_count}")
                            print(f"   ğŸ’¬ ç•™è¨€: {comment_count}")
                            print(f"   ğŸ”„ è½‰ç™¼: {repost_count}")
                            print(f"   ğŸ“¤ åˆ†äº«: {share_count}")
                            print(f"   ğŸ–¼ï¸ åœ–ç‰‡: {len(images)} å€‹")
                            print(f"   ğŸ¥ å½±ç‰‡: {len(videos)} å€‹")
                            
                            if pk == TARGET_PK:
                                print(f"   ğŸ¯ ç¢ºèªæ‰¾åˆ°ç›®æ¨™è²¼æ–‡ï¼")
                                
                                # ä¿å­˜æˆåŠŸæ•¸æ“š
                                success_file = Path(f"successful_content_query_{datetime.now().strftime('%H%M%S')}.json")
                                with open(success_file, 'w', encoding='utf-8') as f:
                                    json.dump({
                                        "doc_id": doc_id,
                                        "variables": variables,
                                        "endpoint": endpoint,
                                        "media_data": media,
                                        "extracted": {
                                            "pk": pk,
                                            "content": content_text,
                                            "like_count": like_count,
                                            "comment_count": comment_count,
                                            "repost_count": repost_count,
                                            "share_count": share_count,
                                            "images": images,
                                            "videos": videos
                                        }
                                    }, f, indent=2, ensure_ascii=False)
                                print(f"   ğŸ“ å·²ä¿å­˜æˆåŠŸé…ç½®åˆ°: {success_file}")
                                
                                return True
                            else:
                                print(f"   âš ï¸ PK ä¸åŒ¹é…ï¼Œé æœŸ: {TARGET_PK}")
                        
                        elif data and "data" in data:
                            # å¯èƒ½æ˜¯å…¶ä»–çµæ§‹
                            print(f"   âš ï¸ æ•¸æ“šçµæ§‹ä¸ç¬¦é æœŸï¼Œdata éµ: {list(data.keys())}")
                            if data["data"]:
                                print(f"   ğŸ“‹ å…§å±¤ data éµ: {list(data['data'].keys())}")
                        else:
                            print(f"   âŒ ç„¡æ•ˆçš„ data çµæ§‹")
                    else:
                        print(f"   âŒ éŸ¿æ‡‰ä¸­ç„¡ data æ¬„ä½")
                    
                    return False
                
                except Exception as e:
                    print(f"   âŒ è§£æéŸ¿æ‡‰å¤±æ•—: {e}")
                    print(f"   ğŸ“„ åŸå§‹éŸ¿æ‡‰: {response.text[:500]}...")
                    return False
            else:
                print(f"   âŒ HTTP éŒ¯èª¤: {response.status_code}")
                if response.status_code == 404:
                    print(f"   ğŸ’¡ å¯èƒ½ endpoint å·²è®Šæ›´")
                return False
                
        except Exception as e:
            print(f"   âŒ è«‹æ±‚å¤±æ•—: {e}")
            return False

async def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸš€ ä½¿ç”¨æ­£ç¢ºæ ¼å¼æ¸¬è©¦å…§å®¹æŸ¥è©¢...")
    
    auth_file = get_auth_file_path()
    if not auth_file.exists():
        print(f"âŒ èªè­‰æª”æ¡ˆ {auth_file} ä¸å­˜åœ¨ã€‚è«‹å…ˆåŸ·è¡Œ save_auth.pyã€‚")
        return

    success = await test_with_known_doc_ids()
    
    if success:
        print(f"\nğŸ‰ å…§å®¹æŸ¥è©¢æˆåŠŸï¼")
        print(f"ğŸ’¡ ç¾åœ¨å¯ä»¥æ•´åˆåˆ°ä¸»è¦çˆ¬èŸ²é‚è¼¯ä¸­")
    else:
        print(f"\nğŸ˜ æ‰€æœ‰å˜—è©¦éƒ½å¤±æ•—äº†")
        print(f"ğŸ’¡ å¯èƒ½éœ€è¦:")
        print(f"   1. æ›´æ–° doc_id")
        print(f"   2. æª¢æŸ¥è®Šæ•¸æ ¼å¼")
        print(f"   3. ç¢ºèª endpoint æ­£ç¢ºæ€§")

if __name__ == "__main__":
    asyncio.run(main())