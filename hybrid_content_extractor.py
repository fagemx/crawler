"""
æ··åˆå…§å®¹æå–å™¨ - çµåˆè¨ˆæ•¸æŸ¥è©¢å’Œ DOM è§£æ
ç²å–æœ€å®Œæ•´å’Œæº–ç¢ºçš„è²¼æ–‡æ•¸æ“š
"""

import asyncio
import json
import httpx
import urllib.parse
import re
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Optional, List, Tuple

import sys
sys.path.append(str(Path(__file__).parent))

from playwright.async_api import async_playwright
from common.config import get_auth_file_path

# æ¸¬è©¦è²¼æ–‡
TEST_POST_URL = "https://www.threads.com/@star_shining0828/post/DMyvZJRz5Cz"
TARGET_PK = "3689219480905289907"

class HybridContentExtractor:
    """æ··åˆå…§å®¹æå–å™¨ - è¨ˆæ•¸æŸ¥è©¢ + DOM è§£æ"""
    
    def __init__(self):
        self.counts_captured = False
        self.counts_headers = {}
        self.counts_payload = ""
        self.auth_header = ""
        self.lsd_token = ""
    
    async def intercept_counts_query(self):
        """æ””æˆªè¨ˆæ•¸æŸ¥è©¢ï¼ˆç”¨æ–¼æº–ç¢ºçš„æ•¸å­—æ•¸æ“šï¼‰"""
        print("ğŸ“Š æ””æˆªè¨ˆæ•¸æŸ¥è©¢...")
        
        auth_file_path = get_auth_file_path()
        
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            context = await browser.new_context(
                storage_state=str(auth_file_path),
                user_agent="Mozilla/5.0 (iPhone; CPU iPhone OS 15_0 like Mac OS X) AppleWebKit/605.1.15",
                viewport={"width": 375, "height": 812}
            )
            
            page = await context.new_page()
            
            async def response_handler(response):
                friendly_name = response.request.headers.get("x-fb-friendly-name", "")
                
                if "useBarcelonaBatchedDynamicPostCountsSubscriptionQuery" in friendly_name:
                    try:
                        data = await response.json()
                        if TARGET_PK in json.dumps(data, ensure_ascii=False):
                            print(f"   âœ… æ””æˆªåˆ°è¨ˆæ•¸æŸ¥è©¢")
                            
                            self.counts_headers = dict(response.request.headers)
                            self.counts_payload = response.request.post_data
                            self.auth_header = self.counts_headers.get("authorization", "")
                            self.lsd_token = self.counts_headers.get("x-fb-lsd", "")
                            
                            self.counts_captured = True
                    except:
                        pass
            
            page.on("response", response_handler)
            await page.goto(TEST_POST_URL, wait_until="networkidle")
            await asyncio.sleep(3)
            await browser.close()
        
        return self.counts_captured
    
    async def get_counts_data(self, target_pk: str) -> Optional[Dict[str, int]]:
        """ä½¿ç”¨è¨ˆæ•¸æŸ¥è©¢ç²å–æº–ç¢ºçš„æ•¸å­—æ•¸æ“š"""
        if not self.counts_captured:
            return None
        
        print(f"   ğŸ“Š æŸ¥è©¢è¨ˆæ•¸æ•¸æ“š...")
        
        # ç²å–èªè­‰
        auth_file_path = get_auth_file_path()
        auth_data = json.loads(auth_file_path.read_text())
        cookies = {cookie['name']: cookie['value'] for cookie in auth_data.get('cookies', [])}
        
        # æº–å‚™è«‹æ±‚
        headers = self.counts_headers.copy()
        for h in ["host", "content-length", "accept-encoding"]:
            headers.pop(h, None)
        
        if not headers.get("authorization"):
            if 'ig_set_authorization' in cookies:
                auth_value = cookies['ig_set_authorization']
                headers["authorization"] = f"Bearer {auth_value}" if not auth_value.startswith('Bearer') else auth_value
        
        payload = self.counts_payload.replace(TARGET_PK, target_pk) if target_pk != TARGET_PK else self.counts_payload
        
        # ç™¼é€è«‹æ±‚
        async with httpx.AsyncClient(headers=headers, cookies=cookies, timeout=30.0, http2=True) as client:
            try:
                response = await client.post("https://www.threads.com/graphql/query", data=payload)
                
                if response.status_code == 200:
                    result = response.json()
                    if "data" in result and result["data"] and "data" in result["data"] and "posts" in result["data"]["data"]:
                        posts = result["data"]["data"]["posts"]
                        
                        for post in posts:
                            if post.get("pk") == target_pk:
                                text_info = post.get("text_post_app_info", {}) or {}
                                return {
                                    "like_count": post.get("like_count", 0),
                                    "comment_count": text_info.get("direct_reply_count", 0),
                                    "repost_count": text_info.get("repost_count", 0),
                                    "share_count": text_info.get("reshare_count", 0)
                                }
            except Exception as e:
                print(f"   âŒ è¨ˆæ•¸æŸ¥è©¢å¤±æ•—: {e}")
        
        return None
    
    async def get_content_and_media_from_dom(self, post_url: str) -> Optional[Dict[str, Any]]:
        """å¾ DOM ç²å–å…§å®¹å’Œåª’é«”æ•¸æ“šï¼ˆå¢å¼·ç‰ˆï¼šæ”¯æ´å½±ç‰‡æ””æˆªï¼‰"""
        print(f"   ğŸŒ å¾ DOM è§£æå…§å®¹å’Œåª’é«”...")
        
        auth_file_path = get_auth_file_path()
        
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            context = await browser.new_context(
                storage_state=str(auth_file_path),
                user_agent="Mozilla/5.0 (iPhone; CPU iPhone OS 15_0 like Mac OS X) AppleWebKit/605.1.15",
                viewport={"width": 375, "height": 812}
            )
            
            page = await context.new_page()
            
            try:
                # æ­¥é©Ÿ 0: è¨­ç½®ç¶²è·¯æ””æˆªå™¨æŠ“å–å½±ç‰‡éŸ¿æ‡‰
                video_urls = set()
                
                async def response_handler(response):
                    try:
                        content_type = response.headers.get("content-type", "")
                        resource_type = response.request.resource_type
                        
                        # æ””æˆªå½±ç‰‡è³‡æº
                        if (resource_type == "media" or 
                            content_type.startswith("video/") or
                            ".mp4" in response.url.lower() or
                            ".m3u8" in response.url.lower() or
                            ".mpd" in response.url.lower()):
                            video_urls.add(response.url)
                            print(f"      ğŸ¥ æ””æˆªåˆ°å½±ç‰‡: {response.url[:80]}...")
                    except Exception as e:
                        pass
                
                page.on("response", response_handler)
                
                # æ­¥é©Ÿ 1: è¼‰å…¥é é¢
                await page.goto(post_url, wait_until="networkidle", timeout=60000)
                await asyncio.sleep(3)
                
                # æ­¥é©Ÿ 2: æ¨¡æ“¬é»æ“Šè§¸ç™¼å½±ç‰‡è¼‰å…¥
                print(f"      ğŸ–±ï¸ å˜—è©¦è§¸ç™¼å½±ç‰‡è¼‰å…¥...")
                try:
                    # å˜—è©¦å¤šç¨®å¯èƒ½çš„å½±ç‰‡è§¸ç™¼å™¨
                    trigger_selectors = [
                        'div[data-testid="media-viewer"]',
                        'video',
                        'div[role="button"][aria-label*="play"]',
                        'div[role="button"][aria-label*="æ’­æ”¾"]',
                        '[data-pressable-container] div[style*="video"]',
                        'div[style*="cursor: pointer"]'
                    ]
                    
                    for selector in trigger_selectors:
                        try:
                            elements = page.locator(selector)
                            count = await elements.count()
                            if count > 0:
                                await elements.first.click(timeout=3000)
                                print(f"      âœ… é»æ“Šè§¸ç™¼å™¨: {selector}")
                                await asyncio.sleep(2)  # ç­‰å¾…å½±ç‰‡è¼‰å…¥
                                break
                        except:
                            continue
                            
                except Exception as e:
                    print(f"      âš ï¸ è§¸ç™¼å½±ç‰‡è¼‰å…¥å¤±æ•—: {e}")
                
                # çµ¦æ›´å¤šæ™‚é–“è®“å½±ç‰‡è¼‰å…¥
                await asyncio.sleep(3)
                
                # æå–ç”¨æˆ¶åï¼ˆå¾ URL ä¸­ç›´æ¥è§£æï¼Œæ›´æº–ç¢ºï¼‰
                username = ""
                try:
                    # å¾ URL ä¸­æå–ç”¨æˆ¶å
                    import re
                    url_match = re.search(r'/@([^/]+)/', post_url)
                    if url_match:
                        username = url_match.group(1)
                    else:
                        # å‚™ç”¨æ–¹æ¡ˆï¼šå¾ DOM ä¸­å°‹æ‰¾
                        username_elem = await page.locator('a[href*="/@"]').first.get_attribute("href")
                        if username_elem:
                            username = username_elem.split("/@")[1].split("/")[0]
                except:
                    pass
                
                # æå–å…§å®¹æ–‡å­—ï¼ˆåŸºæ–¼èª¿è©¦çµæœï¼‰
                content = ""
                try:
                    # ä½¿ç”¨èª¿è©¦ä¸­ç™¼ç¾çš„æœ‰æ•ˆé¸æ“‡å™¨
                    content_selectors = [
                        'div[data-pressable-container] span',  # èª¿è©¦ä¸­æ‰¾åˆ° 305 å€‹å…ƒç´ 
                        '[data-testid="thread-text"]',
                        'article div[dir="auto"]',
                        'div[role="article"] div[dir="auto"]',
                        'span[style*="text-overflow"]'
                    ]
                    
                    for selector in content_selectors:
                        try:
                            elements = page.locator(selector)
                            count = await elements.count()
                            
                            if count > 0:
                                # å°‹æ‰¾åŒ…å«ä¸»è¦å…§å®¹çš„å…ƒç´ ï¼ˆé•·åº¦è¶…é10å­—ç¬¦ä¸”ä¸æ˜¯æ•¸å­—ï¼‰
                                for i in range(min(count, 20)):  # æª¢æŸ¥å‰20å€‹å…ƒç´ 
                                    try:
                                        text = await elements.nth(i).inner_text()
                                        if (text and len(text.strip()) > 10 and 
                                            not text.strip().isdigit() and
                                            "å°æ™‚" not in text and  # æ’é™¤æ™‚é–“
                                            "åˆ†é˜" not in text and
                                            not text.startswith("@")):  # æ’é™¤ç”¨æˆ¶å
                                            content = text.strip()
                                            break
                                    except:
                                        continue
                                
                                if content:
                                    break
                        except:
                            continue
                except:
                    pass
                
                # æ­¥é©Ÿ 4: æå–åœ–ç‰‡ï¼ˆéæ¿¾é ­åƒå’Œ UI å°åœ–ï¼‰
                images = []
                try:
                    print(f"      ğŸ–¼ï¸ æå–åœ–ç‰‡ï¼ˆéæ¿¾é ­åƒï¼‰...")
                    img_elements = page.locator('img')
                    img_count = await img_elements.count()
                    
                    for i in range(min(img_count, 50)):  # æª¢æŸ¥æ›´å¤šåœ–ç‰‡ä½†éæ¿¾
                        try:
                            img_elem = img_elements.nth(i)
                            img_src = await img_elem.get_attribute("src")
                            
                            if not img_src or not ("fbcdn" in img_src or "cdninstagram" in img_src):
                                continue
                            
                            # æ’é™¤ç•Œé¢å…ƒç´ 
                            if ("rsrc.php" in img_src or 
                                "static.cdninstagram.com" in img_src):
                                continue
                            
                            # æª¢æŸ¥åœ–ç‰‡å°ºå¯¸ï¼ˆéæ¿¾é ­åƒï¼‰
                            try:
                                width = int(await img_elem.get_attribute("width") or 0)
                                height = int(await img_elem.get_attribute("height") or 0)
                                max_size = max(width, height)
                                
                                # åªä¿ç•™å°ºå¯¸ > 150x150 çš„åœ–ç‰‡ï¼ˆæ’é™¤é ­åƒï¼‰
                                if max_size > 150 and img_src not in images:
                                    images.append(img_src)
                                    print(f"         ğŸ“¸ åœ–ç‰‡ {len(images)}: {max_size}px")
                            except:
                                # å¦‚æœç„¡æ³•ç²å–å°ºå¯¸ï¼ŒæŒ‰ URL ç‰¹å¾µåˆ¤æ–·
                                if ("t51.2885-15" in img_src or  # è²¼æ–‡åª’é«”
                                    "scontent" in img_src) and img_src not in images:
                                    images.append(img_src)
                        except:
                            continue
                            
                    print(f"      âœ… æ‰¾åˆ° {len(images)} å€‹æœ‰æ•ˆåœ–ç‰‡")
                except Exception as e:
                    print(f"      âŒ åœ–ç‰‡æå–å¤±æ•—: {e}")
                
                # æ­¥é©Ÿ 3: æå–å½±ç‰‡ï¼ˆçµåˆç¶²è·¯æ””æˆªå’Œ DOMï¼‰
                print(f"      ğŸ¥ æå–å½±ç‰‡...")
                videos = list(video_urls)  # å¾ç¶²è·¯æ””æˆªç²å–çš„å½±ç‰‡ URL
                
                try:
                    # æª¢æŸ¥å½±ç‰‡æŒ‡ç¤ºå™¨
                    video_error_text = await page.locator('text="å¾ˆæŠ±æ­‰ï¼Œæ’­æ”¾æ­¤å½±ç‰‡æ™‚ç™¼ç”Ÿå•é¡Œ"').count()
                    if video_error_text > 0:
                        print(f"      ğŸ¥ æª¢æ¸¬åˆ°å½±ç‰‡è¼‰å…¥éŒ¯èª¤è¨Šæ¯")
                    
                    # å¾ DOM ä¸­æå– video æ¨™ç±¤ï¼ˆåŒ…å« posterï¼‰
                    video_elements = page.locator('video')
                    video_count = await video_elements.count()
                    
                    if video_count > 0:
                        print(f"      ğŸ¥ æ‰¾åˆ° {video_count} å€‹ video æ¨™ç±¤")
                        
                        for i in range(video_count):
                            try:
                                video_elem = video_elements.nth(i)
                                
                                # ç²å–å„ç¨®å¯èƒ½çš„å½±ç‰‡ URL
                                src = await video_elem.get_attribute("src")
                                data_src = await video_elem.get_attribute("data-src")
                                poster = await video_elem.get_attribute("poster")
                                
                                if src and src not in videos:
                                    videos.append(src)
                                    print(f"         ğŸ¬ src: {src[:60]}...")
                                
                                if data_src and data_src not in videos:
                                    videos.append(data_src)
                                    print(f"         ğŸ¬ data-src: {data_src[:60]}...")
                                
                                if poster and poster not in videos:
                                    videos.append(f"POSTER::{poster}")
                                    print(f"         ğŸ–¼ï¸ poster: {poster[:60]}...")
                                
                                # æª¢æŸ¥ source å­å…ƒç´ 
                                sources = video_elem.locator('source')
                                source_count = await sources.count()
                                for j in range(source_count):
                                    source_src = await sources.nth(j).get_attribute("src")
                                    if source_src and source_src not in videos:
                                        videos.append(source_src)
                                        print(f"         ğŸ¬ source: {source_src[:60]}...")
                            except:
                                continue
                    
                    # çµ±è¨ˆçµæœ
                    actual_videos = [v for v in videos if not v.startswith("POSTER::")]
                    poster_videos = [v for v in videos if v.startswith("POSTER::")]
                    
                    print(f"      âœ… ç¶²è·¯æ””æˆª: {len(video_urls)} å€‹å½±ç‰‡ URL")
                    print(f"      âœ… DOM å½±ç‰‡: {len(actual_videos)} å€‹")
                    print(f"      âœ… å°é¢åœ–: {len(poster_videos)} å€‹")
                    
                    # å¦‚æœæœ‰å½±ç‰‡æŒ‡ç¤ºå™¨ä½†æ²’æœ‰æ‰¾åˆ°å½±ç‰‡ï¼Œæ¨™è¨˜
                    if video_error_text > 0 and not actual_videos:
                        videos.append("VIDEO_DETECTED_BUT_FAILED_TO_LOAD")
                        
                except Exception as e:
                    print(f"      âŒ å½±ç‰‡æå–å¤±æ•—: {e}")
                
                await browser.close()
                
                return {
                    "username": username,
                    "content": content,
                    "images": images,
                    "videos": videos
                }
            
            except Exception as e:
                print(f"   âŒ DOM è§£æå¤±æ•—: {e}")
                await browser.close()
                return None
    
    def extract_code_from_url(self, url: str) -> str:
        """å¾ URL æå–è²¼æ–‡ä»£ç¢¼"""
        match = re.search(r'/post/([A-Za-z0-9_-]+)', url)
        return match.group(1) if match else ""
    
    async def extract_complete_post(self, post_url: str, target_pk: str = TARGET_PK) -> Optional[Dict[str, Any]]:
        """æå–å®Œæ•´çš„è²¼æ–‡æ•¸æ“šï¼ˆè¨ˆæ•¸ + å…§å®¹ + åª’é«”ï¼‰"""
        print(f"ğŸ¯ æå–å®Œæ•´è²¼æ–‡æ•¸æ“š: {post_url}")
        
        # ç¬¬ä¸€æ­¥ï¼šç²å–æº–ç¢ºçš„è¨ˆæ•¸æ•¸æ“š
        counts_data = await self.get_counts_data(target_pk)
        if not counts_data:
            print(f"   âš ï¸ ç„¡æ³•ç²å–è¨ˆæ•¸æ•¸æ“šï¼Œä½¿ç”¨é è¨­å€¼")
            counts_data = {"like_count": 0, "comment_count": 0, "repost_count": 0, "share_count": 0}
        else:
            print(f"   âœ… è¨ˆæ•¸æ•¸æ“š: è®š{counts_data['like_count']}, ç•™è¨€{counts_data['comment_count']}, è½‰ç™¼{counts_data['repost_count']}, åˆ†äº«{counts_data['share_count']}")
        
        # ç¬¬äºŒæ­¥ï¼šç²å–å…§å®¹å’Œåª’é«”æ•¸æ“š
        content_data = await self.get_content_and_media_from_dom(post_url)
        if not content_data:
            print(f"   âš ï¸ ç„¡æ³•ç²å–å…§å®¹æ•¸æ“šï¼Œä½¿ç”¨é è¨­å€¼")
            content_data = {"username": "", "content": "", "images": [], "videos": []}
        else:
            print(f"   âœ… å…§å®¹æ•¸æ“š: @{content_data['username']}, {len(content_data['content'])}å­—ç¬¦, {len(content_data['images'])}åœ–ç‰‡, {len(content_data['videos'])}å½±ç‰‡")
        
        # åˆä½µæ•¸æ“š
        code = self.extract_code_from_url(post_url)
        
        result = {
            "pk": target_pk,
            "code": code,
            "username": content_data["username"],
            "content": content_data["content"],
            "like_count": counts_data["like_count"],
            "comment_count": counts_data["comment_count"],
            "repost_count": counts_data["repost_count"],
            "share_count": counts_data["share_count"],
            "images": content_data["images"],
            "videos": content_data["videos"],
            "url": post_url,
            "extracted_at": datetime.now().isoformat(),
            "extraction_method": "hybrid_counts_and_dom"
        }
        
        # é¡¯ç¤ºæœ€çµ‚çµæœ
        print(f"\nğŸ“‹ æœ€çµ‚çµæœ:")
        print(f"   ğŸ“„ PK: {result['pk']}")
        print(f"   ğŸ‘¤ ç”¨æˆ¶: @{result['username']}")
        print(f"   ğŸ“ å…§å®¹: {len(result['content'])} å­—ç¬¦")
        print(f"   ğŸ‘ è®šæ•¸: {result['like_count']}")
        print(f"   ğŸ’¬ ç•™è¨€: {result['comment_count']}")
        print(f"   ğŸ”„ è½‰ç™¼: {result['repost_count']}")
        print(f"   ğŸ“¤ åˆ†äº«: {result['share_count']}")
        print(f"   ğŸ–¼ï¸ åœ–ç‰‡: {len(result['images'])} å€‹")
        print(f"   ğŸ¥ å½±ç‰‡: {len(result['videos'])} å€‹")
        
        if result['content']:
            print(f"   ğŸ“„ å…§å®¹é è¦½: {result['content'][:100]}...")
        
        # ä¿å­˜çµæœ
        result_file = Path(f"hybrid_extraction_result_{datetime.now().strftime('%H%M%S')}.json")
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, indent=2, ensure_ascii=False)
        print(f"   ğŸ“ å®Œæ•´çµæœå·²ä¿å­˜: {result_file}")
        
        return result

async def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸš€ æ··åˆå…§å®¹æå–å™¨ - è¨ˆæ•¸æŸ¥è©¢ + DOM è§£æ")
    
    auth_file = get_auth_file_path()
    if not auth_file.exists():
        print(f"âŒ èªè­‰æª”æ¡ˆ {auth_file} ä¸å­˜åœ¨ã€‚è«‹å…ˆåŸ·è¡Œ save_auth.pyã€‚")
        return
    
    extractor = HybridContentExtractor()
    
    # ç¬¬ä¸€æ­¥ï¼šæ””æˆªè¨ˆæ•¸æŸ¥è©¢
    print(f"\nğŸ“¡ ç¬¬ä¸€æ­¥ï¼šæ””æˆªè¨ˆæ•¸æŸ¥è©¢...")
    captured = await extractor.intercept_counts_query()
    
    if not captured:
        print(f"   âš ï¸ æœªæ””æˆªåˆ°è¨ˆæ•¸æŸ¥è©¢ï¼Œå°‡åªä½¿ç”¨ DOM è§£æ")
    
    # ç¬¬äºŒæ­¥ï¼šæå–å®Œæ•´æ•¸æ“š
    print(f"\nğŸ¯ ç¬¬äºŒæ­¥ï¼šæ··åˆæå–...")
    result = await extractor.extract_complete_post(TEST_POST_URL, TARGET_PK)
    
    if result:
        print(f"\nğŸ‰ æ··åˆæå–æˆåŠŸï¼")
        print(f"ğŸ’¡ é€™å€‹æ–¹æ³•çµåˆäº†å…©ç¨®æŠ€è¡“çš„å„ªå‹¢:")
        print(f"   âœ… è¨ˆæ•¸æŸ¥è©¢: æº–ç¢ºçš„æ•¸å­—æ•¸æ“š")
        print(f"   âœ… DOM è§£æ: å®Œæ•´çš„å…§å®¹å’Œåª’é«”")
        print(f"   âœ… ç©©å®šå¯é : å³ä½¿ä¸€å€‹æ–¹æ³•å¤±æ•—ï¼Œå¦ä¸€å€‹å¯ä»¥è£œè¶³")
        print(f"\nğŸ”§ ç¾åœ¨å¯ä»¥å°‡æ­¤æ··åˆæ–¹æ³•æ•´åˆåˆ°ä¸»çˆ¬èŸ²ä¸­ï¼")
    else:
        print(f"\nğŸ˜ æ··åˆæå–å¤±æ•—")

if __name__ == "__main__":
    asyncio.run(main())