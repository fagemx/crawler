"""
å®Œç¾çš„å…§å®¹æ””æˆªå™¨ - å®Œå…¨è¤‡è£½çœŸå¯¦è«‹æ±‚ï¼Œä¸åšä»»ä½•"ç°¡åŒ–"
æŒ‰ç…§ç”¨æˆ¶æŒ‡å°å¯¦ç¾æœ€ç©©å®šçš„æ–¹æ³•
"""

import asyncio
import json
import httpx
import urllib.parse
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Optional, Tuple

import sys
sys.path.append(str(Path(__file__).parent))

from playwright.async_api import async_playwright
from common.config import get_auth_file_path

# æ¸¬è©¦è²¼æ–‡
TEST_POST_URL = "https://www.threads.com/@star_shining0828/post/DMyvZJRz5Cz"
TARGET_PK = "3689219480905289907"

class PerfectContentInterceptor:
    """å®Œç¾æ””æˆªå™¨ - å®Œå…¨è¤‡è£½çœŸå¯¦è«‹æ±‚"""
    
    def __init__(self):
        self.raw_headers = {}
        self.raw_payload = ""
        self.doc_id = ""
        self.auth_header = ""
        self.captured = False
    
    async def intercept_and_copy_everything(self):
        """æ””æˆªä¸¦å®Œæ•´è¤‡è£½æ‰€æœ‰è«‹æ±‚ä¿¡æ¯"""
        print("ğŸ¯ æ””æˆªä¸¦å®Œæ•´è¤‡è£½çœŸå¯¦è«‹æ±‚...")
        
        auth_file_path = get_auth_file_path()
        
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=False)
            context = await browser.new_context(
                storage_state=str(auth_file_path),
                user_agent="Mozilla/5.0 (iPhone; CPU iPhone OS 15_0 like Mac OS X) AppleWebKit/605.1.15",
                viewport={"width": 375, "height": 812},
                locale="zh-TW"
            )
            
            page = await context.new_page()
            
            async def response_handler(response):
                url = response.url.lower()
                friendly_name = response.request.headers.get("x-fb-friendly-name", "")
                
                # æ””æˆªå…§å®¹æŸ¥è©¢ - å°‹æ‰¾ä¸»è²¼æ–‡æŸ¥è©¢è€Œéç•™è¨€æŸ¥è©¢
                if "/graphql/query" in url:
                    print(f"   ğŸ“¡ GraphQL æŸ¥è©¢: {friendly_name}")
                    
                    # æª¢æŸ¥ x-root-field-name ä¾†ç¢ºå®šæŸ¥è©¢é¡å‹
                    root_field = response.request.headers.get("x-root-field-name", "")
                    
                    # å„ªå…ˆæ””æˆªä¸»è²¼æ–‡æŸ¥è©¢ï¼Œé¿å…ç•™è¨€æŸ¥è©¢
                    is_main_post_query = (
                        "BarcelonaPostPageContentQuery" in friendly_name or
                        ("BarcelonaPostPageRefetchableDirectQuery" in friendly_name and 
                         "replies" not in root_field and "media_id__replies" not in root_field)
                    )
                    
                    if is_main_post_query:
                        print(f"   ğŸ¯ æ””æˆªåˆ°ä¸»è²¼æ–‡æŸ¥è©¢: {friendly_name}")
                        print(f"   ğŸ” Root field: {root_field}")
                        
                        # === å®Œæ•´åˆ—å° RAW REQUEST ===
                        print("\n======= RAW POST PAYLOAD =======")
                        print(response.request.post_data)
                        print("\n======= RAW HEADERS =======")
                        for k, v in response.request.headers.items():
                            print(f"{k}: {v}")
                        print("===============================\n")
                        
                        # å®Œæ•´ä¿å­˜ï¼ˆä¸åšä»»ä½•ä¿®æ”¹ï¼‰
                        self.raw_headers = dict(response.request.headers)
                        self.raw_payload = response.request.post_data
                        
                        # æå–é—œéµä¿¡æ¯ç”¨æ–¼è¨˜éŒ„
                        if self.raw_payload and "doc_id=" in self.raw_payload:
                            self.doc_id = self.raw_payload.split("doc_id=")[1].split("&")[0]
                        
                        self.auth_header = self.raw_headers.get("authorization", "")
                        
                        print(f"   ğŸ“‹ doc_id: {self.doc_id}")
                        print(f"   ğŸ”‘ Authorization: {'æ˜¯' if self.auth_header else 'å¦'}")
                        print(f"   ğŸ“¦ å®Œæ•´ä¿å­˜ headers: {len(self.raw_headers)} å€‹")
                        print(f"   ğŸ“¦ å®Œæ•´ä¿å­˜ payload: {len(self.raw_payload)} å­—ç¬¦")
                        
                        self.captured = True
                    elif "replies" in root_field:
                        print(f"   â­ï¸ è·³éç•™è¨€æŸ¥è©¢: {friendly_name} (root: {root_field})")
                    else:
                        print(f"   â­ï¸ è·³éå…¶ä»–æŸ¥è©¢: {friendly_name}")
            
            page.on("response", response_handler)
            
            # å°èˆªä¸¦ç­‰å¾…æ””æˆª
            print(f"   ğŸŒ å°èˆªåˆ°: {TEST_POST_URL}")
            await page.goto(TEST_POST_URL, wait_until="networkidle", timeout=60000)
            await asyncio.sleep(5)
            
            # å¦‚æœæ²’æ””æˆªåˆ°ï¼Œå˜—è©¦åˆ·æ–°
            if not self.captured:
                print(f"   ğŸ”„ åˆ·æ–°é é¢é‡æ–°æ””æˆª...")
                await page.reload(wait_until="networkidle")
                await asyncio.sleep(5)
            
            await browser.close()
        
        return self.captured
    
    async def _process_media_data(self, media: dict, replay_headers: dict, replay_payload: str, full_result: dict) -> bool:
        """è™•ç†åª’é«”æ•¸æ“šä¸¦æå–é—œéµä¿¡æ¯"""
        print(f"   ğŸ‰ æˆåŠŸï¼ç²å–åª’é«”æ•¸æ“š")
        
        # æå–é—œéµä¿¡æ¯
        pk = media.get("pk", "")
        caption = media.get("caption", {}) or {}
        content = caption.get("text", "") if caption else ""
        like_count = media.get("like_count", 0)
        
        text_info = media.get("text_post_app_info", {}) or {}
        comment_count = text_info.get("direct_reply_count", 0)
        repost_count = text_info.get("repost_count", 0)
        share_count = text_info.get("reshare_count", 0)
        
        # åª’é«”ä¿¡æ¯
        images = []
        videos = []
        
        # å–®ä¸€åœ–ç‰‡/å½±ç‰‡
        if "image_versions2" in media and media["image_versions2"]:
            candidates = media["image_versions2"].get("candidates", [])
            if candidates:
                best_image = max(candidates, key=lambda c: c.get("width", 0))
                images.append(best_image.get("url", ""))
        
        if "video_versions" in media and media["video_versions"]:
            videos.append(media["video_versions"][0].get("url", ""))
        
        # è¼ªæ’­åª’é«”
        if "carousel_media" in media and media["carousel_media"]:
            for item in media["carousel_media"]:
                if "image_versions2" in item and item["image_versions2"]:
                    candidates = item["image_versions2"].get("candidates", [])
                    if candidates:
                        best_image = max(candidates, key=lambda c: c.get("width", 0))
                        images.append(best_image.get("url", ""))
                if "video_versions" in item and item["video_versions"]:
                    videos.append(item["video_versions"][0].get("url", ""))
        
        print(f"      ğŸ“„ PK: {pk}")
        print(f"      ğŸ“ å…§å®¹: {len(content)} å­—ç¬¦")
        print(f"      ğŸ‘ è®šæ•¸: {like_count}")
        print(f"      ğŸ’¬ ç•™è¨€: {comment_count}")
        print(f"      ğŸ”„ è½‰ç™¼: {repost_count}")
        print(f"      ğŸ“¤ åˆ†äº«: {share_count}")
        print(f"      ğŸ–¼ï¸ åœ–ç‰‡: {len(images)} å€‹")
        print(f"      ğŸ¥ å½±ç‰‡: {len(videos)} å€‹")
        
        if content:
            print(f"      ğŸ“„ å…§å®¹é è¦½: {content[:100]}...")
        
        # ä¿å­˜æˆåŠŸé…ç½®
        success_file = Path(f"perfect_replay_success_{datetime.now().strftime('%H%M%S')}.json")
        with open(success_file, 'w', encoding='utf-8') as f:
            json.dump({
                "method": "perfect_replay",
                "doc_id": self.doc_id,
                "headers_count": len(replay_headers),
                "payload_size": len(replay_payload),
                "test_result": {
                    "pk": pk,
                    "content": content,
                    "like_count": like_count,
                    "comment_count": comment_count,
                    "repost_count": repost_count,
                    "share_count": share_count,
                    "images_count": len(images),
                    "videos_count": len(videos),
                    "images": images[:3],  # åªä¿å­˜å‰3å€‹URL
                    "videos": videos[:3]
                },
                "raw_headers": replay_headers,
                "raw_payload": replay_payload,
                "media_data": media,
                "full_response": full_result
            }, f, indent=2, ensure_ascii=False)
        
        print(f"      ğŸ“ æˆåŠŸé…ç½®å·²ä¿å­˜: {success_file}")
        return True
    
    async def get_authorization_from_cookies(self) -> Optional[str]:
        """å¾ cookies ç²å– Authorizationï¼ˆå¦‚æœ header ä¸­æ²’æœ‰ï¼‰"""
        auth_file_path = get_auth_file_path()
        auth_data = json.loads(auth_file_path.read_text())
        cookies = {cookie['name']: cookie['value'] for cookie in auth_data.get('cookies', [])}
        
        # æ–¹æ³•1: ig_set_authorization cookie
        if 'ig_set_authorization' in cookies:
            auth_value = cookies['ig_set_authorization']
            if auth_value and not auth_value.startswith('Bearer'):
                return f"Bearer {auth_value}"
            return auth_value
        
        # æ–¹æ³•2: å¾ sessionid æ§‹å»º
        if 'sessionid' in cookies:
            sessionid = cookies['sessionid']
            return f"Bearer IGT:2:{sessionid}"
        
        return None
    
    async def perfect_replay(self, new_pk: str = None):
        """å®Œç¾é‡æ”¾è«‹æ±‚ - å®Œå…¨æŒ‰ç…§æ””æˆªåˆ°çš„æ ¼å¼"""
        if not self.captured:
            print("âŒ æ²’æœ‰æ””æˆªåˆ°è«‹æ±‚")
            return False
        
        print(f"\nğŸ¬ å®Œç¾é‡æ”¾æ””æˆªåˆ°çš„è«‹æ±‚...")
        
        # ç²å– cookies
        auth_file_path = get_auth_file_path()
        auth_data = json.loads(auth_file_path.read_text())
        cookies = {cookie['name']: cookie['value'] for cookie in auth_data.get('cookies', [])}
        
        # æº–å‚™ headersï¼ˆå®Œå…¨è¤‡è£½ï¼Œåªåˆªé™¤æœƒå¹²æ“¾çš„ï¼‰
        replay_headers = self.raw_headers.copy()
        
        # åªåˆªé™¤é€™äº›æœƒè®“ httpx è‡ªå‹•è™•ç†çš„ headers
        for header_to_remove in ["host", "content-length", "accept-encoding"]:
            replay_headers.pop(header_to_remove, None)
        
        print(f"   ğŸ“¦ ä½¿ç”¨ headers: {len(replay_headers)} å€‹")
        
        # æª¢æŸ¥ Authorization
        if not replay_headers.get("authorization"):
            print(f"   ğŸ” header ä¸­ç„¡ Authorizationï¼Œå¾ cookies ç²å–...")
            auth_from_cookies = await self.get_authorization_from_cookies()
            if auth_from_cookies:
                replay_headers["authorization"] = auth_from_cookies
                print(f"   âœ… å¾ cookies è£œå…… Authorization")
            else:
                print(f"   âš ï¸ è­¦å‘Šï¼šç„¡æ³•ç²å– Authorizationï¼Œå¯èƒ½å°è‡´ 403")
        
        # æº–å‚™ payloadï¼ˆå®Œå…¨è¤‡è£½ï¼Œåªæ›¿æ› PK å¦‚æœéœ€è¦ï¼‰
        replay_payload = self.raw_payload
        if new_pk and new_pk != TARGET_PK:
            # åªæ›¿æ› PKï¼Œå…¶ä»–ä¿æŒåŸæ¨£
            replay_payload = replay_payload.replace(TARGET_PK, new_pk)
            print(f"   ğŸ”„ æ›¿æ› PK: {TARGET_PK} â†’ {new_pk}")
        
        print(f"   ğŸ“¦ ä½¿ç”¨ payload: {len(replay_payload)} å­—ç¬¦")
        
        # ç™¼é€è«‹æ±‚ï¼ˆæœ€å°å¯è¡Œç‰ˆæœ¬ï¼‰
        url = "https://www.threads.com/graphql/query"
        
        async with httpx.AsyncClient(
            headers=replay_headers,
            cookies=cookies,
            timeout=30.0,
            follow_redirects=True,
            http2=True
        ) as client:
            try:
                print(f"   ğŸ“¡ ç™¼é€è«‹æ±‚åˆ°: {url}")
                response = await client.post(url, data=replay_payload)
                
                print(f"   ğŸ“¡ HTTP {response.status_code}")
                
                if response.status_code == 200:
                    try:
                        result = response.json()
                        
                        if "errors" in result:
                            print(f"   âŒ GraphQL éŒ¯èª¤: {result['errors'][:1]}")
                            return False
                        
                        if "data" in result and result["data"]:
                            data = result["data"]
                            
                            # æª¢æŸ¥ä¸åŒçš„æ•¸æ“šçµæ§‹
                            if "media" in data and data["media"]:
                                # æ–¹æ³•1: ç›´æ¥çš„ media çµæ§‹
                                media = data["media"]
                                return await self._process_media_data(media, replay_headers, replay_payload, result)
                            
                            elif "data" in data and data["data"]:
                                # æ–¹æ³•2: edges çµæ§‹ï¼ˆç•™è¨€æˆ–ç›¸é—œè²¼æ–‡ï¼‰
                                inner_data = data["data"]
                                if "edges" in inner_data and inner_data["edges"]:
                                    print(f"   ğŸ“ ç™¼ç¾ edges çµæ§‹: {len(inner_data['edges'])} å€‹")
                                    
                                    # å˜—è©¦åœ¨ edges ä¸­æ‰¾åˆ°ç›®æ¨™è²¼æ–‡
                                    target_found = False
                                    for i, edge in enumerate(inner_data["edges"]):
                                        if "node" in edge:
                                            node = edge["node"]
                                            
                                            # æª¢æŸ¥æ˜¯å¦ç‚ºè²¼æ–‡ç¯€é»
                                            if "post" in node:
                                                post = node["post"]
                                                post_pk = post.get("pk", "")
                                                if post_pk == TARGET_PK:
                                                    print(f"   ğŸ¯ åœ¨ edge[{i}] ä¸­æ‰¾åˆ°ç›®æ¨™è²¼æ–‡!")
                                                    return await self._process_media_data(post, replay_headers, replay_payload, result)
                                            
                                            # æª¢æŸ¥æ˜¯å¦ç‚ºç›´æ¥çš„åª’é«”ç¯€é»
                                            elif node.get("pk") == TARGET_PK:
                                                print(f"   ğŸ¯ åœ¨ edge[{i}] ä¸­æ‰¾åˆ°ç›®æ¨™åª’é«”!")
                                                return await self._process_media_data(node, replay_headers, replay_payload, result)
                                            
                                            # æª¢æŸ¥ thread_items çµæ§‹
                                            elif "thread_items" in node:
                                                for j, item in enumerate(node["thread_items"]):
                                                    if "post" in item and item["post"].get("pk") == TARGET_PK:
                                                        print(f"   ğŸ¯ åœ¨ edge[{i}].thread_items[{j}] ä¸­æ‰¾åˆ°ç›®æ¨™è²¼æ–‡!")
                                                        return await self._process_media_data(item["post"], replay_headers, replay_payload, result)
                                    
                                    if not target_found:
                                        print(f"   âš ï¸ edges ä¸­æœªæ‰¾åˆ°ç›®æ¨™è²¼æ–‡ (PK: {TARGET_PK})")
                                        print(f"   ğŸ’¡ é€™å¯èƒ½æ˜¯ç•™è¨€æŸ¥è©¢ï¼Œè€Œä¸æ˜¯ä¸»è²¼æ–‡æŸ¥è©¢")
                                        
                                        # é¡¯ç¤ºæ‰¾åˆ°çš„ PK ç”¨æ–¼èª¿è©¦
                                        found_pks = []
                                        for edge in inner_data["edges"][:3]:  # åªæª¢æŸ¥å‰3å€‹
                                            if "node" in edge:
                                                node = edge["node"]
                                                pk = (node.get("post", {}).get("pk") or 
                                                     node.get("pk") or 
                                                     "unknown")
                                                found_pks.append(pk)
                                        print(f"   ğŸ“‹ æ‰¾åˆ°çš„ PK ç¯„ä¾‹: {found_pks}")
                                
                                print(f"   ğŸ“‹ æ•¸æ“šçµæ§‹: {list(data.keys())}")
                                print(f"   ğŸ“‹ å…§å±¤çµæ§‹: {list(inner_data.keys())}")
                            
                            else:
                                print(f"   ğŸ“‹ æœªçŸ¥æ•¸æ“šçµæ§‹: {list(data.keys())}")
                        
                        else:
                            print(f"   âŒ ç„¡æ•ˆéŸ¿æ‡‰çµæ§‹")
                        
                        return False
                    
                    except Exception as e:
                        print(f"   âŒ è§£æéŸ¿æ‡‰å¤±æ•—: {e}")
                        print(f"   ğŸ“„ éŸ¿æ‡‰ç‰‡æ®µ: {response.text[:300]}...")
                        return False
                
                elif response.status_code == 403:
                    print(f"   âŒ 403 éŒ¯èª¤åˆ†æ:")
                    response_text = response.text
                    
                    if "login_required" in response_text:
                        print(f"      ğŸ’¡ éœ€è¦ç™»å…¥")
                    elif "www_claims" in response_text:
                        print(f"      ğŸ’¡ x-ig-www-claim å•é¡Œ")
                    elif "<!DOCTYPE html>" in response_text:
                        print(f"      ğŸ’¡ è¿”å› HTML é é¢è€Œé JSON")
                    
                    print(f"      ğŸ” å¯èƒ½åŸå› :")
                    print(f"         1. doc_id éæœŸ ({self.doc_id})")
                    print(f"         2. Authorization header æ ¼å¼éŒ¯èª¤")
                    print(f"         3. ç¼ºå°‘é—œéµ headers (x-ig-www-claim ç­‰)")
                    
                    return False
                
                else:
                    print(f"   âŒ HTTP éŒ¯èª¤: {response.status_code}")
                    print(f"   ğŸ“„ éŸ¿æ‡‰: {response.text[:200]}...")
                    return False
            
            except Exception as e:
                print(f"   âŒ è«‹æ±‚å¤±æ•—: {e}")
                return False

async def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸš€ å®Œç¾å…§å®¹æ””æˆªå™¨ - å®Œå…¨è¤‡è£½çœŸå¯¦è«‹æ±‚")
    
    auth_file = get_auth_file_path()
    if not auth_file.exists():
        print(f"âŒ èªè­‰æª”æ¡ˆ {auth_file} ä¸å­˜åœ¨ã€‚è«‹å…ˆåŸ·è¡Œ save_auth.pyã€‚")
        return
    
    interceptor = PerfectContentInterceptor()
    
    # ç¬¬ä¸€æ­¥ï¼šæ””æˆªä¸¦å®Œæ•´è¤‡è£½
    print(f"\nğŸ“¡ ç¬¬ä¸€æ­¥ï¼šæ””æˆªçœŸå¯¦è«‹æ±‚...")
    captured = await interceptor.intercept_and_copy_everything()
    
    if not captured:
        print(f"\nğŸ˜ æœªèƒ½æ””æˆªåˆ°è«‹æ±‚")
        return
    
    # ç¬¬äºŒæ­¥ï¼šå®Œç¾é‡æ”¾
    print(f"\nğŸ¬ ç¬¬äºŒæ­¥ï¼šå®Œç¾é‡æ”¾...")
    success = await interceptor.perfect_replay()
    
    if success:
        print(f"\nğŸ‰ å®Œç¾é‡æ”¾æˆåŠŸï¼")
        print(f"ğŸ’¡ é€™è­‰æ˜äº†ã€Œå®Œå…¨è¤‡è£½ã€çš„æ–¹æ³•æ˜¯æ­£ç¢ºçš„")
        print(f"ğŸ”§ ç¾åœ¨å¯ä»¥:")
        print(f"   1. å°‡æ­¤é‚è¼¯æ•´åˆåˆ°ä¸»çˆ¬èŸ²")
        print(f"   2. å¯¦ç¾æ‰¹é‡å…§å®¹æŸ¥è©¢")
        print(f"   3. æ·»åŠ è‡ªå‹•é‡æ–°æ””æˆªæ©Ÿåˆ¶")
    else:
        print(f"\nğŸ˜ é‡æ”¾å¤±æ•—")
        print(f"ğŸ’¡ å¦‚æœæ˜¯ 403ï¼Œå¯èƒ½éœ€è¦:")
        print(f"   1. é‡æ–°æ””æˆªç²å–æ–°çš„ doc_id")
        print(f"   2. æª¢æŸ¥ Authorization æ ¼å¼")
        print(f"   3. ç¢ºä¿æ‰€æœ‰å¿…è¦ headers éƒ½å·²è¤‡è£½")

if __name__ == "__main__":
    asyncio.run(main())