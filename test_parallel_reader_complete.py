#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å®Œæ•´ç‰ˆä¸‰éšæ®µ Threads Reader è§£æè…³æœ¬
æ•´åˆæ‰€æœ‰å·²é©—è­‰çš„ä¿®æ­£ï¼šNBSPè™•ç†ã€Headersé…ç½®ã€å¢å¼·è§€çœ‹æ•¸æå–ã€ä¸‰éšæ®µé‡è©¦
"""

import json
import re
import requests
import time
import random
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
from typing import Dict, Optional, List

class CompleteThreadsReaderParser:
    """
    å®Œæ•´ç‰ˆ Threads Reader è§£æå™¨
    åŒ…å«ä¸‰éšæ®µè™•ç†æµç¨‹å’Œæ‰€æœ‰å·²é©—è­‰çš„ä¿®æ­£
    """
    
    def __init__(self):
        self.local_reader_url = "http://localhost:8880"
        self.official_reader_url = "https://r.jina.ai"
        
        # NBSP å­—ç¬¦å¸¸é‡ (åƒè€ƒ final ç‰ˆæœ¬)
        self.NBSP = "\u00A0"
        
        # å·²é©—è­‰æœ‰æ•ˆçš„æœ¬åœ° headers é…ç½®
        self.local_headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36',
            'x-wait-for-selector': 'article',
            'x-timeout': '25'
        }
        
        # å®˜æ–¹API headers
        self.official_headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36',
            'X-Return-Format': 'markdown'
        }
        
        # ä¿®æ­£å¾Œçš„è§€çœ‹æ•¸æå–æ¨¡å¼ (åƒè€ƒ final ç‰ˆæœ¬)
        self.view_patterns = [
            # ä¸»è¦æ¨¡å¼ - ä¿®æ­£ NBSP å•é¡Œ
            re.compile(rf'\[Thread[\s{self.NBSP}=]*?(\d+(?:[\.,]\d+)?[KMB]?)\s*views\]', re.IGNORECASE),
            
            # å‚™ç”¨æ¨¡å¼ - è™•ç†åˆ†è¡Œæƒ…æ³
            re.compile(rf'Thread[\s{self.NBSP}=]*?(\d+(?:[\.,]\d+)?[KMB]?)[\s{self.NBSP}]*views', re.IGNORECASE | re.MULTILINE),
            
            # é€šç”¨æ¨¡å¼
            re.compile(r'(\d+(?:[\.,]\d+)?[KMB]?)\s*views?', re.IGNORECASE),
            
            # å®¹éŒ¯æ¨¡å¼
            re.compile(r'(\d+(?:[\.,]\d+)?[KMB]?)\s*view(?:s|ing)', re.IGNORECASE),
            re.compile(r'views?\s*[:\-]\s*(\d+(?:[\.,]\d+)?[KMB]?)', re.IGNORECASE),
        ]
    
    def normalize_content(self, text: str) -> str:
        """æ¨™æº–åŒ–æ–‡æœ¬ (åƒè€ƒ final ç‰ˆæœ¬çš„ normalize å‡½æ•¸)"""
        # â‘  å°‡å„ç¨®ç©ºç™½å­—ç¬¦çµ±ä¸€æ›¿æ›ç‚ºæ¨™æº–ç©ºæ ¼
        text = text.replace(self.NBSP, " ")  # NBSP (U+00A0) 
        text = text.replace("\u2002", " ")   # En Space
        text = text.replace("\u2003", " ")   # Em Space
        text = text.replace("\u2009", " ")   # Thin Space
        text = text.replace("\u200A", " ")   # Hair Space
        text = text.replace("\u3000", " ")   # Ideographic Space
        text = text.replace("\t", " ")       # Tab æ›¿æ›ç‚ºç©ºæ ¼
        
        # â‘¡ æ¨™æº–åŒ–è¡ŒçµæŸç¬¦
        text = text.replace("\r\n", "\n")
        text = text.replace("\r", "\n")
        
        # â‘¢ å£“ç¸®å¤šå€‹é€£çºŒç©ºæ ¼ï¼ˆä½†ä¿ç•™å–®å€‹ç©ºæ ¼ï¼‰
        text = re.sub(r"[ \t]{2,}", " ", text)
        
        return text
    
    def extract_views_count(self, markdown_content: str, post_id: str = "") -> Optional[str]:
        """æœ€çµ‚ç‰ˆè§€çœ‹æ•¸æå– - NBSPä¿®æ­£ (åƒè€ƒ final ç‰ˆæœ¬)"""
        
        # æ¨™æº–åŒ–å…§å®¹
        normalized_content = self.normalize_content(markdown_content)
        
        # 1. å˜—è©¦æ‰€æœ‰æ¨¡å¼åœ¨æ¨™æº–åŒ–å¾Œçš„å…§å®¹ä¸Š
        for i, pattern in enumerate(self.view_patterns):
            match = pattern.search(normalized_content)
            if match:
                views_number = match.group(1)
                if self.validate_views_format(views_number):
                    return views_number
        
        # 2. å¦‚æœé‚„æ˜¯å¤±æ•—ï¼Œå˜—è©¦åœ¨åŸå§‹å…§å®¹ä¸Šæœç´¢ï¼ˆé˜²æ­¢æ¨™æº–åŒ–éåº¦ï¼‰
        for i, pattern in enumerate(self.view_patterns):
            match = pattern.search(markdown_content)
            if match:
                views_number = match.group(1)
                if self.validate_views_format(views_number):
                    return views_number
        
        return None
    
    def validate_views_format(self, views: str) -> bool:
        """é©—è­‰è§€çœ‹æ•¸æ ¼å¼æ˜¯å¦åˆç†"""
        if not views:
            return False
        
        # åŸºæœ¬æ ¼å¼æª¢æŸ¥
        pattern = re.compile(r'^\d+(?:\.\d+)?[KMB]?$', re.IGNORECASE)
        if not pattern.match(views):
            return False
        
        # æ•¸å­—åˆç†æ€§æª¢æŸ¥
        try:
            actual_number = self.convert_to_number(views)
            # è§€çœ‹æ•¸é€šå¸¸åœ¨ 1-100M ç¯„åœå…§
            return 1 <= actual_number <= 100_000_000
        except:
            return False
    
    def convert_to_number(self, number_str: str) -> int:
        """å°‡ K/M/B æ ¼å¼è½‰æ›ç‚ºæ•¸å­—"""
        number_str = number_str.upper()
        if number_str.endswith('K'):
            return int(float(number_str[:-1]) * 1000)
        elif number_str.endswith('M'):
            return int(float(number_str[:-1]) * 1000000)
        elif number_str.endswith('B'):
            return int(float(number_str[:-1]) * 1000000000)
        else:
            return int(number_str)
    
    def extract_post_content(self, content: str) -> Optional[str]:
        """æå–è²¼æ–‡ä¸»è¦å…§å®¹"""
        lines = content.split('\n')
        
        # å°‹æ‰¾å…§å®¹é–‹å§‹ä½ç½®
        content_start = -1
        for i, line in enumerate(lines):
            if 'Markdown Content:' in line:
                content_start = i + 1
                break
        
        if content_start == -1:
            return None
        
        # æå–å‰å¹¾è¡Œä½œç‚ºä¸»è¦å…§å®¹
        content_lines = []
        for i in range(content_start, min(content_start + 10, len(lines))):
            line = lines[i].strip()
            if line and not line.startswith('[![Image') and not line.startswith('[Image'):
                content_lines.append(line)
        
        return '\n'.join(content_lines) if content_lines else None
    
    def fetch_content_local(self, url: str, use_cache: bool = True) -> tuple:
        """å¾æœ¬åœ° Reader æœå‹™ç²å–å…§å®¹"""
        headers = self.local_headers.copy()
        if not use_cache:
            headers['x-no-cache'] = 'true'
        
        try:
            response = requests.get(f"{self.local_reader_url}/{url}", headers=headers, timeout=60)
            
            if response.status_code == 200:
                return True, response.text
            else:
                return False, f"HTTP {response.status_code}"
                
        except Exception as e:
            return False, str(e)
    
    def fetch_content_official(self, url: str) -> tuple:
        """å¾å®˜æ–¹ Jina Reader API ç²å–å…§å®¹"""
        try:
            response = requests.get(f"{self.official_reader_url}/{url}", headers=self.official_headers, timeout=120)
            
            if response.status_code == 200:
                return True, response.text
            else:
                return False, f"HTTP {response.status_code}"
                
        except Exception as e:
            return False, str(e)
    
    def parse_post(self, url: str, content: str, source: str) -> Dict:
        """è§£æè²¼æ–‡å…§å®¹"""
        # æå– post_id
        post_id = url.split('/')[-1] if '/' in url else url
        
        # æå–è§€çœ‹æ•¸å’Œå…§å®¹
        views = self.extract_views_count(content, post_id)
        main_content = self.extract_post_content(content)
        
        return {
            'post_id': post_id,
            'url': url,
            'views': views,
            'content': main_content,
            'source': source,
            'success': views is not None and main_content is not None,
            'has_views': views is not None,
            'has_content': main_content is not None,
            'content_length': len(content)
        }
    
    def process_single_url(self, url: str, request_id: str) -> Dict:
        """è™•ç†å–®å€‹ URLï¼ˆç¬¬ä¸€è¼ªä¸¦è¡Œè™•ç†ï¼‰"""
        success, content = self.fetch_content_local(url, use_cache=True)
        
        if success:
            result = self.parse_post(url, content, "æœ¬åœ°-ç¬¬ä¸€è¼ª")
            result['request_id'] = request_id
            return result
        else:
            return {
                'post_id': url.split('/')[-1],
                'url': url,
                'request_id': request_id,
                'success': False,
                'error': content,
                'source': "æœ¬åœ°-ç¬¬ä¸€è¼ª"
            }
    
    def retry_local(self, url: str, request_id: str) -> Dict:
        """æœ¬åœ°é‡è©¦ï¼ˆç¬¬äºŒè¼ªï¼Œç„¡ç·©å­˜ï¼‰"""
        success, content = self.fetch_content_local(url, use_cache=False)
        
        if success:
            result = self.parse_post(url, content, "æœ¬åœ°-é‡è©¦")
            result['request_id'] = request_id
            return result
        else:
            return {
                'post_id': url.split('/')[-1],
                'url': url,
                'request_id': request_id,
                'success': False,
                'error': content,
                'source': "æœ¬åœ°-é‡è©¦"
            }
    
    def retry_official(self, url: str, request_id: str) -> Dict:
        """å®˜æ–¹APIé‡è©¦ï¼ˆç¬¬ä¸‰è¼ªï¼‰"""
        success, content = self.fetch_content_official(url)
        
        if success:
            result = self.parse_post(url, content, "å®˜æ–¹API")
            result['request_id'] = request_id
            return result
        else:
            return {
                'post_id': url.split('/')[-1],
                'url': url,
                'request_id': request_id,
                'success': False,
                'error': content,
                'source': "å®˜æ–¹API"
            }
    
    def three_stage_processing(self, urls: List[str], max_workers: int = 3) -> List[Dict]:
        """ä¸‰éšæ®µè™•ç†æµç¨‹"""
        total_start_time = time.time()
        all_results = []
        
        print(f"ğŸš€ é–‹å§‹ä¸‰éšæ®µè™•ç† {len(urls)} å€‹ URL")
        print("=" * 80)
        
        # === ç¬¬ä¸€éšæ®µï¼šä¸¦è¡Œè™•ç† ===
        print(f"\nâš¡ (1/3) ç¬¬ä¸€è¼ªä¸¦è¡Œè™•ç† ({max_workers} ä½µç™¼)")
        print("-" * 60)
        
        stage1_start = time.time()
        stage1_results = []
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = []
            for i, url in enumerate(urls):
                future = executor.submit(self.process_single_url, url, f"ä¸¦è¡Œ-{i+1}")
                futures.append(future)
            
            completed = 0
            for future in as_completed(futures):
                result = future.result()
                stage1_results.append(result)
                completed += 1
                
                # é¡¯ç¤ºé€²åº¦
                progress = completed / len(urls) * 100
                status = f"âœ… {result.get('views', 'âŒ')}" if result.get('success') else f"âŒ {result.get('error', 'å¤±æ•—')[:20]}"
                print(f"ğŸ“Š {completed}/{len(urls)} ({progress:.1f}%) | {result['request_id']} | {status}")
        
        stage1_end = time.time()
        stage1_successful = [r for r in stage1_results if r.get('success') and r.get('has_views')]
        
        print(f"\nğŸ¯ ç¬¬ä¸€è¼ªçµæœ: {len(stage1_successful)}/{len(urls)} æˆåŠŸ ({len(stage1_successful)/len(urls)*100:.1f}%)")
        print(f"â±ï¸ ç¬¬ä¸€è¼ªè€—æ™‚: {stage1_end - stage1_start:.1f}s")
        
        all_results.extend(stage1_results)
        
        # === ç¬¬äºŒéšæ®µï¼šæœ¬åœ°é‡è©¦ ===
        failed_or_incomplete = [r for r in stage1_results if not (r.get('success') and r.get('has_views') and r.get('has_content'))]
        
        if failed_or_incomplete:
            print(f"\nğŸ”„ (2/3) ç¬¬äºŒè¼ªæœ¬åœ°é‡è©¦ ({len(failed_or_incomplete)} å€‹é …ç›®)")
            print("-" * 60)
            
            stage2_results = []
            for i, failed_result in enumerate(failed_or_incomplete):
                print(f"   ğŸ”„ æœ¬åœ°é‡è©¦ {i+1}/{len(failed_or_incomplete)}: {failed_result['post_id']}")
                
                result = self.retry_local(failed_result['url'], f"é‡è©¦-{i+1}")
                stage2_results.append(result)
                
                # å°å»¶é²é¿å…éæ–¼é »ç¹
                if i < len(failed_or_incomplete) - 1:
                    time.sleep(1)
            
            stage2_successful = [r for r in stage2_results if r.get('success') and r.get('has_views')]
            print(f"ğŸ¯ ç¬¬äºŒè¼ªæ–°å¢æˆåŠŸ: {len(stage2_successful)} å€‹")
            
            all_results.extend(stage2_results)
        
        # === ç¬¬ä¸‰éšæ®µï¼šå®˜æ–¹APIé‡è©¦ ===
        all_failed = [r for r in all_results if not (r.get('success') and r.get('has_views') and r.get('has_content'))]
        unique_failed_urls = list(set([r['url'] for r in all_failed]))
        
        if unique_failed_urls:
            print(f"\nğŸŒ (3/3) ç¬¬ä¸‰è¼ªå®˜æ–¹APIé‡è©¦ ({len(unique_failed_urls)} å€‹é …ç›®)")
            print("-" * 60)
            
            stage3_results = []
            for i, url in enumerate(unique_failed_urls):
                post_id = url.split('/')[-1]
                print(f"   ğŸŒ å®˜æ–¹APIé‡è©¦ {i+1}/{len(unique_failed_urls)}: {post_id}")
                
                result = self.retry_official(url, f"å®˜æ–¹-{i+1}")
                stage3_results.append(result)
                
                # ç¨é•·å»¶é²é¿å…è§¸ç™¼å®˜æ–¹APIé™åˆ¶
                if i < len(unique_failed_urls) - 1:
                    time.sleep(2)
            
            stage3_successful = [r for r in stage3_results if r.get('success') and r.get('has_views')]
            print(f"ğŸ¯ ç¬¬ä¸‰è¼ªæ–°å¢æˆåŠŸ: {len(stage3_successful)} å€‹")
            
            all_results.extend(stage3_results)
        
        # === çµ±è¨ˆæœ€çµ‚çµæœ ===
        total_end_time = time.time()
        final_successful = self.get_best_results(all_results, urls)
        
        print("\n" + "=" * 80)
        print("ğŸ ä¸‰éšæ®µè™•ç†å®Œæˆ")
        print("=" * 80)
        
        success_count = len([r for r in final_successful if r.get('success') and r.get('has_views')])
        print(f"âœ… æœ€çµ‚æˆåŠŸ: {success_count}/{len(urls)} ({success_count/len(urls)*100:.1f}%)")
        print(f"â±ï¸ ç¸½è€—æ™‚: {total_end_time - total_start_time:.1f}s")
        print(f"ğŸï¸ å¹³å‡é€Ÿåº¦: {len(urls)/(total_end_time - total_start_time):.2f} URL/s")
        
        return final_successful
    
    def get_best_results(self, all_results: List[Dict], original_urls: List[str]) -> List[Dict]:
        """ç²å–æ¯å€‹ URL çš„æœ€ä½³çµæœ"""
        best_results = {}
        
        for result in all_results:
            url = result['url']
            
            # å¦‚æœé‚„æ²’æœ‰çµæœï¼Œæˆ–æ–°çµæœæ›´å¥½ï¼Œå°±æ›´æ–°
            if url not in best_results or self.is_better_result(result, best_results[url]):
                best_results[url] = result
        
        # æŒ‰åŸå§‹é †åºè¿”å›
        return [best_results.get(url, {'url': url, 'success': False}) for url in original_urls]
    
    def is_better_result(self, new_result: Dict, old_result: Dict) -> bool:
        """åˆ¤æ–·æ–°çµæœæ˜¯å¦æ¯”èˆŠçµæœæ›´å¥½"""
        new_score = 0
        old_score = 0
        
        # æœ‰è§€çœ‹æ•¸ +10åˆ†
        if new_result.get('has_views'):
            new_score += 10
        if old_result.get('has_views'):
            old_score += 10
        
        # æœ‰å…§å®¹ +5åˆ†
        if new_result.get('has_content'):
            new_score += 5
        if old_result.get('has_content'):
            old_score += 5
        
        # æˆåŠŸ +1åˆ†
        if new_result.get('success'):
            new_score += 1
        if old_result.get('success'):
            old_score += 1
        
        return new_score > old_score

def load_urls_from_json(file_path: str) -> List[str]:
    """å¾JSONæª”æ¡ˆä¸­æå–æ‰€æœ‰è²¼æ–‡URL (åƒè€ƒ final ç‰ˆæœ¬)"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        urls = [post['url'] for post in data.get('posts', []) if 'url' in post]
        print(f"âœ… å¾ {file_path} æˆåŠŸæå– {len(urls)} å€‹ URLã€‚")
        return urls
    except Exception as e:
        print(f"âŒ æå– URL æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return []

def save_results(results: List[Dict], filename: str = None):
    """ä¿å­˜çµæœåˆ° JSON æ–‡ä»¶"""
    if filename is None:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"complete_reader_results_{timestamp}.json"
    
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump({
                'timestamp': datetime.now().isoformat(),
                'total_urls': len(results),
                'successful_extractions': len([r for r in results if r.get('success') and r.get('has_views')]),
                'results': results
            }, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ çµæœå·²ä¿å­˜åˆ°: {filename}")
        
    except Exception as e:
        print(f"âŒ ä¿å­˜çµæœå¤±æ•—: {e}")

def main():
    """ä¸»ç¨‹å¼"""
    print("ğŸ¯ å®Œæ•´ç‰ˆä¸‰éšæ®µ Threads Reader è§£æå™¨")
    print("æ•´åˆä¿®æ­£: NBSPè™•ç† + Headersé…ç½® + å¢å¼·è§€çœ‹æ•¸æå– + ä¸‰éšæ®µé‡è©¦")
    print("=" * 80)
    
    # è¼‰å…¥ URL åˆ—è¡¨
    json_file = "agents/playwright_crawler/debug/crawl_data_20250803_121452_934d52b1.json"
    urls = load_urls_from_json(json_file)
    
    if not urls:
        print("âŒ æ²’æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„ URLï¼Œç¨‹å¼çµæŸ")
        return
    
    # å‰µå»ºè§£æå™¨ä¸¦é–‹å§‹è™•ç†
    parser = CompleteThreadsReaderParser()
    results = parser.three_stage_processing(urls, max_workers=3)
    
    # ä¿å­˜çµæœ
    save_results(results)
    
    # é¡¯ç¤ºæˆåŠŸæ¡ˆä¾‹
    successful = [r for r in results if r.get('success') and r.get('has_views')]
    if successful:
        print(f"\nğŸ¯ æˆåŠŸæå–è§€çœ‹æ•¸çš„è²¼æ–‡:")
        for r in successful:
            print(f"   âœ… {r['post_id']}: {r['views']} ({r['source']})")
    
    print(f"\nğŸ‰ è™•ç†å®Œæˆï¼å…± {len(successful)}/{len(urls)} å€‹æˆåŠŸ")

if __name__ == '__main__':
    main()