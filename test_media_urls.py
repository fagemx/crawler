#!/usr/bin/env python3
"""
å°ˆé–€æ¸¬è©¦åœ–ç‰‡å’Œå½±ç‰‡ URL æŠ“å–åŠŸèƒ½
"""

import asyncio
import json
import logging
import sys
import os
from pathlib import Path

# è¨­å®šæ—¥èªŒ
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# è·¯å¾‘è¨­å®š
project_root = os.path.abspath(os.path.dirname(__file__))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

try:
    from agents.playwright_crawler.playwright_logic import PlaywrightLogic
    from common.models import PostMetricsBatch
except ModuleNotFoundError as e:
    logging.error(f"âŒ æ¨¡çµ„å°å…¥å¤±æ•—: {e}")
    sys.exit(1)

# æ¸¬è©¦åƒæ•¸ - é¸æ“‡ä¸€å€‹é€šå¸¸æœ‰åœ–ç‰‡/å½±ç‰‡çš„å¸³è™Ÿ
TARGET_USERNAME = "natgeo"  # National Geographic é€šå¸¸æœ‰å¾ˆå¤šåœ–ç‰‡å’Œå½±ç‰‡
MAX_POSTS_TO_CRAWL = 5
AUTH_FILE_PATH = Path(project_root) / "agents" / "playwright_crawler" / "auth.json"

async def test_media_urls():
    """æ¸¬è©¦åœ–ç‰‡å’Œå½±ç‰‡ URL æŠ“å–"""
    print("ğŸ§ª === æ¸¬è©¦åœ–ç‰‡å’Œå½±ç‰‡ URL æŠ“å–åŠŸèƒ½ ===")

    # æª¢æŸ¥èªè­‰æª”æ¡ˆ
    if not AUTH_FILE_PATH.exists():
        print(f"âŒ æ‰¾ä¸åˆ°èªè­‰æª”æ¡ˆ {AUTH_FILE_PATH}")
        return
        
    try:
        with open(AUTH_FILE_PATH, 'r', encoding='utf-8') as f:
            auth_json_content = json.load(f)
        print(f"âœ… æˆåŠŸè®€å–èªè­‰æª”æ¡ˆ")
    except Exception as e:
        print(f"âŒ è®€å–èªè­‰æª”æ¡ˆå¤±æ•—: {e}")
        return

    # åˆå§‹åŒ–çˆ¬èŸ²
    crawler = PlaywrightLogic()
    print("âœ… PlaywrightLogic åˆå§‹åŒ–å®Œæˆ")

    # åŸ·è¡Œçˆ¬å–
    print(f"ğŸš€ é–‹å§‹æ¸¬è©¦ '{TARGET_USERNAME}' çš„åª’é«” URL æŠ“å–...")
    
    try:
        result_batch: PostMetricsBatch = await crawler.fetch_posts(
            username=TARGET_USERNAME,
            max_posts=MAX_POSTS_TO_CRAWL,
            auth_json_content=auth_json_content,
            task_id="test_media_urls"
        )
        
        print("\nâœ… === çˆ¬å–å®Œæˆ ===")
        
        if not result_batch or not result_batch.posts:
            print("âŒ çµæœç‚ºç©ºï¼Œæ²’æœ‰çˆ¬å–åˆ°ä»»ä½•è²¼æ–‡")
            return
            
        print(f"ğŸ“Š å…±çˆ¬å–åˆ° {len(result_batch.posts)} ç¯‡è²¼æ–‡")
        
        # æª¢æŸ¥åª’é«” URL æƒ…æ³
        print("\nğŸ–¼ï¸ === åª’é«” URL æª¢æŸ¥ ===")
        total_images = 0
        total_videos = 0
        posts_with_media = 0
        
        for i, post in enumerate(result_batch.posts, 1):
            print(f"\nè²¼æ–‡ {i}: {post.url.split('/')[-1]}")
            print(f"  å…§å®¹: {post.content[:50]}..." if post.content else "  å…§å®¹: (ç„¡)")
            print(f"  è§€çœ‹æ•¸: {post.views_count}")
            
            # æª¢æŸ¥åœ–ç‰‡
            if post.images:
                print(f"  ğŸ“· åœ–ç‰‡ ({len(post.images)} å¼µ):")
                for j, img_url in enumerate(post.images, 1):
                    print(f"    {j}. {img_url[:80]}...")
                total_images += len(post.images)
            else:
                print("  ğŸ“· åœ–ç‰‡: ç„¡")
            
            # æª¢æŸ¥å½±ç‰‡
            if post.videos:
                print(f"  ğŸ¬ å½±ç‰‡ ({len(post.videos)} å€‹):")
                for j, vid_url in enumerate(post.videos, 1):
                    print(f"    {j}. {vid_url[:80]}...")
                total_videos += len(post.videos)
            else:
                print("  ğŸ¬ å½±ç‰‡: ç„¡")
            
            if post.images or post.videos:
                posts_with_media += 1
        
        # çµ±è¨ˆçµæœ
        print(f"\nğŸ“Š === åª’é«”çµ±è¨ˆ ===")
        print(f"ç¸½åœ–ç‰‡æ•¸: {total_images}")
        print(f"ç¸½å½±ç‰‡æ•¸: {total_videos}")
        print(f"å«åª’é«”çš„è²¼æ–‡: {posts_with_media}/{len(result_batch.posts)}")
        
        if total_images > 0 or total_videos > 0:
            print("ğŸ‰ åª’é«” URL æŠ“å–åŠŸèƒ½æ­£å¸¸å·¥ä½œï¼")
        else:
            print("âš ï¸ æ²’æœ‰æŠ“å–åˆ°ä»»ä½•åª’é«” URLï¼Œå¯èƒ½çš„åŸå› ï¼š")
            print("  1. é¸æ“‡çš„è²¼æ–‡æœ¬èº«æ²’æœ‰åœ–ç‰‡/å½±ç‰‡")
            print("  2. GraphQL API çµæ§‹è®ŠåŒ–")
            print("  3. è§£æé‚è¼¯éœ€è¦èª¿æ•´")
            
        # æª¢æŸ¥æœ€æ–°çš„ debug æ–‡ä»¶
        debug_dir = Path(project_root) / "agents" / "playwright_crawler" / "debug"
        debug_files = list(debug_dir.glob("crawl_data_*_test_med.json"))
        if debug_files:
            latest_debug = max(debug_files, key=lambda f: f.stat().st_mtime)
            print(f"\nğŸ“ æœ€æ–° debug æ–‡ä»¶: {latest_debug.name}")
            
            # æª¢æŸ¥ debug æ–‡ä»¶ä¸­çš„åª’é«”æ•¸æ“š
            try:
                with open(latest_debug, 'r', encoding='utf-8') as f:
                    debug_data = json.load(f)
                
                debug_images = 0
                debug_videos = 0
                for post in debug_data.get("posts", []):
                    debug_images += len(post.get("images", []))
                    debug_videos += len(post.get("videos", []))
                
                print(f"ğŸ“„ Debug æ–‡ä»¶ä¸­çš„åª’é«”æ•¸æ“š:")
                print(f"  åœ–ç‰‡: {debug_images} å¼µ")
                print(f"  å½±ç‰‡: {debug_videos} å€‹")
                
            except Exception as e:
                print(f"âš ï¸ è®€å– debug æ–‡ä»¶å¤±æ•—: {e}")
            
    except Exception as e:
        logging.error(f"âŒ æ¸¬è©¦éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)

if __name__ == "__main__":
    asyncio.run(test_media_urls())