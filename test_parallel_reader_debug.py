#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¤šç·šç¨‹Readerè§£æè…³æœ¬ (V15 - å®Œæ•´èª¿è©¦ç‰ˆ)
ä¿å­˜ç¬¬ä¸€è¼ªæ‰€æœ‰å…§å®¹ä»¥åˆ†æå¤±æ•—åŸå› 
"""

import json
import re
import requests
import time
import subprocess
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Dict, Optional, List

class DebugThreadsReaderParser:
    """
    èª¿è©¦ç‰ˆ Threads Reader è§£æå™¨
    é‡é»ï¼šä¿å­˜æ‰€æœ‰åŸå§‹å…§å®¹é€²è¡Œåˆ†æ
    """
    
    def __init__(self, backend_instances: int = 4):
        self.backend_instances = backend_instances
        self.lb_url = "http://localhost:8880"
        self.official_reader_url = "https://r.jina.ai"
        
        # NBSP å­—ç¬¦å¸¸é‡
        self.NBSP = "\u00A0"
        
        # --- ä¿®æ­£å¾Œçš„è§€çœ‹æ•¸æå–æ¨¡å¼ ---
        self.view_patterns = [
            # ä¸»è¦æ¨¡å¼ - ä¿®æ­£ NBSP å•é¡Œ
            re.compile(rf'\[Thread[\s{self.NBSP}=]*?(\d+(?:[\.,]\d+)?[KMB]?)\s*views\]', re.IGNORECASE),
            
            # å‚™ç”¨æ¨¡å¼ - è™•ç†åˆ†è¡Œæƒ…æ³
            re.compile(rf'Thread[\s{self.NBSP}=]*?(\d+(?:[\.,]\d+)?[KMB]?)[\s{self.NBSP}]*views', re.IGNORECASE | re.MULTILINE),
            
            # é€šç”¨æ¨¡å¼
            re.compile(r'(\d+(?:[\.,]\d+)?[KMB]?)\s*views?', re.IGNORECASE),
            
            # å®¹éŒ¯æ¨¡å¼
            re.compile(r'(\d+(?:[\.,]\d+)?[KMB]?)\s*view(?:s|ing)', re.IGNORECASE),
            re.compile(r'views?\s*[:\-]\s*(\d+(?:[\.,]\d+)?[KMB]?)', re.IGNORECASE),
        ]
        
        self.engagement_pattern = re.compile(r'^\d+(?:\.\d+)?[KMB]?$')
        
        # --- å„ªåŒ–çš„Sessioné…ç½® ---
        self.session = requests.Session()
        adapter = requests.adapters.HTTPAdapter(
            pool_connections=20,
            pool_maxsize=30,
            max_retries=0
        )
        self.session.mount('http://', adapter)
        self.session.mount('https://', adapter)
        
        print(f"ğŸ”§ èª¿è©¦ç‰ˆåˆå§‹åŒ–å®Œæˆ")
        print(f"   å¾Œç«¯å¯¦ä¾‹: {backend_instances} å€‹")
        print(f"   è§€çœ‹æ•¸æ¨¡å¼: {len(self.view_patterns)} å€‹ (å«NBSPä¿®æ­£)")

    def normalize_content(self, text: str) -> str:
        """å…§å®¹æ¨™æº–åŒ– - è™•ç†å„ç¨®ç©ºæ ¼å’Œæ›è¡Œå•é¡Œ"""
        # â‘  å°‡NBSPã€å…¨å½¢ç©ºæ ¼è½‰ç‚ºæ™®é€šç©ºæ ¼
        text = text.replace(self.NBSP, " ").replace("\u3000", " ")
        
        # â‘¡ çµ±ä¸€æ›è¡Œç¬¦
        text = text.replace("\r\n", "\n").replace("\r", "\n")
        
        # â‘¢ å£“ç¸®å¤šå€‹é€£çºŒç©ºæ ¼ï¼ˆä½†ä¿ç•™å–®å€‹ç©ºæ ¼ï¼‰
        text = re.sub(r"[ \t]{2,}", " ", text)
        
        return text

    def analyze_views_extraction_detailed(self, content: str, post_id: str) -> dict:
        """è©³ç´°åˆ†æè§€çœ‹æ•¸æå–å¤±æ•—çš„åŸå› """
        
        # æ¨™æº–åŒ–å…§å®¹
        normalized_content = self.normalize_content(content)
        
        analysis = {
            'post_id': post_id,
            'content_length': len(content),
            'normalized_length': len(normalized_content),
            'pattern_results': [],
            'first_10_lines_raw': [],
            'first_10_lines_normalized': [],
            'lines_with_thread': [],
            'lines_with_view': [],
            'all_numbers_found': [],
            'potential_views': []
        }
        
        # ä¿å­˜å‰10è¡ŒåŸå§‹å…§å®¹
        raw_lines = content.split('\n')[:10]
        for i, line in enumerate(raw_lines):
            analysis['first_10_lines_raw'].append({
                'line_num': i + 1,
                'content': line,
                'repr': repr(line),
                'length': len(line)
            })
        
        # ä¿å­˜å‰10è¡Œæ¨™æº–åŒ–å…§å®¹
        norm_lines = normalized_content.split('\n')[:10]
        for i, line in enumerate(norm_lines):
            analysis['first_10_lines_normalized'].append({
                'line_num': i + 1,
                'content': line,
                'repr': repr(line),
                'length': len(line)
            })
        
        # æ¸¬è©¦æ¯å€‹æ¨¡å¼
        for i, pattern in enumerate(self.view_patterns):
            # åœ¨åŸå§‹å…§å®¹ä¸Šæ¸¬è©¦
            raw_matches = pattern.findall(content)
            # åœ¨æ¨™æº–åŒ–å…§å®¹ä¸Šæ¸¬è©¦
            norm_matches = pattern.findall(normalized_content)
            
            analysis['pattern_results'].append({
                'pattern_index': i,
                'pattern': pattern.pattern,
                'raw_matches': raw_matches,
                'normalized_matches': norm_matches,
                'success': len(norm_matches) > 0 or len(raw_matches) > 0
            })
        
        # æ‰¾å‡ºåŒ…å« "thread" çš„è¡Œ
        all_lines = content.split('\n')
        for i, line in enumerate(all_lines):
            if 'thread' in line.lower():
                analysis['lines_with_thread'].append({
                    'line_num': i + 1,
                    'content': line.strip(),
                    'repr': repr(line)
                })
        
        # æ‰¾å‡ºåŒ…å« "view" çš„è¡Œ
        for i, line in enumerate(all_lines):
            if 'view' in line.lower():
                analysis['lines_with_view'].append({
                    'line_num': i + 1,
                    'content': line.strip(),
                    'repr': repr(line)
                })
        
        # æ‰¾å‡ºæ‰€æœ‰æ•¸å­—
        number_pattern = re.compile(r'(\d+(?:[\.,]\d+)?[KMB]?)', re.IGNORECASE)
        all_numbers = number_pattern.findall(content)
        analysis['all_numbers_found'] = list(set(all_numbers))
        
        # æ‰¾å‡ºå¯èƒ½çš„è§€çœ‹æ•¸
        for num in analysis['all_numbers_found']:
            try:
                actual_val = self.convert_number_to_int(num)
                if actual_val >= 100:  # å¯èƒ½æ˜¯è§€çœ‹æ•¸
                    analysis['potential_views'].append({
                        'number': num,
                        'value': actual_val
                    })
            except:
                pass
        
        return analysis

    def enhanced_extract_views_count(self, markdown_content: str, post_id: str = "", debug: bool = False) -> Optional[str]:
        """å¢å¼·ç‰ˆè§€çœ‹æ•¸æå– - è¿”å›è©³ç´°åˆ†æ"""
        
        # æ¨™æº–åŒ–å…§å®¹
        normalized_content = self.normalize_content(markdown_content)
        
        # 1. å˜—è©¦æ‰€æœ‰æ¨¡å¼åœ¨æ¨™æº–åŒ–å¾Œçš„å…§å®¹ä¸Š
        for i, pattern in enumerate(self.view_patterns):
            match = pattern.search(normalized_content)
            if match:
                views_number = match.group(1)
                if self.validate_number_format(views_number):
                    return f"{views_number} views"
        
        # 2. å¦‚æœé‚„æ˜¯å¤±æ•—ï¼Œå˜—è©¦åœ¨åŸå§‹å…§å®¹ä¸Šæœç´¢
        for i, pattern in enumerate(self.view_patterns):
            match = pattern.search(markdown_content)
            if match:
                views_number = match.group(1)
                if self.validate_number_format(views_number):
                    return f"{views_number} views"
        
        return None

    def validate_number_format(self, number: str) -> bool:
        """é©—è­‰æ•¸å­—æ ¼å¼æ˜¯å¦åˆç†"""
        if not number:
            return False
        
        # åŸºæœ¬æ ¼å¼æª¢æŸ¥ - æ”¯æ´é€—è™Ÿå’Œé»ä½œç‚ºå°æ•¸åˆ†éš”ç¬¦
        pattern = re.compile(r'^\d+(?:[,\.]\d+)?[KMB]?$', re.IGNORECASE)
        if not pattern.match(number):
            return False
        
        # è½‰æ›ç‚ºå¯¦éš›æ•¸å­—é€²è¡Œåˆç†æ€§æª¢æŸ¥
        try:
            actual_number = self.convert_number_to_int(number)
            # è§€çœ‹æ•¸é€šå¸¸åœ¨ 1-100M ç¯„åœå…§
            return 1 <= actual_number <= 100_000_000
        except:
            return False

    def convert_number_to_int(self, number_str: str) -> int:
        """å°‡ K/M/B æ ¼å¼çš„æ•¸å­—è½‰æ›ç‚ºæ•´æ•¸ - æ”¯æ´é€—è™Ÿåˆ†éš”ç¬¦"""
        number_str = number_str.upper().replace(',', '.')  # çµ±ä¸€å°æ•¸åˆ†éš”ç¬¦
        
        if number_str.endswith('K'):
            return int(float(number_str[:-1]) * 1000)
        elif number_str.endswith('M'):
            return int(float(number_str[:-1]) * 1000000)
        elif number_str.endswith('B'):
            return int(float(number_str[:-1]) * 1000000000)
        else:
            return int(float(number_str))

    def convert_url_to_threads_net(self, url: str) -> str:
        """å°‡ threads.com URL è½‰æ›ç‚º threads.net"""
        return url.replace("threads.com", "threads.net")

    def fetch_content_local(self, post_url: str, use_cache: bool = True, timeout: int = 60) -> str:
        """å¾æœ¬åœ°Readeræœå‹™ç²å–å…§å®¹"""
        corrected_url = self.convert_url_to_threads_net(post_url)
        reader_url = f"{self.lb_url}/{corrected_url}"
        
        headers = {}
        if not use_cache:
            headers['x-no-cache'] = 'true'
        
        try:
            response = self.session.get(reader_url, headers=headers, timeout=(10, timeout))
            response.raise_for_status()
            return response.text
        except requests.exceptions.Timeout:
            return ""
        except requests.exceptions.RequestException:
            return ""

    def extract_post_content(self, markdown_content: str) -> Optional[str]:
        """æå–è²¼æ–‡å…§å®¹"""
        normalized_content = self.normalize_content(markdown_content)
        lines = normalized_content.split('\n')
        content_lines = []
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            if line.startswith("Title:") or line.startswith("URL Source:") or line.startswith("Markdown Content:"):
                continue
            if "===============" in line or "---" in line:
                break
            if not line.startswith("[") and not line.startswith("!") and len(line) > 10:
                content_lines.append(line)
                if len(content_lines) >= 3:
                    break
        
        return ' '.join(content_lines) if content_lines else None

    def extract_engagement_numbers(self, markdown_content: str) -> Dict[str, str]:
        """æå–äº’å‹•æ•¸æ“šåºåˆ—"""
        normalized_content = self.normalize_content(markdown_content)
        lines = normalized_content.split('\n')
        
        for i, line in enumerate(lines):
            stripped = line.strip()
            if (stripped.startswith('![Image') and 
                'profile picture' not in stripped and 
                i > 0 and any('Translate' in lines[k] for k in range(max(0, i-3), i+1))):
                
                numbers = []
                for j in range(i + 1, min(i + 15, len(lines))):
                    candidate = lines[j].strip()
                    if self.engagement_pattern.match(candidate):
                        numbers.append(candidate)
                    elif candidate and candidate not in ["Pinned", "", "Translate"]:
                        break
                
                engagement_data = {}
                if len(numbers) >= 1: engagement_data['likes'] = numbers[0]
                if len(numbers) >= 2: engagement_data['comments'] = numbers[1]  
                if len(numbers) >= 3: engagement_data['reposts'] = numbers[2]
                if len(numbers) >= 4: engagement_data['shares'] = numbers[3]
                
                if len(numbers) >= 3:
                    return engagement_data
        return {}

    def parse_post_local_debug(self, post_url: str, use_cache: bool = True, timeout: int = 60) -> Dict:
        """ä½¿ç”¨æœ¬åœ°æœå‹™è§£æ - å®Œæ•´èª¿è©¦ç‰ˆ"""
        content = self.fetch_content_local(post_url, use_cache, timeout)
        post_id = post_url.split('/')[-1]
        
        if not content:
            return {
                "url": post_url,
                "post_id": post_id,
                "error": "ç„¡æ³•ç²å–å…§å®¹",
                "raw_content": "",
                "analysis": None
            }
        
        engagement = self.extract_engagement_numbers(content)
        extracted_content = self.extract_post_content(content)
        extracted_views = self.enhanced_extract_views_count(content, post_id)
        
        # å¦‚æœè§€çœ‹æ•¸æå–å¤±æ•—ï¼Œé€²è¡Œè©³ç´°åˆ†æ
        analysis = None
        if not extracted_views:
            analysis = self.analyze_views_extraction_detailed(content, post_id)
        
        return {
            "url": post_url,
            "post_id": post_id,
            "content": extracted_content,
            "views": extracted_views,
            "likes": engagement.get('likes'),
            "comments": engagement.get('comments'),
            "reposts": engagement.get('reposts'),
            "shares": engagement.get('shares'),
            "raw_length": len(content),
            "raw_content": content,  # ä¿å­˜å®Œæ•´åŸå§‹å…§å®¹
            "analysis": analysis,    # å¤±æ•—åˆ†æï¼ˆåƒ…åœ¨å¤±æ•—æ™‚æœ‰å€¼ï¼‰
            "success": bool(extracted_views and extracted_content),
            "error": None
        }

def load_urls_from_file(file_path: str) -> List[str]:
    """å¾JSONæª”æ¡ˆä¸­æå–æ‰€æœ‰è²¼æ–‡URL"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        urls = [post['url'] for post in data.get('posts', []) if 'url' in post]
        print(f"âœ… å¾ {file_path} æˆåŠŸæå– {len(urls)} å€‹ URLã€‚")
        return urls
    except Exception as e:
        print(f"âŒ æå– URL æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return []

def detect_reader_instances():
    """æª¢æ¸¬Readerå¯¦ä¾‹æ•¸é‡"""
    try:
        result = subprocess.run([
            'docker', 'ps', 
            '--filter', 'name=reader', 
            '--filter', 'status=running',
            '--format', '{{.Names}}'
        ], capture_output=True, text=True, timeout=10)
        
        if result.returncode == 0:
            containers = result.stdout.strip().split('\n')
            reader_containers = [name for name in containers if 'reader-' in name and 'lb' not in name and name.strip()]
            return len(reader_containers)
        return 4  # é è¨­å€¼
    except:
        return 4  # é è¨­å€¼

def main():
    """ä¸»å‡½æ•¸ï¼šå®Œæ•´èª¿è©¦ç‰ˆ"""
    json_file_path = 'agents/playwright_crawler/debug/crawl_data_20250803_121452_934d52b1.json'
    
    # --- æª¢æ¸¬é…ç½® ---
    backend_instances = detect_reader_instances()
    max_workers = backend_instances * 2  # ä¿å®ˆä½µç™¼
    
    urls_to_process = load_urls_from_file(json_file_path)
    if not urls_to_process:
        return

    total_urls = len(urls_to_process)
    parser = DebugThreadsReaderParser(backend_instances)
    results = []
    
    start_time = time.time()
    print(f"\nğŸš€ å®Œæ•´èª¿è©¦ç‰ˆå•Ÿå‹•ï¼ç›®æ¨™ï¼šåˆ†ææ‰€æœ‰å¤±æ•—åŸå› ")
    print(f"ğŸ“Š é…ç½®: {backend_instances}å€‹å¯¦ä¾‹, ä½µç™¼æ•¸: {max_workers}")
    print("ğŸ¯ é‡é»: âœ…ä¿å­˜æ‰€æœ‰å…§å®¹ âœ…è©³ç´°å¤±æ•—åˆ†æ âœ…å®Œæ•´èª¿è©¦ä¿¡æ¯")

    # --- ç¬¬ä¸€å±¤: å¹³è¡Œè™•ç†ä¸¦ä¿å­˜æ‰€æœ‰å…§å®¹ ---
    print(f"\nâš¡ å®Œæ•´èª¿è©¦ä½µç™¼è™•ç† {total_urls} å€‹ URL...")
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_url = {executor.submit(parser.parse_post_local_debug, url, True, 45): url for url in urls_to_process}
        
        completed = 0
        for future in as_completed(future_to_url):
            url = future_to_url[future]
            completed += 1
            progress = completed / total_urls
            bar_length = 40
            filled_length = int(bar_length * progress)
            bar = 'â–ˆ' * filled_length + '-' * (bar_length - filled_length)
            
            try:
                result = future.result()
                results.append(result)
                
                # å³æ™‚ç‹€æ…‹é¡¯ç¤º
                status = "âœ…" if result['success'] else "âŒ"
                post_id = result['post_id']
                print(f'\ré€²åº¦: |{bar}| {completed}/{total_urls} ({progress:.1%}) {status} {post_id}', end='', flush=True)
            except Exception as e:
                error_result = {
                    "url": url,
                    "post_id": url.split('/')[-1],
                    "error": f"åŸ·è¡Œç·’ç•°å¸¸: {str(e)}",
                    "success": False
                }
                results.append(error_result)
                print(f'\ré€²åº¦: |{bar}| {completed}/{total_urls} ({progress:.1%}) âŒ {url.split("/")[-1]}', end='', flush=True)

    end_time = time.time()
    total_time = end_time - start_time
    
    # çµ±è¨ˆçµæœ
    successful = [r for r in results if r['success']]
    failed = [r for r in results if not r['success']]
    
    print(f"\n\nğŸ¯ å®Œæ•´èª¿è©¦ç‰ˆçµæœ:")
    print(f"   ç¸½è™•ç†æ•¸é‡: {total_urls}")
    print(f"   æˆåŠŸ: {len(successful)} ({len(successful)/total_urls*100:.1f}%)")
    print(f"   å¤±æ•—: {len(failed)} ({len(failed)/total_urls*100:.1f}%)")
    print(f"   ç¸½è€—æ™‚: {total_time:.1f}s")
    print(f"   å¹³å‡é€Ÿåº¦: {total_urls/total_time:.2f} URL/ç§’")

    # ä¿å­˜å®Œæ•´çµæœ
    output_filename = 'parallel_reader_debug_results.json'
    with open(output_filename, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    print(f"\nğŸ’¾ å®Œæ•´èª¿è©¦çµæœå·²ä¿å­˜åˆ°: {output_filename}")
    
    # ä¿å­˜å¤±æ•—æ¡ˆä¾‹çš„è©³ç´°åˆ†æ
    if failed:
        failed_analysis = []
        for item in failed:
            if item.get('analysis'):
                failed_analysis.append(item['analysis'])
        
        if failed_analysis:
            analysis_filename = 'failed_views_analysis.json'
            with open(analysis_filename, 'w', encoding='utf-8') as f:
                json.dump(failed_analysis, f, ensure_ascii=False, indent=2)
            print(f"ğŸ” å¤±æ•—æ¡ˆä¾‹è©³ç´°åˆ†æå·²ä¿å­˜åˆ°: {analysis_filename}")
    
    # é¡¯ç¤ºå¤±æ•—æ¡ˆä¾‹æ‘˜è¦
    print(f"\nğŸ“‹ å¤±æ•—æ¡ˆä¾‹æ‘˜è¦:")
    for item in failed:
        post_id = item['post_id']
        if item.get('analysis'):
            analysis = item['analysis']
            potential_views = analysis.get('potential_views', [])
            print(f"   âŒ {post_id}: å…§å®¹é•·åº¦={analysis['content_length']}, æ½›åœ¨è§€çœ‹æ•¸={len(potential_views)}")
            if potential_views:
                for pv in potential_views[:3]:  # åªé¡¯ç¤ºå‰3å€‹
                    print(f"      - {pv['number']} ({pv['value']:,})")
        else:
            error = item.get('error', 'æœªçŸ¥éŒ¯èª¤')
            print(f"   âŒ {post_id}: {error}")
    
    print("\n" + "="*80)
    print("ğŸ” å®Œæ•´èª¿è©¦ç‰ˆåŸ·è¡Œå®Œç•¢ï¼")
    print(f"ğŸ“‚ è«‹æª¢æŸ¥ä»¥ä¸‹æª”æ¡ˆé€²è¡Œæ·±å…¥åˆ†æ:")
    print(f"   â€¢ {output_filename} - å®Œæ•´çµæœï¼ˆåŒ…å«æ‰€æœ‰åŸå§‹å…§å®¹ï¼‰")
    if failed:
        print(f"   â€¢ failed_views_analysis.json - å¤±æ•—æ¡ˆä¾‹è©³ç´°åˆ†æ")
    print("="*80)

if __name__ == "__main__":
    main()