#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
APIå„ªå…ˆå‹•æ…‹ç®¡ç·š - é€Ÿç‡é™åˆ¶ç‰ˆæœ¬
æ¯åˆ†é˜æœ€å¤š60å€‹è«‹æ±‚ï¼Œæ¸¬è©¦100å€‹URLï¼ˆå¾17å€‹åŸå§‹URLæ“´å±•ï¼‰
"""

import json
import re
import requests
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
from typing import Dict, Optional, List
import random

class RateLimitedApiFirstReader:
    """
    APIå„ªå…ˆç®¡ç·š - é€Ÿç‡é™åˆ¶ç‰ˆæœ¬
    é™åˆ¶ï¼šæ¯åˆ†é˜æœ€å¤š60å€‹è«‹æ±‚
    """
    
    def __init__(self):
        self.local_reader_url = "http://localhost:8880"
        self.official_reader_url = "https://r.jina.ai"
        
        # é€Ÿç‡é™åˆ¶è¨­å®šï¼šæ¯åˆ†é˜60å€‹è«‹æ±‚ = æ¯ç§’1å€‹è«‹æ±‚
        self.max_requests_per_minute = 60
        self.min_interval_between_requests = 60.0 / self.max_requests_per_minute  # 1.0ç§’
        self.last_request_time = 0
        
        self.NBSP = "\u00A0"
        
        # å®˜æ–¹API headers (ä¸»è¦ç­–ç•¥)
        self.official_headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36',
            'X-Return-Format': 'markdown'
        }
        
        # æœ¬åœ° headers (å‚™ç”¨ç­–ç•¥)
        self.local_headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36',
            'x-wait-for-selector': 'article',
            'x-timeout': '25'
        }
        
        # è§€çœ‹æ•¸æå–æ¨¡å¼
        self.view_patterns = [
            re.compile(rf'\[Thread[\s{self.NBSP}=]*?(\d+(?:[\.,]\d+)?[KMB]?)\s*views\]', re.IGNORECASE),
            re.compile(rf'Thread[\s{self.NBSP}=]*?(\d+(?:[\.,]\d+)?[KMB]?)[\s{self.NBSP}]*views', re.IGNORECASE | re.MULTILINE),
            re.compile(r'(\d+(?:[\.,]\d+)?[KMB]?)\s*views?', re.IGNORECASE),
            re.compile(r'(\d+(?:[\.,]\d+)?[KMB]?)\s*view(?:s|ing)', re.IGNORECASE),
            re.compile(r'views?\s*[:\-]\s*(\d+(?:[\.,]\d+)?[KMB]?)', re.IGNORECASE),
        ]
    
    def normalize_content(self, text: str) -> str:
        """å…§å®¹æ¨™æº–åŒ–"""
        text = text.replace(self.NBSP, " ").replace("\u2002", " ").replace("\u2003", " ")
        text = text.replace("\u2009", " ").replace("\u200A", " ").replace("\u3000", " ").replace("\t", " ")
        text = text.replace("\r\n", "\n").replace("\r", "\n")
        text = re.sub(r"[ \t]{2,}", " ", text)
        return text

    def extract_views_count(self, markdown_content: str, post_id: str = "") -> Optional[str]:
        """è§€çœ‹æ•¸æå–"""
        normalized_content = self.normalize_content(markdown_content)
        
        for pattern in self.view_patterns:
            match = pattern.search(normalized_content)
            if match:
                views_number = match.group(1)
                if self.validate_views_format(views_number):
                    return views_number
        
        for pattern in self.view_patterns:
            match = pattern.search(markdown_content)
            if match:
                views_number = match.group(1)
                if self.validate_views_format(views_number):
                    return views_number
        return None

    def validate_views_format(self, views: str) -> bool:
        """é©—è­‰è§€çœ‹æ•¸æ ¼å¼"""
        if not views: return False
        pattern = re.compile(r'^\d+(?:\.\d+)?[KMB]?$', re.IGNORECASE)
        if not pattern.match(views): return False
        try:
            actual_number = self.convert_to_number(views)
            return 1 <= actual_number <= 100_000_000
        except: return False
    
    def convert_to_number(self, number_str: str) -> int:
        """K/M/Bè½‰æ•¸å­—"""
        number_str = number_str.upper()
        if number_str.endswith('K'): return int(float(number_str[:-1]) * 1000)
        elif number_str.endswith('M'): return int(float(number_str[:-1]) * 1000000)
        elif number_str.endswith('B'): return int(float(number_str[:-1]) * 1000000000)
        else: return int(number_str)

    def extract_post_content(self, content: str) -> Optional[str]:
        """æå–è²¼æ–‡å…§å®¹"""
        lines = content.split('\n')
        content_start = -1
        for i, line in enumerate(lines):
            if 'Markdown Content:' in line:
                content_start = i + 1
                break
        if content_start == -1: return None
        
        content_lines = []
        for i in range(content_start, min(content_start + 10, len(lines))):
            line = lines[i].strip()
            if line and not line.startswith('[![Image') and not line.startswith('[Image'):
                content_lines.append(line)
        return '\n'.join(content_lines) if content_lines else None

    def wait_for_rate_limit(self):
        """ç­‰å¾…é€Ÿç‡é™åˆ¶"""
        current_time = time.time()
        time_since_last = current_time - self.last_request_time
        if time_since_last < self.min_interval_between_requests:
            sleep_time = self.min_interval_between_requests - time_since_last
            time.sleep(sleep_time)
        self.last_request_time = time.time()

    def fetch_content_official_rate_limited(self, url: str) -> tuple:
        """é€Ÿç‡é™åˆ¶çš„å®˜æ–¹APIè«‹æ±‚"""
        self.wait_for_rate_limit()
        try:
            response = requests.get(f"{self.official_reader_url}/{url}", headers=self.official_headers, timeout=60)
            if response.status_code == 200: return True, response.text
            elif response.status_code == 429: return False, "RATE_LIMITED"
            elif response.status_code == 403: return False, "BLOCKED"
            else: return False, f"HTTP {response.status_code}"
        except requests.exceptions.Timeout: return False, "TIMEOUT"
        except Exception as e: return False, str(e)

    def fetch_content_local(self, url: str, use_cache: bool = True) -> tuple:
        """æœ¬åœ°Readerè«‹æ±‚"""
        headers = self.local_headers.copy()
        if not use_cache: headers['x-no-cache'] = 'true'
        try:
            response = requests.get(f"{self.local_reader_url}/{url}", headers=headers, timeout=30)
            return (True, response.text) if response.status_code == 200 else (False, f"HTTP {response.status_code}")
        except Exception as e: return False, str(e)
    
    def parse_post(self, url: str, content: str, source: str) -> Dict:
        """è§£æè²¼æ–‡"""
        post_id = url.split('/')[-1] if '/' in url else url
        views = self.extract_views_count(content, post_id)
        main_content = self.extract_post_content(content)
        
        return {
            'post_id': post_id, 'url': url, 'views': views, 'content': main_content, 'source': source,
            'success': views is not None and main_content is not None,
            'has_views': views is not None, 'has_content': main_content is not None,
            'content_length': len(content)
        }
    
    def is_rate_limited(self, error_msg: str) -> bool:
        """æª¢æ¸¬æ˜¯å¦è¢«é˜»æ“‹"""
        indicators = ['rate limit', 'too many requests', '429', 'quota exceeded', 'blocked', 'forbidden', '403', 'timeout']
        return any(indicator in error_msg.lower() for indicator in indicators)

    def rate_limited_pipeline(self, urls: List[str]):
        """é€Ÿç‡é™åˆ¶ç‰ˆAPIå„ªå…ˆç®¡ç·š"""
        total_start_time = time.time()
        
        print(f"ğŸŒ é€Ÿç‡é™åˆ¶ç‰ˆAPIå„ªå…ˆç®¡ç·šå•Ÿå‹•")
        print(f"ğŸ“Š è™•ç† {len(urls)} å€‹URLï¼Œé™åˆ¶æ¯åˆ†é˜{self.max_requests_per_minute}å€‹è«‹æ±‚")
        print(f"â±ï¸ é ä¼°æœ€å°‘è€—æ™‚: {len(urls) * self.min_interval_between_requests:.1f}ç§’")
        print("=" * 80)
        
        stage1_results = {}
        blocked_urls_for_local = []
        
        # === ç¬¬ä¸€éšæ®µï¼šåºåˆ—åŒ–APIè™•ç†ï¼ˆé¿å…è¶…éé€Ÿç‡é™åˆ¶ï¼‰ ===
        print(f"ğŸŒ (1/4) åºåˆ—åŒ–å®˜æ–¹APIè™•ç†...")
        
        for i, url in enumerate(urls):
            print(f"ğŸŒ APIè«‹æ±‚ {i+1}/{len(urls)} ({(i+1)/len(urls)*100:.1f}%): {url.split('/')[-1]}", end=" ")
            
            success, content = self.fetch_content_official_rate_limited(url)
            
            if success:
                result = self.parse_post(url, content, "å®˜æ–¹API-ç¬¬ä¸€è¼ª")
                if result['has_views']:
                    stage1_results[url] = result
                    print(f"âœ… ({result['views']})")
                    continue
            
            # APIå¤±æ•—
            blocked_urls_for_local.append(url)
            if self.is_rate_limited(content):
                print(f"ğŸš« è¢«é˜»æ“‹ â†’ è½‰æœ¬åœ°")
            else:
                print(f"âŒ å¤±æ•— â†’ è½‰æœ¬åœ°")
        
        # === ç¬¬äºŒéšæ®µï¼šæœ¬åœ°Readerä¸¦è¡Œè™•ç† ===
        if blocked_urls_for_local:
            print(f"\nâš¡ (2/4) æœ¬åœ°Readerä¸¦è¡Œè™•ç† {len(blocked_urls_for_local)} å€‹é …ç›®...")
            
            # æœ¬åœ°ä¸éœ€è¦é€Ÿç‡é™åˆ¶ï¼Œå¯ä»¥ä¸¦è¡Œ
            max_workers = min(4, len(blocked_urls_for_local))
            with ThreadPoolExecutor(max_workers=max_workers) as local_executor:
                local_future_to_url = {local_executor.submit(self.fetch_content_local, url): url for url in blocked_urls_for_local}
                
                local_completed = 0
                for local_future in as_completed(local_future_to_url):
                    url = local_future_to_url[local_future]
                    local_completed += 1
                    local_progress = local_completed / len(blocked_urls_for_local)
                    
                    local_success, local_content = local_future.result()
                    if local_success:
                        result = self.parse_post(url, local_content, "æœ¬åœ°-å›é€€1")
                        stage1_results[url] = result
                        status = f"âœ… ({result['views']})" if result['has_views'] else "âŒ ç„¡è§€çœ‹æ•¸"
                        print(f"âš¡ {local_completed}/{len(blocked_urls_for_local)} ({local_progress:.1%}) | {status}: {result['post_id']}")
                    else:
                        stage1_results[url] = {'url': url, 'success': False, 'source': 'æœ¬åœ°-å›é€€1', 'error': local_content}
                        print(f"âš¡ {local_completed}/{len(blocked_urls_for_local)} ({local_progress:.1%}) | âŒ æœ¬åœ°å¤±æ•—: {url.split('/')[-1]}")

        remaining_failed = [url for url, res in stage1_results.items() if not res.get('has_views')]
        
        # === ç¬¬ä¸‰éšæ®µï¼šæœ¬åœ°é‡è©¦ä¸¦è¡Œè™•ç† ===
        if remaining_failed:
            print(f"\nğŸ”„ (3/4) æœ¬åœ°é‡è©¦ä¸¦è¡Œè™•ç† ({len(remaining_failed)} å€‹é …ç›®)")
            
            max_workers = min(4, len(remaining_failed))
            with ThreadPoolExecutor(max_workers=max_workers) as retry_executor:
                retry_future_to_url = {retry_executor.submit(self.fetch_content_local, url, False): url for url in remaining_failed}
                
                retry_completed = 0
                for retry_future in as_completed(retry_future_to_url):
                    url = retry_future_to_url[retry_future]
                    retry_completed += 1
                    
                    success, content = retry_future.result()
                    if success:
                        result = self.parse_post(url, content, "æœ¬åœ°-é‡è©¦")
                        stage1_results[url] = result
                        status = f"âœ… ({result['views']})" if result['has_views'] else "âŒ ç„¡è§€çœ‹æ•¸"
                        print(f"ğŸ”„ {retry_completed}/{len(remaining_failed)}: {status} {result['post_id']}")

        final_failed = [url for url, res in stage1_results.items() if not res.get('has_views')]
        
        # === ç¬¬å››éšæ®µï¼šæœ€å¾ŒAPIé‡è©¦ï¼ˆåºåˆ—åŒ–ï¼‰ ===
        if final_failed:
            print(f"\nğŸŒ (4/4) æœ€å¾Œå®˜æ–¹APIé‡è©¦ ({len(final_failed)} å€‹é …ç›®)")
            print("ğŸ•’ åºåˆ—åŒ–è™•ç†é¿å…é »ç‡é™åˆ¶...")
            
            for i, url in enumerate(final_failed):
                print(f"ğŸŒ æœ€å¾ŒAPI {i+1}/{len(final_failed)}: {url.split('/')[-1]}", end=" ")
                
                success, content = self.fetch_content_official_rate_limited(url)
                if success:
                    result = self.parse_post(url, content, "å®˜æ–¹API-å›é€€2")
                    stage1_results[url] = result
                    status = f"âœ… ({result['views']})" if result['has_views'] else "âŒ ç„¡è§€çœ‹æ•¸"
                    print(status)
                else:
                    print("âŒ å¤±æ•—")
        
        total_end_time = time.time()
        final_results = [stage1_results.get(url, {'url': url, 'success': False}) for url in urls]
        
        print("\n" + "=" * 80)
        success_count = len([res for res in final_results if res.get('has_views')])
        api_success_count = len([res for res in final_results if res.get('has_views') and 'API' in res.get('source', '')])
        local_success_count = success_count - api_success_count
        
        print(f"âœ… æœ€çµ‚æˆåŠŸ: {success_count}/{len(urls)} ({success_count/len(urls)*100:.1f}%)")
        print(f"ğŸŒ APIæˆåŠŸ: {api_success_count} | âš¡ æœ¬åœ°æˆåŠŸ: {local_success_count}")
        print(f"â±ï¸ ç¸½è€—æ™‚: {total_end_time - total_start_time:.1f}s")
        print(f"ğŸï¸ å¯¦éš›é€Ÿåº¦: {len(urls)/(total_end_time - total_start_time):.2f} URL/s")
        print(f"ğŸ“Š å¹³å‡APIè«‹æ±‚é–“éš”: {(total_end_time - total_start_time)/len(urls):.2f}s")
        
        return final_results

def generate_test_urls(original_urls: List[str], target_count: int = 100) -> List[str]:
    """å¾åŸå§‹URLç”Ÿæˆæ¸¬è©¦ç”¨çš„URLåˆ—è¡¨"""
    test_urls = []
    
    # å…ˆåŠ å…¥æ‰€æœ‰åŸå§‹URL
    test_urls.extend(original_urls)
    
    # å¦‚æœéœ€è¦æ›´å¤šURLï¼Œé‡è¤‡ä½¿ç”¨åŸå§‹URLï¼ˆå¯ä»¥æ”¹è®Šé †åºï¼‰
    while len(test_urls) < target_count:
        remaining = target_count - len(test_urls)
        if remaining >= len(original_urls):
            # é‡æ–°æ’åˆ—åŸå§‹URLä¸¦å…¨éƒ¨åŠ å…¥
            shuffled_urls = original_urls.copy()
            random.shuffle(shuffled_urls)
            test_urls.extend(shuffled_urls)
        else:
            # éš¨æ©Ÿé¸æ“‡å‰©é¤˜æ•¸é‡çš„URL
            selected_urls = random.sample(original_urls, remaining)
            test_urls.extend(selected_urls)
    
    return test_urls[:target_count]

def load_urls_from_json(file_path: str) -> List[str]:
    try:
        with open(file_path, 'r', encoding='utf-8') as f: data = json.load(f)
        urls = [post['url'] for post in data.get('posts', []) if 'url' in post]
        print(f"âœ… å¾ {file_path} æˆåŠŸæå– {len(urls)} å€‹åŸå§‹ URLã€‚")
        return urls
    except Exception as e:
        print(f"âŒ æå– URL æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return []

def main():
    # è¼‰å…¥åŸå§‹17å€‹URL
    original_urls = load_urls_from_json("agents/playwright_crawler/debug/crawl_data_20250803_121452_934d52b1.json")
    if not original_urls: return
    
    # ç”Ÿæˆ100å€‹æ¸¬è©¦URL
    test_urls = generate_test_urls(original_urls, 100)
    print(f"ğŸ¯ ç”Ÿæˆ {len(test_urls)} å€‹æ¸¬è©¦URL (å¾ {len(original_urls)} å€‹åŸå§‹URLæ“´å±•)")
    
    pipeline = RateLimitedApiFirstReader()
    results = pipeline.rate_limited_pipeline(test_urls)
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    filename = f"rate_limited_api_results_{timestamp}.json"
    with open(filename, 'w', encoding='utf-8') as f: 
        json.dump({
            'total_urls': len(results),
            'successful_extractions': len([r for r in results if r.get('has_views')]),
            'original_url_count': len(original_urls),
            'generated_url_count': len(test_urls),
            'results': results
        }, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ çµæœå·²ä¿å­˜åˆ°: {filename}")

if __name__ == '__main__':
    main()