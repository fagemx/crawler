#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Readerå…§å®¹è§£æå™¨ - å¾Jina Readerè¿”å›çš„markdownä¸­æå–é—œéµæ•¸æ“š
"""

import re
import requests
from typing import Dict, Optional, List
import json

class ThreadsReaderParser:
    """Threadsè²¼æ–‡Readerè§£æå™¨"""
    
    def __init__(self, reader_base_url: str = "http://localhost:8880"):
        self.reader_base_url = reader_base_url
    
    def fetch_content(self, post_url: str) -> str:
        """å¾Readeræœå‹™ç²å–å…§å®¹"""
        reader_url = f"{self.reader_base_url}/{post_url}"
        try:
            response = requests.get(reader_url, timeout=30)
            response.raise_for_status()
            return response.text
        except Exception as e:
            print(f"âŒ ç²å–å…§å®¹å¤±æ•—: {e}")
            return ""
    
    def extract_post_content(self, markdown_content: str) -> Optional[str]:
        """æå–è²¼æ–‡å…§å®¹"""
        lines = markdown_content.split('\n')
        
        # æ–¹æ³•1: æŸ¥æ‰¾ç¬¬ä¸€æ®µæ­£æ–‡å…§å®¹ï¼ˆåœ¨===åˆ†éš”ç¬¦ä¹‹å‰ï¼‰
        content_lines = []
        in_content = False
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            # è·³éæ¨™é¡Œå’ŒURLè¡Œ
            if line.startswith("Title:") or line.startswith("URL Source:") or line.startswith("Markdown Content:"):
                continue
                
            # å¦‚æœé‡åˆ°åˆ†éš”ç¬¦ï¼Œåœæ­¢
            if "===============" in line or "---" in line:
                break
                
            # å¦‚æœæ˜¯è²¼æ–‡å…§å®¹ï¼ˆä¸æ˜¯é€£çµã€ä¸æ˜¯æŒ‰éˆ•æ–‡å­—ï¼‰
            if not line.startswith("[") and not line.startswith("!") and len(line) > 10:
                content_lines.append(line)
                if len(content_lines) >= 3:  # å–å‰å¹¾è¡Œä½œç‚ºå…§å®¹
                    break
        
        return ' '.join(content_lines) if content_lines else None
    
    def extract_views_count(self, markdown_content: str) -> Optional[str]:
        """æå–è§€çœ‹æ•¸"""
        # æŸ¥æ‰¾åŒ…å«"views"çš„è¡Œ
        patterns = [
            r'(\d+(?:\.\d+)?[KMB]?)\s*views',
            r'Thread.*?(\d+(?:\.\d+)?[KMB]?)\s*views',
            r'(\d+(?:,\d+)*)\s*views'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, markdown_content, re.IGNORECASE)
            if match:
                return f"{match.group(1)} views"
        
        return None
    
    def extract_likes_count(self, markdown_content: str) -> Optional[str]:
        """æå–æŒ‰è®šæ•¸ - å¤šç¨®æ ¼å¼æ”¯æ´"""
        lines = markdown_content.split('\n')
        
        # æ–¹æ³•1: æŸ¥æ‰¾"Like"æ¨™ç±¤å¾Œçš„æ•¸å­—ï¼ˆèˆŠæ ¼å¼ï¼‰
        for i, line in enumerate(lines):
            if line.strip() == "Like" and i + 2 < len(lines):
                next_line = lines[i + 2].strip()
                if re.match(r'^\d+(?:\.\d+)?[KMB]?$', next_line):
                    return next_line
        
        # æ–¹æ³•2: æ–°æ ¼å¼ - å¾æ•¸å­—åºåˆ—ä¸­å–ç¬¬ä¸€å€‹
        numbers = self.extract_engagement_numbers(markdown_content)
        if len(numbers) >= 1:
            return numbers[0]
        
        # æ–¹æ³•3: æŸ¥æ‰¾"like"ç›¸é—œæ¨¡å¼
        like_patterns = [
            r'Like\s*\n\s*(\d+(?:\.\d+)?[KMB]?)',
            r'(\d+(?:\.\d+)?[KMB]?)\s*(?:likes?|ğŸ‘|â¤ï¸)'
        ]
        
        for pattern in like_patterns:
            match = re.search(pattern, markdown_content, re.MULTILINE | re.IGNORECASE)
            if match:
                return match.group(1)
        
        return None
    
    def extract_engagement_numbers(self, markdown_content: str) -> List[str]:
        """æå–æ‰€æœ‰çµ±è¨ˆæ•¸å­—åºåˆ—ï¼ˆæŒ‰è®šã€ç•™è¨€ã€è½‰ç™¼ã€åˆ†äº«ï¼‰"""
        lines = markdown_content.split('\n')
        
        # ç­–ç•¥1: æŸ¥æ‰¾è²¼æ–‡å…§å®¹å¾Œçš„ç¬¬ä¸€å€‹åœ–ç‰‡ï¼Œç„¶å¾Œæå–å¾ŒçºŒæ•¸å­—
        for i, line in enumerate(lines):
            stripped = line.strip()
            # æ‰¾åˆ°è²¼æ–‡åœ–ç‰‡ï¼ˆé€šå¸¸åœ¨Translateä¹‹å¾Œï¼‰
            if stripped.startswith('![Image') and not 'profile picture' in stripped:
                numbers = []
                # åœ¨é€™å€‹åœ–ç‰‡å¾ŒæŸ¥æ‰¾é€£çºŒçš„æ•¸å­—
                for j in range(i + 1, min(i + 20, len(lines))):
                    candidate = lines[j].strip()
                    if re.match(r'^\d+(?:\.\d+)?[KMB]?$', candidate):
                        numbers.append(candidate)
                    elif candidate and not re.match(r'^\d+(?:\.\d+)?[KMB]?$', candidate) and candidate != "Pinned":
                        # é‡åˆ°éæ•¸å­—è¡Œï¼ˆä½†è·³éPinnedï¼‰ï¼Œåœæ­¢æ”¶é›†
                        break
                
                # å¦‚æœæ‰¾åˆ°äº†æ•¸å­—åºåˆ—ï¼Œè¿”å›
                if len(numbers) >= 3:
                    return numbers
        
        # ç­–ç•¥2: å¦‚æœç­–ç•¥1å¤±æ•—ï¼ŒæŸ¥æ‰¾ä»»ä½•é€£çºŒçš„æ•¸å­—åºåˆ—
        all_numbers = []
        for i, line in enumerate(lines):
            stripped = line.strip()
            if re.match(r'^\d+(?:\.\d+)?[KMB]?$', stripped):
                # æª¢æŸ¥å‰å¾Œæ–‡ï¼Œç¢ºä¿é€™æ˜¯çµ±è¨ˆæ•¸å­—
                context_valid = False
                # æª¢æŸ¥å‰é¢10è¡Œæ˜¯å¦æœ‰åœ–ç‰‡æˆ–è²¼æ–‡å…§å®¹
                for k in range(max(0, i-10), i):
                    if "![Image" in lines[k] or "Translate" in lines[k]:
                        context_valid = True
                        break
                
                if context_valid:
                    all_numbers.append(stripped)
        
        # å¦‚æœæ‰¾åˆ°4å€‹æˆ–æ›´å¤šæ•¸å­—ï¼Œå–å‰4å€‹
        if len(all_numbers) >= 4:
            return all_numbers[:4]
        elif len(all_numbers) >= 3:
            return all_numbers[:3]
        
        return all_numbers
    
    def extract_comments_count(self, markdown_content: str) -> Optional[str]:
        """æå–ç•™è¨€æ•¸"""
        lines = markdown_content.split('\n')
        
        # æ–¹æ³•1: èˆŠæ ¼å¼ - æŸ¥æ‰¾Commentæ¨™ç±¤
        for i, line in enumerate(lines):
            if line.strip() == "Comment" and i + 2 < len(lines):
                next_line = lines[i + 2].strip()
                if re.match(r'^\d+(?:\.\d+)?[KMB]?$', next_line):
                    return next_line
        
        # æ–¹æ³•2: æ–°æ ¼å¼ - å¾æ•¸å­—åºåˆ—ä¸­å–ç¬¬äºŒå€‹
        numbers = self.extract_engagement_numbers(markdown_content)
        if len(numbers) >= 2:
            return numbers[1]
        
        return None
    
    def extract_reposts_count(self, markdown_content: str) -> Optional[str]:
        """æå–è½‰ç™¼æ•¸"""
        lines = markdown_content.split('\n')
        
        # æ–¹æ³•1: èˆŠæ ¼å¼ - æŸ¥æ‰¾Repostæ¨™ç±¤
        for i, line in enumerate(lines):
            if line.strip() == "Repost" and i + 2 < len(lines):
                next_line = lines[i + 2].strip()
                if re.match(r'^\d+(?:\.\d+)?[KMB]?$', next_line):
                    return next_line
        
        # æ–¹æ³•2: æ–°æ ¼å¼ - å¾æ•¸å­—åºåˆ—ä¸­å–ç¬¬ä¸‰å€‹
        numbers = self.extract_engagement_numbers(markdown_content)
        if len(numbers) >= 3:
            return numbers[2]
        
        return None
    
    def extract_shares_count(self, markdown_content: str) -> Optional[str]:
        """æå–åˆ†äº«æ•¸"""
        lines = markdown_content.split('\n')
        
        # æ–¹æ³•1: èˆŠæ ¼å¼ - æŸ¥æ‰¾Shareæ¨™ç±¤
        for i, line in enumerate(lines):
            if line.strip() == "Share" and i + 2 < len(lines):
                next_line = lines[i + 2].strip()
                if re.match(r'^\d+(?:\.\d+)?[KMB]?$', next_line):
                    return next_line
        
        # æ–¹æ³•2: æ–°æ ¼å¼ - å¾æ•¸å­—åºåˆ—ä¸­å–ç¬¬å››å€‹
        numbers = self.extract_engagement_numbers(markdown_content)
        if len(numbers) >= 4:
            return numbers[3]
        
        return None
    
    def parse_post(self, post_url: str) -> Dict:
        """è§£æå–®ç¯‡è²¼æ–‡ï¼Œè¿”å›çµæ§‹åŒ–æ•¸æ“š"""
        print(f"ğŸ¯ é–‹å§‹è§£æè²¼æ–‡: {post_url}")
        
        # ç²å–å…§å®¹
        content = self.fetch_content(post_url)
        if not content:
            return {"error": "ç„¡æ³•ç²å–å…§å®¹"}
        
        print(f"ğŸ“„ ç²å–åˆ° {len(content)} å­—ç¬¦çš„å…§å®¹")
        
        # æå–å„é …æ•¸æ“š
        result = {
            "url": post_url,
            "content": self.extract_post_content(content),
            "views": self.extract_views_count(content),
            "likes": self.extract_likes_count(content),
            "comments": self.extract_comments_count(content),
            "reposts": self.extract_reposts_count(content),
            "shares": self.extract_shares_count(content),
            "raw_length": len(content)
        }
        
        return result
    
    def debug_content(self, post_url: str, save_raw: bool = True):
        """èª¿è©¦æ¨¡å¼ï¼šé¡¯ç¤ºåŸå§‹å…§å®¹ä»¥ä¾¿åˆ†æ"""
        content = self.fetch_content(post_url)
        if not content:
            return
        
        print("ğŸ” åŸå§‹å…§å®¹åˆ†æ:")
        print("=" * 50)
        
        lines = content.split('\n')
        for i, line in enumerate(lines[:30]):  # åªé¡¯ç¤ºå‰30è¡Œ
            print(f"{i+1:2d}: {line}")
        
        print(f"\n... (ç¸½å…± {len(lines)} è¡Œ)")
        
        if save_raw:
            filename = f"reader_raw_content_{post_url.split('/')[-1]}.txt"
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(content)
            print(f"ğŸ’¾ åŸå§‹å…§å®¹å·²ä¿å­˜åˆ°: {filename}")

def main():
    """æ¸¬è©¦è§£æå™¨"""
    parser = ThreadsReaderParser()
    
    # æ¸¬è©¦è²¼æ–‡
    test_url = "https://www.threads.com/@ttshow.tw/post/DMfOVeqSkM5"
    
    print("ğŸ§ª æ¸¬è©¦Readerè§£æå™¨")
    print("=" * 50)
    
    # èª¿è©¦æ¨¡å¼ï¼šæŸ¥çœ‹åŸå§‹å…§å®¹
    print("ğŸ“‹ èª¿è©¦æ¨¡å¼ - æŸ¥çœ‹åŸå§‹å…§å®¹:")
    parser.debug_content(test_url)
    
    print("\n" + "=" * 50)
    
    # è§£ææ¨¡å¼ï¼šæå–çµæ§‹åŒ–æ•¸æ“š
    print("ğŸ“Š è§£ææ¨¡å¼ - æå–çµæ§‹åŒ–æ•¸æ“š:")
    result = parser.parse_post(test_url)
    
    print(f"ğŸ“ è²¼æ–‡å…§å®¹: {result.get('content', 'N/A')}")
    print(f"ğŸ‘ï¸ è§€çœ‹æ•¸: {result.get('views', 'N/A')}")
    print(f"ğŸ‘ æŒ‰è®šæ•¸: {result.get('likes', 'N/A')}")
    print(f"ğŸ’¬ ç•™è¨€æ•¸: {result.get('comments', 'N/A')}")
    print(f"ğŸ”„ è½‰ç™¼æ•¸: {result.get('reposts', 'N/A')}")
    print(f"ğŸ“¤ åˆ†äº«æ•¸: {result.get('shares', 'N/A')}")
    print(f"ğŸ“„ åŸå§‹é•·åº¦: {result.get('raw_length', 0)} å­—ç¬¦")
    
    # èª¿è©¦ï¼šé¡¯ç¤ºæ‰¾åˆ°çš„æ•¸å­—åºåˆ—
    content = parser.fetch_content(test_url)
    numbers = parser.extract_engagement_numbers(content)
    print(f"ğŸ” æ‰¾åˆ°çš„æ•¸å­—åºåˆ—: {numbers}")
    
    # ä¿å­˜çµæœ
    with open('reader_parse_result.json', 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    
    print("ğŸ’¾ è§£æçµæœå·²ä¿å­˜åˆ°: reader_parse_result.json")

if __name__ == "__main__":
    main()