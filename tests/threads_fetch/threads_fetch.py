# threads_fetch.py
import asyncio, json, pathlib, sys, re
from playwright.async_api import async_playwright
import logging

# è¨­å®šåŸºæœ¬æ—¥èªŒè¨˜éŒ„
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# --- çµ„æ…‹è¨­å®š ---
# AUTH_FILE: é©—è­‰ç‹€æ…‹æª”æ¡ˆçš„è·¯å¾‘ã€‚
# è«‹å…ˆåŸ·è¡Œ save_auth.py æŒ‡ä»¤ç¢¼ä¾†å»ºç«‹æ­¤æª”æ¡ˆã€‚
AUTH_FILE = pathlib.Path("auth.json")

# TARGET_URL: è¦çˆ¬å–çš„ Threads å€‹äººæª”æ¡ˆ URLã€‚
TARGET_URL = "https://www.threads.com/@natgeo"

# MAX_POSTS: è¦çˆ¬å–çš„æœ€å¤§è²¼æ–‡æ•¸é‡ã€‚
MAX_POSTS = 100

# OUTPUT_FILE: å„²å­˜çˆ¬å–è³‡æ–™çš„è·¯å¾‘ã€‚
# å¦‚æœç›®éŒ„ä¸å­˜åœ¨ï¼Œå°‡æœƒè‡ªå‹•å»ºç«‹ã€‚
OUTPUT_FILE = pathlib.Path("results/profile.json")

# --- GraphQL API è©³ç´°è³‡è¨Š ---
# ç”¨æ–¼è­˜åˆ¥è²¼æ–‡ GraphQL API å‘¼å«çš„æ­£è¦è¡¨ç¤ºå¼ã€‚
GRAPHQL_RE = re.compile(r"/graphql/query")

# å·²çŸ¥çš„ GraphQL æŸ¥è©¢åç¨±ï¼Œç”¨æ–¼æ“·å–è²¼æ–‡ã€‚
# æˆ‘å€‘ç›£è½é€™äº›æŸ¥è©¢ä»¥æ“·å–è²¼æ–‡è³‡æ–™ã€‚
POST_QUERY_NAMES = {
    # 2025 å¹´ 7 æœˆç™¼ç¾ï¼Œä¼¼ä¹æ˜¯å€‹äººæª”æ¡ˆè²¼æ–‡çš„ä¸»è¦æŸ¥è©¢
    "BarcelonaProfileThreadsTabRefetchableDirectQuery",
    # èˆŠçš„æŸ¥è©¢åç¨±ï¼Œä¿ç•™ä½œç‚ºå‚™ç”¨
    "BarcelonaProfilePostsTabQuery",
    "BarcelonaProfilePostsPageQuery",
}


def remove_media_versions(data):
    """
    éè¿´åœ°å¾è³‡æ–™ä¸­ç§»é™¤ 'image_versions2' å’Œ 'video_versions'ï¼Œä»¥ç°¡åŒ–è¼¸å‡ºã€‚
    """
    if isinstance(data, dict):
        # ç§»é™¤ç›®å‰å±¤ç´šçš„éµ
        data.pop('image_versions2', None)
        data.pop('video_versions', None)
        # éè¿´è™•ç†å­—å…¸ä¸­çš„å€¼
        for key, value in data.items():
            remove_media_versions(value)
    elif isinstance(data, list):
        # éè¿´è™•ç†åˆ—è¡¨ä¸­çš„æ¯å€‹é …ç›®
        for item in data:
            remove_media_versions(item)


def parse_post_data(post_data: dict) -> dict:
    """
    è§£æå–®ç¯‡è²¼æ–‡çš„ JSON è³‡æ–™ä¸¦æå–ç›¸é—œæ¬„ä½ã€‚
    """
    post = post_data.get('post', {})
    if not post:
        return {}

    # å®‰å…¨åœ°æå–ä½¿ç”¨è€…è³‡è¨Š
    user = post.get('user', {})
    # æå–æ–‡å­—å’Œåª’é«”è³‡è¨Š
    caption = post.get('caption', {})
    text_post_app_info = post.get('text_post_app_info', {})
    share_info = text_post_app_info.get('share_info', {})

    # æå–åœ–ç‰‡å’Œå½±ç‰‡ URL
    image_url = None
    if post.get('image_versions2', {}).get('candidates'):
        # å–å¾—æœ€é«˜è§£æåº¦çš„åœ–ç‰‡
        if post['image_versions2']['candidates']:
            image_url = post['image_versions2']['candidates'][0].get('url')

    video_url = None
    if post.get('video_versions'):
        if post['video_versions']:
            video_url = post['video_versions'][0].get('url')

    # éè¿´è™•ç†å¼•ç”¨çš„è²¼æ–‡
    quoted_post = None
    if share_info.get('quoted_post'):
        # å¼•ç”¨çš„è²¼æ–‡æ˜¯å·¢ç‹€çµæ§‹ï¼Œæ‰€ä»¥æˆ‘å€‘ä¹Ÿå°å…¶é€²è¡Œè§£æã€‚
        # æ³¨æ„ï¼š'quoted_post' å¯èƒ½ä¸åŒ…å«å®Œæ•´çš„ 'post' ç‰©ä»¶ï¼Œ
        # æ‰€ä»¥æˆ‘å€‘å°‡æ•´å€‹å­—å…¸å‚³éçµ¦è§£æå™¨ã€‚
        quoted_post_data = {'post': share_info['quoted_post']}
        quoted_post = parse_post_data(quoted_post_data)


    return {
        'post_id': post.get('pk'),
        'post_code': post.get('code'),
        'post_url': f"https://www.threads.net/t/{post.get('code')}" if post.get('code') else None,
        'user_id': user.get('pk'),
        'username': user.get('username'),
        'user_profile_pic': user.get('profile_pic_url'),
        'is_verified_user': user.get('is_verified'),
        'taken_at': post.get('taken_at'),
        'caption': caption.get('text'),
        'like_count': post.get('like_count'),
        'reply_count': text_post_app_info.get('direct_reply_count'),
        'repost_count': text_post_app_info.get('repost_count'),
        'quote_count': text_post_app_info.get('quote_count'),
        'view_count': post.get('view_count'), # è¨»ï¼šæ­¤æ¬„ä½ç›®å‰åœ¨ API å›æ‡‰ä¸­æ‰¾ä¸åˆ°ï¼Œé ç•™æ¬„ä½
        'image_url': image_url,
        'video_url': video_url,
        'quoted_post': quoted_post,
    }


async def main():
    if not AUTH_FILE.exists():
        logging.error("é©—è­‰æª”æ¡ˆ 'auth.json' ä¸å­˜åœ¨ã€‚")
        sys.exit(f"âŒ è«‹å…ˆåŸ·è¡Œ save_auth.py æŒ‡ä»¤ç¢¼ä»¥å»ºç«‹ {AUTH_FILE}")

    # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
    OUTPUT_FILE.parent.mkdir(exist_ok=True)
    
    posts = {}

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        ctx = await browser.new_context(storage_state=AUTH_FILE)
        page = await ctx.new_page()

        async def on_response(res):
            # æˆ‘å€‘åªå° GraphQL å›æ‡‰æ„Ÿèˆˆè¶£
            if not GRAPHQL_RE.search(res.url):
                return
            
            # æª¢æŸ¥å›æ‡‰æ˜¯å¦ç‚ºæˆ‘å€‘çš„ç›®æ¨™æŸ¥è©¢ä¹‹ä¸€
            qname = res.request.headers.get("x-fb-friendly-name")
            if qname not in POST_QUERY_NAMES:
                return

            logging.info(f"ğŸ¯ å‘½ä¸­ç›®æ¨™æŸ¥è©¢: {qname}")
            try:
                data = await res.json()
                # è²¼æ–‡å·¢ç‹€åœ°ä½æ–¼æ­¤çµæ§‹ä¸­
                edges = data.get('data', {}).get('mediaData', {}).get('edges', [])
                
                parsed_count = 0
                for edge in edges:
                    thread_items = edge.get('node', {}).get('thread_items', [])
                    for item in thread_items:
                        parsed_post = parse_post_data(item)
                        if parsed_post.get('post_id') and parsed_post['post_id'] not in posts:
                            posts[parsed_post['post_id']] = parsed_post
                            parsed_count += 1
                
                if parsed_count > 0:
                    logging.info(f"âœ… å·²è§£æ {parsed_count} å‰‡æ–°è²¼æ–‡ã€‚ç¸½æ•¸: {len(posts)}")

            except Exception as e:
                logging.error(f"è§£ææŸ¥è©¢ {qname} çš„ JSON å¤±æ•—: {e}")

        page.on("response", on_response)
        
        logging.info(f"æ­£åœ¨å°è¦½è‡³ {TARGET_URL}")
        await page.goto(TARGET_URL, wait_until="networkidle")

        logging.info("é–‹å§‹æ»¾å‹•ä¸¦çˆ¬å–è²¼æ–‡...")
        
        # æ»¾å‹•è¿´åœˆä»¥è¼‰å…¥æ›´å¤šè²¼æ–‡
        scroll_attempts = 0
        max_scroll_attempts_without_new_posts = 5

        while len(posts) < MAX_POSTS:
            posts_before_scroll = len(posts)
            
            # æ»¾å‹•åˆ°é é¢åº•éƒ¨
            await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            
            # ç­‰å¾…æ–°å…§å®¹è¼‰å…¥
            try:
                # ä½¿ç”¨ expect_response ç­‰å¾…ç¬¦åˆæ¢ä»¶çš„ç¶²è·¯å›æ‡‰
                async with page.expect_response(lambda res: GRAPHQL_RE.search(res.url), timeout=5000) as response_info:
                    await response_info.value
            except asyncio.TimeoutError:
                logging.warning("æ»¾å‹•å¾Œç­‰å¾…ç¶²è·¯å›æ‡‰é€¾æ™‚ã€‚å¯èƒ½å·²é”é é¢æœ«ç«¯ã€‚")

            # é¡å¤–ç­‰å¾…ä»¥ç¢ºä¿å®¢æˆ¶ç«¯æ¸²æŸ“å®Œæˆ
            await page.wait_for_timeout(2000)

            posts_after_scroll = len(posts)

            if posts_after_scroll == posts_before_scroll:
                scroll_attempts += 1
                logging.info(f"æ»¾å‹•å˜—è©¦ {scroll_attempts}/{max_scroll_attempts_without_new_posts} æœªç™¼ç¾æ–°è²¼æ–‡ã€‚")
                if scroll_attempts >= max_scroll_attempts_without_new_posts:
                    logging.info("å·²é”å€‹äººæª”æ¡ˆæœ«ç«¯æˆ–ç„¡æ–°è²¼æ–‡è¼‰å…¥ã€‚åœæ­¢æ»¾å‹•ã€‚")
                    break
            else:
                # è‹¥æ‰¾åˆ°æ–°è²¼æ–‡å‰‡é‡è¨­è¨ˆæ•¸å™¨
                scroll_attempts = 0

            if len(posts) >= MAX_POSTS:
                logging.info(f"å·²é”åˆ° {MAX_POSTS} çš„ MAX_POSTS é™åˆ¶ã€‚")
                break
        
        await ctx.close()

    logging.info(f"çˆ¬å–å®Œæˆã€‚ç¸½å…±æ”¶é›†åˆ° {len(posts)} å‰‡è²¼æ–‡ã€‚")
    
    # å„²å­˜æ”¶é›†åˆ°çš„è³‡æ–™
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        json.dump(list(posts.values()), f, indent=2, ensure_ascii=False)
    
    logging.info(f"âœ… çµæœå·²å„²å­˜è‡³ {OUTPUT_FILE}")


if __name__ == "__main__":
    asyncio.run(main()) 