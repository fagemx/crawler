以下做法能在 **完全不改動你既有「抓取邏輯」** 的前提下，為 *同一帳號* 增加「已抓取貼文記錄」與「增量爬取」能力。只要在 **資料庫層 + 入口函式** 動一點點手腳即可。

---

## 1 | 資料庫：兩張最小表

> 你已經在用 PostgreSQL ＋ SQLAlchemy (`common.models`) 儲存 `PostMetrics`。
> 只需再加一張「帳號爬取狀態表」並在 `post_id` 上加唯一鍵即可。

```python
# common/models.py  (節錄)
class PostMetrics(Base):
    __tablename__ = "post_metrics"
    id           = Column(Integer, primary_key=True)  # → 內部用
    post_id      = Column(String, unique=True)        # natgeo_DM0X0DTNr_r
    username     = Column(String, index=True)
    created_at   = Column(DateTime)
    # ……其餘欄位照舊……

class CrawlState(Base):
    """
    每個帳號只會有一列，用來記錄上次爬完後的狀態
    """
    __tablename__ = "crawl_state"
    username        = Column(String, primary_key=True)
    last_crawl_at   = Column(DateTime)        # 方便依時間增量
    total_crawled   = Column(Integer, default=0)
```

> **遷移腳本**：若你用 Alembic
> `alembic revision -m "add crawl_state" && alembic upgrade head`

---

## 2 | 入口函式：決定「要抓幾篇？」

在 `PlaywrightLogic.fetch_posts()` 進入點，一開始多做 3 步：

```python
# ① 讀取目前資料庫已抓數量
async with AsyncSession(engine) as sess:
    existing = await sess.execute(
        select(PostMetrics.post_id).where(PostMetrics.username == username)
    )
    existing_post_ids = {row[0] for row in existing}
    already = len(existing_post_ids)

# ② 根據使用者指令算「這次想要的新總量」
target_total = already + wanted_extra   # 例如 already=50, wanted_extra=50 → 100
need_to_fetch = max(0, target_total - already)
if need_to_fetch == 0:
    logging.info(f"🟢 {username} 已有 {already} 篇，無需增量抓取")
    return PostMetricsBatch(posts=[], username=username, total_count=already)

# ③ 把 need_to_fetch 傳進 get_ordered_post_urls_from_page()
ordered_urls = await self.get_ordered_post_urls_from_page(
    page, username, need_to_fetch + 10  # +10 給保險 buffer
)
```

> * 你不必動 **滾動、解析、補齊** 的任何程式；
>   只要在「挑 URL」時丟掉已抓過的 `post_id` 就行。
> * 把 `need_to_fetch` 控制在 **尚未擁有的新貼文** 數量即可。

---

## 3 | 去重：挑掉抓過的網址

在 `ordered_post_urls` 迴圈內，第一步就判斷：

```python
for post_url in ordered_post_urls:
    code = post_url.rstrip('/').split('/')[-1]
    post_id = f"{username}_{code}"
    if post_id in existing_post_ids:
        continue                     # <- 已抓過就跳過
    # …以下流程保持不變…
```

只要 **收集到 `need_to_fetch` 篇** 就 `break`，省下無謂滾動。

---

## 4 | 寫回資料庫 & 更新 CrawlState

`final_posts` 處理完成後：

```python
async with AsyncSession(engine) as sess:
    # ④ UPSERT 新貼文
    sess.add_all(final_posts)               # SQLAlchemy 2.0 可用 bulk_save_objects
    # ⑤ 更新 crawl_state
    await sess.merge(CrawlState(
        username       = username,
        last_crawl_at  = datetime.utcnow(),
        total_crawled  = already + len(final_posts)
    ))
    await sess.commit()
```

---

## 5 | 使用方式示例

```python
# 想多抓 50 篇
await playwright_logic.fetch_posts(
    username="natgeo",
    max_posts=50,        # 這裡改名叫 wanted_extra 也可
    auth_json_content=auth,
    task_id="incremental_natgeo_50"
)
```

首輪會抓 50；第二輪又執行同指令，程式自動偵測已經有 100 篇，所以只會再抓發文期間真正新增的貼文（0 \~ n 篇）。

---

## 6 | 附加優勢

| 功能         | 原理                                       | 需要新碼量 |
| ---------- | ---------------------------------------- | ----- |
| **自動跳過刪文** | `post_id` 唯一鍵，插入時若已存在直接忽略                | 0     |
| **新貼文通知**  | 比對 `created_at > last_crawl_at` 就能判斷     | <10 行 |
| **回補缺漏**   | `need_to_fetch` 會把「先前因錯誤 missed 的貼文」一起補回 | 已內建   |

---

### 一次性改動，長期收益

* **不碰解析器、不碰 Agent flow**：只在「入口」和「資料層」加薄薄一層。
* **保持原本 MCP / NATS 架構**：任何 Agent 要增量重爬，只需把「想加幾篇」寫進指令。
* **擴充容易**：之後若要每天自動增量，只要把 `wanted_extra=∞` 改成「抓所有新貼文直到遇到重複」即可。

---

若要看 **完整 diff** 或 Alembic migration 範例，隨時告訴我！


####


以下做法能在 **完全不改動你既有「抓取邏輯」** 的前提下，為 *同一帳號* 增加「已抓取貼文記錄」與「增量爬取」能力。只要在 **資料庫層 + 入口函式** 動一點點手腳即可。

---

## 1 | 資料庫：兩張最小表

> 你已經在用 PostgreSQL ＋ SQLAlchemy (`common.models`) 儲存 `PostMetrics`。
> 只需再加一張「帳號爬取狀態表」並在 `post_id` 上加唯一鍵即可。

```python
# common/models.py  (節錄)
class PostMetrics(Base):
    __tablename__ = "post_metrics"
    id           = Column(Integer, primary_key=True)  # → 內部用
    post_id      = Column(String, unique=True)        # natgeo_DM0X0DTNr_r
    username     = Column(String, index=True)
    created_at   = Column(DateTime)
    # ……其餘欄位照舊……

class CrawlState(Base):
    """
    每個帳號只會有一列，用來記錄上次爬完後的狀態
    """
    __tablename__ = "crawl_state"
    username        = Column(String, primary_key=True)
    last_crawl_at   = Column(DateTime)        # 方便依時間增量
    total_crawled   = Column(Integer, default=0)
```

> **遷移腳本**：若你用 Alembic
> `alembic revision -m "add crawl_state" && alembic upgrade head`

---

## 2 | 入口函式：決定「要抓幾篇？」

在 `PlaywrightLogic.fetch_posts()` 進入點，一開始多做 3 步：

```python
# ① 讀取目前資料庫已抓數量
async with AsyncSession(engine) as sess:
    existing = await sess.execute(
        select(PostMetrics.post_id).where(PostMetrics.username == username)
    )
    existing_post_ids = {row[0] for row in existing}
    already = len(existing_post_ids)

# ② 根據使用者指令算「這次想要的新總量」
target_total = already + wanted_extra   # 例如 already=50, wanted_extra=50 → 100
need_to_fetch = max(0, target_total - already)
if need_to_fetch == 0:
    logging.info(f"🟢 {username} 已有 {already} 篇，無需增量抓取")
    return PostMetricsBatch(posts=[], username=username, total_count=already)

# ③ 把 need_to_fetch 傳進 get_ordered_post_urls_from_page()
ordered_urls = await self.get_ordered_post_urls_from_page(
    page, username, need_to_fetch + 10  # +10 給保險 buffer
)
```

> * 你不必動 **滾動、解析、補齊** 的任何程式；
>   只要在「挑 URL」時丟掉已抓過的 `post_id` 就行。
> * 把 `need_to_fetch` 控制在 **尚未擁有的新貼文** 數量即可。

---

## 3 | 去重：挑掉抓過的網址

在 `ordered_post_urls` 迴圈內，第一步就判斷：

```python
for post_url in ordered_post_urls:
    code = post_url.rstrip('/').split('/')[-1]
    post_id = f"{username}_{code}"
    if post_id in existing_post_ids:
        continue                     # <- 已抓過就跳過
    # …以下流程保持不變…
```

只要 **收集到 `need_to_fetch` 篇** 就 `break`，省下無謂滾動。

---

## 4 | 寫回資料庫 & 更新 CrawlState

`final_posts` 處理完成後：

```python
async with AsyncSession(engine) as sess:
    # ④ UPSERT 新貼文
    sess.add_all(final_posts)               # SQLAlchemy 2.0 可用 bulk_save_objects
    # ⑤ 更新 crawl_state
    await sess.merge(CrawlState(
        username       = username,
        last_crawl_at  = datetime.utcnow(),
        total_crawled  = already + len(final_posts)
    ))
    await sess.commit()
```

---

## 5 | 使用方式示例

```python
# 想多抓 50 篇
await playwright_logic.fetch_posts(
    username="natgeo",
    max_posts=50,        # 這裡改名叫 wanted_extra 也可
    auth_json_content=auth,
    task_id="incremental_natgeo_50"
)
```

首輪會抓 50；第二輪又執行同指令，程式自動偵測已經有 100 篇，所以只會再抓發文期間真正新增的貼文（0 \~ n 篇）。

---

## 6 | 附加優勢

| 功能         | 原理                                       | 需要新碼量 |
| ---------- | ---------------------------------------- | ----- |
| **自動跳過刪文** | `post_id` 唯一鍵，插入時若已存在直接忽略                | 0     |
| **新貼文通知**  | 比對 `created_at > last_crawl_at` 就能判斷     | <10 行 |
| **回補缺漏**   | `need_to_fetch` 會把「先前因錯誤 missed 的貼文」一起補回 | 已內建   |

---

### 一次性改動，長期收益

* **不碰解析器、不碰 Agent flow**：只在「入口」和「資料層」加薄薄一層。
* **保持原本 MCP / NATS 架構**：任何 Agent 要增量重爬，只需把「想加幾篇」寫進指令。
* **擴充容易**：之後若要每天自動增量，只要把 `wanted_extra=∞` 改成「抓所有新貼文直到遇到重複」即可。

---

若要看 **完整 diff** 或 Alembic migration 範例，隨時告訴我！
