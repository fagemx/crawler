ä»¥ä¸‹åšæ³•èƒ½åœ¨ **å®Œå…¨ä¸æ”¹å‹•ä½ æ—¢æœ‰ã€ŒæŠ“å–é‚è¼¯ã€** çš„å‰æä¸‹ï¼Œç‚º *åŒä¸€å¸³è™Ÿ* å¢åŠ ã€Œå·²æŠ“å–è²¼æ–‡è¨˜éŒ„ã€èˆ‡ã€Œå¢é‡çˆ¬å–ã€èƒ½åŠ›ã€‚åªè¦åœ¨ **è³‡æ–™åº«å±¤ + å…¥å£å‡½å¼** å‹•ä¸€é»é»æ‰‹è…³å³å¯ã€‚

---

## 1 | è³‡æ–™åº«ï¼šå…©å¼µæœ€å°è¡¨

> ä½ å·²ç¶“åœ¨ç”¨ PostgreSQL ï¼‹ SQLAlchemy (`common.models`) å„²å­˜ `PostMetrics`ã€‚
> åªéœ€å†åŠ ä¸€å¼µã€Œå¸³è™Ÿçˆ¬å–ç‹€æ…‹è¡¨ã€ä¸¦åœ¨ `post_id` ä¸ŠåŠ å”¯ä¸€éµå³å¯ã€‚

```python
# common/models.py  (ç¯€éŒ„)
class PostMetrics(Base):
    __tablename__ = "post_metrics"
    id           = Column(Integer, primary_key=True)  # â†’ å…§éƒ¨ç”¨
    post_id      = Column(String, unique=True)        # natgeo_DM0X0DTNr_r
    username     = Column(String, index=True)
    created_at   = Column(DateTime)
    # â€¦â€¦å…¶é¤˜æ¬„ä½ç…§èˆŠâ€¦â€¦

class CrawlState(Base):
    """
    æ¯å€‹å¸³è™Ÿåªæœƒæœ‰ä¸€åˆ—ï¼Œç”¨ä¾†è¨˜éŒ„ä¸Šæ¬¡çˆ¬å®Œå¾Œçš„ç‹€æ…‹
    """
    __tablename__ = "crawl_state"
    username        = Column(String, primary_key=True)
    last_crawl_at   = Column(DateTime)        # æ–¹ä¾¿ä¾æ™‚é–“å¢é‡
    total_crawled   = Column(Integer, default=0)
```

> **é·ç§»è…³æœ¬**ï¼šè‹¥ä½ ç”¨ Alembic
> `alembic revision -m "add crawl_state" && alembic upgrade head`

---

## 2 | å…¥å£å‡½å¼ï¼šæ±ºå®šã€Œè¦æŠ“å¹¾ç¯‡ï¼Ÿã€

åœ¨ `PlaywrightLogic.fetch_posts()` é€²å…¥é»ï¼Œä¸€é–‹å§‹å¤šåš 3 æ­¥ï¼š

```python
# â‘  è®€å–ç›®å‰è³‡æ–™åº«å·²æŠ“æ•¸é‡
async with AsyncSession(engine) as sess:
    existing = await sess.execute(
        select(PostMetrics.post_id).where(PostMetrics.username == username)
    )
    existing_post_ids = {row[0] for row in existing}
    already = len(existing_post_ids)

# â‘¡ æ ¹æ“šä½¿ç”¨è€…æŒ‡ä»¤ç®—ã€Œé€™æ¬¡æƒ³è¦çš„æ–°ç¸½é‡ã€
target_total = already + wanted_extra   # ä¾‹å¦‚ already=50, wanted_extra=50 â†’ 100
need_to_fetch = max(0, target_total - already)
if need_to_fetch == 0:
    logging.info(f"ğŸŸ¢ {username} å·²æœ‰ {already} ç¯‡ï¼Œç„¡éœ€å¢é‡æŠ“å–")
    return PostMetricsBatch(posts=[], username=username, total_count=already)

# â‘¢ æŠŠ need_to_fetch å‚³é€² get_ordered_post_urls_from_page()
ordered_urls = await self.get_ordered_post_urls_from_page(
    page, username, need_to_fetch + 10  # +10 çµ¦ä¿éšª buffer
)
```

> * ä½ ä¸å¿…å‹• **æ»¾å‹•ã€è§£æã€è£œé½Š** çš„ä»»ä½•ç¨‹å¼ï¼›
>   åªè¦åœ¨ã€ŒæŒ‘ URLã€æ™‚ä¸Ÿæ‰å·²æŠ“éçš„ `post_id` å°±è¡Œã€‚
> * æŠŠ `need_to_fetch` æ§åˆ¶åœ¨ **å°šæœªæ“æœ‰çš„æ–°è²¼æ–‡** æ•¸é‡å³å¯ã€‚

---

## 3 | å»é‡ï¼šæŒ‘æ‰æŠ“éçš„ç¶²å€

åœ¨ `ordered_post_urls` è¿´åœˆå…§ï¼Œç¬¬ä¸€æ­¥å°±åˆ¤æ–·ï¼š

```python
for post_url in ordered_post_urls:
    code = post_url.rstrip('/').split('/')[-1]
    post_id = f"{username}_{code}"
    if post_id in existing_post_ids:
        continue                     # <- å·²æŠ“éå°±è·³é
    # â€¦ä»¥ä¸‹æµç¨‹ä¿æŒä¸è®Šâ€¦
```

åªè¦ **æ”¶é›†åˆ° `need_to_fetch` ç¯‡** å°± `break`ï¼Œçœä¸‹ç„¡è¬‚æ»¾å‹•ã€‚

---

## 4 | å¯«å›è³‡æ–™åº« & æ›´æ–° CrawlState

`final_posts` è™•ç†å®Œæˆå¾Œï¼š

```python
async with AsyncSession(engine) as sess:
    # â‘£ UPSERT æ–°è²¼æ–‡
    sess.add_all(final_posts)               # SQLAlchemy 2.0 å¯ç”¨ bulk_save_objects
    # â‘¤ æ›´æ–° crawl_state
    await sess.merge(CrawlState(
        username       = username,
        last_crawl_at  = datetime.utcnow(),
        total_crawled  = already + len(final_posts)
    ))
    await sess.commit()
```

---

## 5 | ä½¿ç”¨æ–¹å¼ç¤ºä¾‹

```python
# æƒ³å¤šæŠ“ 50 ç¯‡
await playwright_logic.fetch_posts(
    username="natgeo",
    max_posts=50,        # é€™è£¡æ”¹åå« wanted_extra ä¹Ÿå¯
    auth_json_content=auth,
    task_id="incremental_natgeo_50"
)
```

é¦–è¼ªæœƒæŠ“ 50ï¼›ç¬¬äºŒè¼ªåˆåŸ·è¡ŒåŒæŒ‡ä»¤ï¼Œç¨‹å¼è‡ªå‹•åµæ¸¬å·²ç¶“æœ‰ 100 ç¯‡ï¼Œæ‰€ä»¥åªæœƒå†æŠ“ç™¼æ–‡æœŸé–“çœŸæ­£æ–°å¢çš„è²¼æ–‡ï¼ˆ0 \~ n ç¯‡ï¼‰ã€‚

---

## 6 | é™„åŠ å„ªå‹¢

| åŠŸèƒ½         | åŸç†                                       | éœ€è¦æ–°ç¢¼é‡ |
| ---------- | ---------------------------------------- | ----- |
| **è‡ªå‹•è·³éåˆªæ–‡** | `post_id` å”¯ä¸€éµï¼Œæ’å…¥æ™‚è‹¥å·²å­˜åœ¨ç›´æ¥å¿½ç•¥                | 0     |
| **æ–°è²¼æ–‡é€šçŸ¥**  | æ¯”å° `created_at > last_crawl_at` å°±èƒ½åˆ¤æ–·     | <10 è¡Œ |
| **å›è£œç¼ºæ¼**   | `need_to_fetch` æœƒæŠŠã€Œå…ˆå‰å› éŒ¯èª¤ missed çš„è²¼æ–‡ã€ä¸€èµ·è£œå› | å·²å…§å»º   |

---

### ä¸€æ¬¡æ€§æ”¹å‹•ï¼Œé•·æœŸæ”¶ç›Š

* **ä¸ç¢°è§£æå™¨ã€ä¸ç¢° Agent flow**ï¼šåªåœ¨ã€Œå…¥å£ã€å’Œã€Œè³‡æ–™å±¤ã€åŠ è–„è–„ä¸€å±¤ã€‚
* **ä¿æŒåŸæœ¬ MCP / NATS æ¶æ§‹**ï¼šä»»ä½• Agent è¦å¢é‡é‡çˆ¬ï¼Œåªéœ€æŠŠã€Œæƒ³åŠ å¹¾ç¯‡ã€å¯«é€²æŒ‡ä»¤ã€‚
* **æ“´å……å®¹æ˜“**ï¼šä¹‹å¾Œè‹¥è¦æ¯å¤©è‡ªå‹•å¢é‡ï¼Œåªè¦æŠŠ `wanted_extra=âˆ` æ”¹æˆã€ŒæŠ“æ‰€æœ‰æ–°è²¼æ–‡ç›´åˆ°é‡åˆ°é‡è¤‡ã€å³å¯ã€‚

---

è‹¥è¦çœ‹ **å®Œæ•´ diff** æˆ– Alembic migration ç¯„ä¾‹ï¼Œéš¨æ™‚å‘Šè¨´æˆ‘ï¼


####


ä»¥ä¸‹åšæ³•èƒ½åœ¨ **å®Œå…¨ä¸æ”¹å‹•ä½ æ—¢æœ‰ã€ŒæŠ“å–é‚è¼¯ã€** çš„å‰æä¸‹ï¼Œç‚º *åŒä¸€å¸³è™Ÿ* å¢åŠ ã€Œå·²æŠ“å–è²¼æ–‡è¨˜éŒ„ã€èˆ‡ã€Œå¢é‡çˆ¬å–ã€èƒ½åŠ›ã€‚åªè¦åœ¨ **è³‡æ–™åº«å±¤ + å…¥å£å‡½å¼** å‹•ä¸€é»é»æ‰‹è…³å³å¯ã€‚

---

## 1 | è³‡æ–™åº«ï¼šå…©å¼µæœ€å°è¡¨

> ä½ å·²ç¶“åœ¨ç”¨ PostgreSQL ï¼‹ SQLAlchemy (`common.models`) å„²å­˜ `PostMetrics`ã€‚
> åªéœ€å†åŠ ä¸€å¼µã€Œå¸³è™Ÿçˆ¬å–ç‹€æ…‹è¡¨ã€ä¸¦åœ¨ `post_id` ä¸ŠåŠ å”¯ä¸€éµå³å¯ã€‚

```python
# common/models.py  (ç¯€éŒ„)
class PostMetrics(Base):
    __tablename__ = "post_metrics"
    id           = Column(Integer, primary_key=True)  # â†’ å…§éƒ¨ç”¨
    post_id      = Column(String, unique=True)        # natgeo_DM0X0DTNr_r
    username     = Column(String, index=True)
    created_at   = Column(DateTime)
    # â€¦â€¦å…¶é¤˜æ¬„ä½ç…§èˆŠâ€¦â€¦

class CrawlState(Base):
    """
    æ¯å€‹å¸³è™Ÿåªæœƒæœ‰ä¸€åˆ—ï¼Œç”¨ä¾†è¨˜éŒ„ä¸Šæ¬¡çˆ¬å®Œå¾Œçš„ç‹€æ…‹
    """
    __tablename__ = "crawl_state"
    username        = Column(String, primary_key=True)
    last_crawl_at   = Column(DateTime)        # æ–¹ä¾¿ä¾æ™‚é–“å¢é‡
    total_crawled   = Column(Integer, default=0)
```

> **é·ç§»è…³æœ¬**ï¼šè‹¥ä½ ç”¨ Alembic
> `alembic revision -m "add crawl_state" && alembic upgrade head`

---

## 2 | å…¥å£å‡½å¼ï¼šæ±ºå®šã€Œè¦æŠ“å¹¾ç¯‡ï¼Ÿã€

åœ¨ `PlaywrightLogic.fetch_posts()` é€²å…¥é»ï¼Œä¸€é–‹å§‹å¤šåš 3 æ­¥ï¼š

```python
# â‘  è®€å–ç›®å‰è³‡æ–™åº«å·²æŠ“æ•¸é‡
async with AsyncSession(engine) as sess:
    existing = await sess.execute(
        select(PostMetrics.post_id).where(PostMetrics.username == username)
    )
    existing_post_ids = {row[0] for row in existing}
    already = len(existing_post_ids)

# â‘¡ æ ¹æ“šä½¿ç”¨è€…æŒ‡ä»¤ç®—ã€Œé€™æ¬¡æƒ³è¦çš„æ–°ç¸½é‡ã€
target_total = already + wanted_extra   # ä¾‹å¦‚ already=50, wanted_extra=50 â†’ 100
need_to_fetch = max(0, target_total - already)
if need_to_fetch == 0:
    logging.info(f"ğŸŸ¢ {username} å·²æœ‰ {already} ç¯‡ï¼Œç„¡éœ€å¢é‡æŠ“å–")
    return PostMetricsBatch(posts=[], username=username, total_count=already)

# â‘¢ æŠŠ need_to_fetch å‚³é€² get_ordered_post_urls_from_page()
ordered_urls = await self.get_ordered_post_urls_from_page(
    page, username, need_to_fetch + 10  # +10 çµ¦ä¿éšª buffer
)
```

> * ä½ ä¸å¿…å‹• **æ»¾å‹•ã€è§£æã€è£œé½Š** çš„ä»»ä½•ç¨‹å¼ï¼›
>   åªè¦åœ¨ã€ŒæŒ‘ URLã€æ™‚ä¸Ÿæ‰å·²æŠ“éçš„ `post_id` å°±è¡Œã€‚
> * æŠŠ `need_to_fetch` æ§åˆ¶åœ¨ **å°šæœªæ“æœ‰çš„æ–°è²¼æ–‡** æ•¸é‡å³å¯ã€‚

---

## 3 | å»é‡ï¼šæŒ‘æ‰æŠ“éçš„ç¶²å€

åœ¨ `ordered_post_urls` è¿´åœˆå…§ï¼Œç¬¬ä¸€æ­¥å°±åˆ¤æ–·ï¼š

```python
for post_url in ordered_post_urls:
    code = post_url.rstrip('/').split('/')[-1]
    post_id = f"{username}_{code}"
    if post_id in existing_post_ids:
        continue                     # <- å·²æŠ“éå°±è·³é
    # â€¦ä»¥ä¸‹æµç¨‹ä¿æŒä¸è®Šâ€¦
```

åªè¦ **æ”¶é›†åˆ° `need_to_fetch` ç¯‡** å°± `break`ï¼Œçœä¸‹ç„¡è¬‚æ»¾å‹•ã€‚

---

## 4 | å¯«å›è³‡æ–™åº« & æ›´æ–° CrawlState

`final_posts` è™•ç†å®Œæˆå¾Œï¼š

```python
async with AsyncSession(engine) as sess:
    # â‘£ UPSERT æ–°è²¼æ–‡
    sess.add_all(final_posts)               # SQLAlchemy 2.0 å¯ç”¨ bulk_save_objects
    # â‘¤ æ›´æ–° crawl_state
    await sess.merge(CrawlState(
        username       = username,
        last_crawl_at  = datetime.utcnow(),
        total_crawled  = already + len(final_posts)
    ))
    await sess.commit()
```

---

## 5 | ä½¿ç”¨æ–¹å¼ç¤ºä¾‹

```python
# æƒ³å¤šæŠ“ 50 ç¯‡
await playwright_logic.fetch_posts(
    username="natgeo",
    max_posts=50,        # é€™è£¡æ”¹åå« wanted_extra ä¹Ÿå¯
    auth_json_content=auth,
    task_id="incremental_natgeo_50"
)
```

é¦–è¼ªæœƒæŠ“ 50ï¼›ç¬¬äºŒè¼ªåˆåŸ·è¡ŒåŒæŒ‡ä»¤ï¼Œç¨‹å¼è‡ªå‹•åµæ¸¬å·²ç¶“æœ‰ 100 ç¯‡ï¼Œæ‰€ä»¥åªæœƒå†æŠ“ç™¼æ–‡æœŸé–“çœŸæ­£æ–°å¢çš„è²¼æ–‡ï¼ˆ0 \~ n ç¯‡ï¼‰ã€‚

---

## 6 | é™„åŠ å„ªå‹¢

| åŠŸèƒ½         | åŸç†                                       | éœ€è¦æ–°ç¢¼é‡ |
| ---------- | ---------------------------------------- | ----- |
| **è‡ªå‹•è·³éåˆªæ–‡** | `post_id` å”¯ä¸€éµï¼Œæ’å…¥æ™‚è‹¥å·²å­˜åœ¨ç›´æ¥å¿½ç•¥                | 0     |
| **æ–°è²¼æ–‡é€šçŸ¥**  | æ¯”å° `created_at > last_crawl_at` å°±èƒ½åˆ¤æ–·     | <10 è¡Œ |
| **å›è£œç¼ºæ¼**   | `need_to_fetch` æœƒæŠŠã€Œå…ˆå‰å› éŒ¯èª¤ missed çš„è²¼æ–‡ã€ä¸€èµ·è£œå› | å·²å…§å»º   |

---

### ä¸€æ¬¡æ€§æ”¹å‹•ï¼Œé•·æœŸæ”¶ç›Š

* **ä¸ç¢°è§£æå™¨ã€ä¸ç¢° Agent flow**ï¼šåªåœ¨ã€Œå…¥å£ã€å’Œã€Œè³‡æ–™å±¤ã€åŠ è–„è–„ä¸€å±¤ã€‚
* **ä¿æŒåŸæœ¬ MCP / NATS æ¶æ§‹**ï¼šä»»ä½• Agent è¦å¢é‡é‡çˆ¬ï¼Œåªéœ€æŠŠã€Œæƒ³åŠ å¹¾ç¯‡ã€å¯«é€²æŒ‡ä»¤ã€‚
* **æ“´å……å®¹æ˜“**ï¼šä¹‹å¾Œè‹¥è¦æ¯å¤©è‡ªå‹•å¢é‡ï¼Œåªè¦æŠŠ `wanted_extra=âˆ` æ”¹æˆã€ŒæŠ“æ‰€æœ‰æ–°è²¼æ–‡ç›´åˆ°é‡åˆ°é‡è¤‡ã€å³å¯ã€‚

---

è‹¥è¦çœ‹ **å®Œæ•´ diff** æˆ– Alembic migration ç¯„ä¾‹ï¼Œéš¨æ™‚å‘Šè¨´æˆ‘ï¼
