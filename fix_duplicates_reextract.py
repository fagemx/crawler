#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è™•ç†é‡è¤‡è²¼æ–‡ï¼šä¿ç•™è§€çœ‹æ•¸é«˜çš„ï¼Œè§€çœ‹æ•¸ä½çš„ç”¨APIé‡æ–°æå–
"""

import json
import requests
import time
from typing import Dict, List, Optional
from collections import defaultdict

class DuplicateProcessor:
    def __init__(self):
        self.official_reader_url = "https://r.jina.ai"
        self.official_headers = {'X-Return-Format': 'markdown'}
        
    def convert_views_to_number(self, views_str: str) -> int:
        """å°‡è§€çœ‹æ•¸å­—ç¬¦ä¸²è½‰æ›ç‚ºæ•¸å­—ä»¥ä¾¿æ¯”è¼ƒ"""
        if not views_str:
            return 0
        
        views_str = views_str.upper().replace(',', '')
        
        if views_str.endswith('K'):
            return int(float(views_str[:-1]) * 1000)
        elif views_str.endswith('M'):
            return int(float(views_str[:-1]) * 1000000)
        elif views_str.endswith('B'):
            return int(float(views_str[:-1]) * 1000000000)
        else:
            try:
                return int(views_str)
            except:
                return 0

    def fetch_content_jina_api(self, url: str) -> tuple:
        """ä½¿ç”¨Jina APIé‡æ–°ç²å–å…§å®¹"""
        try:
            print(f"    ğŸŒ APIé‡æ–°æå–: {url.split('/')[-1]}...", end=" ")
            response = requests.get(f"{self.official_reader_url}/{url}", 
                                  headers=self.official_headers, timeout=30)
            if response.status_code == 200:
                print("âœ…")
                return True, response.text
            else:
                print(f"âŒ HTTP {response.status_code}")
                return False, f"HTTP {response.status_code}"
        except Exception as e:
            print(f"âŒ {str(e)}")
            return False, str(e)

    def extract_post_content_smart(self, content: str) -> Optional[str]:
        """æ™ºèƒ½æå–è²¼æ–‡å…§å®¹"""
        lines = content.split('\n')
        
        # ç­–ç•¥1: å¦‚æœç¬¬ä¸€è¡Œæ˜¯å›è¦†ï¼Œç›´æ¥ä½¿ç”¨
        if lines and lines[0].strip().startswith('>>>'):
            reply_content = lines[0].strip()
            if len(reply_content) > 10:
                return reply_content
        
        # ç­–ç•¥2: å°‹æ‰¾ä¸»è²¼æ–‡å…§å®¹
        for i, line in enumerate(lines):
            stripped = line.strip()
            
            # è·³éæ˜é¡¯çš„éå…§å®¹è¡Œ
            if (not stripped or
                stripped.startswith('[') or
                stripped.startswith('![') or
                stripped.startswith('http') or
                stripped.startswith('Log in') or
                stripped.startswith('Thread') or
                stripped.startswith('gvmonthly') or
                stripped.isdigit() or
                stripped in ['Translate', 'views', '===============']):
                continue
            
            # æª¢æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆçš„è²¼æ–‡å…§å®¹
            if (len(stripped) > 15 and
                not stripped.startswith('>>>') and
                ('ã€‚' in stripped or 'ï¼Œ' in stripped or '!' in stripped or 
                 '?' in stripped or 'ğŸ˜†' in stripped or 'ğŸ˜…' in stripped)):
                
                # æª¢æŸ¥å¾ŒçºŒæ˜¯å¦æœ‰ Translate æ¨™è­˜
                for j in range(i + 1, min(i + 3, len(lines))):
                    if 'Translate' in lines[j]:
                        return stripped
                
                # å¦‚æœå…§å®¹åˆç†ä¸”é•·åº¦è¶³å¤ ï¼Œä¹Ÿè¿”å›
                if len(stripped) > 25:
                    return stripped
        
        return None

    def extract_views_count(self, content: str, post_id: str) -> Optional[str]:
        """æå–è§€çœ‹æ•¸"""
        # æ­£è¦åŒ–å…§å®¹
        content = content.replace('\u00a0', ' ').replace('\xa0', ' ')
        
        # è§€çœ‹æ•¸æ¨¡å¼
        view_patterns = [
            r'(\d+(?:\.\d+)?[KMB]?)\s*views',
            r'Thread\s*=+\s*(\d+(?:\.\d+)?[KMB]?)\s*views',
        ]
        
        import re
        for pattern in view_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            if matches:
                return matches[0]
        
        return None

    def extract_engagement_data(self, content: str) -> Dict[str, Optional[str]]:
        """æå–äº’å‹•æ•¸æ“š"""
        lines = content.split('\n')
        numbers = []
        
        # å°‹æ‰¾æ•¸å­—åºåˆ—
        for i, line in enumerate(lines):
            stripped = line.strip()
            if stripped.startswith('![Image') and 'profile picture' not in stripped:
                # åœ¨åœ–ç‰‡å¾ŒæŸ¥æ‰¾æ•¸å­—
                for j in range(i + 1, min(i + 20, len(lines))):
                    candidate = lines[j].strip()
                    import re
                    if re.match(r'^\d+(?:\.\d+)?[KMB]?$', candidate):
                        numbers.append(candidate)
                    elif candidate and candidate != "Pinned":
                        break
                
                if len(numbers) >= 3:
                    break
        
        return {
            'likes': numbers[0] if len(numbers) >= 1 else None,
            'comments': numbers[1] if len(numbers) >= 2 else None,
            'reposts': numbers[2] if len(numbers) >= 3 else None,
            'shares': numbers[3] if len(numbers) >= 4 else None,
        }

    def parse_post_complete(self, url: str, content: str, source: str) -> Dict:
        """å®Œæ•´è§£æè²¼æ–‡"""
        post_id = url.split('/')[-1] if '/' in url else url
        views = self.extract_views_count(content, post_id)
        main_content = self.extract_post_content_smart(content)
        engagement = self.extract_engagement_data(content)
        
        return {
            'post_id': post_id,
            'url': url,
            'views': views,
            'content': main_content,
            'source': source,
            'likes': engagement['likes'],
            'comments': engagement['comments'],
            'reposts': engagement['reposts'],
            'shares': engagement['shares'],
            'success': views is not None and main_content is not None,
            'has_views': views is not None,
            'has_content': main_content is not None,
            'has_likes': engagement['likes'] is not None,
            'has_comments': engagement['comments'] is not None,
            'has_reposts': engagement['reposts'] is not None,
            'has_shares': engagement['shares'] is not None,
            'content_length': len(content),
            'reextracted': True  # æ¨™è¨˜ç‚ºé‡æ–°æå–
        }

    def process_duplicates(self, filename: str):
        """è™•ç†é‡è¤‡è²¼æ–‡"""
        
        print(f"ğŸ“‚ è™•ç†æ–‡ä»¶: {filename}")
        
        # è®€å–çµæœ
        with open(filename, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        results = data.get('results', [])
        print(f"ğŸ“Š åŸå§‹é …ç›®æ•¸: {len(results)}")
        
        # æŒ‰ post_id åˆ†çµ„ï¼Œæ‰¾å‡ºé‡è¤‡é …
        grouped = defaultdict(list)
        for result in results:
            if result.get('post_id'):
                grouped[result['post_id']].append(result)
        
        # è™•ç†é‡è¤‡é …
        processed_results = []
        duplicates_found = 0
        reextracted_count = 0
        
        for post_id, items in grouped.items():
            if len(items) > 1:
                duplicates_found += 1
                print(f"\nğŸ” ç™¼ç¾é‡è¤‡: {post_id} ({len(items)} å€‹ç‰ˆæœ¬)")
                
                # é¡¯ç¤ºæ‰€æœ‰ç‰ˆæœ¬
                for i, item in enumerate(items):
                    views = item.get('views', 'N/A')
                    views_num = self.convert_views_to_number(views)
                    source = item.get('source', 'unknown')
                    content_preview = item.get('content', 'N/A')[:30] + '...' if item.get('content') else 'N/A'
                    print(f"   ç‰ˆæœ¬{i+1}: {views} ({views_num:,}) | {source} | {content_preview}")
                
                # æ‰¾å‡ºè§€çœ‹æ•¸æœ€é«˜çš„
                best_item = max(items, key=lambda x: self.convert_views_to_number(x.get('views', '0')))
                best_views = self.convert_views_to_number(best_item.get('views', '0'))
                
                print(f"   âœ… ä¿ç•™æœ€é«˜è§€çœ‹æ•¸ç‰ˆæœ¬: {best_item.get('views')}")
                processed_results.append(best_item)
                
                # å°è§€çœ‹æ•¸è¼ƒä½çš„ç‰ˆæœ¬ç”¨APIé‡æ–°æå–
                for item in items:
                    if item != best_item:
                        item_views = self.convert_views_to_number(item.get('views', '0'))
                        print(f"   ğŸ”„ è§€çœ‹æ•¸è¼ƒä½ ({item.get('views')})ï¼ŒAPIé‡æ–°æå–...")
                        
                        # ç”¨APIé‡æ–°æå–
                        success, content = self.fetch_content_jina_api(item['url'])
                        
                        if success:
                            # é‡æ–°è§£æ
                            reextracted_result = self.parse_post_complete(
                                item['url'], content, 'jina_api_reextract'
                            )
                            
                            # æ¯”è¼ƒå…§å®¹æ˜¯å¦ä¸åŒ
                            old_content = item.get('content', '')[:50] + '...' if item.get('content') else 'N/A'
                            new_content = reextracted_result.get('content', '')[:50] + '...' if reextracted_result.get('content') else 'N/A'
                            
                            if old_content != new_content:
                                print(f"      ğŸ“ å…§å®¹å·²æ›´æ–°:")
                                print(f"         èˆŠ: {old_content}")
                                print(f"         æ–°: {new_content}")
                            else:
                                print(f"      ğŸ“ å…§å®¹ç›¸åŒï¼Œæ•¸æ“šå·²æ›´æ–°")
                            
                            processed_results.append(reextracted_result)
                            reextracted_count += 1
                        else:
                            print(f"      âŒ APIé‡æ–°æå–å¤±æ•—: {content}")
                            # ä¿ç•™åŸå§‹æ•¸æ“š
                            item['reextract_failed'] = True
                            item['reextract_error'] = content
                            processed_results.append(item)
                        
                        # çŸ­æš«ç­‰å¾…ï¼Œé¿å…APIé™åˆ¶
                        time.sleep(1)
            
            else:
                # æ²’æœ‰é‡è¤‡ï¼Œç›´æ¥ä¿ç•™
                processed_results.append(items[0])
        
        print(f"\nğŸ“Š è™•ç†çµæœ:")
        print(f"   ğŸ”„ ç™¼ç¾é‡è¤‡çµ„: {duplicates_found}")
        print(f"   ğŸŒ APIé‡æ–°æå–: {reextracted_count}")
        print(f"   ğŸ“ æœ€çµ‚é …ç›®æ•¸: {len(processed_results)}")
        
        # ä¿å­˜çµæœ
        data['results'] = processed_results
        data['total_processed'] = len(processed_results)
        data['duplicates_processed'] = duplicates_found
        data['reextracted_count'] = reextracted_count
        
        # é‡æ–°è¨ˆç®—çµ±è¨ˆ
        successful = [r for r in processed_results if r.get('success', False)]
        data['overall_success_rate'] = len(successful) / len(processed_results) * 100 if processed_results else 0
        
        output_filename = filename.replace('.json', '_dedup.json')
        with open(output_filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ è™•ç†çµæœå·²ä¿å­˜åˆ°: {output_filename}")
        
        # é¡¯ç¤ºæœ€çµ‚çµæœæ¨£æœ¬
        print(f"\nğŸ¯ è™•ç†å¾Œçš„æ¨£æœ¬ (å‰5å€‹):")
        for i, result in enumerate(processed_results[:5]):
            post_id = result.get('post_id', 'N/A')
            views = result.get('views', 'N/A')
            source = result.get('source', 'N/A')
            reextracted = result.get('reextracted', False)
            content = result.get('content', 'N/A')
            content_preview = content[:50] + '...' if len(content) > 50 else content
            reext_mark = " [é‡æ–°æå–]" if reextracted else ""
            
            print(f"   {i+1}. {post_id} | {views} | {source}{reext_mark}")
            print(f"      ğŸ“ {content_preview}")

if __name__ == "__main__":
    processor = DuplicateProcessor()
    
    # è™•ç†æœ€æ–°çš„çµæœæ–‡ä»¶
    import glob
    result_files = glob.glob("realtime_extraction_results_*.json")
    if result_files:
        latest_file = max(result_files, key=lambda x: x.split('_')[-1])
        print(f"ğŸ¯ è™•ç†æœ€æ–°æ–‡ä»¶: {latest_file}")
        processor.process_duplicates(latest_file)
    else:
        print("âŒ æœªæ‰¾åˆ°çµæœæ–‡ä»¶")