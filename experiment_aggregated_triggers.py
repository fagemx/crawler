#!/usr/bin/env python3
import requests
import json
import time
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
import re

class AggregatedPageExperiment:
    def __init__(self):
        self.base_url = "http://localhost:8880"
        self.test_urls = [
            'https://www.threads.com/@ttshow.tw/post/DMfOVeqSkM5',  # ä¹‹å‰å‡ºç¾èšåˆé é¢çš„
            'https://www.threads.com/@ttshow.tw/post/DIfkbgLSjO3',  # ä¹‹å‰æˆåŠŸçš„
            'https://www.threads.com/@ttshow.tw/post/DL_vyT-RZQ6',  # ä¹‹å‰å‡ºç¾èšåˆé é¢çš„
        ]
        self.results = []
        
    def make_request(self, url, request_id=None, delay=0):
        """åŸ·è¡Œå–®æ¬¡è«‹æ±‚ä¸¦è¨˜éŒ„çµæœ"""
        if delay > 0:
            time.sleep(delay)
            
        start_time = time.time()
        try:
            response = requests.get(f"{self.base_url}/{url}", timeout=30)
            end_time = time.time()
            
            content = response.text
            
            # æª¢æŸ¥æ˜¯å¦ç‚ºèšåˆé é¢çš„æŒ‡æ¨™
            is_aggregated = self.check_if_aggregated(content, url)
            
            result = {
                'request_id': request_id,
                'url': url,
                'timestamp': datetime.now().isoformat(),
                'response_time': round(end_time - start_time, 2),
                'status_code': response.status_code,
                'content_length': len(content),
                'is_aggregated': is_aggregated,
                'has_related_threads': 'Related threads' in content,
                'other_post_ids': self.find_other_post_ids(content, url),
                'view_count': self.extract_view_count(content),
                'first_100_chars': content[:100].replace('\n', '\\n'),
            }
            
            return result
            
        except Exception as e:
            return {
                'request_id': request_id,
                'url': url,
                'timestamp': datetime.now().isoformat(),
                'error': str(e),
                'is_aggregated': None,
            }
    
    def check_if_aggregated(self, content, original_url):
        """æª¢æŸ¥å…§å®¹æ˜¯å¦ç‚ºèšåˆé é¢"""
        # å¾åŸå§‹ URL æå– post_id
        post_id_match = re.search(r'/post/([A-Za-z0-9_-]+)', original_url)
        if not post_id_match:
            return False
        
        original_post_id = post_id_match.group(1)
        
        # æª¢æŸ¥å…§å®¹ä¸­æ˜¯å¦åŒ…å«å…¶ä»– post_id
        other_post_ids = re.findall(r'/post/([A-Za-z0-9_-]+)', content)
        other_unique_ids = set(other_post_ids) - {original_post_id}
        
        # å¦‚æœåŒ…å«è¶…é 1 å€‹å…¶ä»– post_idï¼Œå¾ˆå¯èƒ½æ˜¯èšåˆé é¢
        return len(other_unique_ids) > 1
    
    def find_other_post_ids(self, content, original_url):
        """æ‰¾å‡ºå…§å®¹ä¸­çš„å…¶ä»– post_id"""
        post_id_match = re.search(r'/post/([A-Za-z0-9_-]+)', original_url)
        if not post_id_match:
            return []
        
        original_post_id = post_id_match.group(1)
        other_post_ids = re.findall(r'/post/([A-Za-z0-9_-]+)', content)
        other_unique_ids = list(set(other_post_ids) - {original_post_id})
        
        return other_unique_ids[:5]  # åªè¿”å›å‰5å€‹
    
    def extract_view_count(self, content):
        """æå–è§€çœ‹æ•¸"""
        view_patterns = [
            r'Thread\s*={6}\s*([0-9,\.]+[KMB]?)\s*views?',
            r'(\d+(?:\.\d+)?[KMB]?)\s*views?',
        ]
        
        for pattern in view_patterns:
            match = re.search(pattern, content, re.IGNORECASE)
            if match:
                return match.group(1)
        return None

    def experiment_1_concurrency_levels(self):
        """å¯¦é©—1ï¼šæ¸¬è©¦ä¸åŒä½µç™¼ç´šåˆ¥"""
        print("ğŸ§ª å¯¦é©—1ï¼šä½µç™¼ç´šåˆ¥å°èšåˆé é¢çš„å½±éŸ¿")
        print("=" * 60)
        
        concurrency_levels = [1, 2, 4, 6, 8, 10]
        
        for concurrency in concurrency_levels:
            print(f"\nğŸ“Š æ¸¬è©¦ä½µç™¼æ•¸: {concurrency}")
            
            # æº–å‚™ç›¸åŒçš„ URL åˆ—è¡¨
            test_urls = self.test_urls * 3  # æ¯å€‹ URL æ¸¬è©¦3æ¬¡
            
            start_time = time.time()
            with ThreadPoolExecutor(max_workers=concurrency) as executor:
                futures = []
                for i, url in enumerate(test_urls):
                    future = executor.submit(self.make_request, url, f"{concurrency}-{i}")
                    futures.append(future)
                
                results = []
                for future in as_completed(futures):
                    result = future.result()
                    results.append(result)
            
            end_time = time.time()
            
            # çµ±è¨ˆçµæœ
            aggregated_count = sum(1 for r in results if r.get('is_aggregated', False))
            success_count = sum(1 for r in results if r.get('view_count') is not None)
            
            print(f"   â±ï¸  ç¸½è€—æ™‚: {end_time - start_time:.2f}s")
            print(f"   ğŸ“Š ç¸½è«‹æ±‚: {len(results)}")
            print(f"   âŒ èšåˆé é¢: {aggregated_count} ({aggregated_count/len(results)*100:.1f}%)")
            print(f"   âœ… æˆåŠŸæå–è§€çœ‹æ•¸: {success_count} ({success_count/len(results)*100:.1f}%)")
            
            # ä¿å­˜è©³ç´°çµæœ
            self.save_experiment_results(f"concurrency_{concurrency}", results)
            
            # ç­‰å¾…ä¸€ä¸‹é¿å…å½±éŸ¿ä¸‹ä¸€è¼ªæ¸¬è©¦
            time.sleep(2)

    def experiment_2_request_intervals(self):
        """å¯¦é©—2ï¼šæ¸¬è©¦è«‹æ±‚é–“éš”"""
        print("\nğŸ§ª å¯¦é©—2ï¼šè«‹æ±‚é–“éš”å°èšåˆé é¢çš„å½±éŸ¿")
        print("=" * 60)
        
        intervals = [0, 0.5, 1.0, 2.0, 5.0]
        
        for interval in intervals:
            print(f"\nğŸ“Š æ¸¬è©¦é–“éš”: {interval}ç§’")
            
            results = []
            start_time = time.time()
            
            # ä¸²è¡Œè«‹æ±‚ï¼Œä½†æœ‰é–“éš”
            for i, url in enumerate(self.test_urls * 3):
                if i > 0:  # ç¬¬ä¸€å€‹è«‹æ±‚ä¸å»¶é²
                    time.sleep(interval)
                
                result = self.make_request(url, f"interval-{interval}-{i}")
                results.append(result)
            
            end_time = time.time()
            
            # çµ±è¨ˆçµæœ
            aggregated_count = sum(1 for r in results if r.get('is_aggregated', False))
            success_count = sum(1 for r in results if r.get('view_count') is not None)
            
            print(f"   â±ï¸  ç¸½è€—æ™‚: {end_time - start_time:.2f}s")
            print(f"   âŒ èšåˆé é¢: {aggregated_count} ({aggregated_count/len(results)*100:.1f}%)")
            print(f"   âœ… æˆåŠŸæå–è§€çœ‹æ•¸: {success_count} ({success_count/len(results)*100:.1f}%)")
            
            self.save_experiment_results(f"interval_{interval}", results)

    def experiment_3_rapid_same_url(self):
        """å¯¦é©—3ï¼šå¿«é€Ÿé‡è¤‡è«‹æ±‚åŒä¸€ URL"""
        print("\nğŸ§ª å¯¦é©—3ï¼šå¿«é€Ÿé‡è¤‡è«‹æ±‚åŒä¸€ URL")
        print("=" * 60)
        
        test_url = self.test_urls[0]  # ä½¿ç”¨ç¬¬ä¸€å€‹ URL
        
        # å¿«é€Ÿé€£çºŒè«‹æ±‚åŒä¸€ URL 10 æ¬¡
        results = []
        start_time = time.time()
        
        with ThreadPoolExecutor(max_workers=5) as executor:
            futures = []
            for i in range(10):
                future = executor.submit(self.make_request, test_url, f"rapid-{i}")
                futures.append(future)
            
            for future in as_completed(futures):
                result = future.result()
                results.append(result)
        
        end_time = time.time()
        
        # çµ±è¨ˆçµæœ
        aggregated_count = sum(1 for r in results if r.get('is_aggregated', False))
        success_count = sum(1 for r in results if r.get('view_count') is not None)
        
        print(f"   â±ï¸  ç¸½è€—æ™‚: {end_time - start_time:.2f}s")
        print(f"   âŒ èšåˆé é¢: {aggregated_count} ({aggregated_count/len(results)*100:.1f}%)")
        print(f"   âœ… æˆåŠŸæå–è§€çœ‹æ•¸: {success_count} ({success_count/len(results)*100:.1f}%)")
        
        self.save_experiment_results("rapid_same_url", results)

    def experiment_4_cache_headers(self):
        """å¯¦é©—4ï¼šæ¸¬è©¦ç·©å­˜é ­çš„å½±éŸ¿"""
        print("\nğŸ§ª å¯¦é©—4ï¼šç·©å­˜é ­å°èšåˆé é¢çš„å½±éŸ¿")
        print("=" * 60)
        
        cache_strategies = [
            {"name": "é»˜èª", "headers": {}},
            {"name": "ç„¡ç·©å­˜", "headers": {"x-no-cache": "true"}},
            {"name": "å¼·åˆ¶åˆ·æ–°", "headers": {"Cache-Control": "no-cache, no-store, must-revalidate"}},
        ]
        
        for strategy in cache_strategies:
            print(f"\nğŸ“Š æ¸¬è©¦ç­–ç•¥: {strategy['name']}")
            
            results = []
            
            # ä½µç™¼è«‹æ±‚æ¸¬è©¦ç·©å­˜ç­–ç•¥
            with ThreadPoolExecutor(max_workers=4) as executor:
                futures = []
                for i, url in enumerate(self.test_urls * 2):
                    future = executor.submit(self.make_request_with_headers, url, strategy['headers'], f"cache-{strategy['name']}-{i}")
                    futures.append(future)
                
                for future in as_completed(futures):
                    result = future.result()
                    results.append(result)
            
            # çµ±è¨ˆçµæœ
            aggregated_count = sum(1 for r in results if r.get('is_aggregated', False))
            success_count = sum(1 for r in results if r.get('view_count') is not None)
            
            print(f"   âŒ èšåˆé é¢: {aggregated_count} ({aggregated_count/len(results)*100:.1f}%)")
            print(f"   âœ… æˆåŠŸæå–è§€çœ‹æ•¸: {success_count} ({success_count/len(results)*100:.1f}%)")
            
            self.save_experiment_results(f"cache_{strategy['name']}", results)

    def make_request_with_headers(self, url, headers, request_id):
        """å¸¶è‡ªå®šç¾©é ­çš„è«‹æ±‚"""
        try:
            response = requests.get(f"{self.base_url}/{url}", headers=headers, timeout=30)
            content = response.text
            
            is_aggregated = self.check_if_aggregated(content, url)
            
            return {
                'request_id': request_id,
                'url': url,
                'timestamp': datetime.now().isoformat(),
                'is_aggregated': is_aggregated,
                'view_count': self.extract_view_count(content),
                'headers_used': headers,
            }
        except Exception as e:
            return {
                'request_id': request_id,
                'url': url,
                'error': str(e),
                'headers_used': headers,
            }

    def save_experiment_results(self, experiment_name, results):
        """ä¿å­˜å¯¦é©—çµæœ"""
        filename = f"experiment_{experiment_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump({
                'experiment': experiment_name,
                'timestamp': datetime.now().isoformat(),
                'results': results
            }, f, ensure_ascii=False, indent=2)
        
        print(f"   ğŸ’¾ çµæœå·²ä¿å­˜: {filename}")

    def run_all_experiments(self):
        """é‹è¡Œæ‰€æœ‰å¯¦é©—"""
        print("ğŸš€ é–‹å§‹èšåˆé é¢è§¸ç™¼æ¢ä»¶å¯¦é©—")
        print("=" * 60)
        
        self.experiment_1_concurrency_levels()
        time.sleep(5)  # å¯¦é©—é–“ä¼‘æ¯
        
        self.experiment_2_request_intervals()
        time.sleep(5)
        
        self.experiment_3_rapid_same_url()
        time.sleep(5)
        
        self.experiment_4_cache_headers()
        
        print("\nğŸ¯ æ‰€æœ‰å¯¦é©—å®Œæˆï¼")
        print("è«‹æª¢æŸ¥ç”Ÿæˆçš„ experiment_*.json æ–‡ä»¶æŸ¥çœ‹è©³ç´°çµæœ")

if __name__ == '__main__':
    experiment = AggregatedPageExperiment()
    experiment.run_all_experiments()