#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
çµ‚æ¥µç‰ˆå‹•æ…‹ç®¡ç·š Threads Reader è§£æè…³æœ¬
çµåˆäº† final.py çš„æå–é‚è¼¯ å’Œ complete.py çš„ headers
é€Ÿåº¦å„ªå…ˆï¼Œå¿«é€Ÿå¤±æ•—ï¼Œå¿«é€Ÿå›é€€åˆ°å®˜æ–¹API
"""

import json
import re
import requests
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
from typing import Dict, Optional, List
import subprocess

class UltimatePipelineReader:
    """
    çµ‚æ¥µç‰ˆå‹•æ…‹ç®¡ç·š Reader è§£æå™¨
    1. ä¸¦è¡Œè™•ç†ï¼Œå¤±æ•—ç«‹å³é€å®˜æ–¹API
    2. å‰©é¤˜å¤±æ•—çš„ï¼Œæœ¬åœ°é‡è©¦
    3. æœ€å¾Œå¤±æ•—çš„ï¼Œå†æ¬¡é€å®˜æ–¹API
    """
    
    def __init__(self, backend_instances: int = 4):
        self.local_reader_url = "http://localhost:8880"
        self.official_reader_url = "https://r.jina.ai"
        self.backend_instances = backend_instances
        
        self.NBSP = "\u00A0"
        
        # ä¾†è‡ª complete.py çš„å·²é©—è­‰æœ‰æ•ˆçš„æœ¬åœ° headers é…ç½®
        self.local_headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36',
            'x-wait-for-selector': 'article',
            'x-timeout': '25'
        }
        
        self.official_headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36',
            'X-Return-Format': 'markdown'
        }
        
        # ä¾†è‡ª final.py çš„æœ€ robust çš„è§€çœ‹æ•¸æå–æ¨¡å¼
        self.view_patterns = [
            re.compile(rf'\[Thread[\s{self.NBSP}=]*?(\d+(?:[\.,]\d+)?[KMB]?)\s*views\]', re.IGNORECASE),
            re.compile(rf'Thread[\s{self.NBSP}=]*?(\d+(?:[\.,]\d+)?[KMB]?)[\s{self.NBSP}]*views', re.IGNORECASE | re.MULTILINE),
            re.compile(r'(\d+(?:[\.,]\d+)?[KMB]?)\s*views?', re.IGNORECASE),
            re.compile(r'(\d+(?:[\.,]\d+)?[KMB]?)\s*view(?:s|ing)', re.IGNORECASE),
            re.compile(r'views?\s*[:\-]\s*(\d+(?:[\.,]\d+)?[KMB]?)', re.IGNORECASE),
        ]
    
    def normalize_content(self, text: str) -> str:
        """ä¾†è‡ª final.py çš„æœ€ robust çš„å…§å®¹æ¨™æº–åŒ– - ä¸ç°¡åŒ–ç‰ˆæœ¬"""
        # â‘  å°‡å„ç¨®ç©ºç™½å­—ç¬¦çµ±ä¸€æ›¿æ›ç‚ºæ¨™æº–ç©ºæ ¼
        text = text.replace(self.NBSP, " ")  # NBSP (U+00A0) 
        text = text.replace("\u2002", " ")   # En Space
        text = text.replace("\u2003", " ")   # Em Space
        text = text.replace("\u2009", " ")   # Thin Space
        text = text.replace("\u200A", " ")   # Hair Space
        text = text.replace("\u3000", " ")   # Ideographic Space
        text = text.replace("\t", " ")       # Tab æ›¿æ›ç‚ºç©ºæ ¼
        
        # â‘¡ æ¨™æº–åŒ–è¡ŒçµæŸç¬¦
        text = text.replace("\r\n", "\n")
        text = text.replace("\r", "\n")
        
        # â‘¢ å£“ç¸®å¤šå€‹é€£çºŒç©ºæ ¼ï¼ˆä½†ä¿ç•™å–®å€‹ç©ºæ ¼ï¼‰
        text = re.sub(r"[ \t]{2,}", " ", text)
        
        return text

    def extract_views_count(self, markdown_content: str, post_id: str = "") -> Optional[str]:
        """ä¾†è‡ª final.py çš„æœ€ robust çš„è§€çœ‹æ•¸æå– - ä¸ç°¡åŒ–ç‰ˆæœ¬"""
        
        # æ¨™æº–åŒ–å…§å®¹
        normalized_content = self.normalize_content(markdown_content)
        
        # 1. å˜—è©¦æ‰€æœ‰æ¨¡å¼åœ¨æ¨™æº–åŒ–å¾Œçš„å…§å®¹ä¸Š
        for i, pattern in enumerate(self.view_patterns):
            match = pattern.search(normalized_content)
            if match:
                views_number = match.group(1)
                if self.validate_views_format(views_number):
                    return views_number
        
        # 2. å¦‚æœé‚„æ˜¯å¤±æ•—ï¼Œå˜—è©¦åœ¨åŸå§‹å…§å®¹ä¸Šæœç´¢ï¼ˆé˜²æ­¢æ¨™æº–åŒ–éåº¦ï¼‰
        for i, pattern in enumerate(self.view_patterns):
            match = pattern.search(markdown_content)
            if match:
                views_number = match.group(1)
                if self.validate_views_format(views_number):
                    return views_number
        
        return None

    def validate_views_format(self, views: str) -> bool:
        """é©—è­‰è§€çœ‹æ•¸æ ¼å¼æ˜¯å¦åˆç† - å®Œæ•´ç‰ˆæœ¬"""
        if not views:
            return False
        
        # åŸºæœ¬æ ¼å¼æª¢æŸ¥
        pattern = re.compile(r'^\d+(?:\.\d+)?[KMB]?$', re.IGNORECASE)
        if not pattern.match(views):
            return False
        
        # æ•¸å­—åˆç†æ€§æª¢æŸ¥
        try:
            actual_number = self.convert_to_number(views)
            # è§€çœ‹æ•¸é€šå¸¸åœ¨ 1-100M ç¯„åœå…§
            return 1 <= actual_number <= 100_000_000
        except:
            return False
    
    def convert_to_number(self, number_str: str) -> int:
        """å°‡ K/M/B æ ¼å¼è½‰æ›ç‚ºæ•¸å­—"""
        number_str = number_str.upper()
        if number_str.endswith('K'):
            return int(float(number_str[:-1]) * 1000)
        elif number_str.endswith('M'):
            return int(float(number_str[:-1]) * 1000000)
        elif number_str.endswith('B'):
            return int(float(number_str[:-1]) * 1000000000)
        else:
            return int(number_str)

    def extract_post_content(self, content: str) -> Optional[str]:
        """æå–è²¼æ–‡ä¸»è¦å…§å®¹"""
        lines = content.split('\n')
        
        # å°‹æ‰¾å…§å®¹é–‹å§‹ä½ç½®
        content_start = -1
        for i, line in enumerate(lines):
            if 'Markdown Content:' in line:
                content_start = i + 1
                break
        
        if content_start == -1:
            return None
        
        # æå–å‰å¹¾è¡Œä½œç‚ºä¸»è¦å…§å®¹
        content_lines = []
        for i in range(content_start, min(content_start + 10, len(lines))):
            line = lines[i].strip()
            if line and not line.startswith('[![Image') and not line.startswith('[Image'):
                content_lines.append(line)
        
        return '\n'.join(content_lines) if content_lines else None

    def fetch_content_local(self, url: str, use_cache: bool = True) -> tuple:
        headers = self.local_headers.copy()
        if not use_cache: headers['x-no-cache'] = 'true'
        try:
            response = requests.get(f"{self.local_reader_url}/{url}", headers=headers, timeout=30)
            return (True, response.text) if response.status_code == 200 else (False, f"HTTP {response.status_code}")
        except Exception as e:
            return False, str(e)

    def fetch_content_official(self, url: str) -> tuple:
        try:
            response = requests.get(f"{self.official_reader_url}/{url}", headers=self.official_headers, timeout=60)
            return (True, response.text) if response.status_code == 200 else (False, f"HTTP {response.status_code}")
        except Exception as e:
            return False, str(e)
    
    def parse_post(self, url: str, content: str, source: str) -> Dict:
        """è§£æè²¼æ–‡å…§å®¹ - å®Œæ•´ç‰ˆæœ¬"""
        # æå– post_id
        post_id = url.split('/')[-1] if '/' in url else url
        
        # æå–è§€çœ‹æ•¸å’Œå…§å®¹
        views = self.extract_views_count(content, post_id)
        main_content = self.extract_post_content(content)
        
        return {
            'post_id': post_id,
            'url': url,
            'views': views,
            'content': main_content,
            'source': source,
            'success': views is not None and main_content is not None,
            'has_views': views is not None,
            'has_content': main_content is not None,
            'content_length': len(content)
        }
    
    def dynamic_pipeline(self, urls: List[str]):
        """å‹•æ…‹ç®¡ç·šè™•ç† - çœŸæ­£çš„å¹³è¡Œè™•ç†"""
        total_start_time = time.time()
        max_workers = 4
        
        print(f"ğŸš€ çµ‚æ¥µç‰ˆå‹•æ…‹ç®¡ç·šå•Ÿå‹•ï¼Œä½µç™¼æ•¸: {max_workers}")
        print("=" * 80)
        
        stage1_results = {}
        failed_urls_for_api = []
        
        # === ç¬¬ä¸€éšæ®µï¼šæœ¬åœ°ä¸¦è¡Œè™•ç† ===
        print(f"âš¡ (1/3) æœ¬åœ°ä¸¦è¡Œè™•ç† {len(urls)} å€‹ URL...")
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_url = {executor.submit(self.fetch_content_local, url): url for url in urls}
            
            completed = 0
            for future in as_completed(future_to_url):
                url = future_to_url[future]
                completed += 1
                progress = completed / len(urls)
                
                success, content = future.result()
                
                if success:
                    result = self.parse_post(url, content, "æœ¬åœ°-ç¬¬ä¸€è¼ª")
                    if result['has_views']:  # æœ‰è§€çœ‹æ•¸å°±ç®—æˆåŠŸ
                        stage1_results[url] = result
                        print(f"ğŸ“Š {completed}/{len(urls)} ({progress:.1%}) | âœ… æœ¬åœ°æˆåŠŸ: {result['post_id']} ({result['views']})")
                        continue
                
                # æœ¬åœ°å¤±æ•—ï¼ŒåŠ å…¥APIå¾…è™•ç†åˆ—è¡¨
                failed_urls_for_api.append(url)
                print(f"ğŸ“Š {completed}/{len(urls)} ({progress:.1%}) | âš ï¸ æœ¬åœ°å¤±æ•—: {url.split('/')[-1]} â†’ å¾…é€å®˜æ–¹API")
        
        # === ç¬¬äºŒéšæ®µï¼šå®˜æ–¹APIä¸¦è¡Œè™•ç†ï¼ˆæœ¬åœ°å¤±æ•—çš„ï¼‰ ===
        if failed_urls_for_api:
            print(f"\nğŸŒ (2/3) å®˜æ–¹APIä¸¦è¡Œè™•ç† {len(failed_urls_for_api)} å€‹å¤±æ•—é …ç›®...")
            
            with ThreadPoolExecutor(max_workers=max_workers) as api_executor:
                api_future_to_url = {api_executor.submit(self.fetch_content_official, url): url for url in failed_urls_for_api}
                
                api_completed = 0
                for api_future in as_completed(api_future_to_url):
                    url = api_future_to_url[api_future]
                    api_completed += 1
                    api_progress = api_completed / len(failed_urls_for_api)
                    
                    api_success, api_content = api_future.result()
                    if api_success:
                        result = self.parse_post(url, api_content, "å®˜æ–¹API-å›é€€1")
                        stage1_results[url] = result
                        if result['has_views']:
                            print(f"ğŸŒ {api_completed}/{len(failed_urls_for_api)} ({api_progress:.1%}) | âœ… å®˜æ–¹APIæˆåŠŸ: {result['post_id']} ({result['views']})")
                        else:
                            print(f"ğŸŒ {api_completed}/{len(failed_urls_for_api)} ({api_progress:.1%}) | âŒ å®˜æ–¹APIç„¡è§€çœ‹æ•¸: {result['post_id']}")
                    else:
                        stage1_results[url] = {'url': url, 'success': False, 'source': 'å®˜æ–¹API-å›é€€1', 'error': api_content}
                        print(f"ğŸŒ {api_completed}/{len(failed_urls_for_api)} ({api_progress:.1%}) | âŒ å®˜æ–¹APIå¤±æ•—: {url.split('/')[-1]}")

        remaining_failed = [url for url, res in stage1_results.items() if not res.get('has_views')]
        
        # === ç¬¬ä¸‰éšæ®µï¼šæœ¬åœ°é‡è©¦ä¸¦è¡Œè™•ç†ï¼ˆé‚„æ˜¯å¤±æ•—çš„ï¼‰ ===
        if remaining_failed:
            print(f"\nğŸ”„ (3/4) æœ¬åœ°é‡è©¦ä¸¦è¡Œè™•ç† ({len(remaining_failed)} å€‹é …ç›®)")
            
            with ThreadPoolExecutor(max_workers=max_workers) as retry_executor:
                retry_future_to_url = {retry_executor.submit(self.fetch_content_local, url, False): url for url in remaining_failed}
                
                retry_completed = 0
                for retry_future in as_completed(retry_future_to_url):
                    url = retry_future_to_url[retry_future]
                    retry_completed += 1
                    retry_progress = retry_completed / len(remaining_failed)
                    
                    success, content = retry_future.result()
                    if success:
                        result = self.parse_post(url, content, "æœ¬åœ°-é‡è©¦")
                        stage1_results[url] = result
                        if result['has_views']:
                            print(f"ğŸ”„ {retry_completed}/{len(remaining_failed)} ({retry_progress:.1%}) | âœ… æœ¬åœ°é‡è©¦æˆåŠŸ: {result['post_id']} ({result['views']})")
                        else:
                            print(f"ğŸ”„ {retry_completed}/{len(remaining_failed)} ({retry_progress:.1%}) | âŒ æœ¬åœ°é‡è©¦ç„¡è§€çœ‹æ•¸: {result['post_id']}")
                    else:
                        print(f"ğŸ”„ {retry_completed}/{len(remaining_failed)} ({retry_progress:.1%}) | âŒ æœ¬åœ°é‡è©¦å¤±æ•—: {url.split('/')[-1]}")

        final_failed = [url for url, res in stage1_results.items() if not res.get('has_views')]
        
        # === ç¬¬å››éšæ®µï¼šæœ€å¾Œçš„å®˜æ–¹APIä¸¦è¡Œé‡è©¦ ===
        if final_failed:
            print(f"\nğŸŒ (4/4) æœ€å¾Œå®˜æ–¹APIä¸¦è¡Œé‡è©¦ ({len(final_failed)} å€‹é …ç›®)")
            
            with ThreadPoolExecutor(max_workers=max_workers) as final_executor:
                final_future_to_url = {final_executor.submit(self.fetch_content_official, url): url for url in final_failed}
                
                final_completed = 0
                for final_future in as_completed(final_future_to_url):
                    url = final_future_to_url[final_future]
                    final_completed += 1
                    final_progress = final_completed / len(final_failed)
                    
                    success, content = final_future.result()
                    if success:
                        result = self.parse_post(url, content, "å®˜æ–¹API-å›é€€2")
                        stage1_results[url] = result
                        if result['has_views']:
                            print(f"ğŸŒ {final_completed}/{len(final_failed)} ({final_progress:.1%}) | âœ… æœ€å¾ŒAPIæˆåŠŸ: {result['post_id']} ({result['views']})")
                        else:
                            print(f"ğŸŒ {final_completed}/{len(final_failed)} ({final_progress:.1%}) | âŒ æœ€å¾ŒAPIç„¡è§€çœ‹æ•¸: {result['post_id']}")
                    else:
                        print(f"ğŸŒ {final_completed}/{len(final_failed)} ({final_progress:.1%}) | âŒ æœ€å¾ŒAPIå¤±æ•—: {url.split('/')[-1]}")
        
        total_end_time = time.time()
        final_results = [stage1_results.get(url, {'url': url, 'success': False}) for url in urls]
        
        print("\n" + "=" * 80)
        success_count = len([res for res in final_results if res.get('has_views')])
        print(f"âœ… æœ€çµ‚æˆåŠŸ: {success_count}/{len(urls)} ({success_count/len(urls)*100:.1f}%)")
        print(f"â±ï¸ ç¸½è€—æ™‚: {total_end_time - total_start_time:.1f}s")
        print(f"ğŸï¸ å¹³å‡é€Ÿåº¦: {len(urls)/(total_end_time - total_start_time):.2f} URL/s")
        
        return final_results

def load_urls_from_json(file_path: str) -> List[str]:
    try:
        with open(file_path, 'r', encoding='utf-8') as f: data = json.load(f)
        urls = [post['url'] for post in data.get('posts', []) if 'url' in post]
        print(f"âœ… å¾ {file_path} æˆåŠŸæå– {len(urls)} å€‹ URLã€‚")
        return urls
    except Exception as e:
        print(f"âŒ æå– URL æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return []

def main():
    urls = load_urls_from_json("agents/playwright_crawler/debug/crawl_data_20250803_121452_934d52b1.json")
    if not urls: return
    
    pipeline = UltimatePipelineReader()
    results = pipeline.dynamic_pipeline(urls)
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    filename = f"ultimate_pipeline_results_{timestamp}.json"
    with open(filename, 'w', encoding='utf-8') as f: json.dump(results, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ çµæœå·²ä¿å­˜åˆ°: {filename}")

if __name__ == '__main__':
    main()