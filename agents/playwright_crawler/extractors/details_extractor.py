"""
è©³ç´°æ•¸æ“šæå–å™¨

è² è²¬ä½¿ç”¨æ··åˆç­–ç•¥è£œé½Šè²¼æ–‡è©³ç´°æ•¸æ“šï¼š
1. GraphQL è¨ˆæ•¸æŸ¥è©¢ç²å–æº–ç¢ºçš„æ•¸å­—æ•¸æ“š (likes, commentsç­‰)
2. DOM è§£æç²å–å®Œæ•´çš„å…§å®¹å’Œåª’é«” (content, images, videos)
"""

import asyncio
import logging
import random
import re
from datetime import datetime
from typing import Dict, List, Optional, Any, Set
from playwright.async_api import BrowserContext, Page

from common.models import PostMetrics
from common.nats_client import publish_progress
from ..parsers.number_parser import parse_number


class DetailsExtractor:
    """
    è©³ç´°æ•¸æ“šæå–å™¨ - ä½¿ç”¨æ··åˆç­–ç•¥æå–å®Œæ•´çš„è²¼æ–‡æ•¸æ“š
    """
    
    def __init__(self):
        pass
    
    async def fill_post_details_from_page(self, posts_to_fill: List[PostMetrics], context: BrowserContext, task_id: str = None, username: str = None) -> List[PostMetrics]:
        """
        ä½¿ç”¨æ··åˆç­–ç•¥è£œé½Šè²¼æ–‡è©³ç´°æ•¸æ“šï¼š
        1. GraphQL è¨ˆæ•¸æŸ¥è©¢ç²å–æº–ç¢ºçš„æ•¸å­—æ•¸æ“š (likes, commentsç­‰)
        2. DOM è§£æç²å–å®Œæ•´çš„å…§å®¹å’Œåª’é«” (content, images, videos)
        é€™ç¨®æ–¹æ³•çµåˆäº†å…©ç¨®æŠ€è¡“çš„å„ªå‹¢ï¼Œæä¾›æœ€ç©©å®šå¯é çš„æ•¸æ“šæå–ã€‚
        """
        if not context:
            logging.error("âŒ Browser context æœªåˆå§‹åŒ–ï¼Œç„¡æ³•åŸ·è¡Œ fill_post_details_from_pageã€‚")
            return posts_to_fill

        # ä¿å®ˆçš„ä¸¦ç™¼æ•¸æå‡ï¼šå¾1å¢åŠ åˆ°2ï¼Œå¹³è¡¡é€Ÿåº¦èˆ‡å®‰å…¨æ€§
        semaphore = asyncio.Semaphore(2)  # è¼•å¾®æå‡ä¸¦ç™¼ä½†ä¿æŒå®‰å…¨
        
        async def fetch_single_details_hybrid(post: PostMetrics):
            async with semaphore:
                page = None
                try:
                    page = await context.new_page()
                    
                    logging.debug(f"ğŸ“„ ä½¿ç”¨æ··åˆç­–ç•¥è£œé½Šè©³ç´°æ•¸æ“š: {post.url}")
                    
                    # === æ­¥é©Ÿ 1: æ··åˆç­–ç•¥ - æ””æˆª+é‡ç™¼è«‹æ±‚ ===
                    counts_data = {}
                    video_urls = set()
                    captured_graphql_request = {}
                    response_handler_active = True
                    
                    async def handle_counts_response(response):
                        if not response_handler_active:
                            return  # åœæ­¢è™•ç†éŸ¿æ‡‰
                        await self._handle_graphql_response(response, counts_data, video_urls, captured_graphql_request)
                    
                    page.on("response", handle_counts_response)
                    
                    # === æ­¥é©Ÿ 2: å°èˆªå’Œè§¸ç™¼è¼‰å…¥ï¼ˆå„ªåŒ–ç‰ˆï¼šæ›´å¿«ä½†å®‰å…¨çš„è¼‰å…¥ç­–ç•¥ï¼‰ ===
                    await page.goto(post.url, wait_until="domcontentloaded", timeout=45000)
                    
                    # æ™ºèƒ½ç­‰å¾…ï¼šå…ˆçŸ­æš«ç­‰å¾…ï¼Œå¦‚æœæ²’æœ‰æ””æˆªåˆ°æ•¸æ“šå†å»¶é•·
                    await asyncio.sleep(1.5)  # ç¸®çŸ­åˆå§‹ç­‰å¾…æ™‚é–“
                    
                    # æª¢æŸ¥æ˜¯å¦å·²ç¶“æ””æˆªåˆ°æ•¸æ“š
                    if not counts_data:
                        logging.debug(f"   â³ é¦–æ¬¡ç­‰å¾…æœªæ””æˆªåˆ°æ•¸æ“šï¼Œå»¶é•·ç­‰å¾…...")
                        await asyncio.sleep(1.5)  # é¡å¤–ç­‰å¾…1.5ç§’ï¼ˆç¸½å…±3ç§’ï¼‰
                    
                    # === æ­¥é©Ÿ 2.5: æ··åˆç­–ç•¥é‡ç™¼è«‹æ±‚ ===
                    if captured_graphql_request and not counts_data:
                        counts_data = await self._resend_graphql_request(captured_graphql_request, post.url, context)
                    
                    # æˆåŠŸç²å–æ•¸æ“šå¾Œåœæ­¢ç›£è½ï¼Œé¿å…ä¸å¿…è¦çš„æ””æˆª
                    if counts_data and counts_data.get("likes", 0) > 0:
                        response_handler_active = False
                        logging.debug(f"   ğŸ›‘ æˆåŠŸç²å–è¨ˆæ•¸æ•¸æ“šï¼Œåœæ­¢éŸ¿æ‡‰ç›£è½")
                    
                    # å˜—è©¦è§¸ç™¼å½±ç‰‡è¼‰å…¥
                    await self._trigger_video_loading(page)
                    
                    # === æ­¥é©Ÿ 3: DOM å…§å®¹æå– ===
                    content_data = await self._extract_content_from_dom(page, username, video_urls)
                    
                    # === æ­¥é©Ÿ 3.5: DOM è¨ˆæ•¸å¾Œæ´ï¼ˆç•¶ GraphQL æ””æˆªå¤±æ•—æ™‚ï¼‰ ===
                    if not counts_data:
                        counts_data = await self._extract_counts_from_dom_fallback(page)
                    
                    # === æ­¥é©Ÿ 4: æ›´æ–°è²¼æ–‡æ•¸æ“š ===
                    updated = await self._update_post_data(post, counts_data, content_data, task_id, username)
                    
                    # éš¨æ©Ÿå»¶é²é¿å…åçˆ¬èŸ²
                    delay = random.uniform(2, 4)
                    await asyncio.sleep(delay)
                    
                except Exception as e:
                    logging.error(f"  âŒ æ··åˆç­–ç•¥è™•ç† {post.post_id} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                    post.processing_stage = "details_failed"
                finally:
                    if page:
                        await page.close()

        # åºåˆ—è™•ç†ä¿æŒé †åº
        for post in posts_to_fill:
            await fetch_single_details_hybrid(post)
        
        return posts_to_fill
    
    async def _handle_graphql_response(self, response, counts_data: dict, video_urls: set, captured_graphql_request: dict):
        """è™•ç† GraphQL éŸ¿æ‡‰çš„æ””æˆªï¼ˆå„ªåŒ–ç‰ˆï¼šæ”¯æŒå»é‡ï¼‰"""
        try:
            import json
            url = response.url.lower()
            headers = response.request.headers
            query_name = headers.get("x-fb-friendly-name", "")
            
            # æª¢æŸ¥æ˜¯å¦å·²ç¶“æœ‰å®Œæ•´æ•¸æ“šï¼Œé¿å…é‡è¤‡æ””æˆª
            if (counts_data.get("likes", 0) > 0 and 
                counts_data.get("comments", 0) >= 0 and 
                counts_data.get("reposts", 0) >= 0 and 
                counts_data.get("shares", 0) >= 0):
                logging.debug(f"   â© å·²æœ‰å®Œæ•´è¨ˆæ•¸æ•¸æ“šï¼Œè·³éé‡è¤‡æ””æˆª")
                return
            
            # æ””æˆªè¨ˆæ•¸æŸ¥è©¢è«‹æ±‚ï¼ˆä¿å­˜headerså’Œpayloadï¼‰
            if ("/graphql" in url and response.status == 200 and 
                "useBarcelonaBatchedDynamicPostCountsSubscriptionQuery" in query_name):
                
                # åªåœ¨ç¬¬ä¸€æ¬¡æ””æˆªæ™‚è¨˜éŒ„è©³ç´°æ—¥èªŒ
                if not captured_graphql_request.get("headers"):
                    logging.info(f"   ğŸ¯ æ””æˆªåˆ°GraphQLè¨ˆæ•¸æŸ¥è©¢ï¼Œä¿å­˜è«‹æ±‚ä¿¡æ¯...")
                    
                    # ä¿å­˜è«‹æ±‚ä¿¡æ¯ï¼ˆæ¨¡ä»¿hybrid_content_extractor.pyçš„æˆåŠŸç­–ç•¥ï¼‰
                    captured_graphql_request.update({
                        "headers": dict(response.request.headers),
                        "payload": response.request.post_data,
                        "url": "https://www.threads.com/graphql/query"
                    })
                    
                    # æ¸…ç†headers
                    clean_headers = captured_graphql_request["headers"].copy()
                    for h in ["host", "content-length", "accept-encoding"]:
                        clean_headers.pop(h, None)
                    captured_graphql_request["clean_headers"] = clean_headers
                    
                    logging.info(f"   âœ… æˆåŠŸä¿å­˜GraphQLè«‹æ±‚ä¿¡æ¯ï¼Œæº–å‚™é‡ç™¼...")
                else:
                    logging.debug(f"   ğŸ”„ é‡è¤‡GraphQLæ””æˆªï¼Œä½¿ç”¨å·²ä¿å­˜çš„è«‹æ±‚ä¿¡æ¯")
                
                # ä¹Ÿå˜—è©¦ç›´æ¥è§£æç•¶å‰éŸ¿æ‡‰ï¼ˆä½œç‚ºå‚™ç”¨ï¼‰
                try:
                    data = await response.json()
                    if "data" in data and "data" in data["data"] and "posts" in data["data"]["data"]:
                        posts_list = data["data"]["data"]["posts"]
                        if posts_list and len(posts_list) > 0:
                            post_data = posts_list[0]
                            if isinstance(post_data, dict):
                                text_info = post_data.get("text_post_app_info", {}) or {}
                                new_counts = {
                                    "likes": post_data.get("like_count") or 0,
                                    "comments": text_info.get("direct_reply_count") or 0, 
                                    "reposts": text_info.get("repost_count") or 0,
                                    "shares": text_info.get("reshare_count") or 0
                                }
                                
                                # åªåœ¨æ²’æœ‰æ•¸æ“šæˆ–æ•¸æ“šæ›´æ–°æ™‚æ‰æ›´æ–°
                                if not counts_data or any(new_counts.get(k, 0) > counts_data.get(k, 0) for k in new_counts):
                                    counts_data.update(new_counts)
                                    logging.info(f"   âœ… ç›´æ¥æ””æˆªæˆåŠŸ: è®š={counts_data['likes']}, ç•™è¨€={counts_data['comments']}, è½‰ç™¼={counts_data['reposts']}, åˆ†äº«={counts_data['shares']}")
                                else:
                                    logging.debug(f"   â© æ•¸æ“šç„¡æ›´æ–°ï¼Œè·³éé‡è¤‡è¨˜éŒ„")
                except Exception as e:
                    logging.debug(f"   âš ï¸ ç›´æ¥è§£æå¤±æ•—: {e}")
            
            # æ””æˆªå½±ç‰‡è³‡æº
            content_type = response.headers.get("content-type", "")
            resource_type = response.request.resource_type
            if (resource_type == "media" or 
                content_type.startswith("video/") or
                ".mp4" in response.url.lower() or
                ".m3u8" in response.url.lower() or
                ".mpd" in response.url.lower()):
                video_urls.add(response.url)
                logging.debug(f"   ğŸ¥ æ””æˆªåˆ°å½±ç‰‡: {response.url[:60]}...")
                
        except Exception as e:
            logging.debug(f"   âš ï¸ éŸ¿æ‡‰è™•ç†å¤±æ•—: {e}")
    
    async def _resend_graphql_request(self, captured_graphql_request: dict, post_url: str, context: BrowserContext) -> dict:
        """é‡ç™¼ GraphQL è«‹æ±‚"""
        logging.info(f"   ğŸ”„ ä½¿ç”¨ä¿å­˜çš„GraphQLè«‹æ±‚ä¿¡æ¯é‡ç™¼è«‹æ±‚...")
        counts_data = {}
        
        try:
            import httpx
            
            # å¾URLæå–PKï¼ˆå¦‚æœå¯èƒ½ï¼‰
            url_match = re.search(r'/post/([^/?]+)', post_url)
            if url_match:
                logging.info(f"   ğŸ” URLä»£ç¢¼: {url_match.group(1)}")
            
            # æº–å‚™é‡ç™¼è«‹æ±‚
            headers = captured_graphql_request["clean_headers"]
            payload = captured_graphql_request["payload"]
            
            # å¾é é¢contextç²å–cookies
            cookies_list = await context.cookies()
            cookies = {cookie['name']: cookie['value'] for cookie in cookies_list}
            
            # ç¢ºä¿æœ‰èªè­‰
            if not headers.get("authorization") and 'ig_set_authorization' in cookies:
                auth_value = cookies['ig_set_authorization']
                headers["authorization"] = f"Bearer {auth_value}" if not auth_value.startswith('Bearer') else auth_value
            
            # ç™¼é€HTTPè«‹æ±‚åˆ°Threads API
            async with httpx.AsyncClient(headers=headers, cookies=cookies, timeout=30.0, http2=True) as client:
                api_response = await client.post("https://www.threads.com/graphql/query", data=payload)
                
                if api_response.status_code == 200:
                    result = api_response.json()
                    logging.info(f"   âœ… é‡ç™¼è«‹æ±‚æˆåŠŸï¼Œç‹€æ…‹: {api_response.status_code}")
                    
                    if "data" in result and result["data"] and "data" in result["data"] and "posts" in result["data"]["data"]:
                        posts_list = result["data"]["data"]["posts"]
                        logging.info(f"   ğŸ“Š é‡ç™¼è«‹æ±‚éŸ¿æ‡‰åŒ…å« {len(posts_list)} å€‹è²¼æ–‡")
                        
                        # ä½¿ç”¨ç¬¬ä¸€å€‹è²¼æ–‡ï¼ˆç•¶å‰é é¢çš„ä¸»è¦è²¼æ–‡ï¼‰
                        if posts_list and len(posts_list) > 0:
                            post_data = posts_list[0]
                            if isinstance(post_data, dict):
                                text_info = post_data.get("text_post_app_info", {}) or {}
                                counts_data.update({
                                    "likes": post_data.get("like_count") or 0,
                                    "comments": text_info.get("direct_reply_count") or 0, 
                                    "reposts": text_info.get("repost_count") or 0,
                                    "shares": text_info.get("reshare_count") or 0
                                })
                                logging.info(f"   ğŸ¯ é‡ç™¼è«‹æ±‚æˆåŠŸç²å–æ•¸æ“š: è®š={counts_data['likes']}, ç•™è¨€={counts_data['comments']}, è½‰ç™¼={counts_data['reposts']}, åˆ†äº«={counts_data['shares']}")
                else:
                    logging.warning(f"   âš ï¸ é‡ç™¼è«‹æ±‚å¤±æ•—ï¼Œç‹€æ…‹: {api_response.status_code}")
                    
        except Exception as e:
            logging.warning(f"   âš ï¸ é‡ç™¼è«‹æ±‚éç¨‹å¤±æ•—: {e}")
        
        return counts_data
    
    async def _trigger_video_loading(self, page: Page):
        """è§¸ç™¼å½±ç‰‡è¼‰å…¥"""
        try:
            trigger_selectors = [
                'div[data-testid="media-viewer"]',
                'video',
                'div[role="button"][aria-label*="play"]',
                'div[role="button"][aria-label*="æ’­æ”¾"]',
                '[data-pressable-container] div[style*="video"]'
            ]
            
            for selector in trigger_selectors:
                try:
                    elements = page.locator(selector)
                    count = await elements.count()
                    if count > 0:
                        await elements.first.click(timeout=3000)
                        await asyncio.sleep(2)
                        break
                except:
                    continue
        except:
            pass
    
    async def _extract_content_from_dom(self, page: Page, username: str, video_urls: set) -> dict:
        """å¾ DOM æå–å…§å®¹æ•¸æ“š"""
        content_data = {}
        
        try:
            # æå–ç”¨æˆ¶åï¼ˆå¾ URLï¼‰
            url_match = re.search(r'/@([^/]+)/', page.url)
            content_data["username"] = url_match.group(1) if url_match else username or ""
            
            # æå–å…§å®¹æ–‡å­—
            content = ""
            content_selectors = [
                'div[data-pressable-container] span',
                '[data-testid="thread-text"]',
                'article div[dir="auto"]',
                'div[role="article"] div[dir="auto"]'
            ]
            
            for selector in content_selectors:
                try:
                    elements = page.locator(selector)
                    count = await elements.count()
                    
                    for i in range(min(count, 20)):
                        try:
                            text = await elements.nth(i).inner_text()
                            
                            # åŸºæœ¬éæ¿¾æ¢ä»¶
                            if not text or len(text.strip()) <= 10:
                                continue
                            if text.strip().isdigit():
                                continue
                            if text.startswith("@"):
                                continue
                            
                            # éæ¿¾ç”¨æˆ¶åï¼ˆé‡è¦ä¿®å¾©ï¼ï¼‰
                            if text.strip() == username:
                                logging.debug(f"   âš ï¸ éæ¿¾ç”¨æˆ¶åæ–‡æœ¬: {text}")
                                continue
                            
                            # éæ¿¾æ™‚é–“ç›¸é—œ
                            if any(time_word in text for time_word in ["å°æ™‚", "åˆ†é˜", "ç§’å‰", "å¤©å‰", "é€±å‰", "å€‹æœˆå‰"]):
                                continue
                            
                            # éæ¿¾ç³»çµ±éŒ¯èª¤å’Œæç¤ºä¿¡æ¯ï¼ˆé‡é»ä¿®å¾©ï¼ï¼‰
                            system_messages = [
                                "Sorry, we're having trouble playing this video",
                                "Learn more",
                                "Something went wrong",
                                "Video unavailable",
                                "This content isn't available",
                                "Unable to load",
                                "Error loading",
                                "æ’­æ”¾ç™¼ç”ŸéŒ¯èª¤",
                                "ç„¡æ³•æ’­æ”¾",
                                "è¼‰å…¥å¤±æ•—",
                                "ç™¼ç”ŸéŒ¯èª¤",
                                "å…§å®¹ç„¡æ³•é¡¯ç¤º"
                            ]
                            
                            # æª¢æŸ¥æ˜¯å¦åŒ…å«ç³»çµ±éŒ¯èª¤ä¿¡æ¯
                            text_lower = text.lower()
                            if any(msg.lower() in text_lower for msg in system_messages):
                                logging.debug(f"   âš ï¸ éæ¿¾ç³»çµ±éŒ¯èª¤ä¿¡æ¯: {text[:50]}...")
                                continue
                            
                            # éæ¿¾æŒ‰éˆ•æ–‡å­—å’Œå°èˆª
                            button_texts = ["follow", "following", "like", "comment", "share", "more", "options"]
                            if any(btn in text_lower for btn in button_texts):
                                continue
                            
                            # éæ¿¾ç´”æ•¸å­—çµ„åˆï¼ˆè®šæ•¸ã€åˆ†äº«æ•¸ç­‰ï¼‰
                            if re.match(r'^[\d,.\s]+$', text.strip()):
                                continue
                                
                            # éæ¿¾éçŸ­çš„å…§å®¹
                            if len(text.strip()) < 5:
                                continue
                            
                            # é€šéæ‰€æœ‰éæ¿¾æ¢ä»¶ï¼Œæ¥å—æ­¤å…§å®¹
                            content = text.strip()
                            logging.debug(f"   âœ… æ‰¾åˆ°æœ‰æ•ˆå…§å®¹: {content[:50]}...")
                            break
                        except:
                            continue
                    
                    if content:
                        break
                except:
                    continue
            
            # å¦‚æœæ²’æœ‰æ‰¾åˆ°æœ‰æ•ˆå…§å®¹ï¼Œå˜—è©¦å…¶ä»–ç­–ç•¥
            if not content:
                logging.debug(f"   ğŸ” ä¸»è¦å…§å®¹æå–å¤±æ•—ï¼Œå˜—è©¦å‚™ç”¨ç­–ç•¥...")
                
                # å‚™ç”¨ç­–ç•¥1ï¼šæŸ¥æ‰¾ aria-label æˆ– title å±¬æ€§
                backup_selectors = [
                    'div[aria-label]',
                    'span[title]',
                    '[data-testid="thread-description"]',
                    'article[aria-label]'
                ]
                
                for backup_selector in backup_selectors:
                    try:
                        elements = page.locator(backup_selector)
                        backup_count = await elements.count()
                        
                        for i in range(min(backup_count, 10)):
                            try:
                                backup_text = await elements.nth(i).get_attribute("aria-label") or await elements.nth(i).get_attribute("title")
                                if backup_text and len(backup_text.strip()) > 5:
                                    # éæ¿¾ç”¨æˆ¶å
                                    if backup_text.strip() == username:
                                        continue
                                    
                                    # åŒæ¨£éæ¿¾ç³»çµ±éŒ¯èª¤ä¿¡æ¯
                                    backup_text_lower = backup_text.lower()
                                    if not any(msg.lower() in backup_text_lower for msg in [
                                        "sorry, we're having trouble playing this video",
                                        "learn more", "something went wrong", "video unavailable"
                                    ]):
                                        content = backup_text.strip()
                                        logging.debug(f"   âœ… å‚™ç”¨ç­–ç•¥æ‰¾åˆ°å…§å®¹: {content[:50]}...")
                                        break
                            except:
                                continue
                        
                        if content:
                            break
                    except:
                        continue
                
                # å¦‚æœä»ç„¶æ²’æœ‰å…§å®¹ï¼Œæ¨™è¨˜ç‚ºå½±ç‰‡è²¼æ–‡
                if not content:
                    logging.debug(f"   ğŸ“¹ å¯èƒ½æ˜¯ç´”å½±ç‰‡è²¼æ–‡ï¼Œç„¡æ–‡å­—å…§å®¹")
                    content = ""  # ä¿æŒç©ºå­—ç¬¦ä¸²è€Œä¸æ˜¯éŒ¯èª¤ä¿¡æ¯
            
            content_data["content"] = content
            
            # èª¿è©¦ä¿¡æ¯ï¼šç¢ºèªå…§å®¹æå–çµæœ
            logging.info(f"   ğŸ“ [DEBUG] å…§å®¹æå–çµæœ: content='{content}', username='{content_data.get('username', 'N/A')}'")
            if content == content_data.get("username"):
                logging.warning(f"   âš ï¸ [DEBUG] è­¦å‘Šï¼šcontent èˆ‡ username ç›¸åŒï¼å¯èƒ½å­˜åœ¨éŒ¯èª¤è³¦å€¼")
            
            # æå–åœ–ç‰‡ï¼ˆéæ¿¾é ­åƒï¼‰
            images = []
            img_elements = page.locator('img')
            img_count = await img_elements.count()
            
            for i in range(min(img_count, 50)):
                try:
                    img_elem = img_elements.nth(i)
                    img_src = await img_elem.get_attribute("src")
                    
                    if not img_src or not ("fbcdn" in img_src or "cdninstagram" in img_src):
                        continue
                    
                    if ("rsrc.php" in img_src or "static.cdninstagram.com" in img_src):
                        continue
                    
                    # æª¢æŸ¥å°ºå¯¸éæ¿¾é ­åƒ
                    try:
                        width = int(await img_elem.get_attribute("width") or 0)
                        height = int(await img_elem.get_attribute("height") or 0)
                        max_size = max(width, height)
                        
                        if max_size > 150 and img_src not in images:
                            images.append(img_src)
                    except:
                        if ("t51.2885-15" in img_src or "scontent" in img_src) and img_src not in images:
                            images.append(img_src)
                except:
                    continue
            
            content_data["images"] = images
            
            # æå–å½±ç‰‡ï¼ˆçµåˆç¶²è·¯æ””æˆªå’ŒDOMï¼‰
            videos = list(video_urls)
            
            # DOM ä¸­çš„ video æ¨™ç±¤
            video_elements = page.locator('video')
            video_count = await video_elements.count()
            
            for i in range(video_count):
                try:
                    video_elem = video_elements.nth(i)
                    src = await video_elem.get_attribute("src")
                    data_src = await video_elem.get_attribute("data-src")
                    poster = await video_elem.get_attribute("poster")
                    
                    if src and src not in videos:
                        videos.append(src)
                    if data_src and data_src not in videos:
                        videos.append(data_src)
                    if poster and poster not in videos:
                        videos.append(f"POSTER::{poster}")
                    
                    # source å­å…ƒç´ 
                    sources = video_elem.locator('source')
                    source_count = await sources.count()
                    for j in range(source_count):
                        source_src = await sources.nth(j).get_attribute("src")
                        if source_src and source_src not in videos:
                            videos.append(source_src)
                except:
                    continue
            
            content_data["videos"] = videos
            
            # â† æ–°å¢: æå–çœŸå¯¦ç™¼æ–‡æ™‚é–“
            try:
                post_published_at = await self._extract_post_published_at(page)
                if post_published_at:
                    content_data["post_published_at"] = post_published_at
                    logging.debug(f"   âœ… æå–ç™¼æ–‡æ™‚é–“: {post_published_at}")
            except Exception as e:
                logging.debug(f"   âš ï¸ ç™¼æ–‡æ™‚é–“æå–å¤±æ•—: {e}")
            
            # â† æ–°å¢: æå–ä¸»é¡Œæ¨™ç±¤
            try:
                tags = await self._extract_tags_from_dom(page)
                if tags:
                    content_data["tags"] = tags
                    logging.debug(f"   âœ… æå–æ¨™ç±¤: {tags}")
            except Exception as e:
                logging.debug(f"   âš ï¸ æ¨™ç±¤æå–å¤±æ•—: {e}")
            
        except Exception as e:
            logging.debug(f"   âš ï¸ DOM å…§å®¹æå–å¤±æ•—: {e}")
        
        return content_data
    
    async def _extract_counts_from_dom_fallback(self, page: Page) -> dict:
        """DOM è¨ˆæ•¸å¾Œæ´æå–"""
        logging.warning(f"   ğŸ”„ GraphQL è¨ˆæ•¸æ””æˆªå¤±æ•—ï¼Œé–‹å§‹ DOM è¨ˆæ•¸å¾Œæ´...")
        
        # å…ˆæª¢æŸ¥é é¢ç‹€æ…‹
        page_title = await page.title()
        page_url = page.url
        logging.info(f"   ğŸ“„ é é¢ç‹€æ…‹ - æ¨™é¡Œ: {page_title}, URL: {page_url}")
        
        count_selectors = {
            "likes": [
                # English selectors
                "button[aria-label*='likes'] span",
                "button[aria-label*='Like'] span", 
                "span:has-text(' likes')",
                "span:has-text(' like')",
                "button svg[aria-label='Like'] + span",
                "button[aria-label*='like']",
                # Chinese selectors
                "button[aria-label*='å€‹å–œæ­¡'] span",
                "button[aria-label*='å–œæ­¡']",
                # Generic patterns
                "button[data-testid*='like'] span",
                "div[role='button'][aria-label*='like'] span"
            ],
            "comments": [
                # English selectors
                "a[href$='#comments'] span",
                "span:has-text(' comments')",
                "span:has-text(' comment')",
                "a:has-text('comments')",
                "button[aria-label*='comment'] span",
                # Chinese selectors
                "span:has-text(' å‰‡ç•™è¨€')",
                "a:has-text('å‰‡ç•™è¨€')",
                # Generic patterns
                "button[data-testid*='comment'] span",
                "div[role='button'][aria-label*='comment'] span"
            ],
            "reposts": [
                # English selectors
                "span:has-text(' reposts')",
                "span:has-text(' repost')",
                "button[aria-label*='repost'] span",
                "a:has-text('reposts')",
                # Chinese selectors
                "span:has-text(' æ¬¡è½‰ç™¼')",
                "a:has-text('è½‰ç™¼')",
                # Generic patterns
                "button[data-testid*='repost'] span"
            ],
            "shares": [
                # English selectors
                "span:has-text(' shares')",
                "span:has-text(' share')",
                "button[aria-label*='share'] span",
                "a:has-text('shares')",
                # Chinese selectors
                "span:has-text(' æ¬¡åˆ†äº«')",
                "a:has-text('åˆ†äº«')",
                # Generic patterns
                "button[data-testid*='share'] span"
            ],
        }
        
        # å…ˆé€²è¡Œé€šç”¨å…ƒç´ æƒæï¼Œä¸¦æ™ºèƒ½æå–æ•¸å­—
        logging.info(f"   ğŸ” é€šç”¨å…ƒç´ æƒæå’Œæ™ºèƒ½æ•¸å­—æå–...")
        dom_counts = {}
        
        try:
            all_buttons = await page.locator('button').all_inner_texts()
            all_spans = await page.locator('span').all_inner_texts()
            number_elements = [text for text in (all_buttons + all_spans) if any(char.isdigit() for char in text)]
            logging.info(f"   ğŸ”¢ æ‰¾åˆ°åŒ…å«æ•¸å­—çš„å…ƒç´ : {number_elements[:20]}")
            
            # === ğŸ¯ æ™ºèƒ½æ•¸å­—è­˜åˆ¥ï¼šå¾æ‰¾åˆ°çš„æ•¸å­—ä¸­æå–ç¤¾äº¤æ•¸æ“š ===
            pure_numbers = []
            for text in number_elements:
                # è·³éæ˜é¡¯ä¸æ˜¯äº’å‹•æ•¸æ“šçš„æ–‡å­—
                if any(skip in text for skip in ['ç€è¦½', 'æ¬¡ç€è¦½', 'è§€çœ‹', 'å¤©', 'å°æ™‚', 'åˆ†é˜', 'ç§’', 'on.natgeo.com']):
                    continue
                    
                number = parse_number(text)
                if number and number > 0:
                    pure_numbers.append((number, text))
                    logging.info(f"   ğŸ“Š æå–æ•¸å­—: {number} (å¾ '{text}')")
            
            # æ ¹æ“šæ•¸å­—å¤§å°æ™ºèƒ½åˆ†é…ï¼ˆé€šå¸¸ï¼šlikes > comments > reposts > sharesï¼‰
            pure_numbers.sort(reverse=True)  # å¾å¤§åˆ°å°æ’åº
            
            if len(pure_numbers) >= 4:
                dom_counts["likes"] = pure_numbers[0][0]
                dom_counts["comments"] = pure_numbers[1][0] 
                dom_counts["reposts"] = pure_numbers[2][0]
                dom_counts["shares"] = pure_numbers[3][0]
                logging.info(f"   ğŸ¯ æ™ºèƒ½åˆ†é…4å€‹æ•¸å­—: è®š={dom_counts['likes']}, ç•™è¨€={dom_counts['comments']}, è½‰ç™¼={dom_counts['reposts']}, åˆ†äº«={dom_counts['shares']}")
            elif len(pure_numbers) >= 2:
                dom_counts["likes"] = pure_numbers[0][0]
                dom_counts["comments"] = pure_numbers[1][0]
                logging.info(f"   ğŸ¯ æ™ºèƒ½åˆ†é…2å€‹æ•¸å­—: è®š={dom_counts['likes']}, ç•™è¨€={dom_counts['comments']}")
            elif len(pure_numbers) >= 1:
                dom_counts["likes"] = pure_numbers[0][0]
                logging.info(f"   ğŸ¯ æ™ºèƒ½åˆ†é…1å€‹æ•¸å­—: è®š={dom_counts['likes']}")
                
        except Exception as e:
            logging.warning(f"   âš ï¸ æ™ºèƒ½æ•¸å­—æå–å¤±æ•—: {e}")
        
        # å¦‚æœæ™ºèƒ½æå–æˆåŠŸï¼Œè·³éå‚³çµ±é¸æ“‡å™¨ï¼›å¦å‰‡ç¹¼çºŒå˜—è©¦
        if not dom_counts:
            logging.info(f"   âš ï¸ æ™ºèƒ½æå–å¤±æ•—ï¼Œå›åˆ°å‚³çµ±é¸æ“‡å™¨...")
            for key, sels in count_selectors.items():
                logging.info(f"   ğŸ” å˜—è©¦æå– {key} æ•¸æ“š...")
                for i, sel in enumerate(sels):
                    try:
                        el = page.locator(sel).first
                        count = await el.count()
                        if count > 0:
                            text = (await el.inner_text()).strip()
                            logging.info(f"   ğŸ“ é¸æ“‡å™¨ {i+1}/{len(sels)} '{sel}' æ‰¾åˆ°æ–‡å­—: '{text}'")
                            n = parse_number(text)
                            if n and n > 0:
                                dom_counts[key] = n
                                logging.info(f"   âœ… DOM æˆåŠŸæå– {key}: {n} (é¸æ“‡å™¨: {sel})")
                                break
                            else:
                                logging.info(f"   âš ï¸ ç„¡æ³•è§£ææ•¸å­—: '{text}' -> {n}")
                        else:
                            logging.info(f"   âŒ é¸æ“‡å™¨ {i+1}/{len(sels)} æœªæ‰¾åˆ°å…ƒç´ : '{sel}'")
                    except Exception as e:
                        logging.info(f"   âš ï¸ é¸æ“‡å™¨ {i+1}/{len(sels)} '{sel}' éŒ¯èª¤: {e}")
                        continue
                
                if key not in dom_counts:
                    logging.warning(f"   âŒ ç„¡æ³•æ‰¾åˆ° {key} æ•¸æ“š")
        
        if dom_counts:
            counts_data = {
                "likes": dom_counts.get("likes", 0),
                "comments": dom_counts.get("comments", 0),
                "reposts": dom_counts.get("reposts", 0),
                "shares": dom_counts.get("shares", 0),
            }
            logging.info(f"   ğŸ¯ DOM è¨ˆæ•¸å¾Œæ´æˆåŠŸ: {counts_data}")
            return counts_data
        else:
            # æ‰€æœ‰æ–¹æ³•éƒ½å¤±æ•—æ™‚ï¼Œè¨˜éŒ„é é¢ç‹€æ…‹ç”¨æ–¼èª¿è©¦
            await self._debug_failed_page(page)
            return {}
    
    async def _debug_failed_page(self, page: Page):
        """èª¿è©¦å¤±æ•—é é¢çš„ç‹€æ…‹"""
        logging.warning(f"   âŒ GraphQLæ””æˆªå’ŒDOMå¾Œæ´éƒ½å¤±æ•—äº†ï¼")
        try:
            page_title = await page.title()
            page_url = page.url
            logging.info(f"   ğŸ“„ å¤±æ•—é é¢åˆ†æ - æ¨™é¡Œ: {page_title}")
            logging.info(f"   ğŸ”— å¤±æ•—é é¢åˆ†æ - URL: {page_url}")
            
            # æª¢æŸ¥é é¢æ˜¯å¦æ­£å¸¸è¼‰å…¥
            all_text = await page.inner_text('body')
            if "ç™»å…¥" in all_text or "login" in all_text.lower():
                logging.warning(f"   âš ï¸ å¯èƒ½é‡åˆ°ç™»å…¥é é¢")
            elif len(all_text) < 100:
                logging.warning(f"   âš ï¸ é é¢å…§å®¹å¤ªå°‘ï¼Œå¯èƒ½è¼‰å…¥å¤±æ•—")
            else:
                logging.info(f"   ğŸ“ é é¢å…§å®¹é•·åº¦: {len(all_text)} å­—å…ƒ")
                
                # æª¢æŸ¥æ˜¯å¦æœ‰äº’å‹•æŒ‰éˆ•
                like_buttons = await page.locator('[aria-label*="like"], [aria-label*="Like"], [aria-label*="å–œæ­¡"]').count()
                comment_buttons = await page.locator('[aria-label*="comment"], [aria-label*="Comment"], [aria-label*="ç•™è¨€"]').count()
                logging.info(f"   ğŸ“Š æ‰¾åˆ°æŒ‰éˆ•: è®š {like_buttons} å€‹, ç•™è¨€ {comment_buttons} å€‹")
                
                # å˜—è©¦æ‰¾åˆ°ä»»ä½•æ•¸å­—
                all_numbers = await page.locator(':text-matches("\\d+")').all_inner_texts()
                if all_numbers:
                    logging.info(f"   ğŸ”¢ é é¢æ‰€æœ‰æ•¸å­—: {all_numbers[:15]}")  # é¡¯ç¤ºå‰15å€‹
                
                # æª¢æŸ¥æ˜¯å¦æœ‰é˜»æ“‹å…ƒç´ 
                modal_count = await page.locator('[role="dialog"], .modal, [data-testid*="modal"]').count()
                if modal_count > 0:
                    logging.warning(f"   âš ï¸ ç™¼ç¾ {modal_count} å€‹æ¨¡æ…‹æ¡†å¯èƒ½é˜»æ“‹å…§å®¹")
                    
        except Exception as debug_e:
            logging.warning(f"   âš ï¸ å¤±æ•—é é¢åˆ†æéŒ¯èª¤: {debug_e}")
    
    async def _update_post_data(self, post: PostMetrics, counts_data: dict, content_data: dict, task_id: str, username: str) -> bool:
        """æ›´æ–°è²¼æ–‡æ•¸æ“š"""
        updated = False
        
        # æ›´æ–°è¨ˆæ•¸æ•¸æ“š - åªåœ¨ç¾æœ‰æ•¸æ“šç‚º None æˆ– 0 æ™‚æ‰æ›´æ–°
        if counts_data:
            if post.likes_count in (None, 0) and (counts_data.get("likes") or 0) > 0:
                post.likes_count = counts_data["likes"]
                updated = True
            if post.comments_count in (None, 0) and (counts_data.get("comments") or 0) > 0:
                post.comments_count = counts_data["comments"]
                updated = True
            if post.reposts_count in (None, 0) and (counts_data.get("reposts") or 0) > 0:
                post.reposts_count = counts_data["reposts"]
                updated = True
            if post.shares_count in (None, 0) and (counts_data.get("shares") or 0) > 0:
                post.shares_count = counts_data["shares"]
                updated = True
        
        # æ›´æ–°å…§å®¹æ•¸æ“š - åªåœ¨ç¾æœ‰æ•¸æ“šç‚ºç©ºæ™‚æ‰æ›´æ–°
        if content_data.get("content") and not post.content:
            logging.info(f"   ğŸ“ [DEBUG] æ›´æ–° post.content: å¾ '{post.content}' â†’ '{content_data['content']}'")
            post.content = content_data["content"]
            updated = True
        
        if content_data.get("images") and not post.images:
            post.images = content_data["images"]
            updated = True
        
        if content_data.get("videos") and not post.videos:
            # éæ¿¾å¯¦éš›å½±ç‰‡ï¼ˆæ’é™¤ POSTERï¼‰
            actual_videos = [v for v in content_data["videos"] if not v.startswith("POSTER::")]
            if actual_videos:
                post.videos = actual_videos
                updated = True
        
        # â† æ–°å¢: æ›´æ–°çœŸå¯¦ç™¼æ–‡æ™‚é–“
        if content_data.get("post_published_at") and not post.post_published_at:
            post.post_published_at = content_data["post_published_at"]
            updated = True
        
        # â† æ–°å¢: æ›´æ–°ä¸»é¡Œæ¨™ç±¤
        if content_data.get("tags") and not post.tags:
            post.tags = content_data["tags"]
            updated = True
        
        if updated:
            post.processing_stage = "details_filled_hybrid"
            # æ§‹å»ºè£œé½Šä¿¡æ¯
            info_parts = [
                f"è®š={post.likes_count}",
                f"å…§å®¹={len(post.content)}å­—",
                f"åœ–ç‰‡={len(post.images)}å€‹",
                f"å½±ç‰‡={len(post.videos)}å€‹"
            ]
            
            if post.post_published_at:
                info_parts.append(f"ç™¼æ–‡æ™‚é–“={post.post_published_at.strftime('%Y-%m-%d %H:%M')}")
            if post.tags:
                info_parts.append(f"æ¨™ç±¤={post.tags}")
                
            logging.info(f"  âœ… æ··åˆç­–ç•¥æˆåŠŸè£œé½Š {post.post_id}: {', '.join(info_parts)}")
            
            # ç™¼å¸ƒé€²åº¦
            if task_id:
                await publish_progress(
                    task_id, 
                    "details_fetched_hybrid",
                    username=content_data.get("username", username or "unknown"),
                    post_id=post.post_id,
                    likes_count=post.likes_count,
                    content_length=len(post.content),
                    media_count=len(post.images) + len(post.videos)
                )
        else:
            post.processing_stage = "details_failed"
            logging.warning(f"  âš ï¸ æ··åˆç­–ç•¥ç„¡æ³•è£œé½Š {post.post_id} çš„æ•¸æ“š")
        
        return updated
    
    async def _extract_post_published_at(self, page: Page) -> Optional[Any]:
        """æå–è²¼æ–‡çœŸå¯¦ç™¼å¸ƒæ™‚é–“ (å¾DOM)"""
        from datetime import datetime
        import json
        
        try:
            # æ–¹æ³•A: ç›´æ¥æŠ“ <time> çš„ datetime å±¬æ€§
            time_elements = page.locator('time[datetime]')
            count = await time_elements.count()
            
            if count > 0:
                for i in range(min(count, 5)):  # æª¢æŸ¥å‰5å€‹
                    try:
                        time_el = time_elements.nth(i)
                        
                        # datetime å±¬æ€§
                        iso_time = await time_el.get_attribute('datetime')
                        if iso_time:
                            from dateutil import parser
                            return parser.parse(iso_time)
                        
                        # title æˆ– aria-label å±¬æ€§  
                        title_time = (await time_el.get_attribute('title') or 
                                    await time_el.get_attribute('aria-label'))
                        if title_time:
                            parsed_time = self._parse_chinese_time(title_time)
                            if parsed_time:
                                return parsed_time
                    except Exception:
                        continue
            
            # æ–¹æ³•B: è§£æ __NEXT_DATA__
            try:
                script_el = page.locator('#__NEXT_DATA__')
                if await script_el.count() > 0:
                    script_content = await script_el.text_content()
                    data = json.loads(script_content)
                    
                    taken_at = self._find_taken_at(data)
                    if taken_at:
                        return datetime.fromtimestamp(taken_at)
                        
            except Exception:
                pass
            
        except Exception:
            pass
        
        return None
    
    def _parse_chinese_time(self, time_str: str) -> Optional[Any]:
        """è§£æä¸­æ–‡æ™‚é–“æ ¼å¼"""
        from datetime import datetime
        try:
            # è™•ç† "2025å¹´8æœˆ3æ—¥ä¸‹åˆ 2:36" æ ¼å¼
            if "å¹´" in time_str and "æœˆ" in time_str and "æ—¥" in time_str:
                import re
                match = re.search(r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥.*?(\d{1,2}):(\d{2})', time_str)
                if match:
                    year, month, day, hour, minute = map(int, match.groups())
                    
                    # è™•ç†ä¸‹åˆ/ä¸Šåˆ
                    if "ä¸‹åˆ" in time_str and hour < 12:
                        hour += 12
                    elif "ä¸Šåˆ" in time_str and hour == 12:
                        hour = 0
                    
                    return datetime(year, month, day, hour, minute)
        except:
            pass
        return None
    
    def _find_taken_at(self, data: Any, path: str = "") -> Optional[int]:
        """éæ­¸æœç´¢ taken_at æ™‚é–“æˆ³"""
        if isinstance(data, dict):
            for key, value in data.items():
                if key == "taken_at" and isinstance(value, int) and value > 1000000000:
                    return value
                result = self._find_taken_at(value, f"{path}.{key}")
                if result:
                    return result
        elif isinstance(data, list):
            for i, item in enumerate(data):
                result = self._find_taken_at(item, f"{path}[{i}]")
                if result:
                    return result
        return None
    
    async def _extract_tags_from_dom(self, page: Page) -> List[str]:
        """æå–ä¸»é¡Œæ¨™ç±¤ (å°ˆé–€æœç´¢Threadsæ¨™ç±¤é€£çµ)"""
        tags = []
        
        try:
            # ç­–ç•¥1: æœç´¢æ¨™ç±¤é€£çµï¼ˆå„ªå…ˆç´šæœ€é«˜ï¼‰
            tag_link_selectors = [
                'a[href*="/search?q="][href*="serp_type=tags"]',  # æ¨™ç±¤æœç´¢é€£çµ
                'a[href*="/search"][href*="tag_id="]',  # åŒ…å«tag_idçš„é€£çµ
                'a[href*="serp_type=tags"]',  # æ¨™ç±¤é¡å‹é€£çµ
            ]
            
            for selector in tag_link_selectors:
                try:
                    tag_links = page.locator(selector)
                    count = await tag_links.count()
                    
                    if count > 0:
                        # åªæª¢æŸ¥å‰3å€‹ï¼ˆé¿å…å›å¾©ä¸­çš„æ¨™ç±¤ï¼‰
                        for i in range(min(count, 3)):
                            try:
                                link = tag_links.nth(i)
                                href = await link.get_attribute('href')
                                text = await link.inner_text()
                                
                                if href and text:
                                    tag_name = self._extract_tag_name_from_link(href, text)
                                    if tag_name and tag_name not in tags:
                                        tags.append(tag_name)
                                        return tags  # æ‰¾åˆ°ä¸€å€‹å°±è¿”å›
                                        
                            except Exception:
                                continue
                                
                except Exception:
                    continue
            
            # ç­–ç•¥2: æœç´¢ä¸»æ–‡ç« å€åŸŸå…§çš„æ¨™ç±¤å…ƒç´ 
            main_post_selectors = [
                'article:first-of-type',
                '[role="article"]:first-of-type',
                'div[data-pressable-container]:first-of-type',
            ]
            
            for main_selector in main_post_selectors:
                try:
                    main_element = page.locator(main_selector)
                    if await main_element.count() > 0:
                        # åœ¨ä¸»æ–‡ç« å…§æœç´¢æ¨™ç±¤é€£çµ
                        main_tag_links = main_element.locator('a[href*="/search"]')
                        main_count = await main_tag_links.count()
                        
                        if main_count > 0:
                            for i in range(min(main_count, 2)):
                                try:
                                    link = main_tag_links.nth(i)
                                    href = await link.get_attribute('href')
                                    text = await link.inner_text()
                                    
                                    if href and text:
                                        tag_name = self._extract_tag_name_from_link(href, text)
                                        if tag_name and tag_name not in tags:
                                            tags.append(tag_name)
                                            return tags
                                            
                                except Exception:
                                    continue
                        
                except Exception:
                    continue
            
        except Exception:
            pass
        
        return tags[:1] if tags else []  # åªè¿”å›ç¬¬ä¸€å€‹æ¨™ç±¤
    
    def _extract_tag_name_from_link(self, href: str, text: str) -> Optional[str]:
        """å¾æ¨™ç±¤é€£çµä¸­æå–æ¨™ç±¤åç¨±"""
        try:
            # å¾URLçš„qåƒæ•¸ä¸­è§£æ
            if "q=" in href:
                import urllib.parse
                parsed_url = urllib.parse.urlparse(href)
                query_params = urllib.parse.parse_qs(parsed_url.query)
                
                if 'q' in query_params:
                    tag_name = query_params['q'][0]
                    tag_name = urllib.parse.unquote(tag_name)
                    return tag_name
            
            # å¾é€£çµæ–‡æœ¬ä¸­å–å¾—ï¼ˆå‚™ç”¨ï¼‰
            if text and len(text.strip()) > 0 and len(text.strip()) <= 20:
                clean_text = text.strip()
                if clean_text.startswith('#'):
                    clean_text = clean_text[1:]
                return clean_text
                
        except Exception:
            pass
        
        return None