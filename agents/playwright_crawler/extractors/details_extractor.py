"""
è©³ç´°æ•¸æ“šæå–å™¨

è² è²¬ä½¿ç”¨ä¸‰å±¤å‚™ç”¨ç­–ç•¥è£œé½Šè²¼æ–‡è©³ç´°æ•¸æ“šï¼š
1. HTMLæ­£å‰‡è§£æ - æœ€ç©©å®šï¼Œç›´æ¥å¾HTMLæ–‡æœ¬æå– (å„ªå…ˆç´šæœ€é«˜)
2. GraphQL è¨ˆæ•¸æ””æˆª - æº–ç¢ºçš„APIæ•¸æ“š (å‚™ç”¨æ–¹æ¡ˆ1) 
3. DOM é¸æ“‡å™¨è§£æ - é é¢å…ƒç´ å®šä½ (æœ€å¾Œå‚™ç”¨)

åŒæ™‚æå–å…§å®¹å’Œåª’é«”æ•¸æ“š (content, images, videos)
"""

import asyncio
import logging
import random
import re
from datetime import datetime
from typing import Dict, List, Optional, Any, Set
from playwright.async_api import BrowserContext, Page

from common.models import PostMetrics
from common.nats_client import publish_progress
from ..parsers.number_parser import parse_number
from ..parsers.html_parser import HTMLParser


class DetailsExtractor:
    """
    è©³ç´°æ•¸æ“šæå–å™¨ - ä½¿ç”¨æ··åˆç­–ç•¥æå–å®Œæ•´çš„è²¼æ–‡æ•¸æ“š
    """
    
    def __init__(self):
        self.html_parser = HTMLParser()  # åˆå§‹åŒ–HTMLè§£æå™¨
    
    async def fill_post_details_from_page(self, posts_to_fill: List[PostMetrics], context: BrowserContext, task_id: str = None, username: str = None) -> List[PostMetrics]:
        """
        ä½¿ç”¨ä¸‰å±¤å‚™ç”¨ç­–ç•¥è£œé½Šè²¼æ–‡è©³ç´°æ•¸æ“šï¼š
        1. HTMLæ­£å‰‡è§£æ - æœ€ç©©å®šï¼Œé›¶é¡å¤–æˆæœ¬ (å„ªå…ˆç´šæœ€é«˜)
        2. GraphQL è¨ˆæ•¸æ””æˆª - æº–ç¢ºçš„APIæ•¸æ“š (å‚™ç”¨æ–¹æ¡ˆ1)
        3. DOM é¸æ“‡å™¨è§£æ - é é¢å…ƒç´ å®šä½ (æœ€å¾Œå‚™ç”¨)
        
        åŒæ™‚æå–å…§å®¹å’Œåª’é«”æ•¸æ“šï¼Œé€™ç¨®å¤šå±¤æ¶æ§‹æä¾›æœ€ç©©å®šå¯é çš„æ•¸æ“šæå–ã€‚
        """
        if not context:
            logging.error("âŒ Browser context æœªåˆå§‹åŒ–ï¼Œç„¡æ³•åŸ·è¡Œ fill_post_details_from_pageã€‚")
            return posts_to_fill

        # ä¿å®ˆçš„ä¸¦ç™¼æ•¸æå‡ï¼šå¾1å¢åŠ åˆ°2ï¼Œå¹³è¡¡é€Ÿåº¦èˆ‡å®‰å…¨æ€§
        semaphore = asyncio.Semaphore(2)  # è¼•å¾®æå‡ä¸¦ç™¼ä½†ä¿æŒå®‰å…¨
        
        async def fetch_single_details_hybrid(post: PostMetrics):
            async with semaphore:
                page = None
                try:
                    page = await context.new_page()
                    
                    logging.debug(f"ğŸ“„ ä½¿ç”¨æ··åˆç­–ç•¥è£œé½Šè©³ç´°æ•¸æ“š: {post.url}")
                    
                    # === æ­¥é©Ÿ 1: æ··åˆç­–ç•¥ - æ””æˆª+é‡ç™¼è«‹æ±‚ ===
                    counts_data = {}
                    video_urls = set()
                    captured_graphql_request = {}
                    response_handler_active = True
                    
                    async def handle_counts_response(response):
                        if not response_handler_active:
                            return  # åœæ­¢è™•ç†éŸ¿æ‡‰
                        await self._handle_graphql_response(response, counts_data, video_urls, captured_graphql_request)
                    
                    page.on("response", handle_counts_response)
                    
                    # === æ­¥é©Ÿ 1.5: æ³¨å…¥play()åŠ«æŒè…³æœ¬ï¼ˆæ–°ç‰ˆThreadså½±ç‰‡æå–ï¼‰ ===
                    await page.add_init_script("""
                    (function () {
                        // åŠ«æŒHTMLMediaElement.play() æ–¹æ³•æ”¶é›†å½±ç‰‡URL
                        const origPlay = HTMLMediaElement.prototype.play;
                        HTMLMediaElement.prototype.play = function () {
                            if (this.currentSrc || this.src) {
                                const videoUrl = this.currentSrc || this.src;
                                // éæ¿¾çœŸæ­£çš„å½±ç‰‡æ ¼å¼
                                if (videoUrl.includes('.mp4') || 
                                    videoUrl.includes('.m3u8') || 
                                    videoUrl.includes('.mpd') ||
                                    videoUrl.includes('video') ||
                                    videoUrl.includes('/v/') ||
                                    this.tagName.toLowerCase() === 'video') {
                                    window._lastVideoSrc = videoUrl;
                                    window._videoSourceInfo = {
                                        url: videoUrl,
                                        tagName: this.tagName,
                                        duration: this.duration || 0,
                                        videoWidth: this.videoWidth || 0,
                                        videoHeight: this.videoHeight || 0
                                    };
                                    console.log('[Video Hijack] æ•ç²çœŸå¯¦å½±ç‰‡:', videoUrl);
                                }
                            }
                            return origPlay.apply(this, arguments);
                        };
                        
                        // è¦†å¯«IntersectionObserverå¼·åˆ¶å¯è¦‹
                        const origObserver = window.IntersectionObserver;
                        window.IntersectionObserver = function(callback, options) {
                            const fakeObserver = new origObserver(function(entries) {
                                entries.forEach(entry => { entry.isIntersecting = true; });
                                callback(entries);
                            }, options);
                            return fakeObserver;
                        };
                        
                        window._videoHijackReady = true;
                    })();
                    """)
                    
                    # === æ­¥é©Ÿ 2: ç›´æ¥å°èˆªï¼ˆç°¡å–®é«˜æ•ˆï¼‰ ===
                    await page.goto(post.url, wait_until="domcontentloaded", timeout=45000)
                    
                    # === æ­¥é©Ÿ 2.1: HTMLè§£æï¼ˆç¬¬ä¸€å„ªå…ˆç´šï¼Œé›¶é¡å¤–æˆæœ¬ï¼‰ ===
                    html_content = None
                    try:
                        html_content = await page.content()  # ç²å–å®Œæ•´HTML
                        html_counts = self.html_parser.extract_from_html(html_content)
                        if html_counts:
                            counts_data.update(html_counts)
                            logging.info(f"   ğŸ¯ HTMLè§£ææˆåŠŸ: {html_counts}")
                            # å¦‚æœHTMLè§£ææˆåŠŸï¼Œè¨˜éŒ„HTMLå…§å®¹ä¾›èª¿è©¦ä½¿ç”¨
                            post_id = post.post_id if hasattr(post, 'post_id') else 'unknown'
                            logging.debug(f"   ğŸ“ HTMLè§£ææˆåŠŸï¼Œpost_id: {post_id}")
                        else:
                            logging.debug(f"   ğŸ“„ HTMLè§£ææœªæ‰¾åˆ°æ•¸æ“šï¼Œç¹¼çºŒå…¶ä»–æ–¹æ³•...")
                    except Exception as e:
                        logging.warning(f"   âš ï¸ HTMLè§£æå¤±æ•—: {e}")
                    
                    # === æ­¥é©Ÿ 2.2: JavaScriptç€è¦½æ•¸æå–ï¼ˆé‡å°å‹•æ…‹å…§å®¹ï¼‰ ===
                    # èª¿è©¦ï¼šæª¢æŸ¥HTMLè§£ææ˜¯å¦å·²æœ‰ç€è¦½æ•¸
                    existing_views = counts_data.get("views_count")
                    logging.info(f"   ğŸ” [DEBUG] HTMLè§£æç€è¦½æ•¸: {existing_views}")
                    
                    if not existing_views:
                        logging.info(f"   ğŸš€ [DEBUG] é–‹å§‹JavaScriptç€è¦½æ•¸æå–...")
                        try:
                            views_count = await self._extract_views_with_javascript(page)
                            if views_count:
                                counts_data["views_count"] = views_count
                                logging.info(f"   ğŸ‘ï¸ JavaScriptæå–ç€è¦½æ•¸æˆåŠŸ: {views_count}")
                            else:
                                logging.warning(f"   ğŸ“„ JavaScriptæœªæ‰¾åˆ°ç€è¦½æ•¸...")
                        except Exception as e:
                            logging.warning(f"   âš ï¸ JavaScriptç€è¦½æ•¸æå–å¤±æ•—: {e}")
                    else:
                        logging.info(f"   â© [DEBUG] HTMLå·²æœ‰ç€è¦½æ•¸ï¼Œè·³éJavaScriptæå–")
                    
                    # æ™ºèƒ½ç­‰å¾…ï¼šå…ˆçŸ­æš«ç­‰å¾…ï¼Œå¦‚æœæ²’æœ‰æ””æˆªåˆ°æ•¸æ“šå†å»¶é•·
                    await asyncio.sleep(1.5)  # ç¸®çŸ­åˆå§‹ç­‰å¾…æ™‚é–“
                    
                    # æª¢æŸ¥æ˜¯å¦å·²ç¶“æ””æˆªåˆ°æ•¸æ“š
                    if not counts_data:
                        logging.debug(f"   â³ é¦–æ¬¡ç­‰å¾…æœªæ””æˆªåˆ°æ•¸æ“šï¼Œå»¶é•·ç­‰å¾…...")
                        await asyncio.sleep(1.5)  # é¡å¤–ç­‰å¾…1.5ç§’ï¼ˆç¸½å…±3ç§’ï¼‰
                    
                    # === æª¢æŸ¥HTMLè§£ææ˜¯å¦å·²ç¶“æˆåŠŸ ===
                    html_success = counts_data and all(counts_data.get(k, 0) > 0 for k in ["likes", "comments", "reposts", "shares"])
                    if html_success:
                        logging.info(f"   âœ… HTMLè§£æå·²æä¾›å®Œæ•´æ•¸æ“šï¼Œè·³éGraphQLæ””æˆª: {counts_data}")
                        response_handler_active = False
                    else:
                        # === æ­¥é©Ÿ 2.5: æ··åˆç­–ç•¥é‡ç™¼è«‹æ±‚ ===
                        if captured_graphql_request and not counts_data:
                            counts_data = await self._resend_graphql_request(captured_graphql_request, post.url, context)
                        
                        # æˆåŠŸç²å–æ•¸æ“šå¾Œåœæ­¢ç›£è½ï¼Œé¿å…ä¸å¿…è¦çš„æ””æˆª
                        if counts_data and counts_data.get("likes", 0) > 0:
                            response_handler_active = False
                            logging.debug(f"   ğŸ›‘ æˆåŠŸç²å–è¨ˆæ•¸æ•¸æ“šï¼Œåœæ­¢éŸ¿æ‡‰ç›£è½")
                    
                    # å˜—è©¦è§¸ç™¼å½±ç‰‡è¼‰å…¥
                    await self._trigger_video_loading(page)
                    
                    # === æ­¥é©Ÿ 3: DOM å…§å®¹æå– ===
                    content_data = await self._extract_content_from_dom(page, username, video_urls)
                    
                    # === æ­¥é©Ÿ 3.5: DOM è¨ˆæ•¸å¾Œæ´ï¼ˆç•¶ HTMLè§£æ å’Œ GraphQL æ””æˆªéƒ½å¤±æ•—æ™‚ï¼‰ ===
                    if not counts_data or not any(counts_data.values()):
                        logging.info(f"   ğŸ”„ HTMLå’ŒGraphQLéƒ½æœªç²å–æ•¸æ“šï¼Œå•Ÿå‹•DOMå¾Œæ´...")
                        dom_counts = await self._extract_counts_from_dom_fallback(page)
                        if dom_counts:
                            counts_data.update(dom_counts)
                            logging.info(f"   ğŸ¯ DOMå¾Œæ´æˆåŠŸ: {dom_counts}")
                        else:
                            logging.warning(f"   âŒ æ‰€æœ‰æå–æ–¹æ³•éƒ½å¤±æ•—äº†")
                    
                    # === æ­¥é©Ÿ 4: æ›´æ–°è²¼æ–‡æ•¸æ“š ===
                    updated = await self._update_post_data(post, counts_data, content_data, task_id, username)
                    
                    # éš¨æ©Ÿå»¶é²é¿å…åçˆ¬èŸ²
                    delay = random.uniform(2, 4)
                    await asyncio.sleep(delay)
                    
                except Exception as e:
                    logging.error(f"  âŒ æ··åˆç­–ç•¥è™•ç† {post.post_id} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                    post.processing_stage = "details_failed"
                finally:
                    if page:
                        await page.close()

        # åºåˆ—è™•ç†ä¿æŒé †åº
        for post in posts_to_fill:
            await fetch_single_details_hybrid(post)
        
        return posts_to_fill
    
    async def _handle_graphql_response(self, response, counts_data: dict, video_urls: set, captured_graphql_request: dict):
        """è™•ç† GraphQL éŸ¿æ‡‰çš„æ””æˆªï¼ˆå„ªåŒ–ç‰ˆï¼šæ”¯æŒå»é‡ï¼‰"""
        try:
            import json
            url = response.url.lower()
            headers = response.request.headers
            query_name = headers.get("x-fb-friendly-name", "")
            
            # æª¢æŸ¥æ˜¯å¦å·²ç¶“æœ‰å®Œæ•´æ•¸æ“šï¼Œé¿å…é‡è¤‡æ””æˆª
            if (counts_data.get("likes", 0) > 0 and 
                counts_data.get("comments", 0) >= 0 and 
                counts_data.get("reposts", 0) >= 0 and 
                counts_data.get("shares", 0) >= 0):
                logging.debug(f"   â© å·²æœ‰å®Œæ•´è¨ˆæ•¸æ•¸æ“šï¼Œè·³éé‡è¤‡æ””æˆª")
                return
            
            # æ””æˆªè¨ˆæ•¸æŸ¥è©¢è«‹æ±‚ï¼ˆä¿å­˜headerså’Œpayloadï¼‰
            if ("/graphql" in url and response.status == 200 and 
                "useBarcelonaBatchedDynamicPostCountsSubscriptionQuery" in query_name):
                
                # åªåœ¨ç¬¬ä¸€æ¬¡æ””æˆªæ™‚è¨˜éŒ„è©³ç´°æ—¥èªŒ
                if not captured_graphql_request.get("headers"):
                    logging.info(f"   ğŸ¯ æ””æˆªåˆ°GraphQLè¨ˆæ•¸æŸ¥è©¢ï¼Œä¿å­˜è«‹æ±‚ä¿¡æ¯...")
                    
                    # ä¿å­˜è«‹æ±‚ä¿¡æ¯ï¼ˆæ¨¡ä»¿hybrid_content_extractor.pyçš„æˆåŠŸç­–ç•¥ï¼‰
                    captured_graphql_request.update({
                        "headers": dict(response.request.headers),
                        "payload": response.request.post_data,
                        "url": "https://www.threads.com/graphql/query"
                    })
                    
                    # æ¸…ç†headers
                    clean_headers = captured_graphql_request["headers"].copy()
                    for h in ["host", "content-length", "accept-encoding"]:
                        clean_headers.pop(h, None)
                    captured_graphql_request["clean_headers"] = clean_headers
                    
                    logging.info(f"   âœ… æˆåŠŸä¿å­˜GraphQLè«‹æ±‚ä¿¡æ¯ï¼Œæº–å‚™é‡ç™¼...")
                else:
                    logging.debug(f"   ğŸ”„ é‡è¤‡GraphQLæ””æˆªï¼Œä½¿ç”¨å·²ä¿å­˜çš„è«‹æ±‚ä¿¡æ¯")
                
                # ä¹Ÿå˜—è©¦ç›´æ¥è§£æç•¶å‰éŸ¿æ‡‰ï¼ˆä½œç‚ºå‚™ç”¨ï¼‰
                try:
                    data = await response.json()
                    if "data" in data and "data" in data["data"] and "posts" in data["data"]["data"]:
                        posts_list = data["data"]["data"]["posts"]
                        if posts_list and len(posts_list) > 0:
                            post_data = posts_list[0]
                            if isinstance(post_data, dict):
                                text_info = post_data.get("text_post_app_info", {}) or {}
                                new_counts = {
                                    "likes": post_data.get("like_count") or 0,
                                    "comments": text_info.get("direct_reply_count") or 0, 
                                    "reposts": text_info.get("repost_count") or 0,
                                    "shares": text_info.get("reshare_count") or 0
                                }
                                
                                # åªåœ¨æ²’æœ‰æ•¸æ“šæˆ–æ•¸æ“šæ›´æ–°æ™‚æ‰æ›´æ–°
                                if not counts_data or any(new_counts.get(k, 0) > counts_data.get(k, 0) for k in new_counts):
                                    counts_data.update(new_counts)
                                    logging.info(f"   âœ… ç›´æ¥æ””æˆªæˆåŠŸ: è®š={counts_data['likes']}, ç•™è¨€={counts_data['comments']}, è½‰ç™¼={counts_data['reposts']}, åˆ†äº«={counts_data['shares']}")
                                else:
                                    logging.debug(f"   â© æ•¸æ“šç„¡æ›´æ–°ï¼Œè·³éé‡è¤‡è¨˜éŒ„")
                except Exception as e:
                    logging.debug(f"   âš ï¸ ç›´æ¥è§£æå¤±æ•—: {e}")
            
            # ğŸ¬ ç²¾ç¢ºGraphQLæ””æˆªï¼ˆæ ¹æ“šç”¨æˆ¶å»ºè­°æ”¹é€²ï¼‰
            if "GraphVideoPlayback" in response.url or "PolarisGraphVideoPlaybackQuery" in response.url:
                try:
                    data = await response.json()
                    logging.debug(f"   ğŸ” å‘½ä¸­GraphQLå½±ç‰‡æŸ¥è©¢: {response.url}")
                    
                    # ç›´æ¥è·¯å¾‘ï¼šdata.video
                    video_data = data.get("data", {}).get("video", {})
                    if video_data:
                        for key in ("playable_url_hd", "playable_url"):
                            url = video_data.get(key)
                            if url:
                                video_urls.add(url)
                                logging.info(f"   ğŸ¥ GraphQL{key}: {url}")  # é¡¯ç¤ºå®Œæ•´URL
                    else:
                        logging.debug(f"   âš ï¸ GraphQLéŸ¿æ‡‰ç„¡videoå­—æ®µ: {list(data.get('data', {}).keys())}")
                        
                except Exception as e:
                    logging.debug(f"   âš ï¸ GraphQLå½±ç‰‡è§£æå¤±æ•—: {e}")
            
            # ğŸš€ ç¬¬0å±¤ï¼šç›´æ¥æ””æˆªå½±ç‰‡æ–‡ä»¶è«‹æ±‚ï¼ˆæœ€ç›´æ¥æ–¹æ³•ï¼‰
            url_clean = response.url.split("?")[0]  # ç§»é™¤æŸ¥è©¢åƒæ•¸
            if url_clean.endswith((".mp4", ".m3u8", ".mpd", ".webm", ".mov")):
                video_urls.add(response.url)
                logging.info(f"   ğŸ¯ ç¬¬0å±¤ç›´æ¥æ””æˆªå®Œæ•´URL: {response.url}")
            
            # ğŸ¥ å‚³çµ±è³‡æºæ””æˆªï¼ˆå‚™ç”¨ï¼‰
            content_type = response.headers.get("content-type", "")
            resource_type = response.request.resource_type
            if (resource_type == "media" or content_type.startswith("video/")):
                if self._is_valid_video_url(response.url):
                    video_urls.add(response.url)
                    logging.info(f"   ğŸ¥ å‚³çµ±è³‡æºæ””æˆªå®Œæ•´URL: {response.url}")
                else:
                    logging.debug(f"   ğŸš« è·³ééå½±ç‰‡è³‡æº: {response.url[:60]}...")
                
        except Exception as e:
            logging.debug(f"   âš ï¸ éŸ¿æ‡‰è™•ç†å¤±æ•—: {e}")
    
    async def _resend_graphql_request(self, captured_graphql_request: dict, post_url: str, context: BrowserContext) -> dict:
        """é‡ç™¼ GraphQL è«‹æ±‚"""
        logging.info(f"   ğŸ”„ ä½¿ç”¨ä¿å­˜çš„GraphQLè«‹æ±‚ä¿¡æ¯é‡ç™¼è«‹æ±‚...")
        counts_data = {}
        
        try:
            import httpx
            
            # å¾URLæå–PKï¼ˆå¦‚æœå¯èƒ½ï¼‰
            url_match = re.search(r'/post/([^/?]+)', post_url)
            if url_match:
                logging.info(f"   ğŸ” URLä»£ç¢¼: {url_match.group(1)}")
            
            # æº–å‚™é‡ç™¼è«‹æ±‚
            headers = captured_graphql_request["clean_headers"]
            payload = captured_graphql_request["payload"]
            
            # å¾é é¢contextç²å–cookies
            cookies_list = await context.cookies()
            cookies = {cookie['name']: cookie['value'] for cookie in cookies_list}
            
            # ç¢ºä¿æœ‰èªè­‰ - ä¿®å¾©ç‰ˆï¼šåŠ å…¥é—œéµtoken
            # 1. è¨­ç½®authorization
            if not headers.get("authorization") and 'ig_set_authorization' in cookies:
                auth_value = cookies['ig_set_authorization']
                headers["authorization"] = f"Bearer {auth_value}" if not auth_value.startswith('Bearer') else auth_value
            
            # 2. ç¢ºä¿é—œéµçš„fb_dtsgå’Œlsd tokenå­˜åœ¨
            if 'fb_dtsg' in cookies:
                headers["x-fb-dtsg"] = cookies['fb_dtsg']
            elif 'dtsg' in cookies:
                headers["x-fb-dtsg"] = cookies['dtsg']
            
            if 'lsd' in cookies:
                headers["x-fb-lsd"] = cookies['lsd']
            elif '_js_lsd' in cookies:
                headers["x-fb-lsd"] = cookies['_js_lsd']
            
            # 3. ç¢ºä¿User-Agentå’Œå…¶ä»–å¿…è¦header
            if 'user-agent' not in headers:
                headers["user-agent"] = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
            
            logging.debug(f"   ğŸ”§ èªè­‰æª¢æŸ¥: auth={bool(headers.get('authorization'))}, dtsg={bool(headers.get('x-fb-dtsg'))}, lsd={bool(headers.get('x-fb-lsd'))}")
            
            # ç™¼é€HTTPè«‹æ±‚åˆ°Threads API
            async with httpx.AsyncClient(headers=headers, cookies=cookies, timeout=30.0, http2=True) as client:
                api_response = await client.post("https://www.threads.com/graphql/query", data=payload)
                
                if api_response.status_code == 200:
                    result = api_response.json()
                    logging.info(f"   âœ… é‡ç™¼è«‹æ±‚æˆåŠŸï¼Œç‹€æ…‹: {api_response.status_code}")
                    
                    if "data" in result and result["data"] and "data" in result["data"] and "posts" in result["data"]["data"]:
                        posts_list = result["data"]["data"]["posts"]
                        logging.info(f"   ğŸ“Š é‡ç™¼è«‹æ±‚éŸ¿æ‡‰åŒ…å« {len(posts_list)} å€‹è²¼æ–‡")
                        
                        # ä½¿ç”¨ç¬¬ä¸€å€‹è²¼æ–‡ï¼ˆç•¶å‰é é¢çš„ä¸»è¦è²¼æ–‡ï¼‰
                        if posts_list and len(posts_list) > 0:
                            post_data = posts_list[0]
                            if isinstance(post_data, dict):
                                text_info = post_data.get("text_post_app_info", {}) or {}
                                counts_data.update({
                                    "likes": post_data.get("like_count") or 0,
                                    "comments": text_info.get("direct_reply_count") or 0, 
                                    "reposts": text_info.get("repost_count") or 0,
                                    "shares": text_info.get("reshare_count") or 0
                                })
                                logging.info(f"   ğŸ¯ é‡ç™¼è«‹æ±‚æˆåŠŸç²å–æ•¸æ“š: è®š={counts_data['likes']}, ç•™è¨€={counts_data['comments']}, è½‰ç™¼={counts_data['reposts']}, åˆ†äº«={counts_data['shares']}")
                else:
                    logging.warning(f"   âš ï¸ é‡ç™¼è«‹æ±‚å¤±æ•—ï¼Œç‹€æ…‹: {api_response.status_code}")
                    
        except Exception as e:
            logging.warning(f"   âš ï¸ é‡ç™¼è«‹æ±‚éç¨‹å¤±æ•—: {e}")
        
        return counts_data
    
    async def _extract_views_with_javascript(self, page) -> Optional[int]:
        """ä½¿ç”¨JavaScriptå¾æ¸²æŸ“å¾Œçš„DOMä¸­æå–ç€è¦½æ•¸"""
        try:
            # JavaScriptä»£ç¢¼ï¼šæœç´¢æ‰€æœ‰åŒ…å«ç€è¦½æ•¸çš„å…ƒç´ 
            js_code = """
            () => {
                // æœç´¢æ‰€æœ‰å¯èƒ½åŒ…å«ç€è¦½æ•¸çš„æ–‡æœ¬
                const allTexts = [];
                const walker = document.createTreeWalker(
                    document.body,
                    NodeFilter.SHOW_TEXT,
                    null,
                    false
                );
                
                let node;
                while (node = walker.nextNode()) {
                    const text = node.textContent.trim();
                    if (text && (
                        text.includes('views') || 
                        text.includes('ç€è¦½') ||
                        text.includes('æµè§ˆ') ||
                        /\\d+K\\s*views/i.test(text) ||
                        /\\d+M\\s*views/i.test(text) ||
                        /\\d+è¬.*ç€è¦½/i.test(text) ||
                        /\\d+ä¸‡.*æµè§ˆ/i.test(text)
                    )) {
                        allTexts.push(text);
                    }
                }
                
                // ä¹Ÿæœç´¢aria-labelå’Œdataå±¬æ€§
                const elements = document.querySelectorAll('*');
                for (const el of elements) {
                    const ariaLabel = el.getAttribute('aria-label') || '';
                    const title = el.getAttribute('title') || '';
                    const dataText = el.getAttribute('data-text') || '';
                    
                    for (const attr of [ariaLabel, title, dataText, el.textContent || '']) {
                        if (attr && (
                            attr.includes('views') || 
                            attr.includes('ç€è¦½') ||
                            attr.includes('æµè§ˆ') ||
                            /\\d+K\\s*views/i.test(attr) ||
                            /\\d+M\\s*views/i.test(attr) ||
                            /\\d+è¬.*ç€è¦½/i.test(attr) ||
                            /\\d+ä¸‡.*æµè§ˆ/i.test(attr)
                        )) {
                            allTexts.push(attr.trim());
                        }
                    }
                }
                
                return [...new Set(allTexts)]; // å»é‡
            }
            """
            
            # åŸ·è¡ŒJavaScriptç²å–æ‰€æœ‰å¯èƒ½çš„ç€è¦½æ•¸æ–‡æœ¬
            view_texts = await page.evaluate(js_code)
            
            if not view_texts:
                logging.debug(f"   ğŸ” JavaScriptæœªæ‰¾åˆ°ä»»ä½•ç€è¦½ç›¸é—œæ–‡æœ¬")
                return None
            
            logging.debug(f"   ğŸ” JavaScriptæ‰¾åˆ° {len(view_texts)} å€‹ç€è¦½ç›¸é—œæ–‡æœ¬:")
            for i, text in enumerate(view_texts[:5]):  # åªè¨˜éŒ„å‰5å€‹
                logging.debug(f"      {i+1}. '{text}'")
            
            # ä½¿ç”¨ç¾æœ‰çš„ç€è¦½æ•¸è§£æé‚è¼¯
            for text in view_texts:
                views_count = self._parse_views_text(text)
                if views_count and views_count > 1000:  # åˆç†æ€§æª¢æŸ¥
                    logging.info(f"   ğŸ¯ æˆåŠŸè§£æç€è¦½æ•¸: {views_count} (ä¾†æº: '{text}')")
                    return views_count
            
            logging.debug(f"   âŒ æ‰€æœ‰ç€è¦½æ–‡æœ¬éƒ½ç„¡æ³•è§£æå‡ºæœ‰æ•ˆæ•¸å­—")
            return None
            
        except Exception as e:
            logging.warning(f"   âš ï¸ JavaScriptç€è¦½æ•¸æå–éç¨‹å¤±æ•—: {e}")
            return None
    
    def _parse_views_text(self, text: str) -> Optional[int]:
        """è§£æç€è¦½æ•¸æ–‡æœ¬ï¼Œè¿”å›æ•¸å­—"""
        import re
        
        try:
            # è‹±æ–‡æ ¼å¼
            patterns = [
                (r'(\d+(?:\.\d+)?)\s*K\s*views', 1000),
                (r'(\d+(?:\.\d+)?)\s*M\s*views', 1000000),
                (r'(\d+(?:,\d{3})*)\s*views', 1),
                # ä¸­æ–‡æ ¼å¼  
                (r'(\d+(?:\.\d+)?)\s*è¬.*ç€è¦½', 10000),
                (r'(\d+(?:\.\d+)?)\s*ä¸‡.*æµè§ˆ', 10000),
                (r'(\d+(?:,\d{3})*)\s*.*ç€è¦½', 1),
                (r'(\d+(?:,\d{3})*)\s*.*æµè§ˆ', 1),
            ]
            
            for pattern_str, multiplier in patterns:
                pattern = re.compile(pattern_str, re.IGNORECASE)
                match = pattern.search(text)
                if match:
                    try:
                        num = float(match.group(1).replace(',', ''))
                        views = int(num * multiplier)
                        if 1000 <= views <= 50000000:  # åˆç†ç¯„åœ
                            return views
                    except (ValueError, TypeError):
                        continue
            
            return None
            
        except Exception as e:
            logging.debug(f"   âš ï¸ è§£æç€è¦½æ•¸æ–‡æœ¬å¤±æ•—: {e}")
            return None
    
    async def _realistic_navigation_to_post(self, page: Page, post_url: str):
        """ç¾å¯¦å°èˆªè·¯å¾‘ï¼šé¦–é  â†’ ç”¨æˆ¶é  â†’ è²¼æ–‡ (é¿å…åçˆ¬èŸ²)"""
        try:
            import re
            from urllib.parse import urlparse
            
            # å¾è²¼æ–‡URLè§£æç”¨æˆ¶åå’Œè²¼æ–‡ID
            url_match = re.search(r'/@([^/]+)/post/([^/?]+)', post_url)
            if not url_match:
                logging.warning(f"   âš ï¸ ç„¡æ³•è§£æè²¼æ–‡URLï¼Œå›é€€åˆ°ç›´æ¥å°èˆª: {post_url}")
                await page.goto(post_url, wait_until="domcontentloaded", timeout=45000)
                return
                
            username = url_match.group(1)
            post_id = url_match.group(2)
            parsed_url = urlparse(post_url)
            base_domain = f"{parsed_url.scheme}://{parsed_url.netloc}"
            
            logging.info(f"   ğŸŒ é–‹å§‹ç¾å¯¦å°èˆª: {username} â†’ {post_id}")
            
            # æ­¥é©Ÿ1: å°èˆªåˆ°é¦–é 
            logging.info(f"   ğŸ“ æ­¥é©Ÿ1: å°èˆªåˆ°é¦–é ...")
            await page.goto(f"{base_domain}/", wait_until="networkidle", timeout=30000)
            await asyncio.sleep(2)  # ç­‰å¾…é¦–é è¼‰å…¥
            
            # æ­¥é©Ÿ2: æ¨¡æ“¬äººé¡è¡Œç‚ºï¼ˆæ»¾å‹•ä¸€ä¸‹ï¼‰
            logging.debug(f"   ğŸ‘† æ¨¡æ“¬ç”¨æˆ¶æ»¾å‹•...")
            await page.mouse.wheel(0, 300)
            await asyncio.sleep(1)
            
            # æ­¥é©Ÿ3: å°èˆªåˆ°ç”¨æˆ¶é é¢
            user_profile_url = f"{base_domain}/@{username}"
            logging.info(f"   ğŸ“ æ­¥é©Ÿ2: å°èˆªåˆ°ç”¨æˆ¶é é¢: {user_profile_url}")
            await page.goto(user_profile_url, wait_until="networkidle", timeout=30000)
            await asyncio.sleep(2)  # ç­‰å¾…ç”¨æˆ¶é é¢è¼‰å…¥
            
            # æ­¥é©Ÿ4: å˜—è©¦æ‰¾åˆ°ä¸¦é»æ“Šç›®æ¨™è²¼æ–‡
            logging.info(f"   ğŸ“ æ­¥é©Ÿ3: å°‹æ‰¾ç›®æ¨™è²¼æ–‡: {post_id}")
            
            # å˜—è©¦å¤šç¨®æ–¹å¼æ‰¾åˆ°è²¼æ–‡é€£çµ
            post_link_selectors = [
                f'a[href*="{post_id}"]',  # ç›´æ¥åŒ…å«è²¼æ–‡IDçš„é€£çµ
                f'a[href*="/post/{post_id}"]',  # å®Œæ•´è·¯å¾‘
                f'a[href*="/{username}/post/{post_id}"]',  # å®Œæ•´ç”¨æˆ¶è·¯å¾‘
            ]
            
            post_found = False
            for selector in post_link_selectors:
                try:
                    post_links = page.locator(selector)
                    link_count = await post_links.count()
                    
                    if link_count > 0:
                        logging.info(f"   ğŸ¯ æ‰¾åˆ°ç›®æ¨™è²¼æ–‡é€£çµï¼é»æ“Šé€²å…¥...")
                        await post_links.first.click()
                        await page.wait_for_load_state("networkidle", timeout=15000)
                        post_found = True
                        break
                        
                except Exception as e:
                    logging.debug(f"   âŒ è²¼æ–‡é€£çµæŸ¥æ‰¾å¤±æ•—: {e}")
                    continue
            
            # å¦‚æœç„¡æ³•é€šéé»æ“Šæ‰¾åˆ°ï¼Œå›é€€åˆ°ç›´æ¥å°èˆª
            if not post_found:
                logging.warning(f"   âš ï¸ æœªæ‰¾åˆ°è²¼æ–‡é€£çµï¼Œç›´æ¥å°èˆªåˆ°ç›®æ¨™é é¢...")
                await page.goto(post_url, wait_until="networkidle", timeout=30000)
            
            logging.info(f"   âœ… ç¾å¯¦å°èˆªå®Œæˆ")
            
        except Exception as e:
            logging.error(f"   âŒ ç¾å¯¦å°èˆªå¤±æ•—: {e}")
            # å›é€€åˆ°ç›´æ¥å°èˆª
            logging.info(f"   ğŸ”„ å›é€€åˆ°ç›´æ¥å°èˆª...")
            await page.goto(post_url, wait_until="domcontentloaded", timeout=45000)
    
    async def _trigger_video_loading(self, page: Page):
        """è§¸ç™¼å½±ç‰‡è¼‰å…¥ - å¢å¼·ç‰ˆï¼ˆåŸºæ–¼æŠ€è¡“å ±å‘Šï¼‰"""
        try:
            # éšæ®µ1: æ“´å±•è§¸ç™¼é¸æ“‡å™¨ï¼ˆåŸºæ–¼æŠ€è¡“å ±å‘Šï¼‰
            trigger_selectors = [
                'div[data-testid="media-viewer"]',
                'video',  
                'div[role="button"][aria-label*="play"]',
                'div[role="button"][aria-label*="æ’­æ”¾"]', 
                'div[role="button"][aria-label*="Play"]',
                '[data-pressable-container] div[style*="video"]',
                'div[aria-label*="Video"]',  # æ–°å¢
                'div[aria-label*="å½±ç‰‡"]',    # æ–°å¢
                'div[data-testid*="video"]', # æ–°å¢
                'button[aria-label*="æ’­æ”¾"]', # æ–°å¢
                'button[aria-label*="play"]', # æ–°å¢
            ]
            
            logging.info(f"   ğŸ¬ é–‹å§‹è§¸ç™¼å½±ç‰‡è¼‰å…¥...")
            video_triggered = False
            
            for i, selector in enumerate(trigger_selectors):
                try:
                    elements = page.locator(selector)
                    count = await elements.count()
                    if count > 0:
                        logging.info(f"   ğŸ¯ è§¸ç™¼å™¨ {i+1}/{len(trigger_selectors)} æ‰¾åˆ° {count} å€‹å…ƒç´ : {selector}")
                        # å˜—è©¦é»æ“Šç¬¬ä¸€å€‹å…ƒç´ 
                        await elements.first.click(timeout=3000)
                        await asyncio.sleep(1.5)  # çŸ­æš«ç­‰å¾…
                        video_triggered = True
                        break
                except Exception as e:
                    logging.debug(f"   âŒ è§¸ç™¼å™¨ {i+1} å¤±æ•—: {e}")
                    continue
            
            if video_triggered:
                logging.info(f"   âœ… å½±ç‰‡è§¸ç™¼æˆåŠŸï¼Œç­‰å¾…è¼‰å…¥...")
                # éšæ®µ2: å»¶é²äºŒéšæ®µï¼ˆæŠ€è¡“å ±å‘Šå»ºè­°ï¼‰
                await asyncio.sleep(3)  # ç­‰å¾…ç¬¬ä¸€æ®µ MPD/M3U8
                logging.debug(f"   ğŸ”„ å»¶é²äºŒéšæ®µç­‰å¾… MP4 ç‰‡æ®µ...")
            else:
                logging.warning(f"   âš ï¸ æœªæ‰¾åˆ°å½±ç‰‡è§¸ç™¼å…ƒç´ ")
                
        except Exception as e:
            logging.warning(f"   âŒ å½±ç‰‡è§¸ç™¼è¼‰å…¥å¤±æ•—: {e}")
    
    def _clean_content_text(self, text: str) -> str:
        """æ¸…ç†å…§å®¹æ–‡å­—ï¼Œç§»é™¤å¥å°¾çš„ \nTranslate"""
        if not text:
            return text
            
        cleaned = text.strip()
        
        # ç§»é™¤å¥å°¾çš„ç¿»è­¯æ¨™è¨˜ï¼ˆå¤šç¨®æ ¼å¼ï¼‰
        translation_patterns = ['\nTranslate', '\nç¿»è­¯', 'ç¿»è­¯', 'Translate']
        
        for pattern in translation_patterns:
            if cleaned.endswith(pattern):
                cleaned = cleaned[:-len(pattern)].strip()
                logging.debug(f"   ğŸ§¹ ç§»é™¤ç¿»è­¯æ¨™è¨˜: {pattern}")
                break
        
        return cleaned
    
    def _is_valid_video_url(self, url: str) -> bool:
        """é©—è­‰URLæ˜¯å¦ç‚ºæœ‰æ•ˆçš„å½±ç‰‡URL"""
        if not url or not isinstance(url, str):
            return False
            
        url_lower = url.lower()
        
        # æ˜ç¢ºçš„å½±ç‰‡æ ¼å¼
        video_extensions = ['.mp4', '.webm', '.mov', '.avi', '.m3u8', '.mpd']
        if any(ext in url_lower for ext in video_extensions):
            return True
            
        # åŒ…å«videoé—œéµå­—çš„è·¯å¾‘
        video_keywords = ['/video/', '/v/', 'video', 'playback']
        if any(keyword in url_lower for keyword in video_keywords):
            return True
            
        # æ’é™¤æ˜ç¢ºçš„éå½±ç‰‡è³‡æº
        non_video_patterns = [
            'poster', 'thumbnail', 'preview', '.jpg', '.png', '.jpeg', 
            '.gif', '.webp', '_n.jpg', '_n.png', 'stp=dst-jpg'
        ]
        if any(pattern in url_lower for pattern in non_video_patterns):
            return False
            
        # Facebook/Instagram CDNç‰¹æ®Šåˆ¤æ–·
        if 'fbcdn.net' in url_lower:
            # /v/ è·¯å¾‘é€šå¸¸æ˜¯å½±ç‰‡, /p/ æˆ– /t/ é€šå¸¸æ˜¯åœ–ç‰‡
            if '/v/' in url_lower or '/video/' in url_lower:
                return True
            elif '/p/' in url_lower or '/t/' in url_lower:
                return False
            # å…¶ä»–fbcdn URLéœ€è¦æ›´å¤šä¿¡æ¯åˆ¤æ–·
            return False
            
        return True
    
    async def _extract_video_from_next_data(self, page: Page) -> list:
        """å¾__NEXT_DATA__ä¸­æå–å½±ç‰‡URLï¼ˆæ–°ç‰ˆThreads Next.jsæ¶æ§‹ï¼‰"""
        try:
            script_el = page.locator('script#__NEXT_DATA__')
            if await script_el.count() > 0:
                script_content = await script_el.text_content()
                if script_content:
                    import json
                    data = json.loads(script_content)
                    
                    # ç°¡åŒ–è·¯å¾‘è§£æï¼ˆæ ¹æ“šç”¨æˆ¶å»ºè­°æ”¹é€²ï¼‰
                    video_urls = []
                    
                    try:
                        # ä¸»è·¯å¾‘ï¼šå–®è²¼æ–‡æˆ–å¡ç‰‡æµ
                        medias = (
                            data["props"]["pageProps"]["post"].get("media", [])  # å–®è²¼æ–‡
                            if "post" in data.get("props", {}).get("pageProps", {})
                            else data["props"]["pageProps"]["feed"]["edges"][0]["node"]["media"]  # å¡ç‰‡æµ
                        )
                        
                        for item in medias:
                            video_url = item.get("video_url")
                            if video_url:
                                video_urls.append(video_url)
                                logging.info(f"   ğŸ“¹ __NEXT_DATA__å½±ç‰‡: {video_url}")
                                
                    except (KeyError, TypeError, IndexError) as e:
                        logging.debug(f"   âš ï¸ __NEXT_DATA__è·¯å¾‘è§£æå¤±æ•—: {e}")
                        # å‚™ç”¨ï¼šç›´æ¥æœç´¢ä»»ä½•video_url
                        try:
                            import re
                            script_text = script_content
                            video_url_pattern = r'"video_url":"([^"]+)"'
                            matches = re.findall(video_url_pattern, script_text)
                            for match in matches:
                                video_urls.append(match)
                                logging.info(f"   ğŸ“¹ __NEXT_DATA__å‚™ç”¨æœç´¢: {match}")
                        except Exception:
                            pass
                    
                    return video_urls
        except Exception as e:
            logging.debug(f"   âš ï¸ __NEXT_DATA__è§£æå¤±æ•—: {e}")
        
        return []
    
    async def _extract_video_from_hijacked_play(self, page: Page) -> str:
        """å¾åŠ«æŒçš„play()æ–¹æ³•ä¸­ç²å–å½±ç‰‡URL"""
        try:
            # ç­‰å¾…åŠ«æŒè…³æœ¬å°±ç·’
            await page.wait_for_function("window._videoHijackReady === true", timeout=5000)
            
            # å˜—è©¦å¤šç¨®æ–¹å¼è§¸ç™¼å½±ç‰‡æ’­æ”¾ä»¥æ¿€æ´»åŠ«æŒ
            try:
                # æ–¹æ³•1ï¼šä½¿ç”¨éµç›¤å¿«æ·éµï¼ˆå¾ˆå¤šæ’­æ”¾å™¨æ”¯æŒï¼‰
                await page.keyboard.press("k")  # å¸¸è¦‹çš„æ’­æ”¾/æš«åœå¿«æ·éµ
                await asyncio.sleep(0.5)
                
                # æ–¹æ³•2ï¼šé»æ“Šä»»ä½•å¯èƒ½çš„æ’­æ”¾å…ƒç´ 
                trigger_selectors = [
                    'video', 'button[aria-label*="play"]', 'button[aria-label*="æ’­æ”¾"]',
                    'div[role="button"][aria-label*="play"]'
                ]
                
                for selector in trigger_selectors:
                    try:
                        elements = page.locator(selector)
                        if await elements.count() > 0:
                            await elements.first.click(timeout=2000)
                            await asyncio.sleep(0.5)
                            break
                    except:
                        continue
                        
            except Exception as e:
                logging.debug(f"   âš ï¸ è§¸ç™¼æ’­æ”¾å¤±æ•—: {e}")
            
            # ç­‰å¾…åŠ«æŒåˆ°å½±ç‰‡URLæˆ–è¶…æ™‚
            try:
                await page.wait_for_function("window._lastVideoSrc !== undefined", timeout=8000)
                video_url = await page.evaluate("window._lastVideoSrc")
                video_info = await page.evaluate("window._videoSourceInfo || {}")
                
                logging.info(f"   ğŸ” åŠ«æŒè©³æƒ…: æ¨™ç±¤={video_info.get('tagName', 'unknown')} å¯¬åº¦={video_info.get('videoWidth', 0)} é«˜åº¦={video_info.get('videoHeight', 0)}")
                logging.info(f"   ğŸ¬ å®Œæ•´URL: {video_url}")
                
                if video_url:
                    # é©—è­‰æ˜¯å¦ç‚ºçœŸæ­£çš„å½±ç‰‡URL
                    if self._is_valid_video_url(video_url):
                        logging.info(f"   âœ… åŠ«æŒplay()ç²å¾—æœ‰æ•ˆå½±ç‰‡: {video_url[:60]}...")
                        return video_url
                    else:
                        logging.warning(f"   âŒ åŠ«æŒåˆ°éå½±ç‰‡è³‡æº: {video_url[:60]}...")
                        return None
            except Exception:
                logging.debug(f"   â° play()åŠ«æŒè¶…æ™‚ï¼Œæœªæ•ç²åˆ°å½±ç‰‡URL")
                
        except Exception as e:
            logging.debug(f"   âš ï¸ play()åŠ«æŒå¤±æ•—: {e}")
        
        return None
    
    async def _extract_content_from_dom(self, page: Page, username: str, video_urls: set) -> dict:
        """å¾ DOM æå–å…§å®¹æ•¸æ“š"""
        content_data = {}
        
        try:
            # æå–ç”¨æˆ¶åï¼ˆå¾ URLï¼‰
            url_match = re.search(r'/@([^/]+)/', page.url)
            content_data["username"] = url_match.group(1) if url_match else username or ""
            
            # æå–å…§å®¹æ–‡å­—
            content = ""
            content_selectors = [
                'div[data-pressable-container] span',
                '[data-testid="thread-text"]',
                'article div[dir="auto"]',
                'div[role="article"] div[dir="auto"]'
            ]
            
            for selector in content_selectors:
                try:
                    elements = page.locator(selector)
                    count = await elements.count()
                    
                    for i in range(min(count, 20)):
                        try:
                            text = await elements.nth(i).inner_text()
                            
                            # åŸºæœ¬éæ¿¾æ¢ä»¶
                            if not text or len(text.strip()) <= 10:
                                continue
                            if text.strip().isdigit():
                                continue
                            if text.startswith("@"):
                                continue
                            
                            # éæ¿¾ç”¨æˆ¶åï¼ˆé‡è¦ä¿®å¾©ï¼ï¼‰
                            if text.strip() == username:
                                logging.debug(f"   âš ï¸ éæ¿¾ç”¨æˆ¶åæ–‡æœ¬: {text}")
                                continue
                            
                            # éæ¿¾æ™‚é–“ç›¸é—œ
                            if any(time_word in text for time_word in ["å°æ™‚", "åˆ†é˜", "ç§’å‰", "å¤©å‰", "é€±å‰", "å€‹æœˆå‰"]):
                                continue
                            
                            # éæ¿¾ç³»çµ±éŒ¯èª¤å’Œæç¤ºä¿¡æ¯ï¼ˆé‡é»ä¿®å¾©ï¼ï¼‰
                            system_messages = [
                                "Sorry, we're having trouble playing this video",
                                "Learn more",
                                "Something went wrong",
                                "Video unavailable",
                                "This content isn't available",
                                "Unable to load",
                                "Error loading",
                                "æ’­æ”¾ç™¼ç”ŸéŒ¯èª¤",
                                "ç„¡æ³•æ’­æ”¾",
                                "è¼‰å…¥å¤±æ•—",
                                "ç™¼ç”ŸéŒ¯èª¤",
                                "å…§å®¹ç„¡æ³•é¡¯ç¤º"
                            ]
                            
                            # æª¢æŸ¥æ˜¯å¦åŒ…å«ç³»çµ±éŒ¯èª¤ä¿¡æ¯
                            text_lower = text.lower()
                            if any(msg.lower() in text_lower for msg in system_messages):
                                logging.debug(f"   âš ï¸ éæ¿¾ç³»çµ±éŒ¯èª¤ä¿¡æ¯: {text[:50]}...")
                                continue
                            
                            # éæ¿¾æŒ‰éˆ•æ–‡å­—å’Œå°èˆª
                            button_texts = ["follow", "following", "like", "comment", "share", "more", "options"]
                            if any(btn in text_lower for btn in button_texts):
                                continue
                            
                            # éæ¿¾ç´”æ•¸å­—çµ„åˆï¼ˆè®šæ•¸ã€åˆ†äº«æ•¸ç­‰ï¼‰
                            if re.match(r'^[\d,.\s]+$', text.strip()):
                                continue
                                
                            # éæ¿¾éçŸ­çš„å…§å®¹
                            if len(text.strip()) < 5:
                                continue
                            
                            # é€šéæ‰€æœ‰éæ¿¾æ¢ä»¶ï¼Œæ¥å—æ­¤å…§å®¹ä¸¦æ¸…ç†ç¿»è­¯æ¨™è¨˜
                            content = self._clean_content_text(text)
                            logging.debug(f"   âœ… æ‰¾åˆ°æœ‰æ•ˆå…§å®¹: {content[:50]}...")
                            break
                        except:
                            continue
                    
                    if content:
                        break
                except:
                    continue
            
            # å¦‚æœæ²’æœ‰æ‰¾åˆ°æœ‰æ•ˆå…§å®¹ï¼Œå˜—è©¦å…¶ä»–ç­–ç•¥
            if not content:
                logging.debug(f"   ğŸ” ä¸»è¦å…§å®¹æå–å¤±æ•—ï¼Œå˜—è©¦å‚™ç”¨ç­–ç•¥...")
                
                # å‚™ç”¨ç­–ç•¥1ï¼šæŸ¥æ‰¾ aria-label æˆ– title å±¬æ€§
                backup_selectors = [
                    'div[aria-label]',
                    'span[title]',
                    '[data-testid="thread-description"]',
                    'article[aria-label]'
                ]
                
                for backup_selector in backup_selectors:
                    try:
                        elements = page.locator(backup_selector)
                        backup_count = await elements.count()
                        
                        for i in range(min(backup_count, 10)):
                            try:
                                backup_text = await elements.nth(i).get_attribute("aria-label") or await elements.nth(i).get_attribute("title")
                                if backup_text and len(backup_text.strip()) > 5:
                                    # éæ¿¾ç”¨æˆ¶å
                                    if backup_text.strip() == username:
                                        continue
                                    
                                    # åŒæ¨£éæ¿¾ç³»çµ±éŒ¯èª¤ä¿¡æ¯
                                    backup_text_lower = backup_text.lower()
                                    if not any(msg.lower() in backup_text_lower for msg in [
                                        "sorry, we're having trouble playing this video",
                                        "learn more", "something went wrong", "video unavailable"
                                    ]):
                                        content = self._clean_content_text(backup_text)
                                        logging.debug(f"   âœ… å‚™ç”¨ç­–ç•¥æ‰¾åˆ°å…§å®¹: {content[:50]}...")
                                        break
                            except:
                                continue
                        
                        if content:
                            break
                    except:
                        continue
                
                # å¦‚æœä»ç„¶æ²’æœ‰å…§å®¹ï¼Œæ¨™è¨˜ç‚ºå½±ç‰‡è²¼æ–‡
                if not content:
                    logging.debug(f"   ğŸ“¹ å¯èƒ½æ˜¯ç´”å½±ç‰‡è²¼æ–‡ï¼Œç„¡æ–‡å­—å…§å®¹")
                    content = ""  # ä¿æŒç©ºå­—ç¬¦ä¸²è€Œä¸æ˜¯éŒ¯èª¤ä¿¡æ¯
            
            content_data["content"] = content
            
            # èª¿è©¦ä¿¡æ¯ï¼šç¢ºèªå…§å®¹æå–çµæœ
            logging.info(f"   ğŸ“ [DEBUG] å…§å®¹æå–çµæœ: content='{content}', username='{content_data.get('username', 'N/A')}'")
            if content == content_data.get("username"):
                logging.warning(f"   âš ï¸ [DEBUG] è­¦å‘Šï¼šcontent èˆ‡ username ç›¸åŒï¼å¯èƒ½å­˜åœ¨éŒ¯èª¤è³¦å€¼")
            
            # æå–åœ–ç‰‡ - å¢å¼·ç‰ˆï¼ˆå€åˆ†ä¸»è²¼æ–‡ vs å›æ‡‰ï¼‰
            images = []
            main_post_images = []
            
            # ç­–ç•¥1: ç°¡åŒ–çš„ä¸»è²¼æ–‡åœ–ç‰‡æå–ï¼ˆé¿å…è¤‡é›œé¸æ“‡å™¨ï¼‰
            main_post_selectors = [
                'article img',  # æ–‡ç« å…§çš„åœ–ç‰‡
                'main img',     # main æ¨™ç±¤å…§çš„åœ–ç‰‡
                'img[src*="t51.2885-15"]',  # Instagramåœ–ç‰‡æ ¼å¼ï¼ˆç°¡å–®ç›´æ¥ï¼‰
            ]
            
            for selector in main_post_selectors:
                try:
                    main_imgs = page.locator(selector)
                    main_count = await main_imgs.count()
                    logging.debug(f"   ğŸ” é¸æ“‡å™¨ {selector}: æ‰¾åˆ° {main_count} å€‹åœ–ç‰‡")
                    
                    # ç°¡åŒ–é‚è¼¯ï¼šåªæª¢æŸ¥å‰5å€‹åœ–ç‰‡
                    for i in range(min(main_count, 5)):
                        try:
                            img_elem = main_imgs.nth(i)
                            img_src = await img_elem.get_attribute("src")
                            
                            if (img_src and 
                                ("fbcdn" in img_src or "cdninstagram" in img_src) and
                                "rsrc.php" not in img_src and 
                                img_src not in main_post_images):
                                
                                main_post_images.append(img_src)
                                logging.debug(f"   ğŸ–¼ï¸ ä¸»è²¼æ–‡åœ–ç‰‡: {img_src[:50]}...")
                                
                                # é™åˆ¶æ•¸é‡é¿å…éå¤š
                                if len(main_post_images) >= 3:
                                    break
                                    
                        except Exception as e:
                            logging.debug(f"   âš ï¸ åœ–ç‰‡{i}è™•ç†å¤±æ•—: {e}")
                            continue
                            
                    # å¦‚æœæ‰¾åˆ°åœ–ç‰‡å°±åœæ­¢
                    if main_post_images:
                        break
                        
                except Exception as e:
                    logging.debug(f"   âš ï¸ é¸æ“‡å™¨å¤±æ•—: {e}")
                    continue
            
            # ç­–ç•¥2: å¦‚æœä¸»è²¼æ–‡æå–å¤±æ•—ï¼Œç°¡å–®å›é€€
            if not main_post_images:
                logging.debug(f"   ğŸ”„ ä¸»è²¼æ–‡åœ–ç‰‡æå–å¤±æ•—ï¼Œä½¿ç”¨ç°¡å–®å›é€€...")
                img_elements = page.locator('img')
                img_count = await img_elements.count()
                
                # ç°¡å–®æƒæå‰10å€‹åœ–ç‰‡
                for i in range(min(img_count, 10)):
                    try:
                        img_elem = img_elements.nth(i)
                        img_src = await img_elem.get_attribute("src")
                        
                        if (img_src and 
                            ("fbcdn" in img_src or "cdninstagram" in img_src) and
                            "rsrc.php" not in img_src and 
                            img_src not in images):
                            
                            images.append(img_src)
                            
                            # é™åˆ¶æ•¸é‡
                            if len(images) >= 5:
                                break
                                
                    except:
                        continue
            
            # ä½¿ç”¨ä¸»è²¼æ–‡åœ–ç‰‡ï¼ˆå„ªå…ˆï¼‰æˆ–å›é€€åœ–ç‰‡
            final_images = main_post_images if main_post_images else images
            content_data["images"] = final_images
            
            logging.info(f"   ğŸ–¼ï¸ åœ–ç‰‡æå–çµæœ: ä¸»è²¼æ–‡={len(main_post_images)}å€‹, ç¸½è¨ˆ={len(final_images)}å€‹")
            
            # ğŸ¬ å››å±¤å‚™æ´å½±ç‰‡æå–ç³»çµ± - 2025å¹´æ–°ç‰ˆThreadsé©é…
            videos = list(video_urls)
            logging.info(f"   ğŸ¬ å››å±¤å‚™æ´å½±ç‰‡æå–é–‹å§‹...")
            logging.info(f"   ğŸ”¸ ç¬¬1å±¤(GraphQLæ””æˆª): {len(video_urls)}å€‹")
            
            # ç¬¬2å±¤ï¼š__NEXT_DATA__ JSONè§£æ
            next_data_videos = await self._extract_video_from_next_data(page)
            for video_url in next_data_videos:
                if video_url not in videos:
                    videos.append(video_url)
            logging.info(f"   ğŸ”¸ ç¬¬2å±¤(__NEXT_DATA__): {len(next_data_videos)}å€‹")
            
            # ç¬¬3å±¤ï¼šplay()åŠ«æŒ + è‡ªå‹•æ’­æ”¾
            hijacked_video = await self._extract_video_from_hijacked_play(page)
            if hijacked_video and hijacked_video not in videos:
                videos.append(hijacked_video)
            logging.info(f"   ğŸ”¸ ç¬¬3å±¤(play()åŠ«æŒ): {'1' if hijacked_video else '0'}å€‹")
            
            # ç¬¬4å±¤ï¼šå‚³çµ±DOMæå–ï¼ˆå‚™ç”¨ï¼‰
            video_elements = page.locator('video')
            video_count = await video_elements.count()
            logging.info(f"   ğŸ”¸ ç¬¬4å±¤(DOMå‚™ç”¨): {video_count}å€‹videoå…ƒç´ ")
            
            for i in range(video_count):
                try:
                    video_elem = video_elements.nth(i)
                    src = await video_elem.get_attribute("src")
                    data_src = await video_elem.get_attribute("data-src")
                    poster = await video_elem.get_attribute("poster")
                    
                    # é©—è­‰ä¸¦æ·»åŠ æœ‰æ•ˆçš„å½±ç‰‡URL
                    if src and src not in videos:
                        if self._is_valid_video_url(src):
                            videos.append(src)
                            logging.info(f"   ğŸ“¹ DOM video srcå®Œæ•´URL: {src}")
                        else:
                            logging.debug(f"   ğŸš« è·³éç„¡æ•ˆsrc: {src[:60]}...")
                            
                    if data_src and data_src not in videos:
                        if self._is_valid_video_url(data_src):
                            videos.append(data_src)
                            logging.info(f"   ğŸ“¹ DOM video data-srcå®Œæ•´URL: {data_src}")
                        else:
                            logging.debug(f"   ğŸš« è·³éç„¡æ•ˆdata-src: {data_src[:60]}...")
                            
                    # posterå–®ç¨è™•ç†ï¼ˆå§‹çµ‚ä¿ç•™ï¼Œç”¨æ–¼ç¸®åœ–ï¼‰
                    if poster and f"POSTER::{poster}" not in videos:
                        videos.append(f"POSTER::{poster}")
                        logging.debug(f"   ğŸ–¼ï¸ å½±ç‰‡ç¸®åœ–: {poster[:60]}...")
                    
                    # source å­å…ƒç´ 
                    sources = video_elem.locator('source')
                    source_count = await sources.count()
                    for j in range(source_count):
                        source_src = await sources.nth(j).get_attribute("src")
                        if source_src and source_src not in videos:
                            if self._is_valid_video_url(source_src):
                                videos.append(source_src)
                                logging.info(f"   ğŸ“¹ DOM sourceå®Œæ•´URL: {source_src}")
                            else:
                                logging.debug(f"   ğŸš« è·³éç„¡æ•ˆsource: {source_src[:60]}...")
                except Exception as e:
                    logging.debug(f"   âš ï¸ videoå…ƒç´ {i}è™•ç†å¤±æ•—: {e}")
                    continue
            
            # è¨ˆç®—ç¬¬0å±¤ï¼ˆç›´æ¥æ””æˆªï¼‰çš„è²¢ç»
            direct_intercept_count = 0
            for url in video_urls:
                url_clean = url.split("?")[0]
                if url_clean.endswith((".mp4", ".m3u8", ".mpd", ".webm", ".mov")):
                    direct_intercept_count += 1
            
            content_data["videos"] = videos
            logging.info(f"   ğŸ¬ äº”å±¤å‚™æ´å½±ç‰‡æå–å®Œæˆ: ç¸½è¨ˆ={len(videos)}å€‹")
            logging.info(f"   ğŸ“Š å„å±¤æˆæ•ˆçµ±è¨ˆ: ç›´æ¥æ””æˆª={direct_intercept_count} | GraphQL={len(video_urls)-direct_intercept_count} | __NEXT_DATA__={len(next_data_videos)} | play()åŠ«æŒ={'1' if hijacked_video else '0'} | DOM={video_count}")
            
            # èª¿è©¦ï¼šå¦‚æœæ˜¯å½±ç‰‡è²¼æ–‡ä½†æ²’æ‰¾åˆ°å½±ç‰‡URLï¼Œè¨˜éŒ„æ›´å¤šä¿¡æ¯
            if len(videos) == 0:
                logging.warning(f"   âš ï¸ å½±ç‰‡è²¼æ–‡ä½†æœªæ‰¾åˆ°å½±ç‰‡URLï¼")
                logging.debug(f"   ğŸ” é é¢URL: {page.url}")
                logging.debug(f"   ğŸ” ç¶²è·¯æ””æˆªåˆ°çš„URLs: {list(video_urls)}")
                
                # å˜—è©¦æŸ¥æ‰¾å…¶ä»–å¯èƒ½çš„å½±ç‰‡ç·šç´¢
                video_hints = []
                try:
                    # æŸ¥æ‰¾åŒ…å«"video"çš„å…ƒç´ 
                    video_divs = page.locator('div[aria-label*="video"], div[aria-label*="Video"], div[aria-label*="å½±ç‰‡"]')
                    hint_count = await video_divs.count()
                    if hint_count > 0:
                        video_hints.append(f"æ‰¾åˆ°{hint_count}å€‹videoæ¨™ç±¤")
                        
                    # æŸ¥æ‰¾æ’­æ”¾æŒ‰éˆ•
                    play_buttons = page.locator('button[aria-label*="play"], button[aria-label*="Play"], button[aria-label*="æ’­æ”¾"]')
                    play_count = await play_buttons.count()
                    if play_count > 0:
                        video_hints.append(f"æ‰¾åˆ°{play_count}å€‹æ’­æ”¾æŒ‰éˆ•")
                        
                    if video_hints:
                        logging.info(f"   ğŸ’¡ å½±ç‰‡ç·šç´¢: {', '.join(video_hints)}")
                        
                except Exception as e:
                    logging.debug(f"   âš ï¸ å½±ç‰‡ç·šç´¢æŸ¥æ‰¾å¤±æ•—: {e}")
            
            # â† æ–°å¢: æå–çœŸå¯¦ç™¼æ–‡æ™‚é–“
            try:
                post_published_at = await self._extract_post_published_at(page)
                if post_published_at:
                    content_data["post_published_at"] = post_published_at
                    logging.info(f"   ğŸ“… æå–ç™¼æ–‡æ™‚é–“: {post_published_at}")
                else:
                    logging.warning(f"   ğŸ“… æœªæ‰¾åˆ°ç™¼æ–‡æ™‚é–“")
            except Exception as e:
                logging.warning(f"   âš ï¸ ç™¼æ–‡æ™‚é–“æå–å¤±æ•—: {e}")
            
            # â† æ–°å¢: æå–ä¸»é¡Œæ¨™ç±¤
            try:
                tags = await self._extract_tags_from_dom(page)
                if tags:
                    content_data["tags"] = tags
                    logging.debug(f"   âœ… æå–æ¨™ç±¤: {tags}")
            except Exception as e:
                logging.debug(f"   âš ï¸ æ¨™ç±¤æå–å¤±æ•—: {e}")
            
        except Exception as e:
            logging.debug(f"   âš ï¸ DOM å…§å®¹æå–å¤±æ•—: {e}")
        
        return content_data
    
    async def _extract_counts_from_dom_fallback(self, page: Page) -> dict:
        """DOM è¨ˆæ•¸å¾Œæ´æå–"""
        logging.warning(f"   ğŸ”„ GraphQL è¨ˆæ•¸æ””æˆªå¤±æ•—ï¼Œé–‹å§‹ DOM è¨ˆæ•¸å¾Œæ´...")
        
        # å…ˆæª¢æŸ¥é é¢ç‹€æ…‹
        page_title = await page.title()
        page_url = page.url
        logging.info(f"   ğŸ“„ é é¢ç‹€æ…‹ - æ¨™é¡Œ: {page_title}, URL: {page_url}")
        
        count_selectors = {
            "likes": [
                # NEW: åŸºæ–¼é é¢åˆ†æçš„æœ€æ–°é¸æ“‡å™¨
                "svg[aria-label='è®š'] ~ span",
                "svg[aria-label='è®š'] + span", 
                "svg[aria-label='è®š']",
                "span.x1o0tod.x10l6tqk.x13vifvy",  # å¾åˆ†æä¸­ç™¼ç¾çš„åŒ…å«æ•¸å­—çš„span
                "button:has(svg[aria-label='è®š']) span",
                # é€šç”¨æ•¸å­—é¸æ“‡å™¨ï¼ˆä¾†è‡ªåˆ†æï¼‰
                "span:has-text('è¬') span",
                "span:has-text('k') span",
                # English selectors (ä¿ç•™åŸæœ‰çš„)
                "button[aria-label*='likes'] span",
                "button[aria-label*='Like'] span", 
                "span:has-text(' likes')",
                "span:has-text(' like')",
                "button svg[aria-label='Like'] + span",
                "button[aria-label*='like']",
                # Chinese selectors (ä¿ç•™åŸæœ‰çš„)
                "button[aria-label*='å€‹å–œæ­¡'] span",
                "button[aria-label*='å–œæ­¡']",
                # Generic patterns (ä¿ç•™åŸæœ‰çš„)
                "button[data-testid*='like'] span",
                "div[role='button'][aria-label*='like'] span"
            ],
            "comments": [
                # NEW: åŸºæ–¼é é¢åˆ†æçš„æœ€æ–°é¸æ“‡å™¨
                "svg[aria-label='ç•™è¨€'] ~ span",
                "svg[aria-label='ç•™è¨€'] + span",
                "svg[aria-label='ç•™è¨€']",
                "svg[aria-label='comment'] ~ span",
                "svg[aria-label='comment'] + span", 
                "button:has(svg[aria-label='ç•™è¨€']) span",
                "button:has(svg[aria-label='comment']) span",
                # English selectors (ä¿ç•™åŸæœ‰çš„)
                "a[href$='#comments'] span",
                "span:has-text(' comments')",
                "span:has-text(' comment')",
                "a:has-text('comments')",
                "button[aria-label*='comment'] span",
                # Chinese selectors (ä¿ç•™åŸæœ‰çš„)
                "span:has-text(' å‰‡ç•™è¨€')",
                "a:has-text('å‰‡ç•™è¨€')",
                # Generic patterns (ä¿ç•™åŸæœ‰çš„)
                "button[data-testid*='comment'] span",
                "div[role='button'][aria-label*='comment'] span"
            ],
            "reposts": [
                # NEW: åŸºæ–¼é é¢åˆ†æçš„æœ€æ–°é¸æ“‡å™¨
                "svg[aria-label='è½‰ç™¼'] ~ span",
                "svg[aria-label='è½‰ç™¼'] + span",
                "svg[aria-label='è½‰ç™¼']",
                "button:has(svg[aria-label='è½‰ç™¼']) span",
                "div.x1i10hfl.x1qjc9v5.xjbqb8w span",  # å¾åˆ†æä¸­ç™¼ç¾çš„è½‰ç™¼æŒ‰éˆ•
                # English selectors (ä¿ç•™åŸæœ‰çš„)
                "span:has-text(' reposts')",
                "span:has-text(' repost')",
                "button[aria-label*='repost'] span",
                "a:has-text('reposts')",
                # Chinese selectors (ä¿ç•™åŸæœ‰çš„)
                "span:has-text(' æ¬¡è½‰ç™¼')",
                "a:has-text('è½‰ç™¼')",
                # Generic patterns (ä¿ç•™åŸæœ‰çš„)
                "button[data-testid*='repost'] span"
            ],
            "shares": [
                # NEW: åŸºæ–¼é é¢åˆ†æçš„æœ€æ–°é¸æ“‡å™¨
                "svg[aria-label='åˆ†äº«'] ~ span",
                "svg[aria-label='åˆ†äº«'] + span",
                "svg[aria-label='åˆ†äº«']",
                "svg[aria-label='è²¼æ–‡å·²åˆ†äº«åˆ°è¯é‚¦å®‡å®™'] ~ span",
                "svg[aria-label='è²¼æ–‡å·²åˆ†äº«åˆ°è¯é‚¦å®‡å®™'] + span",
                "button:has(svg[aria-label='åˆ†äº«']) span",
                "div.x1i10hfl.x1qjc9v5.xjbqb8w span",  # å…±ç”¨çš„æŒ‰éˆ•å®¹å™¨é¡
                # English selectors (ä¿ç•™åŸæœ‰çš„)
                "span:has-text(' shares')",
                "span:has-text(' share')",
                "button[aria-label*='share'] span",
                "a:has-text('shares')",
                # Chinese selectors (ä¿ç•™åŸæœ‰çš„)
                "span:has-text(' æ¬¡åˆ†äº«')",
                "a:has-text('åˆ†äº«')",
                # Generic patterns (ä¿ç•™åŸæœ‰çš„)
                "button[data-testid*='share'] span"
            ],
        }
        
        # å…ˆé€²è¡Œé€šç”¨å…ƒç´ æƒæï¼Œä¸¦æ™ºèƒ½æå–æ•¸å­—
        logging.info(f"   ğŸ” é€šç”¨å…ƒç´ æƒæå’Œæ™ºèƒ½æ•¸å­—æå–...")
        dom_counts = {}
        
        try:
            all_buttons = await page.locator('button').all_inner_texts()
            all_spans = await page.locator('span').all_inner_texts()
            number_elements = [text for text in (all_buttons + all_spans) if any(char.isdigit() for char in text)]
            logging.info(f"   ğŸ”¢ æ‰¾åˆ°åŒ…å«æ•¸å­—çš„å…ƒç´ : {number_elements[:20]}")
            
            # === ğŸ¯ æ™ºèƒ½æ•¸å­—è­˜åˆ¥ï¼šå¾æ‰¾åˆ°çš„æ•¸å­—ä¸­æå–ç¤¾äº¤æ•¸æ“š ===
            pure_numbers = []
            combo_found = False
            
            # ğŸ¯ å„ªå…ˆæª¢æŸ¥çµ„åˆæ•¸å­—æ ¼å¼ (ä¾‹å¦‚: "1,230\n31\n53\n68")
            for text in number_elements:
                if '\n' in text and text.count('\n') >= 2:
                    numbers = []
                    for line in text.split('\n'):
                        line_num = parse_number(line.strip())
                        if line_num and line_num > 0:
                            numbers.append(line_num)
                    
                    if len(numbers) >= 3:  # è‡³å°‘3å€‹æ•¸å­—æ‰èªç‚ºæ˜¯çµ„åˆæ ¼å¼
                        logging.info(f"   ğŸ¯ ç™¼ç¾çµ„åˆæ•¸å­—æ ¼å¼: {numbers} (å¾ '{text}')")
                        # é€šå¸¸é †åºï¼šæŒ‰è®š, ç•™è¨€, è½‰ç™¼, åˆ†äº«
                        if len(numbers) >= 1:
                            dom_counts["likes"] = numbers[0]
                            logging.info(f"   â¤ï¸ æŒ‰è®šæ•¸: {numbers[0]}")
                        if len(numbers) >= 2:
                            dom_counts["comments"] = numbers[1] 
                            logging.info(f"   ğŸ’¬ ç•™è¨€æ•¸: {numbers[1]}")
                        if len(numbers) >= 3:
                            dom_counts["reposts"] = numbers[2]
                            logging.info(f"   ğŸ”„ è½‰ç™¼æ•¸: {numbers[2]}")
                        if len(numbers) >= 4:
                            dom_counts["shares"] = numbers[3]
                            logging.info(f"   ğŸ“¤ åˆ†äº«æ•¸: {numbers[3]}")
                        combo_found = True
                        break
            
            # å¦‚æœæ²’æ‰¾åˆ°çµ„åˆæ ¼å¼ï¼Œä½¿ç”¨å‚³çµ±æ–¹æ³•
            if not combo_found:
                for text in number_elements:
                    # è·³éæ˜é¡¯ä¸æ˜¯äº’å‹•æ•¸æ“šçš„æ–‡å­—ï¼ˆä½†ä¸è·³éç€è¦½æ•¸ï¼‰
                    if any(skip in text for skip in ['å¤©', 'å°æ™‚', 'åˆ†é˜', 'ç§’', 'on.natgeo.com', 'px', 'ms', '%']):
                        continue
                        
                    # ç‰¹æ®Šè™•ç†ï¼šç€è¦½æ•¸å¯èƒ½åŒ…å«æŒ‰è®šæ•¸ç­‰ä¿¡æ¯
                    if 'ç€è¦½' in text or 'æ¬¡ç€è¦½' in text:
                        # å¦‚æœæ˜¯ç€è¦½æ•¸ä½†åŒ…å«æœ‰æ•ˆæ•¸å­—ï¼Œä¹Ÿæå–ï¼ˆå¯èƒ½æ˜¯æŒ‰è®šæ•¸ï¼‰
                        number = parse_number(text)
                        if number and number > 0:
                            pure_numbers.append((number, text))
                            logging.info(f"   ğŸ“Š æå–ç€è¦½æ•¸å­—: {number} (å¾ '{text}')")
                    else:
                        number = parse_number(text)
                        if number and number > 0:
                            pure_numbers.append((number, text))
                            logging.info(f"   ğŸ“Š æå–æ•¸å­—: {number} (å¾ '{text}')")
                
                # æ ¹æ“šæ•¸å­—å¤§å°æ™ºèƒ½åˆ†é…ï¼ˆé€šå¸¸ï¼šlikes > comments > reposts > sharesï¼‰
                pure_numbers.sort(reverse=True)  # å¾å¤§åˆ°å°æ’åº
                
                if len(pure_numbers) >= 4:
                    dom_counts["likes"] = pure_numbers[0][0]
                    dom_counts["comments"] = pure_numbers[1][0] 
                    dom_counts["reposts"] = pure_numbers[2][0]
                    dom_counts["shares"] = pure_numbers[3][0]
                    logging.info(f"   ğŸ¯ æ™ºèƒ½åˆ†é…4å€‹æ•¸å­—: è®š={dom_counts['likes']}, ç•™è¨€={dom_counts['comments']}, è½‰ç™¼={dom_counts['reposts']}, åˆ†äº«={dom_counts['shares']}")
                elif len(pure_numbers) >= 2:
                    dom_counts["likes"] = pure_numbers[0][0]
                    dom_counts["comments"] = pure_numbers[1][0]
                    logging.info(f"   ğŸ¯ æ™ºèƒ½åˆ†é…2å€‹æ•¸å­—: è®š={dom_counts['likes']}, ç•™è¨€={dom_counts['comments']}")
                elif len(pure_numbers) >= 1:
                    dom_counts["likes"] = pure_numbers[0][0]
                    logging.info(f"   ğŸ¯ æ™ºèƒ½åˆ†é…1å€‹æ•¸å­—: è®š={dom_counts['likes']}")
                
        except Exception as e:
            logging.warning(f"   âš ï¸ æ™ºèƒ½æ•¸å­—æå–å¤±æ•—: {e}")
        
        # å¦‚æœæ™ºèƒ½æå–æˆåŠŸï¼Œè·³éå‚³çµ±é¸æ“‡å™¨ï¼›å¦å‰‡ç¹¼çºŒå˜—è©¦
        if not dom_counts:
            logging.info(f"   âš ï¸ æ™ºèƒ½æå–å¤±æ•—ï¼Œå›åˆ°å‚³çµ±é¸æ“‡å™¨...")
            for key, sels in count_selectors.items():
                logging.info(f"   ğŸ” å˜—è©¦æå– {key} æ•¸æ“š...")
                for i, sel in enumerate(sels):
                    try:
                        el = page.locator(sel).first
                        count = await el.count()
                        if count > 0:
                            text = (await el.inner_text()).strip()
                            logging.info(f"   ğŸ“ é¸æ“‡å™¨ {i+1}/{len(sels)} '{sel}' æ‰¾åˆ°æ–‡å­—: '{text}'")
                            n = parse_number(text)
                            if n and n > 0:
                                dom_counts[key] = n
                                logging.info(f"   âœ… DOM æˆåŠŸæå– {key}: {n} (é¸æ“‡å™¨: {sel})")
                                break
                            else:
                                logging.info(f"   âš ï¸ ç„¡æ³•è§£ææ•¸å­—: '{text}' -> {n}")
                        else:
                            logging.info(f"   âŒ é¸æ“‡å™¨ {i+1}/{len(sels)} æœªæ‰¾åˆ°å…ƒç´ : '{sel}'")
                    except Exception as e:
                        logging.info(f"   âš ï¸ é¸æ“‡å™¨ {i+1}/{len(sels)} '{sel}' éŒ¯èª¤: {e}")
                        continue
                
                if key not in dom_counts:
                    logging.warning(f"   âŒ ç„¡æ³•æ‰¾åˆ° {key} æ•¸æ“š")
        
        if dom_counts:
            counts_data = {
                "likes": dom_counts.get("likes", 0),
                "comments": dom_counts.get("comments", 0),
                "reposts": dom_counts.get("reposts", 0),
                "shares": dom_counts.get("shares", 0),
            }
            logging.info(f"   ğŸ¯ DOM è¨ˆæ•¸å¾Œæ´æˆåŠŸ: {counts_data}")
            return counts_data
        else:
            # æ‰€æœ‰æ–¹æ³•éƒ½å¤±æ•—æ™‚ï¼Œè¨˜éŒ„é é¢ç‹€æ…‹ç”¨æ–¼èª¿è©¦
            await self._debug_failed_page(page)
            return {}
    
    async def _debug_failed_page(self, page: Page):
        """èª¿è©¦å¤±æ•—é é¢çš„ç‹€æ…‹"""
        logging.warning(f"   âŒ GraphQLæ””æˆªå’ŒDOMå¾Œæ´éƒ½å¤±æ•—äº†ï¼")
        try:
            page_title = await page.title()
            page_url = page.url
            logging.info(f"   ğŸ“„ å¤±æ•—é é¢åˆ†æ - æ¨™é¡Œ: {page_title}")
            logging.info(f"   ğŸ”— å¤±æ•—é é¢åˆ†æ - URL: {page_url}")
            
            # æª¢æŸ¥é é¢æ˜¯å¦æ­£å¸¸è¼‰å…¥
            all_text = await page.inner_text('body')
            if "ç™»å…¥" in all_text or "login" in all_text.lower():
                logging.warning(f"   âš ï¸ å¯èƒ½é‡åˆ°ç™»å…¥é é¢")
            elif len(all_text) < 100:
                logging.warning(f"   âš ï¸ é é¢å…§å®¹å¤ªå°‘ï¼Œå¯èƒ½è¼‰å…¥å¤±æ•—")
            else:
                logging.info(f"   ğŸ“ é é¢å…§å®¹é•·åº¦: {len(all_text)} å­—å…ƒ")
                
                # æª¢æŸ¥æ˜¯å¦æœ‰äº’å‹•æŒ‰éˆ•
                like_buttons = await page.locator('[aria-label*="like"], [aria-label*="Like"], [aria-label*="å–œæ­¡"]').count()
                comment_buttons = await page.locator('[aria-label*="comment"], [aria-label*="Comment"], [aria-label*="ç•™è¨€"]').count()
                logging.info(f"   ğŸ“Š æ‰¾åˆ°æŒ‰éˆ•: è®š {like_buttons} å€‹, ç•™è¨€ {comment_buttons} å€‹")
                
                # å˜—è©¦æ‰¾åˆ°ä»»ä½•æ•¸å­—
                all_numbers = await page.locator(':text-matches("\\d+")').all_inner_texts()
                if all_numbers:
                    logging.info(f"   ğŸ”¢ é é¢æ‰€æœ‰æ•¸å­—: {all_numbers[:15]}")  # é¡¯ç¤ºå‰15å€‹
                
                # æª¢æŸ¥æ˜¯å¦æœ‰é˜»æ“‹å…ƒç´ 
                modal_count = await page.locator('[role="dialog"], .modal, [data-testid*="modal"]').count()
                if modal_count > 0:
                    logging.warning(f"   âš ï¸ ç™¼ç¾ {modal_count} å€‹æ¨¡æ…‹æ¡†å¯èƒ½é˜»æ“‹å…§å®¹")
                    
        except Exception as debug_e:
            logging.warning(f"   âš ï¸ å¤±æ•—é é¢åˆ†æéŒ¯èª¤: {debug_e}")
    
    async def _update_post_data(self, post: PostMetrics, counts_data: dict, content_data: dict, task_id: str, username: str) -> bool:
        """æ›´æ–°è²¼æ–‡æ•¸æ“š"""
        updated = False
        
        # æ›´æ–°è¨ˆæ•¸æ•¸æ“š - åªåœ¨ç¾æœ‰æ•¸æ“šç‚º None æˆ– 0 æ™‚æ‰æ›´æ–°
        if counts_data:
            if post.likes_count in (None, 0) and (counts_data.get("likes") or 0) > 0:
                post.likes_count = counts_data["likes"]
                updated = True
            if post.comments_count in (None, 0) and (counts_data.get("comments") or 0) > 0:
                post.comments_count = counts_data["comments"]
                updated = True
            if post.reposts_count in (None, 0) and (counts_data.get("reposts") or 0) > 0:
                post.reposts_count = counts_data["reposts"]
                updated = True
            if post.shares_count in (None, 0) and (counts_data.get("shares") or 0) > 0:
                post.shares_count = counts_data["shares"]
                updated = True
            # æ–°å¢ï¼šæ›´æ–°ç€è¦½æ•¸
            if post.views_count in (None, 0) and (counts_data.get("views_count") or 0) > 0:
                post.views_count = counts_data["views_count"]
                updated = True
        
        # æ›´æ–°å…§å®¹æ•¸æ“š - åªåœ¨ç¾æœ‰æ•¸æ“šç‚ºç©ºæ™‚æ‰æ›´æ–°
        if content_data.get("content") and not post.content:
            logging.info(f"   ğŸ“ [DEBUG] æ›´æ–° post.content: å¾ '{post.content}' â†’ '{content_data['content']}'")
            post.content = content_data["content"]
            updated = True
        
        if content_data.get("images") and not post.images:
            post.images = content_data["images"]
            updated = True
        
        if content_data.get("videos") and not post.videos:
            # éæ¿¾å¯¦éš›å½±ç‰‡ï¼ˆæ’é™¤ POSTERï¼‰
            actual_videos = [v for v in content_data["videos"] if not v.startswith("POSTER::")]
            if actual_videos:
                post.videos = actual_videos
                updated = True
        
        # â† æ–°å¢: æ›´æ–°çœŸå¯¦ç™¼æ–‡æ™‚é–“
        if content_data.get("post_published_at") and not post.post_published_at:
            post.post_published_at = content_data["post_published_at"]
            updated = True
        
        # â† æ–°å¢: æ›´æ–°ä¸»é¡Œæ¨™ç±¤
        if content_data.get("tags") and not post.tags:
            post.tags = content_data["tags"]
            updated = True
        
        if updated:
            post.processing_stage = "details_filled_hybrid"
            
            # è¨ˆç®—åˆ†æ•¸ (åŸºæ–¼æ‰€æœ‰äº’å‹•æ•¸æ“š)
            calculated_score = post.calculate_score()
            post.calculated_score = calculated_score  # å­˜å„²è¨ˆç®—åˆ†æ•¸
            
            # æ§‹å»ºè£œé½Šä¿¡æ¯
            info_parts = [
                f"è®š={post.likes_count}",
                f"å…§å®¹={len(post.content)}å­—",
                f"åœ–ç‰‡={len(post.images)}å€‹",
                f"å½±ç‰‡={len(post.videos)}å€‹"
            ]
            
            # å¦‚æœæœ‰ç€è¦½æ•¸ï¼Œæ·»åŠ åˆ°ä¿¡æ¯ä¸­
            if post.views_count:
                info_parts.insert(1, f"ç€è¦½={post.views_count}")
                
            # æ·»åŠ è¨ˆç®—åˆ†æ•¸åˆ°ä¿¡æ¯ä¸­
            info_parts.append(f"åˆ†æ•¸={calculated_score:.1f}")
            
            if post.post_published_at:
                info_parts.append(f"ç™¼æ–‡æ™‚é–“={post.post_published_at.strftime('%Y-%m-%d %H:%M')}")
            if post.tags:
                info_parts.append(f"æ¨™ç±¤={post.tags}")
                
            logging.info(f"  âœ… æ··åˆç­–ç•¥æˆåŠŸè£œé½Š {post.post_id}: {', '.join(info_parts)}")
            
            # ç™¼å¸ƒé€²åº¦
            if task_id:
                await publish_progress(
                    task_id, 
                    "details_fetched_hybrid",
                    username=content_data.get("username", username or "unknown"),
                    post_id=post.post_id,
                    likes_count=post.likes_count,
                    content_length=len(post.content),
                    media_count=len(post.images) + len(post.videos)
                )
        else:
            post.processing_stage = "details_failed"
            logging.warning(f"  âš ï¸ æ··åˆç­–ç•¥ç„¡æ³•è£œé½Š {post.post_id} çš„æ•¸æ“š")
        
        return updated
    
    async def _extract_post_published_at(self, page: Page) -> Optional[Any]:
        """æå–è²¼æ–‡çœŸå¯¦ç™¼å¸ƒæ™‚é–“ (å¾DOM)"""
        from datetime import datetime
        import json
        import logging
        
        try:
            logging.info(f"   ğŸ•’ [DEBUG] é–‹å§‹æ™‚é–“æå–...")
            
            # æ–¹æ³•A: ç›´æ¥æŠ“ <time> çš„ datetime å±¬æ€§
            time_elements = page.locator('time[datetime]')
            count = await time_elements.count()
            logging.info(f"   ğŸ•’ [DEBUG] æ‰¾åˆ° {count} å€‹timeå…ƒç´ ")
            
            if count > 0:
                for i in range(min(count, 5)):  # æª¢æŸ¥å‰5å€‹
                    try:
                        time_el = time_elements.nth(i)
                        
                        # datetime å±¬æ€§
                        iso_time = await time_el.get_attribute('datetime')
                        logging.info(f"   ğŸ•’ [DEBUG] time[{i}] datetimeå±¬æ€§: {iso_time}")
                        if iso_time:
                            from dateutil import parser
                            parsed_time = parser.parse(iso_time)
                            
                            # ç«‹å³è½‰æ›ç‚ºå°åŒ—æ™‚é–“
                            from datetime import timezone, timedelta
                            taipei_tz = timezone(timedelta(hours=8))
                            taipei_time = parsed_time.astimezone(taipei_tz).replace(tzinfo=None)
                            
                            logging.info(f"   ğŸ“… [DEBUG] è§£ææˆåŠŸæ™‚é–“: {parsed_time} â†’ å°åŒ—æ™‚é–“: {taipei_time}")
                            return taipei_time
                        
                        # title æˆ– aria-label å±¬æ€§  
                        title_time = (await time_el.get_attribute('title') or 
                                    await time_el.get_attribute('aria-label'))
                        logging.info(f"   ğŸ•’ [DEBUG] time[{i}] title/aria-label: {title_time}")
                        if title_time:
                            parsed_time = self._parse_chinese_time(title_time)
                            if parsed_time:
                                # ç«‹å³è½‰æ›ç‚ºå°åŒ—æ™‚é–“ï¼ˆä¸­æ–‡æ™‚é–“é€šå¸¸å·²ç¶“æ˜¯å°åŒ—æ™‚é–“ï¼‰
                                from datetime import timezone, timedelta
                                taipei_tz = timezone(timedelta(hours=8))
                                if parsed_time.tzinfo is None:
                                    # å‡è¨­ç„¡æ™‚å€ä¿¡æ¯çš„æ˜¯å°åŒ—æ™‚é–“
                                    taipei_time = parsed_time
                                else:
                                    taipei_time = parsed_time.astimezone(taipei_tz).replace(tzinfo=None)
                                
                                logging.info(f"   ğŸ“… [DEBUG] ä¸­æ–‡æ™‚é–“è§£ææˆåŠŸ: {parsed_time} â†’ å°åŒ—æ™‚é–“: {taipei_time}")
                                return taipei_time
                    except Exception as e:
                        logging.info(f"   ğŸ•’ [DEBUG] time[{i}] è§£æå¤±æ•—: {e}")
                        continue
            
            # æ–¹æ³•B: è§£æ __NEXT_DATA__
            logging.info(f"   ğŸ•’ [DEBUG] å˜—è©¦__NEXT_DATA__æ–¹æ³•...")
            try:
                script_el = page.locator('#__NEXT_DATA__')
                count = await script_el.count()
                logging.info(f"   ğŸ•’ [DEBUG] æ‰¾åˆ° {count} å€‹__NEXT_DATA__å…ƒç´ ")
                if count > 0:
                    script_content = await script_el.text_content()
                    if script_content:
                        data = json.loads(script_content)
                        logging.info(f"   ğŸ•’ [DEBUG] __NEXT_DATA__è§£ææˆåŠŸï¼Œé–‹å§‹æŸ¥æ‰¾taken_at...")
                        
                        taken_at = self._find_taken_at(data)
                        if taken_at:
                            result_time = datetime.fromtimestamp(taken_at)
                            
                            # ç«‹å³è½‰æ›ç‚ºå°åŒ—æ™‚é–“
                            from datetime import timezone, timedelta
                            taipei_tz = timezone(timedelta(hours=8))
                            # æ™‚é–“æˆ³é€šå¸¸æ˜¯UTCï¼Œè½‰æ›ç‚ºå°åŒ—æ™‚é–“
                            utc_time = result_time.replace(tzinfo=timezone.utc)
                            taipei_time = utc_time.astimezone(taipei_tz).replace(tzinfo=None)
                            
                            logging.info(f"   ğŸ“… [DEBUG] __NEXT_DATA__æ™‚é–“è§£ææˆåŠŸ: {result_time} â†’ å°åŒ—æ™‚é–“: {taipei_time}")
                            return taipei_time
                        else:
                            logging.info(f"   ğŸ•’ [DEBUG] åœ¨__NEXT_DATA__ä¸­æœªæ‰¾åˆ°taken_at")
                    else:
                        logging.info(f"   ğŸ•’ [DEBUG] __NEXT_DATA__å…§å®¹ç‚ºç©º")
                        
            except Exception as e:
                logging.info(f"   ğŸ•’ [DEBUG] __NEXT_DATA__è§£æå¤±æ•—: {e}")
                pass
            
        except Exception as e:
            logging.info(f"   ğŸ•’ [DEBUG] æ™‚é–“æå–ç¸½é«”å¤±æ•—: {e}")
            pass
        
        logging.info(f"   ğŸ•’ [DEBUG] æ‰€æœ‰æ™‚é–“æå–æ–¹æ³•éƒ½å¤±æ•—äº†")
        return None
    
    def _parse_chinese_time(self, time_str: str) -> Optional[Any]:
        """è§£æä¸­æ–‡æ™‚é–“æ ¼å¼"""
        from datetime import datetime
        try:
            # è™•ç† "2025å¹´8æœˆ3æ—¥ä¸‹åˆ 2:36" æ ¼å¼
            if "å¹´" in time_str and "æœˆ" in time_str and "æ—¥" in time_str:
                import re
                match = re.search(r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥.*?(\d{1,2}):(\d{2})', time_str)
                if match:
                    year, month, day, hour, minute = map(int, match.groups())
                    
                    # è™•ç†ä¸‹åˆ/ä¸Šåˆ
                    if "ä¸‹åˆ" in time_str and hour < 12:
                        hour += 12
                    elif "ä¸Šåˆ" in time_str and hour == 12:
                        hour = 0
                    
                    return datetime(year, month, day, hour, minute)
        except:
            pass
        return None
    
    def _find_taken_at(self, data: Any, path: str = "") -> Optional[int]:
        """éæ­¸æœç´¢ taken_at æ™‚é–“æˆ³"""
        if isinstance(data, dict):
            for key, value in data.items():
                if key == "taken_at" and isinstance(value, int) and value > 1000000000:
                    return value
                result = self._find_taken_at(value, f"{path}.{key}")
                if result:
                    return result
        elif isinstance(data, list):
            for i, item in enumerate(data):
                result = self._find_taken_at(item, f"{path}[{i}]")
                if result:
                    return result
        return None
    
    async def _extract_tags_from_dom(self, page: Page) -> List[str]:
        """æå–ä¸»é¡Œæ¨™ç±¤ (å°ˆé–€æœç´¢Threadsæ¨™ç±¤é€£çµ)"""
        tags = []
        
        try:
            # ç­–ç•¥1: æœç´¢æ¨™ç±¤é€£çµï¼ˆå„ªå…ˆç´šæœ€é«˜ï¼‰
            tag_link_selectors = [
                'a[href*="/search?q="][href*="serp_type=tags"]',  # æ¨™ç±¤æœç´¢é€£çµ
                'a[href*="/search"][href*="tag_id="]',  # åŒ…å«tag_idçš„é€£çµ
                'a[href*="serp_type=tags"]',  # æ¨™ç±¤é¡å‹é€£çµ
            ]
            
            for selector in tag_link_selectors:
                try:
                    tag_links = page.locator(selector)
                    count = await tag_links.count()
                    
                    if count > 0:
                        # åªæª¢æŸ¥å‰3å€‹ï¼ˆé¿å…å›å¾©ä¸­çš„æ¨™ç±¤ï¼‰
                        for i in range(min(count, 3)):
                            try:
                                link = tag_links.nth(i)
                                href = await link.get_attribute('href')
                                text = await link.inner_text()
                                
                                if href and text:
                                    tag_name = self._extract_tag_name_from_link(href, text)
                                    if tag_name and tag_name not in tags:
                                        tags.append(tag_name)
                                        return tags  # æ‰¾åˆ°ä¸€å€‹å°±è¿”å›
                                        
                            except Exception:
                                continue
                                
                except Exception:
                    continue
            
            # ç­–ç•¥2: æœç´¢ä¸»æ–‡ç« å€åŸŸå…§çš„æ¨™ç±¤å…ƒç´ 
            main_post_selectors = [
                'article:first-of-type',
                '[role="article"]:first-of-type',
                'div[data-pressable-container]:first-of-type',
            ]
            
            for main_selector in main_post_selectors:
                try:
                    main_element = page.locator(main_selector)
                    if await main_element.count() > 0:
                        # åœ¨ä¸»æ–‡ç« å…§æœç´¢æ¨™ç±¤é€£çµ
                        main_tag_links = main_element.locator('a[href*="/search"]')
                        main_count = await main_tag_links.count()
                        
                        if main_count > 0:
                            for i in range(min(main_count, 2)):
                                try:
                                    link = main_tag_links.nth(i)
                                    href = await link.get_attribute('href')
                                    text = await link.inner_text()
                                    
                                    if href and text:
                                        tag_name = self._extract_tag_name_from_link(href, text)
                                        if tag_name and tag_name not in tags:
                                            tags.append(tag_name)
                                            return tags
                                            
                                except Exception:
                                    continue
                        
                except Exception:
                    continue
            
        except Exception:
            pass
        
        return tags[:1] if tags else []  # åªè¿”å›ç¬¬ä¸€å€‹æ¨™ç±¤
    
    def _extract_tag_name_from_link(self, href: str, text: str) -> Optional[str]:
        """å¾æ¨™ç±¤é€£çµä¸­æå–æ¨™ç±¤åç¨±"""
        try:
            # å¾URLçš„qåƒæ•¸ä¸­è§£æ
            if "q=" in href:
                import urllib.parse
                parsed_url = urllib.parse.urlparse(href)
                query_params = urllib.parse.parse_qs(parsed_url.query)
                
                if 'q' in query_params:
                    tag_name = query_params['q'][0]
                    tag_name = urllib.parse.unquote(tag_name)
                    return tag_name
            
            # å¾é€£çµæ–‡æœ¬ä¸­å–å¾—ï¼ˆå‚™ç”¨ï¼‰
            if text and len(text.strip()) > 0 and len(text.strip()) <= 20:
                clean_text = text.strip()
                if clean_text.startswith('#'):
                    clean_text = clean_text[1:]
                return clean_text
                
        except Exception:
            pass
        
        return None