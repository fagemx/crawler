"""
è©³ç´°æ•¸æ“šæå–å™¨

è² è²¬ä½¿ç”¨æ··åˆç­–ç•¥è£œé½Šè²¼æ–‡è©³ç´°æ•¸æ“šï¼š
1. GraphQL è¨ˆæ•¸æŸ¥è©¢ç²å–æº–ç¢ºçš„æ•¸å­—æ•¸æ“š (likes, commentsç­‰)
2. DOM è§£æç²å–å®Œæ•´çš„å…§å®¹å’Œåª’é«” (content, images, videos)
"""

import asyncio
import logging
import random
import re
from datetime import datetime
from typing import Dict, List, Optional, Any, Set
from playwright.async_api import BrowserContext, Page

from common.models import PostMetrics
from common.nats_client import publish_progress
from ..parsers.number_parser import parse_number


class DetailsExtractor:
    """
    è©³ç´°æ•¸æ“šæå–å™¨ - ä½¿ç”¨æ··åˆç­–ç•¥æå–å®Œæ•´çš„è²¼æ–‡æ•¸æ“š
    """
    
    def __init__(self):
        pass
    
    async def fill_post_details_from_page(self, posts_to_fill: List[PostMetrics], context: BrowserContext, task_id: str = None, username: str = None) -> List[PostMetrics]:
        """
        ä½¿ç”¨æ··åˆç­–ç•¥è£œé½Šè²¼æ–‡è©³ç´°æ•¸æ“šï¼š
        1. GraphQL è¨ˆæ•¸æŸ¥è©¢ç²å–æº–ç¢ºçš„æ•¸å­—æ•¸æ“š (likes, commentsç­‰)
        2. DOM è§£æç²å–å®Œæ•´çš„å…§å®¹å’Œåª’é«” (content, images, videos)
        é€™ç¨®æ–¹æ³•çµåˆäº†å…©ç¨®æŠ€è¡“çš„å„ªå‹¢ï¼Œæä¾›æœ€ç©©å®šå¯é çš„æ•¸æ“šæå–ã€‚
        """
        if not context:
            logging.error("âŒ Browser context æœªåˆå§‹åŒ–ï¼Œç„¡æ³•åŸ·è¡Œ fill_post_details_from_pageã€‚")
            return posts_to_fill

        # æ¸›å°‘ä¸¦ç™¼æ•¸ä»¥é¿å…è§¸ç™¼åçˆ¬èŸ²æ©Ÿåˆ¶
        semaphore = asyncio.Semaphore(1)  # æ›´ä¿å®ˆçš„ä¸¦ç™¼æ•¸
        
        async def fetch_single_details_hybrid(post: PostMetrics):
            async with semaphore:
                page = None
                try:
                    page = await context.new_page()
                    
                    logging.debug(f"ğŸ“„ ä½¿ç”¨æ··åˆç­–ç•¥è£œé½Šè©³ç´°æ•¸æ“š: {post.url}")
                    
                    # === æ­¥é©Ÿ 1: æ··åˆç­–ç•¥ - æ””æˆª+é‡ç™¼è«‹æ±‚ ===
                    counts_data = {}
                    video_urls = set()
                    captured_graphql_request = {}
                    
                    async def handle_counts_response(response):
                        await self._handle_graphql_response(response, counts_data, video_urls, captured_graphql_request)
                    
                    page.on("response", handle_counts_response)
                    
                    # === æ­¥é©Ÿ 2: å°èˆªå’Œè§¸ç™¼è¼‰å…¥ ===
                    await page.goto(post.url, wait_until="networkidle", timeout=60000)
                    await asyncio.sleep(3)
                    
                    # === æ­¥é©Ÿ 2.5: æ··åˆç­–ç•¥é‡ç™¼è«‹æ±‚ ===
                    if captured_graphql_request and not counts_data:
                        counts_data = await self._resend_graphql_request(captured_graphql_request, post.url)
                    
                    # å˜—è©¦è§¸ç™¼å½±ç‰‡è¼‰å…¥
                    await self._trigger_video_loading(page)
                    
                    # === æ­¥é©Ÿ 3: DOM å…§å®¹æå– ===
                    content_data = await self._extract_content_from_dom(page, username, video_urls)
                    
                    # === æ­¥é©Ÿ 3.5: DOM è¨ˆæ•¸å¾Œæ´ï¼ˆç•¶ GraphQL æ””æˆªå¤±æ•—æ™‚ï¼‰ ===
                    if not counts_data:
                        counts_data = await self._extract_counts_from_dom_fallback(page)
                    
                    # === æ­¥é©Ÿ 4: æ›´æ–°è²¼æ–‡æ•¸æ“š ===
                    updated = await self._update_post_data(post, counts_data, content_data, task_id, username)
                    
                    # éš¨æ©Ÿå»¶é²é¿å…åçˆ¬èŸ²
                    delay = random.uniform(2, 4)
                    await asyncio.sleep(delay)
                    
                except Exception as e:
                    logging.error(f"  âŒ æ··åˆç­–ç•¥è™•ç† {post.post_id} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                    post.processing_stage = "details_failed"
                finally:
                    if page:
                        await page.close()

        # åºåˆ—è™•ç†ä¿æŒé †åº
        for post in posts_to_fill:
            await fetch_single_details_hybrid(post)
        
        return posts_to_fill
    
    async def _handle_graphql_response(self, response, counts_data: dict, video_urls: set, captured_graphql_request: dict):
        """è™•ç† GraphQL éŸ¿æ‡‰çš„æ””æˆª"""
        try:
            import json
            url = response.url.lower()
            headers = response.request.headers
            query_name = headers.get("x-fb-friendly-name", "")
            
            # æ””æˆªè¨ˆæ•¸æŸ¥è©¢è«‹æ±‚ï¼ˆä¿å­˜headerså’Œpayloadï¼‰
            if ("/graphql" in url and response.status == 200 and 
                "useBarcelonaBatchedDynamicPostCountsSubscriptionQuery" in query_name):
                logging.info(f"   ğŸ¯ æ””æˆªåˆ°GraphQLè¨ˆæ•¸æŸ¥è©¢ï¼Œä¿å­˜è«‹æ±‚ä¿¡æ¯...")
                
                # ä¿å­˜è«‹æ±‚ä¿¡æ¯ï¼ˆæ¨¡ä»¿hybrid_content_extractor.pyçš„æˆåŠŸç­–ç•¥ï¼‰
                captured_graphql_request.update({
                    "headers": dict(response.request.headers),
                    "payload": response.request.post_data,
                    "url": "https://www.threads.com/graphql/query"
                })
                
                # æ¸…ç†headers
                clean_headers = captured_graphql_request["headers"].copy()
                for h in ["host", "content-length", "accept-encoding"]:
                    clean_headers.pop(h, None)
                captured_graphql_request["clean_headers"] = clean_headers
                
                logging.info(f"   âœ… æˆåŠŸä¿å­˜GraphQLè«‹æ±‚ä¿¡æ¯ï¼Œæº–å‚™é‡ç™¼...")
                
                # ä¹Ÿå˜—è©¦ç›´æ¥è§£æç•¶å‰éŸ¿æ‡‰ï¼ˆä½œç‚ºå‚™ç”¨ï¼‰
                try:
                    data = await response.json()
                    if "data" in data and "data" in data["data"] and "posts" in data["data"]["data"]:
                        posts_list = data["data"]["data"]["posts"]
                        if posts_list and len(posts_list) > 0:
                            post_data = posts_list[0]
                            if isinstance(post_data, dict):
                                text_info = post_data.get("text_post_app_info", {}) or {}
                                counts_data.update({
                                    "likes": post_data.get("like_count") or 0,
                                    "comments": text_info.get("direct_reply_count") or 0, 
                                    "reposts": text_info.get("repost_count") or 0,
                                    "shares": text_info.get("reshare_count") or 0
                                })
                                logging.info(f"   âœ… ç›´æ¥æ””æˆªæˆåŠŸ: è®š={counts_data['likes']}, ç•™è¨€={counts_data['comments']}, è½‰ç™¼={counts_data['reposts']}, åˆ†äº«={counts_data['shares']}")
                except Exception as e:
                    logging.debug(f"   âš ï¸ ç›´æ¥è§£æå¤±æ•—: {e}")
            
            # æ””æˆªå½±ç‰‡è³‡æº
            content_type = response.headers.get("content-type", "")
            resource_type = response.request.resource_type
            if (resource_type == "media" or 
                content_type.startswith("video/") or
                ".mp4" in response.url.lower() or
                ".m3u8" in response.url.lower() or
                ".mpd" in response.url.lower()):
                video_urls.add(response.url)
                logging.debug(f"   ğŸ¥ æ””æˆªåˆ°å½±ç‰‡: {response.url[:60]}...")
                
        except Exception as e:
            logging.debug(f"   âš ï¸ éŸ¿æ‡‰è™•ç†å¤±æ•—: {e}")
    
    async def _resend_graphql_request(self, captured_graphql_request: dict, post_url: str) -> dict:
        """é‡ç™¼ GraphQL è«‹æ±‚"""
        logging.info(f"   ğŸ”„ ä½¿ç”¨ä¿å­˜çš„GraphQLè«‹æ±‚ä¿¡æ¯é‡ç™¼è«‹æ±‚...")
        counts_data = {}
        
        try:
            import httpx
            
            # å¾URLæå–PKï¼ˆå¦‚æœå¯èƒ½ï¼‰
            url_match = re.search(r'/post/([^/?]+)', post_url)
            if url_match:
                logging.info(f"   ğŸ” URLä»£ç¢¼: {url_match.group(1)}")
            
            # æº–å‚™é‡ç™¼è«‹æ±‚
            headers = captured_graphql_request["clean_headers"]
            payload = captured_graphql_request["payload"]
            
            # å¾é é¢contextç²å–cookies
            cookies_list = await context.cookies()
            cookies = {cookie['name']: cookie['value'] for cookie in cookies_list}
            
            # ç¢ºä¿æœ‰èªè­‰
            if not headers.get("authorization") and 'ig_set_authorization' in cookies:
                auth_value = cookies['ig_set_authorization']
                headers["authorization"] = f"Bearer {auth_value}" if not auth_value.startswith('Bearer') else auth_value
            
            # ç™¼é€HTTPè«‹æ±‚åˆ°Threads API
            async with httpx.AsyncClient(headers=headers, cookies=cookies, timeout=30.0, http2=True) as client:
                api_response = await client.post("https://www.threads.com/graphql/query", data=payload)
                
                if api_response.status_code == 200:
                    result = api_response.json()
                    logging.info(f"   âœ… é‡ç™¼è«‹æ±‚æˆåŠŸï¼Œç‹€æ…‹: {api_response.status_code}")
                    
                    if "data" in result and result["data"] and "data" in result["data"] and "posts" in result["data"]["data"]:
                        posts_list = result["data"]["data"]["posts"]
                        logging.info(f"   ğŸ“Š é‡ç™¼è«‹æ±‚éŸ¿æ‡‰åŒ…å« {len(posts_list)} å€‹è²¼æ–‡")
                        
                        # ä½¿ç”¨ç¬¬ä¸€å€‹è²¼æ–‡ï¼ˆç•¶å‰é é¢çš„ä¸»è¦è²¼æ–‡ï¼‰
                        if posts_list and len(posts_list) > 0:
                            post_data = posts_list[0]
                            if isinstance(post_data, dict):
                                text_info = post_data.get("text_post_app_info", {}) or {}
                                counts_data.update({
                                    "likes": post_data.get("like_count") or 0,
                                    "comments": text_info.get("direct_reply_count") or 0, 
                                    "reposts": text_info.get("repost_count") or 0,
                                    "shares": text_info.get("reshare_count") or 0
                                })
                                logging.info(f"   ğŸ¯ é‡ç™¼è«‹æ±‚æˆåŠŸç²å–æ•¸æ“š: è®š={counts_data['likes']}, ç•™è¨€={counts_data['comments']}, è½‰ç™¼={counts_data['reposts']}, åˆ†äº«={counts_data['shares']}")
                else:
                    logging.warning(f"   âš ï¸ é‡ç™¼è«‹æ±‚å¤±æ•—ï¼Œç‹€æ…‹: {api_response.status_code}")
                    
        except Exception as e:
            logging.warning(f"   âš ï¸ é‡ç™¼è«‹æ±‚éç¨‹å¤±æ•—: {e}")
        
        return counts_data
    
    async def _trigger_video_loading(self, page: Page):
        """è§¸ç™¼å½±ç‰‡è¼‰å…¥"""
        try:
            trigger_selectors = [
                'div[data-testid="media-viewer"]',
                'video',
                'div[role="button"][aria-label*="play"]',
                'div[role="button"][aria-label*="æ’­æ”¾"]',
                '[data-pressable-container] div[style*="video"]'
            ]
            
            for selector in trigger_selectors:
                try:
                    elements = page.locator(selector)
                    count = await elements.count()
                    if count > 0:
                        await elements.first.click(timeout=3000)
                        await asyncio.sleep(2)
                        break
                except:
                    continue
        except:
            pass
    
    async def _extract_content_from_dom(self, page: Page, username: str, video_urls: set) -> dict:
        """å¾ DOM æå–å…§å®¹æ•¸æ“š"""
        content_data = {}
        
        try:
            # æå–ç”¨æˆ¶åï¼ˆå¾ URLï¼‰
            url_match = re.search(r'/@([^/]+)/', page.url)
            content_data["username"] = url_match.group(1) if url_match else username or ""
            
            # æå–å…§å®¹æ–‡å­—
            content = ""
            content_selectors = [
                'div[data-pressable-container] span',
                '[data-testid="thread-text"]',
                'article div[dir="auto"]',
                'div[role="article"] div[dir="auto"]'
            ]
            
            for selector in content_selectors:
                try:
                    elements = page.locator(selector)
                    count = await elements.count()
                    
                    for i in range(min(count, 20)):
                        try:
                            text = await elements.nth(i).inner_text()
                            if (text and len(text.strip()) > 10 and 
                                not text.strip().isdigit() and
                                "å°æ™‚" not in text and "åˆ†é˜" not in text and
                                not text.startswith("@")):
                                content = text.strip()
                                break
                        except:
                            continue
                    
                    if content:
                        break
                except:
                    continue
            
            content_data["content"] = content
            
            # æå–åœ–ç‰‡ï¼ˆéæ¿¾é ­åƒï¼‰
            images = []
            img_elements = page.locator('img')
            img_count = await img_elements.count()
            
            for i in range(min(img_count, 50)):
                try:
                    img_elem = img_elements.nth(i)
                    img_src = await img_elem.get_attribute("src")
                    
                    if not img_src or not ("fbcdn" in img_src or "cdninstagram" in img_src):
                        continue
                    
                    if ("rsrc.php" in img_src or "static.cdninstagram.com" in img_src):
                        continue
                    
                    # æª¢æŸ¥å°ºå¯¸éæ¿¾é ­åƒ
                    try:
                        width = int(await img_elem.get_attribute("width") or 0)
                        height = int(await img_elem.get_attribute("height") or 0)
                        max_size = max(width, height)
                        
                        if max_size > 150 and img_src not in images:
                            images.append(img_src)
                    except:
                        if ("t51.2885-15" in img_src or "scontent" in img_src) and img_src not in images:
                            images.append(img_src)
                except:
                    continue
            
            content_data["images"] = images
            
            # æå–å½±ç‰‡ï¼ˆçµåˆç¶²è·¯æ””æˆªå’ŒDOMï¼‰
            videos = list(video_urls)
            
            # DOM ä¸­çš„ video æ¨™ç±¤
            video_elements = page.locator('video')
            video_count = await video_elements.count()
            
            for i in range(video_count):
                try:
                    video_elem = video_elements.nth(i)
                    src = await video_elem.get_attribute("src")
                    data_src = await video_elem.get_attribute("data-src")
                    poster = await video_elem.get_attribute("poster")
                    
                    if src and src not in videos:
                        videos.append(src)
                    if data_src and data_src not in videos:
                        videos.append(data_src)
                    if poster and poster not in videos:
                        videos.append(f"POSTER::{poster}")
                    
                    # source å­å…ƒç´ 
                    sources = video_elem.locator('source')
                    source_count = await sources.count()
                    for j in range(source_count):
                        source_src = await sources.nth(j).get_attribute("src")
                        if source_src and source_src not in videos:
                            videos.append(source_src)
                except:
                    continue
            
            content_data["videos"] = videos
            
        except Exception as e:
            logging.debug(f"   âš ï¸ DOM å…§å®¹æå–å¤±æ•—: {e}")
        
        return content_data
    
    async def _extract_counts_from_dom_fallback(self, page: Page) -> dict:
        """DOM è¨ˆæ•¸å¾Œæ´æå–"""
        logging.warning(f"   ğŸ”„ GraphQL è¨ˆæ•¸æ””æˆªå¤±æ•—ï¼Œé–‹å§‹ DOM è¨ˆæ•¸å¾Œæ´...")
        
        # å…ˆæª¢æŸ¥é é¢ç‹€æ…‹
        page_title = await page.title()
        page_url = page.url
        logging.info(f"   ğŸ“„ é é¢ç‹€æ…‹ - æ¨™é¡Œ: {page_title}, URL: {page_url}")
        
        count_selectors = {
            "likes": [
                # English selectors
                "button[aria-label*='likes'] span",
                "button[aria-label*='Like'] span", 
                "span:has-text(' likes')",
                "span:has-text(' like')",
                "button svg[aria-label='Like'] + span",
                "button[aria-label*='like']",
                # Chinese selectors
                "button[aria-label*='å€‹å–œæ­¡'] span",
                "button[aria-label*='å–œæ­¡']",
                # Generic patterns
                "button[data-testid*='like'] span",
                "div[role='button'][aria-label*='like'] span"
            ],
            "comments": [
                # English selectors
                "a[href$='#comments'] span",
                "span:has-text(' comments')",
                "span:has-text(' comment')",
                "a:has-text('comments')",
                "button[aria-label*='comment'] span",
                # Chinese selectors
                "span:has-text(' å‰‡ç•™è¨€')",
                "a:has-text('å‰‡ç•™è¨€')",
                # Generic patterns
                "button[data-testid*='comment'] span",
                "div[role='button'][aria-label*='comment'] span"
            ],
            "reposts": [
                # English selectors
                "span:has-text(' reposts')",
                "span:has-text(' repost')",
                "button[aria-label*='repost'] span",
                "a:has-text('reposts')",
                # Chinese selectors
                "span:has-text(' æ¬¡è½‰ç™¼')",
                "a:has-text('è½‰ç™¼')",
                # Generic patterns
                "button[data-testid*='repost'] span"
            ],
            "shares": [
                # English selectors
                "span:has-text(' shares')",
                "span:has-text(' share')",
                "button[aria-label*='share'] span",
                "a:has-text('shares')",
                # Chinese selectors
                "span:has-text(' æ¬¡åˆ†äº«')",
                "a:has-text('åˆ†äº«')",
                # Generic patterns
                "button[data-testid*='share'] span"
            ],
        }
        
        # å…ˆé€²è¡Œé€šç”¨å…ƒç´ æƒæï¼Œä¸¦æ™ºèƒ½æå–æ•¸å­—
        logging.info(f"   ğŸ” é€šç”¨å…ƒç´ æƒæå’Œæ™ºèƒ½æ•¸å­—æå–...")
        dom_counts = {}
        
        try:
            all_buttons = await page.locator('button').all_inner_texts()
            all_spans = await page.locator('span').all_inner_texts()
            number_elements = [text for text in (all_buttons + all_spans) if any(char.isdigit() for char in text)]
            logging.info(f"   ğŸ”¢ æ‰¾åˆ°åŒ…å«æ•¸å­—çš„å…ƒç´ : {number_elements[:20]}")
            
            # === ğŸ¯ æ™ºèƒ½æ•¸å­—è­˜åˆ¥ï¼šå¾æ‰¾åˆ°çš„æ•¸å­—ä¸­æå–ç¤¾äº¤æ•¸æ“š ===
            pure_numbers = []
            for text in number_elements:
                # è·³éæ˜é¡¯ä¸æ˜¯äº’å‹•æ•¸æ“šçš„æ–‡å­—
                if any(skip in text for skip in ['ç€è¦½', 'æ¬¡ç€è¦½', 'è§€çœ‹', 'å¤©', 'å°æ™‚', 'åˆ†é˜', 'ç§’', 'on.natgeo.com']):
                    continue
                    
                number = parse_number(text)
                if number and number > 0:
                    pure_numbers.append((number, text))
                    logging.info(f"   ğŸ“Š æå–æ•¸å­—: {number} (å¾ '{text}')")
            
            # æ ¹æ“šæ•¸å­—å¤§å°æ™ºèƒ½åˆ†é…ï¼ˆé€šå¸¸ï¼šlikes > comments > reposts > sharesï¼‰
            pure_numbers.sort(reverse=True)  # å¾å¤§åˆ°å°æ’åº
            
            if len(pure_numbers) >= 4:
                dom_counts["likes"] = pure_numbers[0][0]
                dom_counts["comments"] = pure_numbers[1][0] 
                dom_counts["reposts"] = pure_numbers[2][0]
                dom_counts["shares"] = pure_numbers[3][0]
                logging.info(f"   ğŸ¯ æ™ºèƒ½åˆ†é…4å€‹æ•¸å­—: è®š={dom_counts['likes']}, ç•™è¨€={dom_counts['comments']}, è½‰ç™¼={dom_counts['reposts']}, åˆ†äº«={dom_counts['shares']}")
            elif len(pure_numbers) >= 2:
                dom_counts["likes"] = pure_numbers[0][0]
                dom_counts["comments"] = pure_numbers[1][0]
                logging.info(f"   ğŸ¯ æ™ºèƒ½åˆ†é…2å€‹æ•¸å­—: è®š={dom_counts['likes']}, ç•™è¨€={dom_counts['comments']}")
            elif len(pure_numbers) >= 1:
                dom_counts["likes"] = pure_numbers[0][0]
                logging.info(f"   ğŸ¯ æ™ºèƒ½åˆ†é…1å€‹æ•¸å­—: è®š={dom_counts['likes']}")
                
        except Exception as e:
            logging.warning(f"   âš ï¸ æ™ºèƒ½æ•¸å­—æå–å¤±æ•—: {e}")
        
        # å¦‚æœæ™ºèƒ½æå–æˆåŠŸï¼Œè·³éå‚³çµ±é¸æ“‡å™¨ï¼›å¦å‰‡ç¹¼çºŒå˜—è©¦
        if not dom_counts:
            logging.info(f"   âš ï¸ æ™ºèƒ½æå–å¤±æ•—ï¼Œå›åˆ°å‚³çµ±é¸æ“‡å™¨...")
            for key, sels in count_selectors.items():
                logging.info(f"   ğŸ” å˜—è©¦æå– {key} æ•¸æ“š...")
                for i, sel in enumerate(sels):
                    try:
                        el = page.locator(sel).first
                        count = await el.count()
                        if count > 0:
                            text = (await el.inner_text()).strip()
                            logging.info(f"   ğŸ“ é¸æ“‡å™¨ {i+1}/{len(sels)} '{sel}' æ‰¾åˆ°æ–‡å­—: '{text}'")
                            n = parse_number(text)
                            if n and n > 0:
                                dom_counts[key] = n
                                logging.info(f"   âœ… DOM æˆåŠŸæå– {key}: {n} (é¸æ“‡å™¨: {sel})")
                                break
                            else:
                                logging.info(f"   âš ï¸ ç„¡æ³•è§£ææ•¸å­—: '{text}' -> {n}")
                        else:
                            logging.info(f"   âŒ é¸æ“‡å™¨ {i+1}/{len(sels)} æœªæ‰¾åˆ°å…ƒç´ : '{sel}'")
                    except Exception as e:
                        logging.info(f"   âš ï¸ é¸æ“‡å™¨ {i+1}/{len(sels)} '{sel}' éŒ¯èª¤: {e}")
                        continue
                
                if key not in dom_counts:
                    logging.warning(f"   âŒ ç„¡æ³•æ‰¾åˆ° {key} æ•¸æ“š")
        
        if dom_counts:
            counts_data = {
                "likes": dom_counts.get("likes", 0),
                "comments": dom_counts.get("comments", 0),
                "reposts": dom_counts.get("reposts", 0),
                "shares": dom_counts.get("shares", 0),
            }
            logging.info(f"   ğŸ¯ DOM è¨ˆæ•¸å¾Œæ´æˆåŠŸ: {counts_data}")
            return counts_data
        else:
            # æ‰€æœ‰æ–¹æ³•éƒ½å¤±æ•—æ™‚ï¼Œè¨˜éŒ„é é¢ç‹€æ…‹ç”¨æ–¼èª¿è©¦
            await self._debug_failed_page(page)
            return {}
    
    async def _debug_failed_page(self, page: Page):
        """èª¿è©¦å¤±æ•—é é¢çš„ç‹€æ…‹"""
        logging.warning(f"   âŒ GraphQLæ””æˆªå’ŒDOMå¾Œæ´éƒ½å¤±æ•—äº†ï¼")
        try:
            page_title = await page.title()
            page_url = page.url
            logging.info(f"   ğŸ“„ å¤±æ•—é é¢åˆ†æ - æ¨™é¡Œ: {page_title}")
            logging.info(f"   ğŸ”— å¤±æ•—é é¢åˆ†æ - URL: {page_url}")
            
            # æª¢æŸ¥é é¢æ˜¯å¦æ­£å¸¸è¼‰å…¥
            all_text = await page.inner_text('body')
            if "ç™»å…¥" in all_text or "login" in all_text.lower():
                logging.warning(f"   âš ï¸ å¯èƒ½é‡åˆ°ç™»å…¥é é¢")
            elif len(all_text) < 100:
                logging.warning(f"   âš ï¸ é é¢å…§å®¹å¤ªå°‘ï¼Œå¯èƒ½è¼‰å…¥å¤±æ•—")
            else:
                logging.info(f"   ğŸ“ é é¢å…§å®¹é•·åº¦: {len(all_text)} å­—å…ƒ")
                
                # æª¢æŸ¥æ˜¯å¦æœ‰äº’å‹•æŒ‰éˆ•
                like_buttons = await page.locator('[aria-label*="like"], [aria-label*="Like"], [aria-label*="å–œæ­¡"]').count()
                comment_buttons = await page.locator('[aria-label*="comment"], [aria-label*="Comment"], [aria-label*="ç•™è¨€"]').count()
                logging.info(f"   ğŸ“Š æ‰¾åˆ°æŒ‰éˆ•: è®š {like_buttons} å€‹, ç•™è¨€ {comment_buttons} å€‹")
                
                # å˜—è©¦æ‰¾åˆ°ä»»ä½•æ•¸å­—
                all_numbers = await page.locator(':text-matches("\\d+")').all_inner_texts()
                if all_numbers:
                    logging.info(f"   ğŸ”¢ é é¢æ‰€æœ‰æ•¸å­—: {all_numbers[:15]}")  # é¡¯ç¤ºå‰15å€‹
                
                # æª¢æŸ¥æ˜¯å¦æœ‰é˜»æ“‹å…ƒç´ 
                modal_count = await page.locator('[role="dialog"], .modal, [data-testid*="modal"]').count()
                if modal_count > 0:
                    logging.warning(f"   âš ï¸ ç™¼ç¾ {modal_count} å€‹æ¨¡æ…‹æ¡†å¯èƒ½é˜»æ“‹å…§å®¹")
                    
        except Exception as debug_e:
            logging.warning(f"   âš ï¸ å¤±æ•—é é¢åˆ†æéŒ¯èª¤: {debug_e}")
    
    async def _update_post_data(self, post: PostMetrics, counts_data: dict, content_data: dict, task_id: str, username: str) -> bool:
        """æ›´æ–°è²¼æ–‡æ•¸æ“š"""
        updated = False
        
        # æ›´æ–°è¨ˆæ•¸æ•¸æ“š - åªåœ¨ç¾æœ‰æ•¸æ“šç‚º None æˆ– 0 æ™‚æ‰æ›´æ–°
        if counts_data:
            if post.likes_count in (None, 0) and (counts_data.get("likes") or 0) > 0:
                post.likes_count = counts_data["likes"]
                updated = True
            if post.comments_count in (None, 0) and (counts_data.get("comments") or 0) > 0:
                post.comments_count = counts_data["comments"]
                updated = True
            if post.reposts_count in (None, 0) and (counts_data.get("reposts") or 0) > 0:
                post.reposts_count = counts_data["reposts"]
                updated = True
            if post.shares_count in (None, 0) and (counts_data.get("shares") or 0) > 0:
                post.shares_count = counts_data["shares"]
                updated = True
        
        # æ›´æ–°å…§å®¹æ•¸æ“š - åªåœ¨ç¾æœ‰æ•¸æ“šç‚ºç©ºæ™‚æ‰æ›´æ–°
        if content_data.get("content") and not post.content:
            post.content = content_data["content"]
            updated = True
        
        if content_data.get("images") and not post.images:
            post.images = content_data["images"]
            updated = True
        
        if content_data.get("videos") and not post.videos:
            # éæ¿¾å¯¦éš›å½±ç‰‡ï¼ˆæ’é™¤ POSTERï¼‰
            actual_videos = [v for v in content_data["videos"] if not v.startswith("POSTER::")]
            if actual_videos:
                post.videos = actual_videos
                updated = True
        
        if updated:
            post.processing_stage = "details_filled_hybrid"
            logging.info(f"  âœ… æ··åˆç­–ç•¥æˆåŠŸè£œé½Š {post.post_id}: è®š={post.likes_count}, å…§å®¹={len(post.content)}å­—, åœ–ç‰‡={len(post.images)}å€‹, å½±ç‰‡={len(post.videos)}å€‹")
            
            # ç™¼å¸ƒé€²åº¦
            if task_id:
                await publish_progress(
                    task_id, 
                    "details_fetched_hybrid",
                    username=content_data.get("username", username or "unknown"),
                    post_id=post.post_id,
                    likes_count=post.likes_count,
                    content_length=len(post.content),
                    media_count=len(post.images) + len(post.videos)
                )
        else:
            post.processing_stage = "details_failed"
            logging.warning(f"  âš ï¸ æ··åˆç­–ç•¥ç„¡æ³•è£œé½Š {post.post_id} çš„æ•¸æ“š")
        
        return updated