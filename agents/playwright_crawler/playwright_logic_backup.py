import json
import asyncio
import logging
import random
import re
import tempfile
import uuid
from pathlib import Path
from typing import Dict, List, Optional, AsyncIterable, Callable, Any
from datetime import datetime
import httpx

from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeoutError, Browser, BrowserContext, Page, Route, APIResponse

from common.settings import get_settings
from common.a2a import stream_text, stream_data, stream_status, stream_error, TaskState
from common.models import PostMetrics, PostMetricsBatch
from common.nats_client import publish_progress
from common.utils import (
    get_best_video_url,
    get_best_image_url,
    generate_post_url,
    first_of,
    parse_thread_item
)
from common.history import crawl_history

# å°å…¥ Hybrid Extractorï¼ˆå¦‚æœå­˜åœ¨ï¼‰
try:
    import sys
    sys.path.append(str(Path(__file__).parent.parent.parent))
    from hybrid_content_extractor import HybridContentExtractor
    HYBRID_AVAILABLE = True
except ImportError:
    HYBRID_AVAILABLE = False
    logging.warning("âš ï¸ Hybrid Extractor ä¸å¯ç”¨ï¼Œå°‡ä½¿ç”¨åŸå§‹æ–¹æ³•")

# Vision Agent URL - æŒ‡å‘æˆ‘å€‘å³å°‡å»ºç«‹çš„æ–°ç«¯é»
VISION_AGENT_URL = "http://vision-agent:8005/v1/vision/extract-views-from-image"

# èª¿è©¦æª”æ¡ˆè·¯å¾‘
DEBUG_DIR = Path(__file__).parent / "debug"
DEBUG_DIR.mkdir(exist_ok=True)
DEBUG_FAILED_ITEM_FILE = DEBUG_DIR / "failed_post_sample.json"
SAMPLE_THREAD_ITEM_FILE = DEBUG_DIR / "sample_thread_item.json"
RAW_CRAWL_DATA_FILE = DEBUG_DIR / "raw_crawl_data.json"

# è¨­å®šæ—¥èªŒï¼ˆé¿å…é‡è¤‡é…ç½®ï¼‰
if not logging.getLogger().handlers:
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# --- ç§»é™¤ GraphQL ä¾è³´ï¼Œæ”¹ç”¨ç´” DOM æ–¹æ¡ˆ ---

# --- çµ±ä¸€æ•¸å­—è§£æå·¥å…· ---
def parse_number(val):
    """
    çµ±ä¸€è§£æå„ç¨®æ•¸å­—æ ¼å¼ â†’ int
    - int / float ç›´æ¥å›å‚³
    - dict æœƒå˜—è©¦æŠ“ 'count'ã€'total'ã€ç¬¬ä¸€å€‹ value
    - å…¶é¤˜å­—ä¸²èµ° K / M / è¬ / é€—è™Ÿ æµç¨‹
    """
    # 1) å·²ç¶“æ˜¯æ•¸å€¼
    if isinstance(val, (int, float)):
        return int(val)
    
    # 2) å¦‚æœæ˜¯ dict å…ˆæŒ–æ•¸å­—å†éè¿´
    if isinstance(val, dict):
        for key in ("count", "total", "value"):
            if key in val:
                return parse_number(val[key])
        # æ‰¾ä¸åˆ°å¸¸è¦‹éµ â†’ æŠ“ç¬¬ä¸€å€‹ value
        if val:
            return parse_number(next(iter(val.values())))
        return 0  # ç©º dict
    
    # 3) None æˆ–ç©ºå­—ä¸²
    if not val:
        return 0
    
    try:
        # --- ä»¥ä¸‹è·ŸåŸæœ¬ä¸€æ¨£ ---
        text = str(val).strip().replace('&nbsp;', ' ')
        text = re.sub(r'ä¸²æ–‡\s*', '', text)
        text = re.sub(r'æ¬¡ç€è¦½.*$', '', text)
        text = re.sub(r'views?.*$', '', text, flags=re.IGNORECASE).strip()
        
        # ä¸­æ–‡è¬ / å„„
        if 'è¬' in text:
            m = re.search(r'([\d.,]+)\s*è¬', text)
            if m: 
                return int(float(m.group(1).replace(',', '')) * 1e4)
        if 'å„„' in text:
            m = re.search(r'([\d.,]+)\s*å„„', text)
            if m: 
                return int(float(m.group(1).replace(',', '')) * 1e8)
        
        # è‹±æ–‡ K / M
        up = text.upper()
        if 'M' in up:
            m = re.search(r'([\d.,]+)\s*M', up)
            if m: 
                return int(float(m.group(1).replace(',', '')) * 1e6)
        if 'K' in up:
            m = re.search(r'([\d.,]+)\s*K', up)
            if m: 
                return int(float(m.group(1).replace(',', '')) * 1e3)
        
        # å–®ç´”æ•¸å­—å«é€—è™Ÿ
        m = re.search(r'[\d,]+', text)
        return int(m.group(0).replace(',', '')) if m else 0
        
    except (ValueError, IndexError) as e:
        logging.debug(f"âš ï¸ ç„¡æ³•è§£ææ•¸å­—: '{val}' - {e}")
        return 0

# --- å¼·å¥çš„æ¬„ä½å°ç…§è¡¨ ---
FIELD_MAP = {
    "like_count": [
        "like_count", "likeCount", 
        ["feedback_info", "aggregated_like_count"],
        ["feedback_info", "aggregated_like_count", "count"],  # æ›´æ·±å±¤è·¯å¾‘
        ["like_info", "count"]
    ],
    "comment_count": [
        ["text_post_app_info", "direct_reply_count"],
        "comment_count", "commentCount",
        ["reply_info", "count"]
    ],
    "share_count": [
        ["text_post_app_info", "reshare_count"],  # çœŸæ­£çš„ shares æ¬„ä½
        "reshareCount", "share_count", "shareCount"
    ],
    "repost_count": [
        ["text_post_app_info", "repost_count"],  # reposts æ¬„ä½
        "repostCount", "repost_count"
    ],
    "content": [
        ["caption", "text"],
        "caption", "text", "content"
    ],
    "author": [
        ["user", "username"], 
        ["owner", "username"],
        ["user", "handle"]
    ],
    "created_at": [
        "taken_at", "taken_at_timestamp", 
        "publish_date", "created_time"
    ],
    "post_id": [
        "pk", "id", "post_id"
    ],
    "code": [
        "code", "shortcode", "media_code"
    ],
    # å¢åŠ ç›´æ¥å¾ API å˜—è©¦ç²å– views çš„è·¯å¾‘
    "view_count": [
        ["feedback_info", "view_count"],
        ["video_info", "play_count"],
        "view_count",
        "views",
        "impression_count"
    ],
}

def parse_post_data(thread_item: Dict[str, Any], username: str) -> Optional[PostMetrics]:
    """
    å¼·å¥çš„è²¼æ–‡è§£æå™¨ï¼Œä½¿ç”¨å¤šéµ fallback æ©Ÿåˆ¶è™•ç†æ¬„ä½è®Šå‹•ã€‚
    æ”¯æ´ Threads GraphQL API çš„ä¸åŒç‰ˆæœ¬å’Œæ¬„ä½å‘½åè®ŠåŒ–ã€‚
    """
    # ä½¿ç”¨æ™ºèƒ½æœå°‹æ‰¾åˆ°çœŸæ­£çš„è²¼æ–‡ç‰©ä»¶
    post = parse_thread_item(thread_item)
    
    if not post:
        logging.info(f"âŒ æ‰¾ä¸åˆ°æœ‰æ•ˆçš„ post ç‰©ä»¶ï¼Œæ”¶åˆ°çš„è³‡æ–™éµ: {list(thread_item.keys())}")
        logging.info(f"âŒ thread_item å…§å®¹ç¯„ä¾‹: {str(thread_item)[:300]}...")
        
        # è‡ªå‹•å„²å­˜ç¬¬ä¸€ç­† raw JSON ä¾›åˆ†æ
        try:
            if not DEBUG_FAILED_ITEM_FILE.exists():
                DEBUG_FAILED_ITEM_FILE.write_text(
                    json.dumps(thread_item, indent=2, ensure_ascii=False),
                    encoding="utf-8" # æ˜ç¢ºæŒ‡å®š UTF-8
                )
                logging.info(f"ğŸ“ å·²å„²å­˜å¤±æ•—ç¯„ä¾‹è‡³ {DEBUG_FAILED_ITEM_FILE}")
        except Exception:
            pass  # å¯«æª”å¤±æ•—ä¸å½±éŸ¿ä¸»è¦åŠŸèƒ½
            
        return None

    # ä½¿ç”¨å¼·å¥çš„æ¬„ä½è§£æ
    post_id = first_of(post, *FIELD_MAP["post_id"])
    code = first_of(post, *FIELD_MAP["code"])
    
    if not post_id:
        logging.info(f"âŒ æ‰¾ä¸åˆ°æœ‰æ•ˆçš„ post_idï¼Œå¯ç”¨æ¬„ä½: {list(post.keys())}")
        return None
        
    if not code:
        logging.info(f"âŒ æ‰¾ä¸åˆ°æœ‰æ•ˆçš„ codeï¼Œå¯ç”¨æ¬„ä½: {list(post.keys())}")
        return None

    # --- URL ä¿®å¾©ï¼šä½¿ç”¨æ­£ç¢ºçš„æ ¼å¼ ---
    # èˆŠæ ¼å¼ (éŒ¯èª¤): f"https://www.threads.net/t/{code}"
    # æ–°æ ¼å¼ (æ­£ç¢º): f"https://www.threads.com/@{username}/post/{code}"
    url = generate_post_url(username, code)

    # ä½¿ç”¨å¤šéµ fallback è§£ææ‰€æœ‰æ¬„ä½
    author = first_of(post, *FIELD_MAP["author"]) or username
    content = first_of(post, *FIELD_MAP["content"]) or ""
    like_count = parse_number(first_of(post, *FIELD_MAP["like_count"]))
    comment_count = parse_number(first_of(post, *FIELD_MAP["comment_count"]))
    share_count = parse_number(first_of(post, *FIELD_MAP["share_count"]))
    repost_count = parse_number(first_of(post, *FIELD_MAP["repost_count"]))
    created_at = first_of(post, *FIELD_MAP["created_at"])
    
    # ğŸ”¥ èª¿è©¦ä¿¡æ¯ï¼šæª¢æŸ¥åŸå§‹æ•¸æ“š
    logging.debug(f"ğŸ”¥ like={first_of(post, *FIELD_MAP['like_count'])!r} comment={first_of(post, *FIELD_MAP['comment_count'])!r} from raw post_id={post_id}")
    logging.debug(f"ğŸ”¥ è§£æå¾Œ: like={like_count} comment={comment_count} share={share_count} repost={repost_count}")
    
    # åµæ¸¬æœªçŸ¥æ¬„ä½ï¼ˆç”¨æ–¼èª¿è©¦å’Œæ”¹é€²ï¼‰
    unknown_keys = set(post.keys()) - {
        'pk', 'id', 'post_id', 'code', 'shortcode', 'media_code',
        'user', 'owner', 'caption', 'text', 'content',
        'like_count', 'likeCount', 'feedback_info', 'like_info',
        'text_post_app_info', 'comment_count', 'commentCount', 'reply_info',
        'taken_at', 'taken_at_timestamp', 'publish_date', 'created_time',
        # å¸¸è¦‹ä½†ä¸é‡è¦çš„æ¬„ä½
        'media_type', 'has_liked', 'image_versions2', 'video_versions',
        'carousel_media', 'location', 'user_tags', 'usertags'
    }
    
    if unknown_keys:
        logging.debug(f"ğŸ” ç™¼ç¾æœªçŸ¥æ¬„ä½: {unknown_keys}")
        # å¯ä»¥é¸æ“‡å¯«å…¥æª”æ¡ˆä»¥ä¾›åˆ†æ
        try:
            with open("unknown_fields.log", "a", encoding="utf-8") as f:
                f.write(f"{json.dumps(list(unknown_keys))}\n")
        except Exception:
            pass  # å¯«æª”å¤±æ•—ä¸å½±éŸ¿ä¸»è¦åŠŸèƒ½

    # --- MEDIA -------------------------------------------------------------
    images, videos = [], []

    def append_image(url):
        if url and url not in images:
            images.append(url)

    def append_video(url):
        if url and url not in videos:
            videos.append(url)

    def best_candidate(candidates: list[dict], prefer_mp4=False):
        """
        å¾ image_versions2 æˆ– video_versions è£¡æŒ‘ã€Œå¯¬åº¦æœ€å¤§çš„é‚£å€‹ã€URL
        """
        if not candidates:
            return None
        key = "url"
        return max(candidates, key=lambda c: c.get("width", 0)).get(key)

    # 1) å–®åœ– / å–®ç‰‡
    if "image_versions2" in post:
        append_image(best_candidate(post["image_versions2"].get("candidates", [])))
    if "video_versions" in post:
        append_video(best_candidate(post.get("video_versions", []), prefer_mp4=True))

    # 2) è¼ªæ’­ carousel_media (åŠ å…¥ None çš„å®‰å…¨è™•ç†)
    for media in post.get("carousel_media") or []:
        if "image_versions2" in media:
            append_image(best_candidate(media["image_versions2"].get("candidates", [])))
        if "video_versions" in media:
            append_video(best_candidate(media.get("video_versions", []), prefer_mp4=True))
    # -----------------------------------------------------------------------


    # æˆåŠŸè§£æï¼Œè¨˜éŒ„éƒ¨åˆ†è³‡è¨Šä¾›é™¤éŒ¯
    logging.info(f"âœ… æˆåŠŸè§£æè²¼æ–‡ {post_id}: ä½œè€…={author}, è®šæ•¸={like_count}, åœ–ç‰‡={len(images)}, å½±ç‰‡={len(videos)}")
    
    # çµ±ä¸€ post_id æ ¼å¼ï¼šä½¿ç”¨ username_code è€Œä¸æ˜¯åŸå§‹çš„æ•¸å­— ID
    unified_post_id = f"{username}_{code}"
    
    return PostMetrics(
        url=url,
        post_id=unified_post_id,  # ä½¿ç”¨çµ±ä¸€æ ¼å¼çš„ post_id
        username=username,
        source="playwright",
        processing_stage="playwright_crawled",
        likes_count=like_count,      # å·²ç¶“é€šé parse_number() è™•ç†
        comments_count=comment_count, # å·²ç¶“é€šé parse_number() è™•ç†
        reposts_count=repost_count,   # å·²ç¶“é€šé parse_number() è™•ç†
        shares_count=share_count,     # å·²ç¶“é€šé parse_number() è™•ç†
        content=content,
        created_at=datetime.fromtimestamp(created_at) if created_at and isinstance(created_at, (int, float)) else datetime.utcnow(),
        images=images,
        videos=videos,
        # æ–°å¢ï¼šç›´æ¥å¾ API è§£æ views_countï¼ˆæŒ‰æŒ‡å¼•å„ªå…ˆå˜—è©¦ APIï¼‰
        views_count=parse_number(first_of(post, *FIELD_MAP["view_count"])),
    )

# +++ æ–°å¢ï¼šå¾å‰ç«¯è§£æç€è¦½æ•¸çš„è¼”åŠ©å‡½å¼ï¼ˆä¿®å¾©ç©ºæ ¼å•é¡Œï¼‰ +++
def parse_views_text(text: Optional[str]) -> Optional[int]:
    """å°‡ '161.9è¬æ¬¡ç€è¦½' æˆ– '4 è¬æ¬¡ç€è¦½' æˆ– '1.2M views' é€™é¡æ–‡å­—è½‰æ›ç‚ºæ•´æ•¸"""
    if not text:
        return None
    try:
        original_text = text
        
        # ç§»é™¤ä¸å¿…è¦çš„æ–‡å­—ï¼Œä¿ç•™æ•¸å­—å’Œå–®ä½
        text = re.sub(r'ä¸²æ–‡\s*', '', text)  # ç§»é™¤ "ä¸²æ–‡"
        
        # è™•ç†ä¸­æ–‡æ ¼å¼ï¼š1.2è¬ã€4 è¬æ¬¡ç€è¦½ã€5000æ¬¡ç€è¦½
        if 'è¬' in text:
            match = re.search(r'([\d.]+)\s*è¬', text)  # å…è¨±æ•¸å­—å’Œè¬ä¹‹é–“æœ‰ç©ºæ ¼
            if match:
                return int(float(match.group(1)) * 10000)
        elif 'å„„' in text:
            match = re.search(r'([\d.]+)\s*å„„', text)  # å…è¨±æ•¸å­—å’Œå„„ä¹‹é–“æœ‰ç©ºæ ¼
            if match:
                return int(float(match.group(1)) * 100000000)
        
        # è™•ç†è‹±æ–‡æ ¼å¼ï¼š1.2M views, 500K views
        text_upper = text.upper()
        if 'M' in text_upper:
            match = re.search(r'([\d.]+)M', text_upper)
            if match:
                return int(float(match.group(1)) * 1000000)
        elif 'K' in text_upper:
            match = re.search(r'([\d.]+)K', text_upper)
            if match:
                return int(float(match.group(1)) * 1000)
        
        # è™•ç†ç´”æ•¸å­—æ ¼å¼ï¼ˆå¯èƒ½åŒ…å«é€—è™Ÿï¼‰
        match = re.search(r'[\d,]+', text)
        if match:
            return int(match.group(0).replace(',', ''))
        
        logging.debug(f"ğŸ” ç„¡æ³•è§£æç€è¦½æ•¸æ–‡å­—: '{original_text}'")
        return None
        
    except (ValueError, IndexError) as e:
        logging.warning(f"âš ï¸ ç„¡æ³•è§£æç€è¦½æ•¸æ–‡å­—: '{text}' - {e}")
        return None

class PlaywrightLogic:
    """
    ä½¿ç”¨ Playwright é€²è¡Œçˆ¬èŸ²çš„æ ¸å¿ƒé‚è¼¯ã€‚
    """
    def __init__(self):
        self.settings = get_settings().playwright
        self.context = None  # åˆå§‹åŒ– context

    # GraphQL éŸ¿æ‡‰è™•ç†å™¨å·²ç§»é™¤ - æ”¹ç”¨ç´” DOM æ–¹æ¡ˆ

    async def get_ordered_post_urls_from_page(self, page: Page, username: str, max_posts: int) -> List[str]:
        """
        å¾ç”¨æˆ¶é é¢ç›´æ¥æå–è²¼æ–‡ URLsï¼Œä¿æŒæ™‚é–“é †åº
        é€™æ˜¯è§£æ±ºè²¼æ–‡é †åºæ··äº‚å•é¡Œçš„é—œéµæ–¹æ³•
        """
        user_url = f"https://www.threads.com/@{username}"
        logging.info(f"ğŸ” æ­£åœ¨å¾ç”¨æˆ¶é é¢ç²å–æœ‰åºçš„è²¼æ–‡ URLs: {user_url}")
        
        await page.goto(user_url, wait_until="networkidle")
        
        # ç­‰å¾…é é¢è¼‰å…¥
        await asyncio.sleep(3)
        
        # æ»¾å‹•ä»¥è¼‰å…¥æ›´å¤šè²¼æ–‡ï¼ˆä½†ä¸è¦æ»¾å‹•å¤ªå¤šæ¬¡é¿å…è¼‰å…¥éèˆŠçš„è²¼æ–‡ï¼‰
        scroll_count = min(3, max(1, max_posts // 10))  # æ ¹æ“šéœ€æ±‚å‹•æ…‹èª¿æ•´æ»¾å‹•æ¬¡æ•¸
        for i in range(scroll_count):
            await page.mouse.wheel(0, 1000)
            await asyncio.sleep(2)
        
        # æå–è²¼æ–‡ URLsï¼Œä¿æŒåŸå§‹é †åº
        post_urls = await page.evaluate("""
            () => {
                // ç²å–æ‰€æœ‰è²¼æ–‡é€£çµï¼Œä¿æŒDOMä¸­çš„åŸå§‹é †åº
                const links = Array.from(document.querySelectorAll('a[href*="/post/"]'));
                const urls = [];
                const seen = new Set();
                
                // éæ­·æ™‚ä¿æŒé †åºï¼Œåªå»é‡ä½†ä¸é‡æ’
                for (const link of links) {
                    const url = link.href;
                    if (url.includes('/post/') && !seen.has(url)) {
                        seen.add(url);
                        urls.push(url);
                    }
                }
                
                return urls;
            }
        """)
        
        # é™åˆ¶æ•¸é‡ä½†ä¿æŒé †åº
        post_urls = post_urls[:max_posts]
        logging.info(f"   âœ… æŒ‰æ™‚é–“é †åºæ‰¾åˆ° {len(post_urls)} å€‹è²¼æ–‡ URLs")
        
        return post_urls

    async def fetch_posts(
        self,
        username: str,
        extra_posts: int,
        auth_json_content: Dict,
        task_id: str = None
    ) -> PostMetricsBatch:
        """
        ä½¿ç”¨æŒ‡å®šçš„èªè­‰å…§å®¹é€²è¡Œå¢é‡çˆ¬å–ã€‚
        
        Args:
            username: ç›®æ¨™ç”¨æˆ¶å
            extra_posts: æƒ³è¦é¡å¤–æŠ“å–çš„è²¼æ–‡æ•¸é‡ï¼ˆå¢é‡èªç¾©ï¼‰
            auth_json_content: èªè­‰ä¿¡æ¯
            task_id: ä»»å‹™ID
            
        Returns:
            PostMetricsBatch: æ–°æŠ“å–çš„è²¼æ–‡æ‰¹æ¬¡
            
        æ ¸å¿ƒå„ªåŒ–ï¼ˆåŸºæ–¼ç”¨æˆ¶å»ºè­°ï¼‰ï¼š
        - extra_posts=0 è‡ªå‹•è·³éçˆ¬å–
        - é å…ˆè¨ˆç®—needï¼Œå¯¦ç¾ç²¾ç¢ºæ—©åœ
        - ä½¿ç”¨latest_post_idå„ªåŒ–æŸ¥è©¢
        """
        if task_id is None:
            task_id = str(uuid.uuid4())
        
        # ğŸš€ æ ¸å¿ƒå„ªåŒ–ï¼šå¢é‡çˆ¬å–é‚è¼¯ï¼ˆåŸºæ–¼ç”¨æˆ¶å»ºè­°ï¼‰
        # â‘  æª¢æŸ¥æ˜¯å¦éœ€è¦çˆ¬å–
        if extra_posts <= 0:
            logging.info(f"ğŸŸ¢ {username} ç„¡éœ€é¡å¤–çˆ¬å– (extra_posts={extra_posts})")
            existing_state = await crawl_history.get_crawl_state(username)
            total_existing = existing_state.get("total_crawled", 0) if existing_state else 0
            return PostMetricsBatch(posts=[], username=username, total_count=total_existing)
        
        # â‘¡ è®€å–ç¾æœ‰post_idé›†åˆï¼ˆé¿å…é‡è¤‡æŠ“å–ï¼‰
        existing_post_ids = await crawl_history.get_existing_post_ids(username)
        already_count = len(existing_post_ids)
        need_to_fetch = extra_posts  # ç²¾ç¢ºæ§åˆ¶ï¼šå°±æ˜¯è¦é€™éº¼å¤šç¯‡æ–°çš„
        
        logging.info(f"ğŸ“Š {username} å¢é‡ç‹€æ…‹: å·²æœ‰={already_count}, éœ€è¦æ–°å¢={need_to_fetch}")
        
        target_url = f"https://www.threads.com/@{username}"
        posts: Dict[str, PostMetrics] = {}
        
        # ç™¼å¸ƒé–‹å§‹çˆ¬å–çš„é€²åº¦
        await publish_progress(task_id, "fetch_start", username=username, extra_posts=extra_posts)
        
        # 1. å®‰å…¨åœ°å°‡ auth.json å…§å®¹å¯«å…¥è‡¨æ™‚æª”æ¡ˆ
        auth_file = Path(tempfile.gettempdir()) / f"{task_id or uuid.uuid4()}_auth.json"
        try:
            with open(auth_file, 'w', encoding='utf-8') as f:
                json.dump(auth_json_content, f)

            async with async_playwright() as p:
                browser = await p.chromium.launch(
                    headless=self.settings.headless,
                    timeout=self.settings.navigation_timeout,
                    args=["--no-sandbox", "--disable-dev-shm-usage", "--disable-blink-features=AutomationControlled"]
                )
                self.context = await browser.new_context(
                    storage_state=str(auth_file),
                    user_agent=self.settings.user_agent, # å¾è¨­å®šè®€å–
                    viewport={"width": 1920, "height": 1080},
                    locale="zh-TW",  # è¨­å®šç‚ºç¹é«”ä¸­æ–‡
                    has_touch=True,
                    accept_downloads=False,
                    bypass_csp=True  # æ–°å¢ï¼šç¹é CSP é™åˆ¶
                )
                # æ–°å¢ï¼šéš±è— webdriver å±¬æ€§
                await self.context.add_init_script(
                    "Object.defineProperty(navigator,'webdriver',{get:()=>undefined})"
                )
                
                page = await self.context.new_page()
                page.on("console", lambda m: logging.info(f"CONSOLE [{m.type}] {m.text}"))

                # --- ğŸš€ å„ªåŒ–æ–¹æ³•ï¼šä½¿ç”¨å¢é‡éœ€æ±‚+buffer ---
                logging.info(f"ğŸ¯ [Task: {task_id}] å¢é‡çˆ¬å–: éœ€è¦{need_to_fetch}ç¯‡æ–°è²¼æ–‡")
                buffer_size = min(need_to_fetch + 10, 50)  # åˆç†bufferï¼Œé¿å…éåº¦æŠ“å–
                ordered_post_urls = await self.get_ordered_post_urls_from_page(page, username, buffer_size)
                
                if not ordered_post_urls:
                    logging.warning(f"âš ï¸ [Task: {task_id}] ç„¡æ³•å¾ç”¨æˆ¶é é¢ç²å–è²¼æ–‡ URLs")
                    # GraphQL æ–¹æ³•å·²ç§»é™¤ï¼Œç´” DOM æ–¹æ¡ˆä¸éœ€è¦å›é€€

                logging.info(f"å°è¦½è‡³ç›®æ¨™é é¢: {target_url}")
                await page.goto(target_url, wait_until="networkidle", timeout=self.settings.navigation_timeout)

                # --- æ»¾å‹•èˆ‡å»¶é²é‚è¼¯ ---
                scroll_attempts_without_new_posts = 0
                max_retries = 5

                # æ³¨æ„ï¼šæ­¤æ»¾å‹•é‚è¼¯åœ¨å¢é‡æ¨¡å¼ä¸‹å·²ä¸éœ€è¦ï¼Œå› ç‚ºæˆ‘å€‘å·²é€šéget_ordered_post_urls_from_pageç²å–URL
                while len(posts) < need_to_fetch and False:  # æš«æ™‚ç¦ç”¨æ»¾å‹•
                    posts_before_scroll = len(posts)
                    await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")

                    await asyncio.sleep(0.5)  # çœŸçš„è¦ç­‰ networkidle çš„è©±è‡ªå·±ç¡ä¸€ä¸‹å°±å¥½

                    delay = random.uniform(self.settings.scroll_delay_min, self.settings.scroll_delay_max) # å¾è¨­å®šè®€å–
                    await asyncio.sleep(delay)

                    if len(posts) == posts_before_scroll:
                        scroll_attempts_without_new_posts += 1
                        logging.info(f"æ»¾å‹•å¾Œæœªç™¼ç¾æ–°è²¼æ–‡ (å˜—è©¦ {scroll_attempts_without_new_posts}/{max_retries})")
                        if scroll_attempts_without_new_posts >= max_retries:
                            logging.info("å·²é”é é¢æœ«ç«¯æˆ–ç„¡æ–°å…§å®¹ï¼Œåœæ­¢æ»¾å‹•ã€‚")
                            break
                    else:
                        scroll_attempts_without_new_posts = 0
                        # ç™¼å¸ƒé€²åº¦æ›´æ–°
                        await publish_progress(
                            task_id, "fetch_progress", 
                            username=username,
                            current=len(posts), 
                            total=need_to_fetch,
                            progress=min(len(posts) / need_to_fetch, 1.0) if need_to_fetch > 0 else 1.0
                        )

                # --- ğŸš€ å¢é‡å„ªåŒ–ï¼šå»é‡+ç²¾ç¢ºæ—©åœ ---
                logging.info(f"âœ… [Task: {task_id}] å¢é‡ç¯©é¸: å¾{len(ordered_post_urls)}å€‹URLä¸­å°‹æ‰¾{need_to_fetch}ç¯‡æ–°è²¼æ–‡")
                ordered_posts = []  # ä¿æŒé †åºçš„é™£åˆ—
                processed_count = 0
                new_posts_found = 0
                
                for i, post_url in enumerate(ordered_post_urls):
                    # å¾ URL ä¸­æå– post_id å’Œ code
                    url_parts = post_url.split('/')
                    if len(url_parts) >= 2:
                        code = url_parts[-1] if url_parts[-1] != 'media' else url_parts[-2]  # è™•ç† /media çµå°¾
                        post_id = f"{username}_{code}"  # ç”Ÿæˆå”¯ä¸€çš„ post_id
                        processed_count += 1
                        
                        # ğŸš€ æ ¸å¿ƒå»é‡é‚è¼¯
                        if post_id in existing_post_ids:
                            logging.debug(f"â­ï¸ è·³éå·²å­˜åœ¨: {post_id}")
                            continue
                        
                        # ğŸ¯ ç²¾ç¢ºæ—©åœæ©Ÿåˆ¶
                        if new_posts_found >= need_to_fetch:
                            logging.info(f"ğŸ¯ ææ—©åœæ­¢: å·²æ”¶é›†åˆ°{need_to_fetch}ç¯‡æ–°è²¼æ–‡")
                            break
                        
                        # â‘  å‰µå»ºæ–°çš„PostMetrics
                        post_metrics = PostMetrics(
                            url=post_url,
                            post_id=post_id,
                            username=username,
                            source="playwright_incremental",
                            processing_stage="url_extracted",
                            likes_count=0,
                            comments_count=0,
                            reposts_count=0,
                            shares_count=0,
                            content="",
                            created_at=datetime.utcnow(),
                            images=[],
                            videos=[],
                            views_count=None,
                        )
                        posts[post_id] = post_metrics
                        ordered_posts.append(post_metrics)
                        new_posts_found += 1
                        
                        logging.info(f"âœ… ç™¼ç¾æ–°è²¼æ–‡ {new_posts_found}/{need_to_fetch}: {post_id}")
                        
                        # åªåœ¨æœ€å¾Œä¸€å€‹æˆ–æ¯2å€‹ç™¼å¸ƒé€²åº¦ï¼Œæ¸›å°‘æ—¥èªŒå™ªéŸ³
                        if new_posts_found == need_to_fetch or (new_posts_found % 2 == 0):
                            await publish_progress(
                                task_id, 
                                "post_url_extracted",
                                username=username,
                                post_id=post_id,
                                current=new_posts_found,
                                total=need_to_fetch,
                                progress=new_posts_found / need_to_fetch,
                                urls_processed=i + 1
                            )
                
                # ä½¿ç”¨ ordered_posts è€Œä¸æ˜¯ posts.values() ä¾†ä¿æŒé †åº
                logging.info(f"âœ… [Task: {task_id}] å‰µå»ºäº† {len(ordered_posts)} å€‹æœ‰åºçš„åŸºç¤PostMetrics")
                
                # é—œé–‰ page ä½†ä¿ç•™ context ä¾› fill_views_from_page ä½¿ç”¨
                await page.close()

                # --- ğŸš€ å¢é‡æ¨¡å¼ï¼šä½¿ç”¨ç²¾ç¢ºæ”¶é›†çš„çµæœ ---
                # åœ¨å¢é‡æ¨¡å¼ä¸‹ï¼Œordered_postså·²ç¶“æ˜¯ç²¾ç¢ºæ§åˆ¶çš„çµæœ
                if 'ordered_posts' in locals():
                    final_posts = ordered_posts  # å·²ç¶“æ˜¯ç²¾ç¢ºæ•¸é‡ï¼Œç„¡éœ€æˆªæ–·
                    total_found = len(ordered_posts)
                    logging.info(f"ğŸ¯ [Task: {task_id}] å¢é‡æ¨¡å¼: ç²¾ç¢ºæ”¶é›†åˆ°{total_found}ç¯‡æ–°è²¼æ–‡")
                else:
                    # å›é€€åˆ°åŸå§‹æ–¹æ³•ï¼ˆä¿ç•™å…¼å®¹æ€§ï¼‰
                    final_posts = list(posts.values())[:need_to_fetch]
                    total_found = len(final_posts)
                    logging.info(f"ğŸ”„ [Task: {task_id}] å›é€€æ¨¡å¼: é™åˆ¶åˆ°{need_to_fetch}ç¯‡")
                
                logging.info(f"ğŸ”„ [Task: {task_id}] æº–å‚™å›å‚³æœ€çµ‚è³‡æ–™ï¼šå…±ç™¼ç¾ {total_found} å‰‡è²¼æ–‡, å›å‚³ {len(final_posts)} å‰‡")
                
                # --- è£œé½Šè©³ç´°æ•¸æ“šå’Œè§€çœ‹æ•¸ (åœ¨ browser context é‚„å­˜åœ¨æ™‚åŸ·è¡Œ) ---
                # ç´” DOM æ–¹æ¡ˆï¼šä¸€å®šè¦åŸ·è¡Œè©³ç´°æ•¸æ“šè£œé½Šä¾†ç²å–è¨ˆæ•¸
                logging.info(f"ğŸ” [Task: {task_id}] é–‹å§‹ DOM æ•¸æ“šè£œé½Šï¼ˆlikes, comments, contentç­‰ï¼‰...")
                await publish_progress(task_id, "fill_details_start", username=username, posts_count=len(final_posts))
                
                try:
                    final_posts = await self.fill_post_details_from_page(final_posts, task_id=task_id, username=username)
                    logging.info(f"âœ… [Task: {task_id}] è©³ç´°æ•¸æ“šè£œé½Šå®Œæˆ")
                    await publish_progress(task_id, "fill_details_completed", username=username, posts_count=len(final_posts))
                except Exception as e:
                    logging.warning(f"âš ï¸ [Task: {task_id}] è£œé½Šè©³ç´°æ•¸æ“šæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                    await publish_progress(task_id, "fill_details_error", username=username, error=str(e))
                
                # è£œé½Šè§€çœ‹æ•¸ï¼ˆå…©ç¨®æ–¹æ³•éƒ½éœ€è¦ï¼‰
                logging.info(f"ğŸ” [Task: {task_id}] é–‹å§‹è£œé½Šè§€çœ‹æ•¸...")
                await publish_progress(task_id, "fill_views_start", username=username, posts_count=len(final_posts))
                
                try:
                    final_posts = await self.fill_views_from_page(final_posts, task_id=task_id, username=username)
                    logging.info(f"âœ… [Task: {task_id}] è§€çœ‹æ•¸è£œé½Šå®Œæˆ")
                    await publish_progress(task_id, "fill_views_completed", username=username, posts_count=len(final_posts))
                except Exception as e:
                    logging.warning(f"âš ï¸ [Task: {task_id}] è£œé½Šè§€çœ‹æ•¸æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                    await publish_progress(task_id, "fill_views_error", username=username, error=str(e))
                    # å³ä½¿è£œé½Šå¤±æ•—ï¼Œä¹Ÿç¹¼çºŒè¿”å›åŸºæœ¬æ•¸æ“š

                # æ‰‹å‹•é—œé–‰ browser å’Œ context
                await self.context.close()
                await browser.close()
                self.context = None
            
            # ä¿å­˜åŸå§‹æŠ“å–è³‡æ–™ä¾›èª¿è©¦
            try:
                raw_data = {
                    "task_id": task_id,
                    "username": username, 
                    "timestamp": datetime.now().isoformat(),
                    "total_found": total_found,
                    "returned_count": len(final_posts),
                    "posts": [
                        {
                            "url": post.url,
                            "post_id": post.post_id,
                            "likes_count": post.likes_count,
                            "comments_count": post.comments_count,
                            "reposts_count": post.reposts_count,
                            "shares_count": post.shares_count,
                            "views_count": post.views_count,
                            "calculated_score": post.calculate_score(),  # ğŸ†• ç§»åˆ° views_count ä¸‹é¢
                            "content": post.content,
                            "created_at": post.created_at.isoformat() if post.created_at else None,
                            "images": post.images,  # æ·»åŠ åœ–ç‰‡ URL
                            "videos": post.videos   # æ·»åŠ å½±ç‰‡ URL
                        } for post in final_posts
                    ]
                }
                
                # ä½¿ç”¨æ™‚é–“æˆ³è¨˜é¿å…æª”æ¡ˆè¡çª
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                raw_file = DEBUG_DIR / f"crawl_data_{timestamp}_{task_id[:8]}.json"
                raw_file.write_text(
                    json.dumps(raw_data, indent=2, ensure_ascii=False),
                    encoding="utf-8" # æ˜ç¢ºæŒ‡å®š UTF-8
                )
                logging.info(f"ğŸ’¾ [Task: {task_id}] å·²ä¿å­˜åŸå§‹æŠ“å–è³‡æ–™è‡³: {raw_file}")
                
            except Exception as e:
                logging.warning(f"âš ï¸ [Task: {task_id}] ä¿å­˜èª¿è©¦è³‡æ–™å¤±æ•—: {e}")
            
            # ç™¼å¸ƒå®Œæˆé€²åº¦
            await publish_progress(
                task_id, "completed", 
                username=username,
                total_posts=len(final_posts),
                success=True
            )

            # ğŸš€ æ›´æ–°çˆ¬å–ç‹€æ…‹ï¼ˆé—œéµå„ªåŒ–ï¼‰
            if final_posts:
                # ä¿å­˜æ–°è²¼æ–‡åˆ°æ•¸æ“šåº«
                saved_count = await crawl_history.upsert_posts(final_posts)
                
                # æ›´æ–°crawl_stateï¼ˆä½¿ç”¨æœ€æ–°çš„post_idï¼‰
                latest_post_id = final_posts[0].post_id if final_posts else None
                if latest_post_id:
                    await crawl_history.update_crawl_state(username, latest_post_id, saved_count)
                
                # ç”Ÿæˆä»»å‹™ç›£æ§æŒ‡æ¨™
                task_metrics = await crawl_history.get_task_metrics(username, need_to_fetch, len(final_posts))
                logging.info(f"ğŸ“Š ä»»å‹™å®Œæˆ: {task_metrics}")
            
            batch = PostMetricsBatch(
                posts=final_posts,
                username=username,
                total_count=already_count + len(final_posts),  # æ›´æ–°ç¸½æ•¸
                processing_stage="playwright_incremental_completed"
            )
            return batch
            
        except Exception as e:
            error_message = f"Playwright æ ¸å¿ƒé‚è¼¯å‡ºéŒ¯: {e}"
            logging.error(error_message, exc_info=True)
            # ç™¼å¸ƒéŒ¯èª¤é€²åº¦
            await publish_progress(
                task_id, "error", 
                username=username,
                error=error_message,
                success=False
            )
            raise
        
        finally:
            # æ¸…ç†è‡¨æ™‚èªè­‰æª”æ¡ˆ
            if auth_file.exists():
                auth_file.unlink()
                logging.info(f"ğŸ—‘ï¸ [Task: {task_id}] å·²åˆªé™¤è‡¨æ™‚èªè­‰æª”æ¡ˆ: {auth_file}")
            
            # ç¢ºä¿ context è¢«é‡ç½®ï¼ˆbrowser å·²åœ¨ä¸Šé¢æ‰‹å‹•é—œé–‰ï¼‰
            self.context = None 

    # +++ æ–°å¢ï¼šå¾å‰ç«¯è£œé½Šç€è¦½æ•¸çš„æ ¸å¿ƒæ–¹æ³•ï¼ˆæ•´åˆ Gate é é¢è™•ç†ï¼‰ +++
    async def fill_views_from_page(self, posts_to_fill: List[PostMetrics], task_id: str = None, username: str = None) -> List[PostMetrics]:
        """
        éæ­·è²¼æ–‡åˆ—è¡¨ï¼Œå°èˆªåˆ°æ¯å€‹è²¼æ–‡çš„é é¢ä»¥è£œé½Š views_countã€‚
        æ•´åˆäº†æˆåŠŸçš„ Gate é é¢è™•ç†å’Œé›™ç­–ç•¥æå–æ–¹æ³•ã€‚
        """
        if not self.context:
            logging.error("âŒ Browser context æœªåˆå§‹åŒ–ï¼Œç„¡æ³•åŸ·è¡Œ fill_views_from_pageã€‚")
            return posts_to_fill

        # æ¸›å°‘ä¸¦ç™¼æ•¸ä»¥é¿å…è§¸ç™¼åçˆ¬èŸ²æ©Ÿåˆ¶
        semaphore = asyncio.Semaphore(2)
        
        async def fetch_single_view(post: PostMetrics):
            async with semaphore:
                page = None
                try:
                    page = await self.context.new_page()
                    # ç¦ç”¨åœ–ç‰‡å’Œå½±ç‰‡è¼‰å…¥ä»¥åŠ é€Ÿ
                    await page.route("**/*.{png,jpg,jpeg,gif,mp4,webp}", lambda r: r.abort())
                    
                    logging.debug(f"ğŸ“„ æ­£åœ¨è™•ç†: {post.url}")
                    
                    # å°èˆªåˆ°è²¼æ–‡é é¢
                    await page.goto(post.url, wait_until="networkidle", timeout=30000)
                    
                    # æª¢æŸ¥é é¢é¡å‹ï¼ˆå®Œæ•´é é¢ vs Gate é é¢ï¼‰
                    page_content = await page.content()
                    is_gate_page = "__NEXT_DATA__" not in page_content
                    
                    if is_gate_page:
                        logging.debug(f"   âš ï¸ æª¢æ¸¬åˆ° Gate é é¢ï¼Œç›´æ¥ä½¿ç”¨ DOM é¸æ“‡å™¨...")
                    
                    views_count = None
                    extraction_method = None
                    
                    # ç­–ç•¥ 1: GraphQL æ””æˆªï¼ˆåªåœ¨é Gate é é¢æ™‚ï¼‰
                    if not is_gate_page:
                        try:
                            response = await page.wait_for_response(
                                lambda r: "containing_thread" in r.url and r.status == 200, 
                                timeout=8000
                            )
                            data = await response.json()
                            
                            # è§£æç€è¦½æ•¸
                            thread_items = data["data"]["containing_thread"]["thread_items"]
                            post_data = thread_items[0]["post"]
                            views_count = (post_data.get("feedback_info", {}).get("view_count") or
                                          post_data.get("video_info", {}).get("play_count") or 0)
                            
                            if views_count > 0:
                                extraction_method = "graphql_api"
                                logging.debug(f"   âœ… GraphQL API ç²å–ç€è¦½æ•¸: {views_count:,}")
                        except Exception as e:
                            logging.debug(f"   âš ï¸ GraphQL æ””æˆªå¤±æ•—: {str(e)[:100]}")
                    
                    # ç­–ç•¥ 2: DOM é¸æ“‡å™¨ï¼ˆGate é é¢çš„ä¸»è¦æ–¹æ³•ï¼‰
                    if views_count is None or views_count == 0:
                        selectors = [
                            "a:has-text(' æ¬¡ç€è¦½'), a:has-text(' views')",    # ä¸»è¦é¸æ“‡å™¨
                            "*:has-text('æ¬¡ç€è¦½'), *:has-text('views')",      # é€šç”¨é¸æ“‡å™¨
                            "span:has-text('æ¬¡ç€è¦½'), span:has-text('views')", # span å…ƒç´ 
                            "text=/\\d+[\\.\\d]*[^\\d]?æ¬¡ç€è¦½/, text=/\\d+.*views?/",  # è™•ç†ã€Œ4 è¬æ¬¡ç€è¦½ã€ç©ºæ ¼å•é¡Œ
                        ]
                        
                        for i, selector in enumerate(selectors):
                            try:
                                element = await page.wait_for_selector(selector, timeout=3000)
                                if element:
                                    view_text = await element.inner_text()
                                    parsed_views = parse_number(view_text)
                                    if parsed_views and parsed_views > 0:
                                        views_count = parsed_views
                                        extraction_method = f"dom_selector_{i+1}"
                                        logging.debug(f"   âœ… DOM é¸æ“‡å™¨ {i+1} ç²å–ç€è¦½æ•¸: {views_count:,}")
                                        break
                            except Exception:
                                continue
                    
                    # æ›´æ–°çµæœ - åªåœ¨ç¾æœ‰ç€è¦½æ•¸ç‚º None æˆ– <= 0 æ™‚æ‰æ›´æ–°
                    if views_count and views_count > 0:
                        if post.views_count is None or post.views_count <= 0:
                            post.views_count = views_count
                            post.views_fetched_at = datetime.utcnow()
                            logging.info(f"  âœ… æˆåŠŸç²å– {post.post_id} çš„ç€è¦½æ•¸: {views_count:,} (æ–¹æ³•: {extraction_method})")
                            
                            # ç™¼å¸ƒé€²åº¦
                            if task_id:
                                await publish_progress(
                                    task_id, 
                                    "views_fetched",
                                    username=username or "unknown",
                                    post_id=post.post_id,
                                    views_count=views_count,
                                    extraction_method=extraction_method,
                                    is_gate_page=is_gate_page
                                )
                        else:
                            logging.info(f"  â„¹ï¸ {post.post_id} å·²æœ‰ç€è¦½æ•¸ {post.views_count:,}ï¼Œè·³éæ›´æ–°")
                    else:
                        if post.views_count is None:
                            logging.warning(f"  âŒ ç„¡æ³•ç²å– {post.post_id} çš„ç€è¦½æ•¸")
                            post.views_count = -1
                            post.views_fetched_at = datetime.utcnow()
                    
                    # éš¨æ©Ÿå»¶é²é¿å…åçˆ¬èŸ²
                    delay = random.uniform(2, 4)
                    await asyncio.sleep(delay)
                    
                except Exception as e:
                    logging.error(f"  âŒ è™•ç† {post.post_id} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                    post.views_count = -1
                    post.views_fetched_at = datetime.utcnow()
                finally:
                    if page:
                        await page.close()

        # åºåˆ—è™•ç†é¿å…ä¸¦ç™¼å•é¡Œï¼ˆæ ¹æ“šæˆåŠŸç¶“é©—ï¼‰
        for post in posts_to_fill:
            await fetch_single_view(post)
        
        return posts_to_fill

    # +++ æ–°å¢ï¼šæ··åˆæå–å™¨ - çµåˆè¨ˆæ•¸æŸ¥è©¢å’ŒDOMè§£æ +++
    async def fill_post_details_from_page(self, posts_to_fill: List[PostMetrics], task_id: str = None, username: str = None) -> List[PostMetrics]:
        """
        ä½¿ç”¨æ··åˆç­–ç•¥è£œé½Šè²¼æ–‡è©³ç´°æ•¸æ“šï¼š
        1. GraphQL è¨ˆæ•¸æŸ¥è©¢ç²å–æº–ç¢ºçš„æ•¸å­—æ•¸æ“š (likes, commentsç­‰)
        2. DOM è§£æç²å–å®Œæ•´çš„å…§å®¹å’Œåª’é«” (content, images, videos)
        é€™ç¨®æ–¹æ³•çµåˆäº†å…©ç¨®æŠ€è¡“çš„å„ªå‹¢ï¼Œæä¾›æœ€ç©©å®šå¯é çš„æ•¸æ“šæå–ã€‚
        """
        if not self.context:
            logging.error("âŒ Browser context æœªåˆå§‹åŒ–ï¼Œç„¡æ³•åŸ·è¡Œ fill_post_details_from_pageã€‚")
            return posts_to_fill

        # æ¸›å°‘ä¸¦ç™¼æ•¸ä»¥é¿å…è§¸ç™¼åçˆ¬èŸ²æ©Ÿåˆ¶
        semaphore = asyncio.Semaphore(1)  # æ›´ä¿å®ˆçš„ä¸¦ç™¼æ•¸
        
        async def fetch_single_details_hybrid(post: PostMetrics):
            async with semaphore:
                page = None
                try:
                    page = await self.context.new_page()
                    
                    logging.debug(f"ğŸ“„ ä½¿ç”¨æ··åˆç­–ç•¥è£œé½Šè©³ç´°æ•¸æ“š: {post.url}")
                    
                    # === æ­¥é©Ÿ 1: æ··åˆç­–ç•¥ - æ””æˆª+é‡ç™¼è«‹æ±‚ï¼ˆæ¨¡ä»¿æˆåŠŸçš„hybridæ–¹æ³•ï¼‰ ===
                    counts_data = {}
                    video_urls = set()
                    captured_graphql_request = {}
                    
                    async def handle_counts_response(response):
                        try:
                            import json
                            url = response.url.lower()
                            headers = response.request.headers
                            query_name = headers.get("x-fb-friendly-name", "")
                            
                            # æ””æˆªè¨ˆæ•¸æŸ¥è©¢è«‹æ±‚ï¼ˆä¿å­˜headerså’Œpayloadï¼‰
                            if ("/graphql" in url and response.status == 200 and 
                                "useBarcelonaBatchedDynamicPostCountsSubscriptionQuery" in query_name):
                                logging.info(f"   ğŸ¯ æ””æˆªåˆ°GraphQLè¨ˆæ•¸æŸ¥è©¢ï¼Œä¿å­˜è«‹æ±‚ä¿¡æ¯...")
                                
                                # ä¿å­˜è«‹æ±‚ä¿¡æ¯ï¼ˆæ¨¡ä»¿hybrid_content_extractor.pyçš„æˆåŠŸç­–ç•¥ï¼‰
                                captured_graphql_request.update({
                                    "headers": dict(response.request.headers),
                                    "payload": response.request.post_data,
                                    "url": "https://www.threads.com/graphql/query"
                                })
                                
                                # æ¸…ç†headers
                                clean_headers = captured_graphql_request["headers"].copy()
                                for h in ["host", "content-length", "accept-encoding"]:
                                    clean_headers.pop(h, None)
                                captured_graphql_request["clean_headers"] = clean_headers
                                
                                logging.info(f"   âœ… æˆåŠŸä¿å­˜GraphQLè«‹æ±‚ä¿¡æ¯ï¼Œæº–å‚™é‡ç™¼...")
                                
                                # ä¹Ÿå˜—è©¦ç›´æ¥è§£æç•¶å‰éŸ¿æ‡‰ï¼ˆä½œç‚ºå‚™ç”¨ï¼‰
                                try:
                                    data = await response.json()
                                    if "data" in data and "data" in data["data"] and "posts" in data["data"]["data"]:
                                        posts_list = data["data"]["data"]["posts"]
                                        if posts_list and len(posts_list) > 0:
                                            post_data = posts_list[0]
                                            if isinstance(post_data, dict):
                                                text_info = post_data.get("text_post_app_info", {}) or {}
                                                counts_data.update({
                                                    "likes": post_data.get("like_count") or 0,
                                                    "comments": text_info.get("direct_reply_count") or 0, 
                                                    "reposts": text_info.get("repost_count") or 0,
                                                    "shares": text_info.get("reshare_count") or 0
                                                })
                                                logging.info(f"   âœ… ç›´æ¥æ””æˆªæˆåŠŸ: è®š={counts_data['likes']}, ç•™è¨€={counts_data['comments']}, è½‰ç™¼={counts_data['reposts']}, åˆ†äº«={counts_data['shares']}")
                                except Exception as e:
                                    logging.debug(f"   âš ï¸ ç›´æ¥è§£æå¤±æ•—: {e}")
                            
                            # æ””æˆªå½±ç‰‡è³‡æº
                            content_type = response.headers.get("content-type", "")
                            resource_type = response.request.resource_type
                            if (resource_type == "media" or 
                                content_type.startswith("video/") or
                                ".mp4" in response.url.lower() or
                                ".m3u8" in response.url.lower() or
                                ".mpd" in response.url.lower()):
                                video_urls.add(response.url)
                                logging.debug(f"   ğŸ¥ æ””æˆªåˆ°å½±ç‰‡: {response.url[:60]}...")
                                
                        except Exception as e:
                            logging.debug(f"   âš ï¸ éŸ¿æ‡‰è™•ç†å¤±æ•—: {e}")
                    
                    page.on("response", handle_counts_response)
                    
                    # === æ­¥é©Ÿ 2: å°èˆªå’Œè§¸ç™¼è¼‰å…¥ ===
                    await page.goto(post.url, wait_until="networkidle", timeout=60000)
                    await asyncio.sleep(3)
                    
                    # === æ­¥é©Ÿ 2.5: æ··åˆç­–ç•¥é‡ç™¼è«‹æ±‚ï¼ˆæ¨¡ä»¿hybrid_content_extractor.pyï¼‰ ===
                    if captured_graphql_request and not counts_data:
                        logging.info(f"   ğŸ”„ ä½¿ç”¨ä¿å­˜çš„GraphQLè«‹æ±‚ä¿¡æ¯é‡ç™¼è«‹æ±‚...")
                        try:
                            import httpx
                            
                            # å¾URLæå–PKï¼ˆå¦‚æœå¯èƒ½ï¼‰
                            target_pk = None
                            url_match = re.search(r'/post/([^/?]+)', post.url)
                            if url_match:
                                # é€™è£¡æˆ‘å€‘éœ€è¦å¾æŸå€‹åœ°æ–¹ç²å–PKï¼Œæˆ–è€…ç”¨å…¶ä»–æ–¹å¼
                                logging.info(f"   ğŸ” URLä»£ç¢¼: {url_match.group(1)}")
                            
                            # æº–å‚™é‡ç™¼è«‹æ±‚
                            headers = captured_graphql_request["clean_headers"]
                            payload = captured_graphql_request["payload"]
                            
                            # å¾é é¢contextç²å–cookies
                            cookies_list = await self.context.cookies()
                            cookies = {cookie['name']: cookie['value'] for cookie in cookies_list}
                            
                            # ç¢ºä¿æœ‰èªè­‰
                            if not headers.get("authorization") and 'ig_set_authorization' in cookies:
                                auth_value = cookies['ig_set_authorization']
                                headers["authorization"] = f"Bearer {auth_value}" if not auth_value.startswith('Bearer') else auth_value
                            
                            # ç™¼é€HTTPè«‹æ±‚åˆ°Threads API
                            async with httpx.AsyncClient(headers=headers, cookies=cookies, timeout=30.0, http2=True) as client:
                                api_response = await client.post("https://www.threads.com/graphql/query", data=payload)
                                
                                if api_response.status_code == 200:
                                    result = api_response.json()
                                    logging.info(f"   âœ… é‡ç™¼è«‹æ±‚æˆåŠŸï¼Œç‹€æ…‹: {api_response.status_code}")
                                    
                                    if "data" in result and result["data"] and "data" in result["data"] and "posts" in result["data"]["data"]:
                                        posts_list = result["data"]["data"]["posts"]
                                        logging.info(f"   ğŸ“Š é‡ç™¼è«‹æ±‚éŸ¿æ‡‰åŒ…å« {len(posts_list)} å€‹è²¼æ–‡")
                                        
                                        # ä½¿ç”¨ç¬¬ä¸€å€‹è²¼æ–‡ï¼ˆç•¶å‰é é¢çš„ä¸»è¦è²¼æ–‡ï¼‰
                                        if posts_list and len(posts_list) > 0:
                                            post_data = posts_list[0]
                                            if isinstance(post_data, dict):
                                                text_info = post_data.get("text_post_app_info", {}) or {}
                                                counts_data.update({
                                                    "likes": post_data.get("like_count") or 0,
                                                    "comments": text_info.get("direct_reply_count") or 0, 
                                                    "reposts": text_info.get("repost_count") or 0,
                                                    "shares": text_info.get("reshare_count") or 0
                                                })
                                                logging.info(f"   ğŸ¯ é‡ç™¼è«‹æ±‚æˆåŠŸç²å–æ•¸æ“š: è®š={counts_data['likes']}, ç•™è¨€={counts_data['comments']}, è½‰ç™¼={counts_data['reposts']}, åˆ†äº«={counts_data['shares']}")
                                else:
                                    logging.warning(f"   âš ï¸ é‡ç™¼è«‹æ±‚å¤±æ•—ï¼Œç‹€æ…‹: {api_response.status_code}")
                                
                        except Exception as e:
                            logging.warning(f"   âš ï¸ é‡ç™¼è«‹æ±‚éç¨‹å¤±æ•—: {e}")
                    
                    # å˜—è©¦è§¸ç™¼å½±ç‰‡è¼‰å…¥
                    try:
                        trigger_selectors = [
                            'div[data-testid="media-viewer"]',
                            'video',
                            'div[role="button"][aria-label*="play"]',
                            'div[role="button"][aria-label*="æ’­æ”¾"]',
                            '[data-pressable-container] div[style*="video"]'
                        ]
                        
                        for selector in trigger_selectors:
                            try:
                                elements = page.locator(selector)
                                count = await elements.count()
                                if count > 0:
                                    await elements.first.click(timeout=3000)
                                    await asyncio.sleep(2)
                                    break
                            except:
                                continue
                    except:
                        pass
                    
                    # === æ­¥é©Ÿ 3: DOM å…§å®¹æå– ===
                    content_data = {}
                    
                    try:
                        # æå–ç”¨æˆ¶åï¼ˆå¾ URLï¼‰
                        import re as regex
                        url_match = regex.search(r'/@([^/]+)/', post.url)
                        content_data["username"] = url_match.group(1) if url_match else username or ""
                        
                        # æå–å…§å®¹æ–‡å­—
                        content = ""
                        content_selectors = [
                            'div[data-pressable-container] span',
                            '[data-testid="thread-text"]',
                            'article div[dir="auto"]',
                            'div[role="article"] div[dir="auto"]'
                        ]
                        
                        for selector in content_selectors:
                            try:
                                elements = page.locator(selector)
                                count = await elements.count()
                                
                                for i in range(min(count, 20)):
                                    try:
                                        text = await elements.nth(i).inner_text()
                                        if (text and len(text.strip()) > 10 and 
                                            not text.strip().isdigit() and
                                            "å°æ™‚" not in text and "åˆ†é˜" not in text and
                                            not text.startswith("@")):
                                            content = text.strip()
                                            break
                                    except:
                                        continue
                                
                                if content:
                                    break
                            except:
                                continue
                        
                        content_data["content"] = content
                        
                        # æå–åœ–ç‰‡ï¼ˆéæ¿¾é ­åƒï¼‰
                        images = []
                        img_elements = page.locator('img')
                        img_count = await img_elements.count()
                        
                        for i in range(min(img_count, 50)):
                            try:
                                img_elem = img_elements.nth(i)
                                img_src = await img_elem.get_attribute("src")
                                
                                if not img_src or not ("fbcdn" in img_src or "cdninstagram" in img_src):
                                    continue
                                
                                if ("rsrc.php" in img_src or "static.cdninstagram.com" in img_src):
                                    continue
                                
                                # æª¢æŸ¥å°ºå¯¸éæ¿¾é ­åƒ
                                try:
                                    width = int(await img_elem.get_attribute("width") or 0)
                                    height = int(await img_elem.get_attribute("height") or 0)
                                    max_size = max(width, height)
                                    
                                    if max_size > 150 and img_src not in images:
                                        images.append(img_src)
                                except:
                                    if ("t51.2885-15" in img_src or "scontent" in img_src) and img_src not in images:
                                        images.append(img_src)
                            except:
                                continue
                        
                        content_data["images"] = images
                        
                        # æå–å½±ç‰‡ï¼ˆçµåˆç¶²è·¯æ””æˆªå’ŒDOMï¼‰
                        videos = list(video_urls)
                        
                        # DOM ä¸­çš„ video æ¨™ç±¤
                        video_elements = page.locator('video')
                        video_count = await video_elements.count()
                        
                        for i in range(video_count):
                            try:
                                video_elem = video_elements.nth(i)
                                src = await video_elem.get_attribute("src")
                                data_src = await video_elem.get_attribute("data-src")
                                poster = await video_elem.get_attribute("poster")
                                
                                if src and src not in videos:
                                    videos.append(src)
                                if data_src and data_src not in videos:
                                    videos.append(data_src)
                                if poster and poster not in videos:
                                    videos.append(f"POSTER::{poster}")
                                
                                # source å­å…ƒç´ 
                                sources = video_elem.locator('source')
                                source_count = await sources.count()
                                for j in range(source_count):
                                    source_src = await sources.nth(j).get_attribute("src")
                                    if source_src and source_src not in videos:
                                        videos.append(source_src)
                            except:
                                continue
                        
                        content_data["videos"] = videos
                        
                    except Exception as e:
                        logging.debug(f"   âš ï¸ DOM å…§å®¹æå–å¤±æ•—: {e}")
                    
                    # === æ­¥é©Ÿ 3.5: DOM è¨ˆæ•¸å¾Œæ´ï¼ˆç•¶ GraphQL æ””æˆªå¤±æ•—æ™‚ï¼‰ ===
                    if not counts_data:
                        logging.warning(f"   ğŸ”„ GraphQL è¨ˆæ•¸æ””æˆªå¤±æ•—ï¼Œé–‹å§‹ DOM è¨ˆæ•¸å¾Œæ´...")
                        
                        # å…ˆæª¢æŸ¥é é¢ç‹€æ…‹
                        page_title = await page.title()
                        page_url = page.url
                        logging.info(f"   ğŸ“„ é é¢ç‹€æ…‹ - æ¨™é¡Œ: {page_title}, URL: {page_url}")
                        
                        count_selectors = {
                            "likes": [
                                # English selectors
                                "button[aria-label*='likes'] span",
                                "button[aria-label*='Like'] span", 
                                "span:has-text(' likes')",
                                "span:has-text(' like')",
                                "button svg[aria-label='Like'] + span",
                                "button[aria-label*='like']",
                                # Chinese selectors
                                "button[aria-label*='å€‹å–œæ­¡'] span",
                                "button[aria-label*='å–œæ­¡']",
                                # Generic patterns
                                "button[data-testid*='like'] span",
                                "div[role='button'][aria-label*='like'] span"
                            ],
                            "comments": [
                                # English selectors
                                "a[href$='#comments'] span",
                                "span:has-text(' comments')",
                                "span:has-text(' comment')",
                                "a:has-text('comments')",
                                "button[aria-label*='comment'] span",
                                # Chinese selectors
                                "span:has-text(' å‰‡ç•™è¨€')",
                                "a:has-text('å‰‡ç•™è¨€')",
                                # Generic patterns
                                "button[data-testid*='comment'] span",
                                "div[role='button'][aria-label*='comment'] span"
                            ],
                            "reposts": [
                                # English selectors
                                "span:has-text(' reposts')",
                                "span:has-text(' repost')",
                                "button[aria-label*='repost'] span",
                                "a:has-text('reposts')",
                                # Chinese selectors
                                "span:has-text(' æ¬¡è½‰ç™¼')",
                                "a:has-text('è½‰ç™¼')",
                                # Generic patterns
                                "button[data-testid*='repost'] span"
                            ],
                            "shares": [
                                # English selectors
                                "span:has-text(' shares')",
                                "span:has-text(' share')",
                                "button[aria-label*='share'] span",
                                "a:has-text('shares')",
                                # Chinese selectors
                                "span:has-text(' æ¬¡åˆ†äº«')",
                                "a:has-text('åˆ†äº«')",
                                # Generic patterns
                                "button[data-testid*='share'] span"
                            ],
                        }
                        
                        # å…ˆé€²è¡Œé€šç”¨å…ƒç´ æƒæï¼Œä¸¦æ™ºèƒ½æå–æ•¸å­—
                        logging.info(f"   ğŸ” é€šç”¨å…ƒç´ æƒæå’Œæ™ºèƒ½æ•¸å­—æå–...")
                        dom_counts = {}
                        try:
                            all_buttons = await page.locator('button').all_inner_texts()
                            all_spans = await page.locator('span').all_inner_texts()
                            number_elements = [text for text in (all_buttons + all_spans) if any(char.isdigit() for char in text)]
                            logging.info(f"   ğŸ”¢ æ‰¾åˆ°åŒ…å«æ•¸å­—çš„å…ƒç´ : {number_elements[:20]}")
                            
                            # === ğŸ¯ æ™ºèƒ½æ•¸å­—è­˜åˆ¥ï¼šå¾æ‰¾åˆ°çš„æ•¸å­—ä¸­æå–ç¤¾äº¤æ•¸æ“š ===
                            pure_numbers = []
                            for text in number_elements:
                                # è·³éæ˜é¡¯ä¸æ˜¯äº’å‹•æ•¸æ“šçš„æ–‡å­—
                                if any(skip in text for skip in ['ç€è¦½', 'æ¬¡ç€è¦½', 'è§€çœ‹', 'å¤©', 'å°æ™‚', 'åˆ†é˜', 'ç§’', 'on.natgeo.com']):
                                    continue
                                    
                                number = parse_number(text)
                                if number and number > 0:
                                    pure_numbers.append((number, text))
                                    logging.info(f"   ğŸ“Š æå–æ•¸å­—: {number} (å¾ '{text}')")
                            
                            # æ ¹æ“šæ•¸å­—å¤§å°æ™ºèƒ½åˆ†é…ï¼ˆé€šå¸¸ï¼šlikes > comments > reposts > sharesï¼‰
                            pure_numbers.sort(reverse=True)  # å¾å¤§åˆ°å°æ’åº
                            
                            if len(pure_numbers) >= 4:
                                dom_counts["likes"] = pure_numbers[0][0]
                                dom_counts["comments"] = pure_numbers[1][0] 
                                dom_counts["reposts"] = pure_numbers[2][0]
                                dom_counts["shares"] = pure_numbers[3][0]
                                logging.info(f"   ğŸ¯ æ™ºèƒ½åˆ†é…4å€‹æ•¸å­—: è®š={dom_counts['likes']}, ç•™è¨€={dom_counts['comments']}, è½‰ç™¼={dom_counts['reposts']}, åˆ†äº«={dom_counts['shares']}")
                            elif len(pure_numbers) >= 2:
                                dom_counts["likes"] = pure_numbers[0][0]
                                dom_counts["comments"] = pure_numbers[1][0]
                                logging.info(f"   ğŸ¯ æ™ºèƒ½åˆ†é…2å€‹æ•¸å­—: è®š={dom_counts['likes']}, ç•™è¨€={dom_counts['comments']}")
                            elif len(pure_numbers) >= 1:
                                dom_counts["likes"] = pure_numbers[0][0]
                                logging.info(f"   ğŸ¯ æ™ºèƒ½åˆ†é…1å€‹æ•¸å­—: è®š={dom_counts['likes']}")
                                
                        except Exception as e:
                            logging.warning(f"   âš ï¸ æ™ºèƒ½æ•¸å­—æå–å¤±æ•—: {e}")
                        
                        # å¦‚æœæ™ºèƒ½æå–æˆåŠŸï¼Œè·³éå‚³çµ±é¸æ“‡å™¨ï¼›å¦å‰‡ç¹¼çºŒå˜—è©¦
                        if not dom_counts:
                            logging.info(f"   âš ï¸ æ™ºèƒ½æå–å¤±æ•—ï¼Œå›åˆ°å‚³çµ±é¸æ“‡å™¨...")
                            for key, sels in count_selectors.items():
                                logging.info(f"   ğŸ” å˜—è©¦æå– {key} æ•¸æ“š...")
                                for i, sel in enumerate(sels):
                                    try:
                                        el = page.locator(sel).first
                                        count = await el.count()
                                        if count > 0:
                                            text = (await el.inner_text()).strip()
                                            logging.info(f"   ğŸ“ é¸æ“‡å™¨ {i+1}/{len(sels)} '{sel}' æ‰¾åˆ°æ–‡å­—: '{text}'")
                                            n = parse_number(text)
                                            if n and n > 0:
                                                dom_counts[key] = n
                                                logging.info(f"   âœ… DOM æˆåŠŸæå– {key}: {n} (é¸æ“‡å™¨: {sel})")
                                                break
                                            else:
                                                logging.info(f"   âš ï¸ ç„¡æ³•è§£ææ•¸å­—: '{text}' -> {n}")
                                        else:
                                            logging.info(f"   âŒ é¸æ“‡å™¨ {i+1}/{len(sels)} æœªæ‰¾åˆ°å…ƒç´ : '{sel}'")
                                    except Exception as e:
                                        logging.info(f"   âš ï¸ é¸æ“‡å™¨ {i+1}/{len(sels)} '{sel}' éŒ¯èª¤: {e}")
                                        continue
                                
                                if key not in dom_counts:
                                    logging.warning(f"   âŒ ç„¡æ³•æ‰¾åˆ° {key} æ•¸æ“š")
                        
                        if dom_counts:
                            counts_data = {
                                "likes": dom_counts.get("likes", 0),
                                "comments": dom_counts.get("comments", 0),
                                "reposts": dom_counts.get("reposts", 0),
                                "shares": dom_counts.get("shares", 0),
                            }
                            logging.info(f"   ğŸ¯ DOM è¨ˆæ•¸å¾Œæ´æˆåŠŸ: {counts_data}")
                        else:
                            # æ‰€æœ‰æ–¹æ³•éƒ½å¤±æ•—æ™‚ï¼Œè¨˜éŒ„é é¢ç‹€æ…‹ç”¨æ–¼èª¿è©¦
                            logging.warning(f"   âŒ GraphQLæ””æˆªå’ŒDOMå¾Œæ´éƒ½å¤±æ•—äº†ï¼")
                            try:
                                page_title = await page.title()
                                page_url = page.url
                                logging.info(f"   ğŸ“„ å¤±æ•—é é¢åˆ†æ - æ¨™é¡Œ: {page_title}")
                                logging.info(f"   ğŸ”— å¤±æ•—é é¢åˆ†æ - URL: {page_url}")
                                
                                # æª¢æŸ¥é é¢æ˜¯å¦æ­£å¸¸è¼‰å…¥
                                all_text = await page.inner_text('body')
                                if "ç™»å…¥" in all_text or "login" in all_text.lower():
                                    logging.warning(f"   âš ï¸ å¯èƒ½é‡åˆ°ç™»å…¥é é¢")
                                elif len(all_text) < 100:
                                    logging.warning(f"   âš ï¸ é é¢å…§å®¹å¤ªå°‘ï¼Œå¯èƒ½è¼‰å…¥å¤±æ•—")
                                else:
                                    logging.info(f"   ğŸ“ é é¢å…§å®¹é•·åº¦: {len(all_text)} å­—å…ƒ")
                                    
                                    # æª¢æŸ¥æ˜¯å¦æœ‰äº’å‹•æŒ‰éˆ•
                                    like_buttons = await page.locator('[aria-label*="like"], [aria-label*="Like"], [aria-label*="å–œæ­¡"]').count()
                                    comment_buttons = await page.locator('[aria-label*="comment"], [aria-label*="Comment"], [aria-label*="ç•™è¨€"]').count()
                                    logging.info(f"   ğŸ“Š æ‰¾åˆ°æŒ‰éˆ•: è®š {like_buttons} å€‹, ç•™è¨€ {comment_buttons} å€‹")
                                    
                                    # å˜—è©¦æ‰¾åˆ°ä»»ä½•æ•¸å­—
                                    all_numbers = await page.locator(':text-matches("\\d+")').all_inner_texts()
                                    if all_numbers:
                                        logging.info(f"   ğŸ”¢ é é¢æ‰€æœ‰æ•¸å­—: {all_numbers[:15]}")  # é¡¯ç¤ºå‰15å€‹
                                    
                                    # æª¢æŸ¥æ˜¯å¦æœ‰é˜»æ“‹å…ƒç´ 
                                    modal_count = await page.locator('[role="dialog"], .modal, [data-testid*="modal"]').count()
                                    if modal_count > 0:
                                        logging.warning(f"   âš ï¸ ç™¼ç¾ {modal_count} å€‹æ¨¡æ…‹æ¡†å¯èƒ½é˜»æ“‹å…§å®¹")
                                        
                            except Exception as debug_e:
                                logging.warning(f"   âš ï¸ å¤±æ•—é é¢åˆ†æéŒ¯èª¤: {debug_e}")
                    
                    # === æ­¥é©Ÿ 4: æ›´æ–°è²¼æ–‡æ•¸æ“š ===
                    updated = False
                    
                    # æ›´æ–°è¨ˆæ•¸æ•¸æ“š - åªåœ¨ç¾æœ‰æ•¸æ“šç‚º None æˆ– 0 æ™‚æ‰æ›´æ–°
                    if counts_data:
                        if post.likes_count in (None, 0) and (counts_data.get("likes") or 0) > 0:
                            post.likes_count = counts_data["likes"]
                            updated = True
                        if post.comments_count in (None, 0) and (counts_data.get("comments") or 0) > 0:
                            post.comments_count = counts_data["comments"]
                            updated = True
                        if post.reposts_count in (None, 0) and (counts_data.get("reposts") or 0) > 0:
                            post.reposts_count = counts_data["reposts"]
                            updated = True
                        if post.shares_count in (None, 0) and (counts_data.get("shares") or 0) > 0:
                            post.shares_count = counts_data["shares"]
                            updated = True
                    
                    # æ›´æ–°å…§å®¹æ•¸æ“š - åªåœ¨ç¾æœ‰æ•¸æ“šç‚ºç©ºæ™‚æ‰æ›´æ–°
                    if content_data.get("content") and not post.content:
                        post.content = content_data["content"]
                        updated = True
                    
                    if content_data.get("images") and not post.images:
                        post.images = content_data["images"]
                        updated = True
                    
                    if content_data.get("videos") and not post.videos:
                        # éæ¿¾å¯¦éš›å½±ç‰‡ï¼ˆæ’é™¤ POSTERï¼‰
                        actual_videos = [v for v in content_data["videos"] if not v.startswith("POSTER::")]
                        if actual_videos:
                            post.videos = actual_videos
                            updated = True
                    
                    if updated:
                        post.processing_stage = "details_filled_hybrid"
                        logging.info(f"  âœ… æ··åˆç­–ç•¥æˆåŠŸè£œé½Š {post.post_id}: è®š={post.likes_count}, å…§å®¹={len(post.content)}å­—, åœ–ç‰‡={len(post.images)}å€‹, å½±ç‰‡={len(post.videos)}å€‹")
                        
                        # ç™¼å¸ƒé€²åº¦
                        if task_id:
                            await publish_progress(
                                task_id, 
                                "details_fetched_hybrid",
                                username=content_data.get("username", username or "unknown"),
                                post_id=post.post_id,
                                likes_count=post.likes_count,
                                content_length=len(post.content),
                                media_count=len(post.images) + len(post.videos)
                            )
                    else:
                        post.processing_stage = "details_failed"
                        logging.warning(f"  âš ï¸ æ··åˆç­–ç•¥ç„¡æ³•è£œé½Š {post.post_id} çš„æ•¸æ“š")
                    
                    # éš¨æ©Ÿå»¶é²é¿å…åçˆ¬èŸ²
                    delay = random.uniform(2, 4)
                    await asyncio.sleep(delay)
                    
                except Exception as e:
                    logging.error(f"  âŒ æ··åˆç­–ç•¥è™•ç† {post.post_id} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                    post.processing_stage = "details_failed"
                finally:
                    if page:
                        await page.close()

        # åºåˆ—è™•ç†ä¿æŒé †åº
        for post in posts_to_fill:
            await fetch_single_details_hybrid(post)
        
        return posts_to_fill


