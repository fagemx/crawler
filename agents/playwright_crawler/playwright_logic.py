import json
import asyncio
import logging
import random
import re
import tempfile
import uuid
from pathlib import Path
from typing import Dict, List, Optional, AsyncIterable, Callable, Any
from datetime import datetime

from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeoutError, Browser, BrowserContext, Page, Route, APIResponse

from common.settings import get_settings
from common.a2a import stream_text, stream_data, stream_status, stream_error, TaskState
from common.models import PostMetrics, PostMetricsBatch
from common.nats_client import publish_progress
from common.utils import (
    get_best_video_url,
    get_best_image_url,
    generate_post_url,
    first_of,
    parse_thread_item
)

# èª¿è©¦æª”æ¡ˆè·¯å¾‘
DEBUG_DIR = Path(__file__).parent / "debug"
DEBUG_DIR.mkdir(exist_ok=True)
DEBUG_FAILED_ITEM_FILE = DEBUG_DIR / "failed_post_sample.json"
SAMPLE_THREAD_ITEM_FILE = DEBUG_DIR / "sample_thread_item.json"
RAW_CRAWL_DATA_FILE = DEBUG_DIR / "raw_crawl_data.json"

# è¨­å®šæ—¥èªŒ
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# --- GraphQL API è©³ç´°è³‡è¨Š ---
GRAPHQL_RE = re.compile(r"/graphql/query")
# ä¿ç•™ç”¨ä¾† debugï¼›å¯¦éš›é‚è¼¯æ”¹æˆã€Œçœ‹çµæ§‹ã€å³å¯
POST_QUERY_NAMES = set()

# --- å¼·å¥çš„æ¬„ä½å°ç…§è¡¨ ---
FIELD_MAP = {
    "like_count": [
        "like_count", "likeCount", 
        ["feedback_info", "aggregated_like_count"],
        ["like_info", "count"]
    ],
    "comment_count": [
        ["text_post_app_info", "direct_reply_count"],
        "comment_count", "commentCount",
        ["reply_info", "count"]
    ],
    "share_count": [
        ["text_post_app_info", "reshare_count"],  # çœŸæ­£çš„ shares æ¬„ä½
        "reshareCount", "share_count", "shareCount"
    ],
    "repost_count": [
        ["text_post_app_info", "repost_count"],  # reposts æ¬„ä½
        "repostCount", "repost_count"
    ],
    "content": [
        ["caption", "text"],
        "caption", "text", "content"
    ],
    "author": [
        ["user", "username"], 
        ["owner", "username"],
        ["user", "handle"]
    ],
    "created_at": [
        "taken_at", "taken_at_timestamp", 
        "publish_date", "created_time"
    ],
    "post_id": [
        "pk", "id", "post_id"
    ],
    "code": [
        "code", "shortcode", "media_code"
    ],
    # å¢åŠ ç›´æ¥å¾ API å˜—è©¦ç²å– views çš„è·¯å¾‘
    "view_count": [
        ["feedback_info", "view_count"],
        ["video_info", "play_count"],
        "view_count",
        "views",
        "impression_count"
    ],
}

def parse_post_data(thread_item: Dict[str, Any], username: str) -> Optional[PostMetrics]:
    """
    å¼·å¥çš„è²¼æ–‡è§£æå™¨ï¼Œä½¿ç”¨å¤šéµ fallback æ©Ÿåˆ¶è™•ç†æ¬„ä½è®Šå‹•ã€‚
    æ”¯æ´ Threads GraphQL API çš„ä¸åŒç‰ˆæœ¬å’Œæ¬„ä½å‘½åè®ŠåŒ–ã€‚
    """
    # ä½¿ç”¨æ™ºèƒ½æœå°‹æ‰¾åˆ°çœŸæ­£çš„è²¼æ–‡ç‰©ä»¶
    post = parse_thread_item(thread_item)
    
    if not post:
        logging.info(f"âŒ æ‰¾ä¸åˆ°æœ‰æ•ˆçš„ post ç‰©ä»¶ï¼Œæ”¶åˆ°çš„è³‡æ–™éµ: {list(thread_item.keys())}")
        logging.info(f"âŒ thread_item å…§å®¹ç¯„ä¾‹: {str(thread_item)[:300]}...")
        
        # è‡ªå‹•å„²å­˜ç¬¬ä¸€ç­† raw JSON ä¾›åˆ†æ
        try:
            if not DEBUG_FAILED_ITEM_FILE.exists():
                DEBUG_FAILED_ITEM_FILE.write_text(
                    json.dumps(thread_item, indent=2, ensure_ascii=False),
                    encoding="utf-8" # æ˜ç¢ºæŒ‡å®š UTF-8
                )
                logging.info(f"ğŸ“ å·²å„²å­˜å¤±æ•—ç¯„ä¾‹è‡³ {DEBUG_FAILED_ITEM_FILE}")
        except Exception:
            pass  # å¯«æª”å¤±æ•—ä¸å½±éŸ¿ä¸»è¦åŠŸèƒ½
            
        return None

    # ä½¿ç”¨å¼·å¥çš„æ¬„ä½è§£æ
    post_id = first_of(post, *FIELD_MAP["post_id"])
    code = first_of(post, *FIELD_MAP["code"])
    
    if not post_id:
        logging.info(f"âŒ æ‰¾ä¸åˆ°æœ‰æ•ˆçš„ post_idï¼Œå¯ç”¨æ¬„ä½: {list(post.keys())}")
        return None
        
    if not code:
        logging.info(f"âŒ æ‰¾ä¸åˆ°æœ‰æ•ˆçš„ codeï¼Œå¯ç”¨æ¬„ä½: {list(post.keys())}")
        return None

    # --- URL ä¿®å¾©ï¼šä½¿ç”¨æ­£ç¢ºçš„æ ¼å¼ ---
    # èˆŠæ ¼å¼ (éŒ¯èª¤): f"https://www.threads.net/t/{code}"
    # æ–°æ ¼å¼ (æ­£ç¢º): f"https://www.threads.com/@{username}/post/{code}"
    url = generate_post_url(username, code)

    # ä½¿ç”¨å¤šéµ fallback è§£ææ‰€æœ‰æ¬„ä½
    author = first_of(post, *FIELD_MAP["author"]) or username
    content = first_of(post, *FIELD_MAP["content"]) or ""
    like_count = first_of(post, *FIELD_MAP["like_count"]) or 0
    comment_count = first_of(post, *FIELD_MAP["comment_count"]) or 0
    share_count = first_of(post, *FIELD_MAP["share_count"]) or 0
    repost_count = first_of(post, *FIELD_MAP["repost_count"]) or 0
    created_at = first_of(post, *FIELD_MAP["created_at"])
    
    # åµæ¸¬æœªçŸ¥æ¬„ä½ï¼ˆç”¨æ–¼èª¿è©¦å’Œæ”¹é€²ï¼‰
    unknown_keys = set(post.keys()) - {
        'pk', 'id', 'post_id', 'code', 'shortcode', 'media_code',
        'user', 'owner', 'caption', 'text', 'content',
        'like_count', 'likeCount', 'feedback_info', 'like_info',
        'text_post_app_info', 'comment_count', 'commentCount', 'reply_info',
        'taken_at', 'taken_at_timestamp', 'publish_date', 'created_time',
        # å¸¸è¦‹ä½†ä¸é‡è¦çš„æ¬„ä½
        'media_type', 'has_liked', 'image_versions2', 'video_versions',
        'carousel_media', 'location', 'user_tags', 'usertags'
    }
    
    if unknown_keys:
        logging.debug(f"ğŸ” ç™¼ç¾æœªçŸ¥æ¬„ä½: {unknown_keys}")
        # å¯ä»¥é¸æ“‡å¯«å…¥æª”æ¡ˆä»¥ä¾›åˆ†æ
        try:
            with open("unknown_fields.log", "a", encoding="utf-8") as f:
                f.write(f"{json.dumps(list(unknown_keys))}\n")
        except Exception:
            pass  # å¯«æª”å¤±æ•—ä¸å½±éŸ¿ä¸»è¦åŠŸèƒ½

    # --- MEDIA -------------------------------------------------------------
    images, videos = [], []

    def append_image(url):
        if url and url not in images:
            images.append(url)

    def append_video(url):
        if url and url not in videos:
            videos.append(url)

    def best_candidate(candidates: list[dict], prefer_mp4=False):
        """
        å¾ image_versions2 æˆ– video_versions è£¡æŒ‘ã€Œå¯¬åº¦æœ€å¤§çš„é‚£å€‹ã€URL
        """
        if not candidates:
            return None
        key = "url"
        return max(candidates, key=lambda c: c.get("width", 0)).get(key)

    # 1) å–®åœ– / å–®ç‰‡
    if "image_versions2" in post:
        append_image(best_candidate(post["image_versions2"].get("candidates", [])))
    if "video_versions" in post:
        append_video(best_candidate(post.get("video_versions", []), prefer_mp4=True))

    # 2) è¼ªæ’­ carousel_media (åŠ å…¥ None çš„å®‰å…¨è™•ç†)
    for media in post.get("carousel_media") or []:
        if "image_versions2" in media:
            append_image(best_candidate(media["image_versions2"].get("candidates", [])))
        if "video_versions" in media:
            append_video(best_candidate(media.get("video_versions", []), prefer_mp4=True))
    # -----------------------------------------------------------------------


    # æˆåŠŸè§£æï¼Œè¨˜éŒ„éƒ¨åˆ†è³‡è¨Šä¾›é™¤éŒ¯
    logging.info(f"âœ… æˆåŠŸè§£æè²¼æ–‡ {post_id}: ä½œè€…={author}, è®šæ•¸={like_count}, åœ–ç‰‡={len(images)}, å½±ç‰‡={len(videos)}")
    
    return PostMetrics(
        url=url,
        post_id=str(post_id),
        username=username,
        source="playwright",
        processing_stage="playwright_crawled",
        likes_count=int(like_count) if isinstance(like_count, (int, str)) and str(like_count).isdigit() else 0,
        comments_count=int(comment_count) if isinstance(comment_count, (int, str)) and str(comment_count).isdigit() else 0,
        reposts_count=int(repost_count) if isinstance(repost_count, (int, str)) and str(repost_count).isdigit() else 0,
        shares_count=int(share_count) if isinstance(share_count, (int, str)) and str(share_count).isdigit() else 0,
        content=content,
        created_at=created_at,
        images=images,
        videos=videos,
        # æ–°å¢ï¼šç›´æ¥å¾ API è§£æ views_countï¼ˆæŒ‰æŒ‡å¼•å„ªå…ˆå˜—è©¦ APIï¼‰
        views_count=first_of(post, *FIELD_MAP["view_count"]) if first_of(post, *FIELD_MAP["view_count"]) is not None else None,
    )

# +++ æ–°å¢ï¼šå¾å‰ç«¯è§£æç€è¦½æ•¸çš„è¼”åŠ©å‡½å¼ +++
def parse_views_text(text: Optional[str]) -> Optional[int]:
    """å°‡ '161.9è¬æ¬¡ç€è¦½' æˆ– '1.2M views' é€™é¡æ–‡å­—è½‰æ›ç‚ºæ•´æ•¸"""
    if not text:
        return None
    try:
        original_text = text
        text_lower = text.lower().replace(",", "").strip()
        
        # ç§»é™¤ã€Œæ¬¡ç€è¦½ã€å’Œã€Œviewsã€ç­‰å¾Œç¶´
        text_clean = text_lower.replace("æ¬¡ç€è¦½", "").replace("views", "").replace("view", "").strip()
        
        # æå–æ•¸å­—éƒ¨åˆ†
        num_part = re.findall(r"[\d\.]+", text_clean)
        if not num_part:
            logging.debug(f"ğŸ” ç„¡æ³•å¾ '{original_text}' ä¸­æå–æ•¸å­—")
            return None
        
        num = float(num_part[0])
        
        # è™•ç†ä¸­æ–‡å–®ä½
        if "è¬" in text:
            result = int(num * 10000)
        elif "å„„" in text:
            result = int(num * 100000000)
        # è™•ç†è‹±æ–‡å–®ä½
        elif "m" in text_lower:
            result = int(num * 1000000)
        elif "k" in text_lower:
            result = int(num * 1000)
        else:
            result = int(num)
            
        logging.debug(f"ğŸ” æˆåŠŸè§£æ '{original_text}' -> {result}")
        return result
        
    except (ValueError, IndexError) as e:
        logging.warning(f"âš ï¸ ç„¡æ³•è§£æç€è¦½æ•¸æ–‡å­—: '{text}' - {e}")
        return None

class PlaywrightLogic:
    """
    ä½¿ç”¨ Playwright é€²è¡Œçˆ¬èŸ²çš„æ ¸å¿ƒé‚è¼¯ã€‚
    """
    def __init__(self):
        self.settings = get_settings().playwright
        self.known_queries = set()  # è¿½è¹¤å·²è¦‹éçš„æŸ¥è©¢åç¨±
        self.context = None  # åˆå§‹åŒ– context

    def _build_response_handler(self, username: str, posts: dict, task_id: str, max_posts: int, stream_callback):
        """å»ºç«‹ GraphQL å›æ‡‰è™•ç†å™¨ - ä½¿ç”¨çµæ§‹åˆ¤æ–·è€Œéåç¨±ç™½åå–®"""
        async def _handle_response(res):
            if not GRAPHQL_RE.search(res.url):
                return
                
            qname = res.request.headers.get("x-fb-friendly-name", "UNKNOWN")
            try:
                data = await res.json()
            except Exception:
                return  # é JSON å›æ‡‰ï¼Œç›´æ¥è·³é
                
            # çµæ§‹åˆ¤æ–·ï¼šåªè¦æœ‰ data.mediaData.edges å°±æ˜¯è²¼æ–‡è³‡æ–™
            edges = data.get("data", {}).get("mediaData", {}).get("edges", [])
            if not edges:
                return  # è·Ÿè²¼æ–‡ç„¡é—œï¼Œå¿½ç•¥
                
            # è§£æè²¼æ–‡ - æ”¯æ´æ–°èˆŠçµæ§‹çš„ fallback
            new_count = 0
            for edge in edges:
                # æ”¯æ´ thread_items / items çš„ fallback
                node = edge.get("node", {})
                thread_items = node.get("thread_items") or node.get("items") or []
                
                # è‡ªå‹•å„²å­˜ç¬¬ä¸€ç­†æˆåŠŸçš„ thread_item ä¾›åˆ†æ
                if thread_items and new_count == 0:
                    try:
                        if not SAMPLE_THREAD_ITEM_FILE.exists():
                            SAMPLE_THREAD_ITEM_FILE.write_text(
                                json.dumps(thread_items[0], indent=2, ensure_ascii=False),
                                encoding="utf-8" # æ˜ç¢ºæŒ‡å®š UTF-8
                            )
                            logging.info(f"ğŸ“ å·²å„²å­˜æˆåŠŸç¯„ä¾‹è‡³ {SAMPLE_THREAD_ITEM_FILE}")
                    except Exception:
                        pass

                for item in thread_items:
                    parsed_post = parse_post_data(item, username)
                    if parsed_post and parsed_post.post_id not in posts:
                        posts[parsed_post.post_id] = parsed_post
                        new_count += 1
                        
            if new_count > 0:
                logging.info(f"âœ… [{qname}] +{new_count} (ç¸½ {len(posts)}/{max_posts})")
                # ä½¿ç”¨å›èª¿å‡½æ•¸ç™¼é€ä¸²æµè¨Šæ¯ (å¦‚æœæœ‰çš„è©±)
                if stream_callback:
                    stream_callback(f"âœ… å¾ {qname} è§£æåˆ° {new_count} å‰‡æ–°è²¼æ–‡ï¼Œç¸½æ•¸: {len(posts)}")
            
            # æª¢æŸ¥æ˜¯å¦å·²é”åˆ°ç›®æ¨™æ•¸é‡
            if len(posts) >= max_posts:
                logging.info(f"é”åˆ°ç›®æ¨™è²¼æ–‡æ•¸ {max_posts}ï¼Œå°‡ç›¡å¿«åœæ­¢æ»¾å‹•ã€‚")
                # é€™è£¡å¯ä»¥è§¸ç™¼ä¸€å€‹äº‹ä»¶ä¾†åœæ­¢æ»¾å‹•ï¼Œä½†ç›®å‰æ¶æ§‹ä¸‹æœƒè‡ªç„¶åœæ­¢
                pass
                
            # è¨˜éŒ„æ–°ç™¼ç¾çš„æŸ¥è©¢åç¨±ï¼ˆç”¨æ–¼é™¤éŒ¯ï¼‰
            if qname not in self.known_queries:
                self.known_queries.add(qname)
                # å¯é¸ï¼šå¯«å…¥æª”æ¡ˆä»¥ä¾›æ—¥å¾Œåˆ†æ
                try:
                    with open("seen_queries.txt", "a", encoding="utf-8") as f:
                        f.write(f"{qname}\n")
                except Exception:
                    pass  # å¯«æª”å¤±æ•—ä¸å½±éŸ¿ä¸»è¦åŠŸèƒ½
                    
        return _handle_response

    async def fetch_posts(
        self,
        username: str,
        max_posts: int,
        auth_json_content: Dict,
        task_id: str = None
    ) -> PostMetricsBatch:
        """
        ä½¿ç”¨æŒ‡å®šçš„èªè­‰å…§å®¹çˆ¬å–è²¼æ–‡ã€‚
        æ­¤ç‰ˆæœ¬æœƒèšåˆæ‰€æœ‰çµæœï¼Œä¸¦ä¸€æ¬¡æ€§è¿”å› PostMetricsBatchã€‚
        """
        if task_id is None:
            task_id = str(uuid.uuid4())
            
        target_url = f"https://www.threads.com/@{username}"
        posts: Dict[str, PostMetrics] = {}
        
        # ç™¼å¸ƒé–‹å§‹çˆ¬å–çš„é€²åº¦
        await publish_progress(task_id, "fetch_start", username=username, max_posts=max_posts)
        
        # 1. å®‰å…¨åœ°å°‡ auth.json å…§å®¹å¯«å…¥è‡¨æ™‚æª”æ¡ˆ
        auth_file = Path(tempfile.gettempdir()) / f"{task_id or uuid.uuid4()}_auth.json"
        try:
            with open(auth_file, 'w', encoding='utf-8') as f:
                json.dump(auth_json_content, f)

            async with async_playwright() as p:
                browser = await p.chromium.launch(
                    headless=self.settings.headless,
                    timeout=self.settings.navigation_timeout,
                    args=["--no-sandbox", "--disable-dev-shm-usage", "--disable-blink-features=AutomationControlled"]
                )
                self.context = await browser.new_context(
                    storage_state=str(auth_file),
                    user_agent=self.settings.user_agent, # å¾è¨­å®šè®€å–
                    viewport={"width": 1920, "height": 1080},
                    locale="zh-TW",  # è¨­å®šç‚ºç¹é«”ä¸­æ–‡
                    has_touch=True,
                    accept_downloads=False
                )
                page = await self.context.new_page()
                page.on("console", lambda m: logging.info(f"CONSOLE [{m.type}] {m.text}"))

                # Response handler
                response_handler = self._build_response_handler(username, posts, task_id, max_posts, stream_callback=None)
                page.on("response", response_handler)

                logging.info(f"å°è¦½è‡³ç›®æ¨™é é¢: {target_url}")
                await page.goto(target_url, wait_until="networkidle", timeout=self.settings.navigation_timeout)

                # --- æ»¾å‹•èˆ‡å»¶é²é‚è¼¯ ---
                scroll_attempts_without_new_posts = 0
                max_retries = 5

                while len(posts) < max_posts:
                    posts_before_scroll = len(posts)
                    await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")

                    try:
                        async with page.expect_response(lambda res: GRAPHQL_RE.search(res.url), timeout=15000):
                            pass
                    except PlaywrightTimeoutError:
                        logging.warning(f"â³ [Task: {task_id}] æ»¾å‹•å¾Œç­‰å¾…ç¶²è·¯å›æ‡‰é€¾æ™‚ã€‚")

                    delay = random.uniform(self.settings.scroll_delay_min, self.settings.scroll_delay_max) # å¾è¨­å®šè®€å–
                    await asyncio.sleep(delay)

                    if len(posts) == posts_before_scroll:
                        scroll_attempts_without_new_posts += 1
                        logging.info(f"æ»¾å‹•å¾Œæœªç™¼ç¾æ–°è²¼æ–‡ (å˜—è©¦ {scroll_attempts_without_new_posts}/{max_retries})")
                        if scroll_attempts_without_new_posts >= max_retries:
                            logging.info("å·²é”é é¢æœ«ç«¯æˆ–ç„¡æ–°å…§å®¹ï¼Œåœæ­¢æ»¾å‹•ã€‚")
                            break
                    else:
                        scroll_attempts_without_new_posts = 0
                        # ç™¼å¸ƒé€²åº¦æ›´æ–°
                        await publish_progress(
                            task_id, "fetch_progress", 
                            username=username,
                            current=len(posts), 
                            total=max_posts,
                            progress=min(len(posts) / max_posts, 1.0)
                        )
                
                # é—œé–‰ page ä½†ä¿ç•™ context ä¾› fill_views_from_page ä½¿ç”¨
                await page.close()

                # --- æ•´ç†ä¸¦å›å‚³çµæœ ---
                final_posts = list(posts.values())
                total_found = len(final_posts)
                
                # æ ¹æ“š max_posts æˆªæ–·çµæœ
                if total_found > max_posts:
                    try:
                        final_posts.sort(key=lambda p: p.created_at or datetime.min, reverse=True)
                    except Exception:
                        pass 
                    final_posts = final_posts[:max_posts]
                
                logging.info(f"ğŸ”„ [Task: {task_id}] æº–å‚™å›å‚³æœ€çµ‚è³‡æ–™ï¼šå…±ç™¼ç¾ {total_found} å‰‡è²¼æ–‡, å›å‚³ {len(final_posts)} å‰‡")
                
                # --- è£œé½Šè§€çœ‹æ•¸ (åœ¨ browser context é‚„å­˜åœ¨æ™‚åŸ·è¡Œ) ---
                logging.info(f"ğŸ” [Task: {task_id}] é–‹å§‹è£œé½Šè§€çœ‹æ•¸...")
                await publish_progress(task_id, "fill_views_start", username=username, posts_count=len(final_posts))
                
                try:
                    final_posts = await self.fill_views_from_page(final_posts)
                    logging.info(f"âœ… [Task: {task_id}] è§€çœ‹æ•¸è£œé½Šå®Œæˆ")
                    await publish_progress(task_id, "fill_views_completed", username=username, posts_count=len(final_posts))
                except Exception as e:
                    logging.warning(f"âš ï¸ [Task: {task_id}] è£œé½Šè§€çœ‹æ•¸æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                    await publish_progress(task_id, "fill_views_error", username=username, error=str(e))
                    # å³ä½¿è£œé½Šå¤±æ•—ï¼Œä¹Ÿç¹¼çºŒè¿”å›åŸºæœ¬æ•¸æ“š

                # æ‰‹å‹•é—œé–‰ browser å’Œ context
                await self.context.close()
                await browser.close()
                self.context = None
            
            # ä¿å­˜åŸå§‹æŠ“å–è³‡æ–™ä¾›èª¿è©¦
            try:
                raw_data = {
                    "task_id": task_id,
                    "username": username, 
                    "timestamp": datetime.now().isoformat(),
                    "total_found": total_found,
                    "returned_count": len(final_posts),
                    "posts": [
                        {
                            "url": post.url,
                            "post_id": post.post_id,
                            "likes_count": post.likes_count,
                            "comments_count": post.comments_count,
                            "reposts_count": post.reposts_count,
                            "shares_count": post.shares_count,
                            "views_count": post.views_count,
                            "calculated_score": post.calculate_score(),  # ğŸ†• ç§»åˆ° views_count ä¸‹é¢
                            "content": post.content,
                            "created_at": post.created_at.isoformat() if post.created_at else None,
                            "images": post.images,  # æ·»åŠ åœ–ç‰‡ URL
                            "videos": post.videos   # æ·»åŠ å½±ç‰‡ URL
                        } for post in final_posts
                    ]
                }
                
                # ä½¿ç”¨æ™‚é–“æˆ³è¨˜é¿å…æª”æ¡ˆè¡çª
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                raw_file = DEBUG_DIR / f"crawl_data_{timestamp}_{task_id[:8]}.json"
                raw_file.write_text(
                    json.dumps(raw_data, indent=2, ensure_ascii=False),
                    encoding="utf-8" # æ˜ç¢ºæŒ‡å®š UTF-8
                )
                logging.info(f"ğŸ’¾ [Task: {task_id}] å·²ä¿å­˜åŸå§‹æŠ“å–è³‡æ–™è‡³: {raw_file}")
                
            except Exception as e:
                logging.warning(f"âš ï¸ [Task: {task_id}] ä¿å­˜èª¿è©¦è³‡æ–™å¤±æ•—: {e}")
            
            # ç™¼å¸ƒå®Œæˆé€²åº¦
            await publish_progress(
                task_id, "completed", 
                username=username,
                total_posts=len(final_posts),
                success=True
            )

            batch = PostMetricsBatch(
                posts=final_posts,
                username=username,
                total_count=total_found,
                processing_stage="playwright_completed"
            )
            return batch
            
        except Exception as e:
            error_message = f"Playwright æ ¸å¿ƒé‚è¼¯å‡ºéŒ¯: {e}"
            logging.error(error_message, exc_info=True)
            # ç™¼å¸ƒéŒ¯èª¤é€²åº¦
            await publish_progress(
                task_id, "error", 
                username=username,
                error=error_message,
                success=False
            )
            raise
        
        finally:
            # æ¸…ç†è‡¨æ™‚èªè­‰æª”æ¡ˆ
            if auth_file.exists():
                auth_file.unlink()
                logging.info(f"ğŸ—‘ï¸ [Task: {task_id}] å·²åˆªé™¤è‡¨æ™‚èªè­‰æª”æ¡ˆ: {auth_file}")
            
            # ç¢ºä¿ context è¢«é‡ç½®ï¼ˆbrowser å·²åœ¨ä¸Šé¢æ‰‹å‹•é—œé–‰ï¼‰
            self.context = None 

    # +++ æ–°å¢ï¼šå¾å‰ç«¯è£œé½Šç€è¦½æ•¸çš„æ ¸å¿ƒæ–¹æ³• +++
    async def fill_views_from_page(self, posts_to_fill: List[PostMetrics]) -> List[PostMetrics]:
        """
        éæ­·è²¼æ–‡åˆ—è¡¨ï¼Œå°èˆªåˆ°æ¯å€‹è²¼æ–‡çš„é é¢ä»¥è£œé½Š views_countã€‚
        ä½¿ç”¨ä¸¦ç™¼è™•ç†ä¾†åŠ é€Ÿæ­¤éç¨‹ã€‚
        """
        if not self.context:
            logging.error("âŒ Browser context æœªåˆå§‹åŒ–ï¼Œç„¡æ³•åŸ·è¡Œ fill_views_from_pageã€‚")
            return posts_to_fill

        # ä½¿ç”¨ Semaphore é™åˆ¶ä¸¦ç™¼æ•¸ï¼Œé¿å…è¢«ä¼ºæœå™¨å°é–
        semaphore = asyncio.Semaphore(5)
        
        async def fetch_single_view(post: PostMetrics):
            async with semaphore:
                page = None
                try:
                    page = await self.context.new_page()
                    # ç¦ç”¨åœ–ç‰‡å’Œå½±ç‰‡è¼‰å…¥ä»¥åŠ é€Ÿ
                    await page.route("**/*.{png,jpg,jpeg,gif,mp4,webp}", lambda r: r.abort())
                    
                    for attempt in range(3): # æœ€å¤šé‡è©¦3æ¬¡
                        try:
                            logging.debug(f"  â¡ï¸ (Attempt {attempt+1}) æ­£åœ¨å°èˆªè‡³: {post.url}")
                            await page.goto(post.url, timeout=20000, wait_until='domcontentloaded')
                            
                            # ä½¿ç”¨æ‚¨å»ºè­°çš„ã€æœ€ç©©å¥çš„ Selector
                            selector = "span:has-text('æ¬¡ç€è¦½'), span:has-text('views')"
                            
                            # ç­‰å¾…å…ƒç´ å‡ºç¾ï¼Œæœ€å¤š10ç§’
                            element = await page.wait_for_selector(selector, timeout=10000)
                            
                            if element:
                                view_text = await element.inner_text()
                                views_count = parse_views_text(view_text)
                                if views_count is not None:
                                    post.views_count = views_count
                                    post.views_fetched_at = datetime.utcnow()
                                    logging.info(f"  âœ… æˆåŠŸç²å– {post.post_id} çš„ç€è¦½æ•¸: {views_count}")
                                    return # æˆåŠŸå¾Œé€€å‡ºé‡è©¦å¾ªç’°
                            break # æ‰¾åˆ°å…ƒç´ ä½†è§£æå¤±æ•—ä¹Ÿè·³å‡º
                        except Exception as e:
                            logging.warning(f"  âš ï¸ (Attempt {attempt+1}) ç²å– {post.post_id} ç€è¦½æ•¸å¤±æ•—: {type(e).__name__}")
                            if attempt < 2:
                                await asyncio.sleep(2) # é‡è©¦å‰ç­‰å¾…
                            else:
                                post.views_count = -1 # æœ€çµ‚å¤±æ•—ï¼Œæ¨™è¨˜ç‚º-1
                                post.views_fetched_at = datetime.utcnow()
                except Exception as e:
                    logging.error(f"  âŒ è™•ç† {post.post_id} æ™‚ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}")
                    post.views_count = -1
                    post.views_fetched_at = datetime.utcnow()
                finally:
                    if page:
                        await page.close()

        tasks = []
        for post in posts_to_fill:
            tasks.append(fetch_single_view(post))
            
        await asyncio.gather(*tasks)
        
        return posts_to_fill


    async def _scroll_and_collect(self, page: Page, username: str, max_posts: int):
        pass


    async def _handle_response(self, response: APIResponse, username: str):
        """è™•ç† GraphQL API å›æ‡‰"""
        # ... (é€™è£¡çš„ _handle_response ç¶­æŒåŸæ¨£) ...
# ... existing code ... 