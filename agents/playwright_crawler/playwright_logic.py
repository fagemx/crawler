"""
Playwright çˆ¬èŸ²æ ¸å¿ƒé‚è¼¯ï¼ˆé‡æ§‹ç‰ˆï¼‰
ä½¿ç”¨æ¨¡å¡ŠåŒ–æ¶æ§‹ï¼Œå¾1377è¡Œç¸®æ¸›åˆ°ç²¾ç°¡çš„å”èª¿å±¤
"""

import asyncio
import json
import logging
import tempfile
import uuid
from pathlib import Path
from typing import Dict, List, Optional
from datetime import datetime

from playwright.async_api import async_playwright, BrowserContext

from common.settings import get_settings
from common.models import PostMetrics, PostMetricsBatch
from common.nats_client import publish_progress
from common.history import crawl_history

# å°å…¥æ‹†åˆ†å¾Œçš„æ¨¡å¡Š
from .extractors import URLExtractor, ViewsExtractor, DetailsExtractor

# èª¿è©¦æª”æ¡ˆè·¯å¾‘
DEBUG_DIR = Path(__file__).parent / "debug"
DEBUG_DIR.mkdir(exist_ok=True)

# è¨­å®šæ—¥èªŒï¼ˆé¿å…é‡è¤‡é…ç½®ï¼‰
if not logging.getLogger().handlers:
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')


class PlaywrightLogic:
    """ä½¿ç”¨ Playwright é€²è¡Œçˆ¬èŸ²çš„æ ¸å¿ƒé‚è¼¯ï¼ˆé‡æ§‹ç‰ˆï¼‰"""
    
    def __init__(self):
        self.settings = get_settings().playwright
        self.context: Optional[BrowserContext] = None
        
        # æå–å™¨å°‡åœ¨contextå»ºç«‹å¾Œåˆå§‹åŒ–
        self.url_extractor = None
        self.views_extractor = None
        self.details_extractor = None

    def _init_extractors(self):
        """åˆå§‹åŒ–æ‰€æœ‰æå–å™¨ï¼ˆéœ€è¦åœ¨contextå»ºç«‹å¾Œå‘¼å«ï¼‰"""
        if self.context:
            self.url_extractor = URLExtractor()
            self.views_extractor = ViewsExtractor(self.context)
            self.details_extractor = DetailsExtractor(self.context)

    async def fetch_posts(
        self,
        username: str,
        extra_posts: int,
        auth_json_content: Dict,
        task_id: str = None
    ) -> PostMetricsBatch:
        """ä½¿ç”¨æŒ‡å®šçš„èªè­‰å…§å®¹é€²è¡Œå¢é‡çˆ¬å–ã€‚"""
        if task_id is None:
            task_id = str(uuid.uuid4())
        
        # å¢é‡çˆ¬å–é‚è¼¯
        if extra_posts <= 0:
            logging.info(f"ğŸŸ¢ {username} ç„¡éœ€é¡å¤–çˆ¬å– (extra_posts={extra_posts})")
            existing_state = await crawl_history.get_crawl_state(username)
            total_existing = existing_state.get("total_crawled", 0) if existing_state else 0
            return PostMetricsBatch(posts=[], username=username, total_count=total_existing)
        
        existing_post_ids = await crawl_history.get_existing_post_ids(username)
        already_count = len(existing_post_ids)
        need_to_fetch = extra_posts
        
        logging.info(f"ğŸ“Š {username} å¢é‡ç‹€æ…‹: å·²æœ‰={already_count}, éœ€è¦æ–°å¢={need_to_fetch}")
        
        await publish_progress(task_id, "fetch_start", username=username, extra_posts=extra_posts)
        
        # å®‰å…¨åœ°å°‡ auth.json å…§å®¹å¯«å…¥è‡¨æ™‚æª”æ¡ˆ
        auth_file = Path(tempfile.gettempdir()) / f"{task_id or uuid.uuid4()}_auth.json"
        try:
            with open(auth_file, 'w', encoding='utf-8') as f:
                json.dump(auth_json_content, f)

            async with async_playwright() as p:
                browser = await p.chromium.launch(
                    headless=self.settings.headless,
                    timeout=self.settings.navigation_timeout,
                    args=["--no-sandbox", "--disable-dev-shm-usage", "--disable-blink-features=AutomationControlled"]
                )
                self.context = await browser.new_context(
                    storage_state=str(auth_file),
                    user_agent=self.settings.user_agent,
                    viewport={"width": 1920, "height": 1080},
                    locale="zh-TW",
                    has_touch=True,
                    accept_downloads=False,
                    bypass_csp=True
                )
                await self.context.add_init_script(
                    "Object.defineProperty(navigator,'webdriver',{get:()=>undefined})"
                )
                
                # åˆå§‹åŒ–æå–å™¨
                self._init_extractors()
                
                page = await self.context.new_page()
                page.on("console", lambda m: logging.info(f"CONSOLE [{m.type}] {m.text}"))

                # æ­¥é©Ÿ1: ä½¿ç”¨URLExtractorç²å–æœ‰åºURLs
                logging.info(f"ğŸ¯ [Task: {task_id}] å¢é‡çˆ¬å–: éœ€è¦{need_to_fetch}ç¯‡æ–°è²¼æ–‡")
                buffer_size = min(need_to_fetch + 10, 50)
                ordered_post_urls = await self.url_extractor.get_ordered_post_urls_from_page(page, username, buffer_size)
                
                if not ordered_post_urls:
                    logging.warning(f"âš ï¸ [Task: {task_id}] ç„¡æ³•å¾ç”¨æˆ¶é é¢ç²å–è²¼æ–‡ URLs")
                    return PostMetricsBatch(posts=[], username=username, total_count=already_count)

                # æ­¥é©Ÿ2: å¢é‡å„ªåŒ–ï¼šå»é‡+ç²¾ç¢ºæ—©åœ
                logging.info(f"âœ… [Task: {task_id}] å¢é‡ç¯©é¸: å¾{len(ordered_post_urls)}å€‹URLä¸­å°‹æ‰¾{need_to_fetch}ç¯‡æ–°è²¼æ–‡")
                ordered_posts = []
                new_posts_found = 0
                
                for i, post_url in enumerate(ordered_post_urls):
                    url_parts = post_url.split('/')
                    if len(url_parts) >= 2:
                        code = url_parts[-1] if url_parts[-1] != 'media' else url_parts[-2]
                        post_id = f"{username}_{code}"
                        
                        # æ ¸å¿ƒå»é‡é‚è¼¯
                        if post_id in existing_post_ids:
                            logging.debug(f"â­ï¸ è·³éå·²å­˜åœ¨: {post_id}")
                            continue
                        
                        # ç²¾ç¢ºæ—©åœæ©Ÿåˆ¶
                        if new_posts_found >= need_to_fetch:
                            logging.info(f"ğŸ¯ ææ—©åœæ­¢: å·²æ”¶é›†åˆ°{need_to_fetch}ç¯‡æ–°è²¼æ–‡")
                            break
                        
                        # å‰µå»ºæ–°çš„PostMetrics
                        post_metrics = PostMetrics(
                            url=post_url,
                            post_id=post_id,
                            username=username,
                            source="playwright_incremental",
                            processing_stage="url_extracted",
                            likes_count=0,
                            comments_count=0,
                            reposts_count=0,
                            shares_count=0,
                            content="",
                            created_at=datetime.utcnow(),
                            images=[],
                            videos=[],
                            views_count=None,
                        )
                        ordered_posts.append(post_metrics)
                        new_posts_found += 1
                        
                        logging.info(f"âœ… ç™¼ç¾æ–°è²¼æ–‡ {new_posts_found}/{need_to_fetch}: {post_id}")
                
                logging.info(f"âœ… [Task: {task_id}] å‰µå»ºäº† {len(ordered_posts)} å€‹æœ‰åºçš„åŸºç¤PostMetrics")
                await page.close()

                # æ­¥é©Ÿ3: ä½¿ç”¨DetailsExtractorè£œé½Šè©³ç´°æ•¸æ“š
                final_posts = ordered_posts
                logging.info(f"ğŸ” [Task: {task_id}] é–‹å§‹ DOM æ•¸æ“šè£œé½Š...")
                await publish_progress(task_id, "fill_details_start", username=username, posts_count=len(final_posts))
                
                try:
                    final_posts = await self.details_extractor.fill_post_details_from_page(final_posts, task_id=task_id, username=username)
                    logging.info(f"âœ… [Task: {task_id}] è©³ç´°æ•¸æ“šè£œé½Šå®Œæˆ")
                    await publish_progress(task_id, "fill_details_completed", username=username, posts_count=len(final_posts))
                except Exception as e:
                    logging.warning(f"âš ï¸ [Task: {task_id}] è£œé½Šè©³ç´°æ•¸æ“šæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                
                # æ­¥é©Ÿ4: ä½¿ç”¨ViewsExtractorè£œé½Šè§€çœ‹æ•¸
                logging.info(f"ğŸ” [Task: {task_id}] é–‹å§‹è£œé½Šè§€çœ‹æ•¸...")
                await publish_progress(task_id, "fill_views_start", username=username, posts_count=len(final_posts))
                
                try:
                    final_posts = await self.views_extractor.fill_views_from_page(final_posts, task_id=task_id, username=username)
                    logging.info(f"âœ… [Task: {task_id}] è§€çœ‹æ•¸è£œé½Šå®Œæˆ")
                    await publish_progress(task_id, "fill_views_completed", username=username, posts_count=len(final_posts))
                except Exception as e:
                    logging.warning(f"âš ï¸ [Task: {task_id}] è£œé½Šè§€çœ‹æ•¸æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

                await self.context.close()
                await browser.close()
                self.context = None
            
            # ä¿å­˜èª¿è©¦æ•¸æ“š
            await self._save_debug_data(task_id, username, len(final_posts), final_posts)
            
            await publish_progress(task_id, "completed", username=username, total_posts=len(final_posts), success=True)

            # æ›´æ–°çˆ¬å–ç‹€æ…‹
            if final_posts:
                saved_count = await crawl_history.upsert_posts(final_posts)
                latest_post_id = final_posts[0].post_id if final_posts else None
                if latest_post_id:
                    await crawl_history.update_crawl_state(username, latest_post_id, saved_count)
                
                task_metrics = await crawl_history.get_task_metrics(username, need_to_fetch, len(final_posts))
                logging.info(f"ğŸ“Š ä»»å‹™å®Œæˆ: {task_metrics}")
            
            return PostMetricsBatch(
                posts=final_posts,
                username=username,
                total_count=already_count + len(final_posts),
                processing_stage="playwright_incremental_completed"
            )
            
        except Exception as e:
            error_message = f"Playwright æ ¸å¿ƒé‚è¼¯å‡ºéŒ¯: {e}"
            logging.error(error_message, exc_info=True)
            await publish_progress(task_id, "error", username=username, error=error_message, success=False)
            raise
        
        finally:
            if auth_file.exists():
                auth_file.unlink()
                logging.info(f"ğŸ—‘ï¸ [Task: {task_id}] å·²åˆªé™¤è‡¨æ™‚èªè­‰æª”æ¡ˆ: {auth_file}")
            self.context = None 

    async def _save_debug_data(self, task_id: str, username: str, total_found: int, final_posts: List[PostMetrics]):
        """ä¿å­˜èª¿è©¦æ•¸æ“š"""
        try:
            raw_data = {
                "task_id": task_id,
                "username": username, 
                "timestamp": datetime.now().isoformat(),
                "total_found": total_found,
                "returned_count": len(final_posts),
                "posts": [
                    {
                        "url": post.url,
                        "post_id": post.post_id,
                        "likes_count": post.likes_count,
                        "comments_count": post.comments_count,
                        "reposts_count": post.reposts_count,
                        "shares_count": post.shares_count,
                        "views_count": post.views_count,
                        "calculated_score": post.calculate_score(),
                        "content": post.content,
                        "created_at": post.created_at.isoformat() if post.created_at else None,
                        "images": post.images,
                        "videos": post.videos
                    } for post in final_posts
                ]
            }
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            raw_file = DEBUG_DIR / f"crawl_data_{timestamp}_{task_id[:8]}.json"
            raw_file.write_text(json.dumps(raw_data, indent=2, ensure_ascii=False), encoding="utf-8")
            logging.info(f"ğŸ’¾ [Task: {task_id}] å·²ä¿å­˜åŸå§‹æŠ“å–è³‡æ–™è‡³: {raw_file}")
            
        except Exception as e:
            logging.warning(f"âš ï¸ [Task: {task_id}] ä¿å­˜èª¿è©¦è³‡æ–™å¤±æ•—: {e}")