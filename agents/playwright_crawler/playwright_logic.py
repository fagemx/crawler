"""
Playwright çˆ¬èŸ²æ ¸å¿ƒé‚è¼¯ï¼ˆé‡æ§‹ç‰ˆï¼‰
"""
import json
import asyncio
import logging
import tempfile
from pathlib import Path
from typing import Dict, List, Optional, Any, Literal
from datetime import datetime, timezone, timedelta

from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeoutError

from common.settings import get_settings
from common.models import PostMetrics, PostMetricsBatch
from common.nats_client import publish_progress
from common.utils import generate_post_url, first_of, parse_thread_item
from common.history import crawl_history

def get_taipei_time():
    """ç²å–ç•¶å‰å°åŒ—æ™‚é–“ï¼ˆç„¡æ™‚å€ä¿¡æ¯ï¼‰"""
    taipei_tz = timezone(timedelta(hours=8))
    return datetime.now(taipei_tz).replace(tzinfo=None)

# å°å…¥é‡æ§‹å¾Œçš„æ¨¡çµ„
from .parsers.number_parser import parse_number
from .parsers.post_parser import parse_post_data
from .extractors.url_extractor import URLExtractor
from .extractors.views_extractor import ViewsExtractor
from .extractors.details_extractor import DetailsExtractor
from .config.field_mappings import FIELD_MAP
from .utils.post_deduplicator import apply_deduplication
from .helpers.scrolling import (
    extract_current_post_ids, check_page_bottom, scroll_once, 
    is_anchor_visible, collect_urls_from_dom, 
    should_stop_new_mode, should_stop_hist_mode,
    enhanced_scroll_with_strategy, wait_for_content_loading,
    final_attempt_scroll, progressive_wait, should_stop_incremental_mode
)

# èª¿è©¦æª”æ¡ˆè·¯å¾‘
DEBUG_DIR = Path(__file__).parent / "debug"
DEBUG_DIR.mkdir(exist_ok=True)

# è¨­å®šæ—¥èªŒï¼ˆé¿å…é‡è¤‡é…ç½®ï¼‰ï¼Œå¼·åˆ¶ä½¿ç”¨å°åŒ—æ™‚å€é¡¯ç¤º asctime
class TaipeiTimezoneFormatter(logging.Formatter):
    def converter(self, timestamp):
        tz = timezone(timedelta(hours=8))
        return datetime.fromtimestamp(timestamp, tz).timetuple()

if not logging.getLogger().handlers:
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    for handler in logging.getLogger().handlers:
        handler.setFormatter(TaipeiTimezoneFormatter('%(asctime)s - %(levelname)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S'))


class PlaywrightLogic:
    """ä½¿ç”¨ Playwright é€²è¡Œçˆ¬èŸ²çš„æ ¸å¿ƒé‚è¼¯ï¼ˆé‡æ§‹ç‰ˆï¼‰"""
    def __init__(self):
        self.playwright = None
        self.browser = None
        self.context = None
    
    def __init__(self):
        self.browser = None
        self.context = None
        self.settings = get_settings()
        
        # åˆå§‹åŒ–æå–å™¨
        self.url_extractor = URLExtractor()
        self.views_extractor = ViewsExtractor()
        self.details_extractor = DetailsExtractor()

    async def fetch_posts(
        self,
        username: str,
        extra_posts: int,  # éœ€è¦çš„è²¼æ–‡æ•¸é‡
        auth_json_content: Dict,
        task_id: str = None,
        mode: Literal["new", "hist"] = "new",  # çˆ¬å–æ¨¡å¼
        anchor_post_id: str = None,            # éŒ¨é»è²¼æ–‡ID  
        max_scroll_rounds: int = 30,           # æœ€å¤§æ»¾å‹•è¼ªæ¬¡
        incremental: bool = True,              # æ–°å¢ï¼šå¢é‡æ¨¡å¼
        enable_deduplication: bool = True,     # æ–°å¢ï¼šå»é‡é–‹é—œ
        realtime_download: bool = False        # ğŸ†• æ–°å¢ï¼šå³æ™‚ä¸‹è¼‰åª’é«”
    ) -> PostMetricsBatch:
        """
        æ™ºèƒ½å¢é‡çˆ¬å–è²¼æ–‡ - æ”¯æŒæ–°è²¼æ–‡è£œè¶³å’Œæ­·å²å›æº¯
        
        Args:
            username: ç›®æ¨™ç”¨æˆ¶å
            extra_posts: éœ€è¦é¡å¤–æŠ“å–çš„è²¼æ–‡æ•¸é‡
            auth_json_content: èªè­‰è³‡è¨Š
            task_id: ä»»å‹™ID
            mode: çˆ¬å–æ¨¡å¼ ("new"=æ–°è²¼æ–‡è£œè¶³, "hist"=æ­·å²å›æº¯)
            anchor_post_id: éŒ¨é»è²¼æ–‡IDï¼Œè‡ªå‹•å¾crawl_stateç²å–
            max_scroll_rounds: æœ€å¤§æ»¾å‹•è¼ªæ¬¡ï¼Œé˜²æ­¢ç„¡é™æ»¾å‹•
        """
        if task_id is None:
            task_id = f"task_{get_taipei_time().strftime('%Y%m%d_%H%M%S')}"
        
        # å…§éƒ¨å°åŒ—æ™‚é–“å‡½æ•¸
        def get_taipei_time():
            """ç²å–ç•¶å‰å°åŒ—æ™‚é–“ï¼ˆç„¡æ™‚å€ä¿¡æ¯ï¼‰"""
            taipei_tz = timezone(timedelta(hours=8))
            return datetime.now(taipei_tz).replace(tzinfo=None)
        
        # å…§éƒ¨æ¢ä»¶å»é‡å‡½æ•¸
        def conditional_deduplication(posts_list):
            """æ ¹æ“š enable_deduplication åƒæ•¸æ±ºå®šæ˜¯å¦åŸ·è¡Œå»é‡"""
            if enable_deduplication:
                return apply_deduplication(posts_list)
            else:
                logging.info(f"âš ï¸ [Task: {task_id}] å»é‡å·²é—œé–‰ï¼Œä¿ç•™æ‰€æœ‰ {len(posts_list)} ç¯‡è²¼æ–‡")
                return posts_list
            
        logging.info(f"ğŸš€ [Task: {task_id}] é–‹å§‹{mode.upper()}æ¨¡å¼çˆ¬å– @{username}ï¼Œç›®æ¨™: {extra_posts} ç¯‡")
        logging.info(f"ğŸ§¹ [Task: {task_id}] å»é‡åŠŸèƒ½: {'å•Ÿç”¨' if enable_deduplication else 'é—œé–‰'}")
        logging.info(f"â¬‡ï¸ [Task: {task_id}] å³æ™‚ä¸‹è¼‰: {'å•Ÿç”¨' if realtime_download else 'é—œé–‰'}")
        
        try:
            # æ­¥é©Ÿ1: åˆå§‹åŒ–ç€è¦½å™¨å’Œèªè­‰
            await self._setup_browser_and_auth(auth_json_content, task_id)
            
            # æ­¥é©Ÿ2: ç²å–ç¾æœ‰è²¼æ–‡IDå’Œçˆ¬å–ç‹€æ…‹ï¼ˆå¢é‡æ¨¡å¼ï¼‰
            existing_post_ids = set()
            if incremental:
                existing_post_ids = await crawl_history.get_existing_post_ids(username)
                crawl_state = await crawl_history.get_crawl_state(username)
                
                # ç¢ºå®šéŒ¨é»è²¼æ–‡ID
                if anchor_post_id is None and crawl_state:
                    anchor_post_id = crawl_state.get('latest_post_id')
                    
                logging.info(f"ğŸ” å¢é‡æ¨¡å¼: å·²çˆ¬å– {len(existing_post_ids)} å€‹è²¼æ–‡")
                logging.info(f"ğŸ“ éŒ¨é»è¨­å®š: {anchor_post_id or 'ç„¡'}")
                
                # èª¿è©¦ï¼šé¡¯ç¤ºå·²å­˜åœ¨çš„è²¼æ–‡IDï¼ˆæœ€å¤šé¡¯ç¤º10å€‹ï¼‰
                if existing_post_ids:
                    sample_ids = list(existing_post_ids)[:10]
                    logging.info(f"ğŸ” [DEBUG] å·²å­˜åœ¨è²¼æ–‡ç¯„ä¾‹: {sample_ids}")
                else:
                    logging.info(f"ğŸ” [DEBUG] è³‡æ–™åº«ä¸­ç„¡ç¾æœ‰è²¼æ–‡")
            else:
                logging.info(f"ğŸ“‹ å…¨é‡æ¨¡å¼: çˆ¬å–æ‰€æœ‰æ‰¾åˆ°çš„è²¼æ–‡")
            
            need_to_fetch = extra_posts
            
            # æ­¥é©Ÿ3: æ¡ç”¨Realtimeç­–ç•¥ - è¶³é¡æ”¶é›†URLs
            logging.info(f"ğŸ”„ [Task: {task_id}] é–‹å§‹æ™ºèƒ½URLæ”¶é›†ï¼ˆç›®æ¨™: {need_to_fetch} ç¯‡ï¼‰...")
            page = await self.context.new_page()
            await page.goto(f"https://www.threads.com/@{username}", wait_until="domcontentloaded")
            await asyncio.sleep(2)
            
            # ä½¿ç”¨Realtimeé¢¨æ ¼çš„URLæ”¶é›†
            all_collected_urls = await self._collect_urls_realtime_style(
                page, username, need_to_fetch, existing_post_ids, incremental, max_scroll_rounds
            )
            await page.close()
            
            if not all_collected_urls:
                logging.warning(f"âŒ [Task: {task_id}] æ²’æœ‰æ”¶é›†åˆ°ä»»ä½•æ–°çš„URL")
                return PostMetricsBatch(posts=[], username=username, total_count=0)
            
            logging.info(f"âœ… [Task: {task_id}] URLæ”¶é›†å®Œæˆï¼æ”¶é›†åˆ° {len(all_collected_urls)} å€‹URL")
            
            # æ­¥é©Ÿ4: æ™ºèƒ½åˆ†æ‰¹è™•ç†ç­–ç•¥ï¼ˆé‡é»ï¼šä¸è¶³æ™‚å¾å‰©é¤˜URLè£œè¶³ï¼‰
            final_posts = []
            url_pool = all_collected_urls.copy()
            
            # åˆå§‹åŒ–å·²è™•ç†çš„è²¼æ–‡IDé›†åˆï¼ˆåŒ…å«è³‡æ–™åº«ä¸­çš„ï¼‰
            processed_post_ids = set()
            if incremental:
                # å¢é‡æ¨¡å¼ï¼šå°‡è³‡æ–™åº«ä¸­å·²å­˜åœ¨çš„è²¼æ–‡IDåŠ å…¥å·²è™•ç†é›†åˆ
                for existing_id in existing_post_ids:
                    # existing_post_ids å·²ç¶“æ˜¯ username_postid æ ¼å¼
                    processed_post_ids.add(existing_id)
                logging.info(f"ğŸ” [Task: {task_id}] å¢é‡æ¨¡å¼ï¼šé å…ˆæ’é™¤ {len(processed_post_ids)} å€‹å·²å­˜åœ¨è²¼æ–‡")
            
            max_process_rounds = 3  # æœ€å¤šè™•ç†3è¼ª
            
            for process_round in range(1, max_process_rounds + 1):
                if len(final_posts) >= need_to_fetch or not url_pool:
                    break
                    
                # éæ¿¾æ‰å·²è™•ç†éçš„URLï¼ˆé‡é»ä¿®å¾©ï¼ï¼‰
                filtered_url_pool = []
                for url in url_pool:
                    post_id = url.split('/')[-1] if url else None
                    full_post_id = f"{username}_{post_id}"
                    
                    # è·³éå·²è™•ç†çš„è²¼æ–‡ID
                    if full_post_id not in processed_post_ids:
                        filtered_url_pool.append(url)
                    else:
                        logging.debug(f"   â­ï¸ è·³éå·²è™•ç†çš„è²¼æ–‡: {full_post_id}")
                
                url_pool = filtered_url_pool
                
                if not url_pool:
                    logging.warning(f"âš ï¸ [Task: {task_id}] ç¬¬ {process_round} è¼ªï¼šéæ¿¾å¾Œç„¡å‰©é¤˜URLå¯è™•ç†")
                    break
                    
                # è¨ˆç®—é€™è¼ªéœ€è¦è™•ç†çš„æ•¸é‡
                shortage = need_to_fetch - len(final_posts)
                batch_size = min(shortage + 2, len(url_pool))  # å¤šè™•ç†2å€‹é˜²æ­¢å»é‡å¾Œä¸è¶³
                current_batch_urls = url_pool[:batch_size]
                url_pool = url_pool[batch_size:]
                
                # è¨˜éŒ„é€™è¼ªå°‡è¦è™•ç†çš„è²¼æ–‡ID
                for url in current_batch_urls:
                    post_id = url.split('/')[-1] if url else None
                    full_post_id = f"{username}_{post_id}"
                    processed_post_ids.add(full_post_id)
                
                logging.info(f"ğŸ”„ [Task: {task_id}] ç¬¬ {process_round} è¼ªè™•ç†ï¼š{len(current_batch_urls)} å€‹URL (é‚„éœ€ {shortage} ç¯‡)")
                
                # è½‰æ›URLsç‚ºPostMetricsä¸¦éæ¿¾éç›®æ¨™ç”¨æˆ¶
                batch_posts = await self._convert_urls_to_posts(current_batch_urls, username, mode, task_id)
                
                if not batch_posts:
                    logging.warning(f"âš ï¸ [Task: {task_id}] ç¬¬ {process_round} è¼ªï¼šè½‰æ›å¾Œç„¡æœ‰æ•ˆè²¼æ–‡")
                    continue
                
                # è©³ç´°æ•¸æ“šè£œé½Š
                logging.info(f"ğŸ” [Task: {task_id}] ç¬¬ {process_round} è¼ªï¼šé–‹å§‹æ•¸æ“šè£œé½Š...")
                await publish_progress(task_id, f"process_round_{process_round}_details", username=username, posts_count=len(batch_posts))
                
                try:
                    batch_posts = await self.details_extractor.fill_post_details_from_page(batch_posts, self.context, task_id=task_id, username=username)
                    batch_posts = await self.views_extractor.fill_views_from_page(batch_posts, self.context, task_id=task_id, username=username)
                    logging.info(f"âœ… [Task: {task_id}] ç¬¬ {process_round} è¼ªï¼šæ•¸æ“šè£œé½Šå®Œæˆ")
                    
                    # ğŸ†• å³æ™‚ä¸‹è¼‰é‚è¼¯
                    if realtime_download:
                        await self._realtime_download_media(batch_posts, task_id, username)
                        
                except Exception as e:
                    logging.warning(f"âš ï¸ [Task: {task_id}] ç¬¬ {process_round} è¼ªæ•¸æ“šè£œé½Šå¤±æ•—: {e}")
                
                # åˆä½µä¸¦å»é‡è™•ç†ï¼ˆé‡é»ï¼šæ¯è¼ªéƒ½è¦å»é‡æª¢æŸ¥ï¼‰
                combined_posts = final_posts + batch_posts
                before_dedup_count = len(combined_posts)
                combined_posts = conditional_deduplication(combined_posts)
                after_dedup_count = len(combined_posts)
                
                added_count = after_dedup_count - len(final_posts)
                removed_count = before_dedup_count - after_dedup_count
                final_posts = combined_posts
                
                logging.info(f"âœ… [Task: {task_id}] ç¬¬ {process_round} è¼ªå®Œæˆï¼šæ–°å¢ {added_count} ç¯‡ï¼Œå»é‡ç§»é™¤ {removed_count} ç¯‡ï¼Œç´¯è¨ˆ {len(final_posts)} ç¯‡")
                
                # æª¢æŸ¥æ˜¯å¦å·²è¶³å¤ 
                if len(final_posts) >= need_to_fetch:
                    logging.info(f"ğŸ¯ [Task: {task_id}] å·²é”ç›®æ¨™æ•¸é‡ï¼æœ€çµ‚: {len(final_posts)} ç¯‡")
                    break
                elif not url_pool:
                    logging.warning(f"âš ï¸ [Task: {task_id}] URLæ± å·²ç©ºï¼Œä½†æ•¸é‡ä¸è¶³ï¼ˆ{len(final_posts)}/{need_to_fetch}ï¼‰")
                    break
            
            # æœ€çµ‚æª¢æŸ¥å’Œçµ±è¨ˆ
            if len(final_posts) < need_to_fetch:
                shortage = need_to_fetch - len(final_posts)
                logging.info(f"âš ï¸ [Task: {task_id}] å»é‡å¾Œè²¼æ–‡ä¸è¶³ï¼éœ€è¦: {need_to_fetch}ï¼Œå¯¦éš›: {len(final_posts)}ï¼Œç¼ºå°‘: {shortage}")
                
                # å¯¦æ–½è£œè¶³ç­–ç•¥
                max_supplement_rounds = 2  # æœ€å¤š2è¼ªè£œè¶³
                
                for supplement_round in range(1, max_supplement_rounds + 1):
                    current_shortage = need_to_fetch - len(final_posts)
                    if current_shortage <= 0:
                        break
                        
                    # å‹•æ…‹å¢åŠ çˆ¬å–æ•¸é‡ï¼šç¼ºå¹¾å‰‡å°±å¤šçˆ¬å¹¾å€
                    supplement_target = current_shortage * (2 + supplement_round)  # ç¬¬1è¼ªÃ—3ï¼Œç¬¬2è¼ªÃ—4
                    
                    logging.info(f"ğŸ”„ [Task: {task_id}] è£œè¶³ç¬¬ {supplement_round} è¼ªï¼šé‚„ç¼º {current_shortage} å‰‡ï¼Œå°‡çˆ¬ {supplement_target} å‰‡")
                    
                    try:
                        # é‡æ–°çˆ¬å–æ›´å¤šè²¼æ–‡
                        supplement_page = await self.context.new_page()
                        await supplement_page.goto(f"https://www.threads.com/@{username}")
                        
                        supplement_urls = await self.url_extractor.get_ordered_post_urls_from_page(
                            supplement_page, username, max_posts=supplement_target
                        )
                        await supplement_page.close()
                        
                        # éæ¿¾å‡ºæ–°çš„è²¼æ–‡URLs
                        existing_post_ids_expanded = existing_post_ids | {p.post_id for p in final_posts}
                        supplement_posts = []
                        
                        logging.info(f"ğŸ” [Task: {task_id}] è£œè¶³éæ¿¾ï¼šæ‰¾åˆ° {len(supplement_urls)} å€‹URLsï¼Œå·²æœ‰ {len(existing_post_ids_expanded)} å€‹ID")
                        
                        for url in supplement_urls:
                            # é©—è­‰URLæ˜¯å¦ç¢ºå¯¦å±¬æ–¼ç›®æ¨™ç”¨æˆ¶
                            if f"@{username}/post/" not in url:
                                logging.warning(f"âš ï¸ è£œè¶³éšæ®µè·³ééç›®æ¨™ç”¨æˆ¶çš„URL: {url}")
                                continue
                                
                            post_id = url.split('/')[-1]
                            full_post_id = f"{username}_{post_id}"
                            
                            logging.debug(f"ğŸ” æª¢æŸ¥URL: {url} â†’ {full_post_id} â†’ å­˜åœ¨: {full_post_id in existing_post_ids_expanded}")
                            
                            # è‡¨æ™‚ä¿®å¾©ï¼šåœ¨æ¸¬è©¦æ¨¡å¼ä¸‹å…è¨±é‡æ–°çˆ¬å–ï¼ˆå¦‚æœexistingå¾ˆå°‘ï¼‰
                            is_test_mode = len(existing_post_ids_expanded) < 50  # å°æ–¼50å€‹IDèªç‚ºæ˜¯æ¸¬è©¦
                            should_include = (full_post_id not in existing_post_ids_expanded) or is_test_mode
                            
                            if should_include:
                                supplement_posts.append(PostMetrics(
                                    post_id=full_post_id,
                                    username=username,
                                    url=url,
                                    content="",
                                    created_at=get_taipei_time(),
                                    fetched_at=get_taipei_time(),
                                    source=f"playwright_supplement_r{supplement_round}",
                                    processing_stage="urls_extracted",
                                    is_complete=False,
                                    likes_count=0, comments_count=0, reposts_count=0, shares_count=0, views_count=None,
                                ))
                        
                        if supplement_posts:
                            # è£œé½Šæ•¸æ“š
                            supplement_posts = await self.details_extractor.fill_post_details_from_page(supplement_posts, self.context, task_id=task_id, username=username)
                            supplement_posts = await self.views_extractor.fill_views_from_page(supplement_posts, self.context, task_id=task_id, username=username)
                            
                            # æœ¬è¼ªå»é‡
                            supplement_posts = conditional_deduplication(supplement_posts)
                            
                            # èˆ‡ç¾æœ‰è²¼æ–‡åˆä½µå»é‡
                            combined_posts = final_posts + supplement_posts
                            combined_posts = conditional_deduplication(combined_posts)
                            
                            added_count = len(combined_posts) - len(final_posts)
                            final_posts = combined_posts
                            
                            logging.info(f"âœ… [Task: {task_id}] è£œè¶³ç¬¬ {supplement_round} è¼ªå®Œæˆï¼šæ–°å¢ {added_count} å‰‡ï¼Œç´¯è¨ˆ {len(final_posts)} å‰‡")
                            
                        else:
                            logging.info(f"âš ï¸ [Task: {task_id}] è£œè¶³ç¬¬ {supplement_round} è¼ªï¼šç„¡æ–°è²¼æ–‡å¯è£œå……")
                            break
                            
                    except Exception as e:
                        logging.warning(f"âš ï¸ [Task: {task_id}] è£œè¶³ç¬¬ {supplement_round} è¼ªå¤±æ•—: {e}")
                        break
                
                final_count = len(final_posts)
                if final_count >= need_to_fetch:
                    logging.info(f"ğŸ¯ [Task: {task_id}] è£œè¶³æˆåŠŸï¼ç›®æ¨™: {need_to_fetch}ï¼Œæœ€çµ‚: {final_count}")
                else:
                    final_shortage = need_to_fetch - final_count
                    logging.warning(f"âš ï¸ [Task: {task_id}] è£œè¶³æœªé”æ¨™ï¼šç›®æ¨™: {need_to_fetch}ï¼Œæœ€çµ‚: {final_count}ï¼Œä»ç¼º: {final_shortage}")
                    
                    # === æ–°å¢ï¼šå•Ÿå‹•å¢å¼·æ»¾å‹•æ”¶é›†ç­–ç•¥ ===
                    logging.info(f"ğŸš€ [Task: {task_id}] å•Ÿå‹•å¢å¼·æ»¾å‹•æ”¶é›†ï¼Œä½¿ç”¨Realtimeç­–ç•¥...")
                    
                    try:
                        # å‰µå»ºæ–°é é¢é€²è¡Œå¢å¼·æ”¶é›†
                        enhanced_page = await self.context.new_page()
                        await enhanced_page.goto(f"https://www.threads.com/@{username}")
                        await asyncio.sleep(3)  # ç­‰å¾…é é¢è¼‰å…¥
                        
                        # ä½¿ç”¨å¢å¼·çš„æ”¶é›†ç­–ç•¥
                        existing_post_ids_final = existing_post_ids | {p.post_id for p in final_posts}
                        extended_max_scroll_rounds = max_scroll_rounds + 30  # é¡å¤–30è¼ªæ»¾å‹•
                        
                        additional_urls = await self._collect_urls_realtime_style(
                            enhanced_page, username, final_shortage + 10,  # å¤šæ”¶é›†10å€‹ä½œç‚ºç·©è¡
                            existing_post_ids_final, incremental, extended_max_scroll_rounds
                        )
                        
                        await enhanced_page.close()
                        
                        if additional_urls:
                            logging.info(f"ğŸ¯ [Task: {task_id}] å¢å¼·æ”¶é›†åˆ° {len(additional_urls)} å€‹é¡å¤–URL")
                            
                            # è™•ç†é¡å¤–æ”¶é›†çš„URLs
                            additional_posts = await self._convert_urls_to_posts(additional_urls, username, mode, task_id)
                            additional_posts = await self.details_extractor.process_posts(
                                additional_posts, task_id=task_id
                            )
                            
                            # å»é‡ä¸¦åˆä½µ
                            additional_posts = conditional_deduplication(additional_posts)
                            combined_posts = final_posts + additional_posts
                            combined_posts = conditional_deduplication(combined_posts)
                            
                            final_added = len(combined_posts) - len(final_posts)
                            final_posts = combined_posts
                            
                            logging.info(f"ğŸ‰ [Task: {task_id}] å¢å¼·æ”¶é›†å®Œæˆï¼šæ–°å¢ {final_added} ç¯‡ï¼Œæœ€çµ‚ç´¯è¨ˆ {len(final_posts)} ç¯‡")
                            
                            # æ›´æ–°æœ€çµ‚è¨ˆæ•¸
                            final_count = len(final_posts)
                            if final_count >= need_to_fetch:
                                logging.info(f"âœ… [Task: {task_id}] å¢å¼·æ”¶é›†é”æ¨™ï¼ç›®æ¨™: {need_to_fetch}ï¼Œæœ€çµ‚: {final_count}")
                            else:
                                remaining_shortage = need_to_fetch - final_count
                                logging.warning(f"âš ï¸ [Task: {task_id}] å¢å¼·æ”¶é›†å¾Œä»æœªé”æ¨™ï¼šé‚„ç¼º {remaining_shortage} ç¯‡")
                        else:
                            logging.warning(f"âš ï¸ [Task: {task_id}] å¢å¼·æ”¶é›†ç„¡æ–°URLï¼Œå¯èƒ½å·²åˆ°é”çœŸæ­£åº•éƒ¨")
                            
                    except Exception as e:
                        logging.error(f"âŒ [Task: {task_id}] å¢å¼·æ”¶é›†å¤±æ•—: {e}")

            # æ­¥é©Ÿ5: ä¿å­˜èª¿è©¦æ•¸æ“š
            await self._save_debug_data(task_id, username, len(all_collected_urls), final_posts)
            
            # æº–å‚™å®Œæ•´çš„æœ€çµ‚çµæœæ•¸æ“š
            final_data = {
                "total_processed": len(final_posts),
                "username": username,
                "need_to_fetch": need_to_fetch,
                "success": True,
                "results": [
                    {
                        "post_id": post.post_id,
                        "url": post.url,
                        "content": post.content,
                        "views_count": post.views_count,  # ğŸ”§ ä¿®å¾©ï¼šä½¿ç”¨æ­£ç¢ºçš„keyåç¨±
                        "likes_count": post.likes_count,
                        "comments_count": post.comments_count,
                        "reposts_count": post.reposts_count,
                        "shares_count": post.shares_count,
                        "calculated_score": post.calculate_score(),  # ğŸ”§ ä¿®å¾©ï¼šèª¿ç”¨æ–¹æ³•è¨ˆç®—åˆ†æ•¸
                        "post_published_at": post.post_published_at.isoformat() if post.post_published_at else None,  # ğŸ”§ ä¿®å¾©keyåç¨±
                        "created_at": post.created_at.isoformat() if post.created_at else None,  # ğŸ”§ æ·»åŠ å‰µå»ºæ™‚é–“
                        "tags": post.tags or [],  # ğŸ”§ æ·»åŠ æ¨™ç±¤
                        "images": post.images or [],  # ğŸ”§ æ·»åŠ åœ–ç‰‡
                        "videos": post.videos or [],  # ğŸ”§ æ·»åŠ å½±ç‰‡
                        "is_complete": post.is_complete
                    }
                    for post in final_posts
                ]
            }
            
            # ç™¼å¸ƒå®Œæˆç‹€æ…‹å’Œå®Œæ•´çµæœåˆ° Redis
            await publish_progress(task_id, "completed", username=username, posts_count=len(final_posts), final_data=final_data)

            # æ­¥é©Ÿ9: æ¨™è¨˜DOMè™•ç†ç‹€æ…‹ä¸¦ä¿å­˜åˆ°æ•¸æ“šåº«
            # ç‚ºæ‰€æœ‰å®Œæ•´è™•ç†çš„è²¼æ–‡æ¨™è¨˜DOMç‹€æ…‹ç‚ºsuccess
            for post in final_posts:
                if post.is_complete:
                    # ä¸è¨­ç½® dom_statusï¼Œå› ç‚º Playwright å°ˆç”¨è¡¨æ ¼æ²’æœ‰é€™äº›å­—æ®µ
                    # post.dom_status = "success"
                    # post.dom_processed_at = datetime.utcnow()
                    # å¦‚æœæœ‰å…§å®¹ä½†Readerç‹€æ…‹æœªè¨­å®šï¼Œæ¨æ–·ç‚ºDOMæå–çš„å…§å®¹
                    if post.content: # and post.reader_status == "pending":
                        # ä¸è¨­ç½® reader_statusï¼Œå› ç‚º Playwright å°ˆç”¨è¡¨æ ¼æ²’æœ‰é€™äº›å­—æ®µ
                        # post.reader_status = "success"
                        # post.reader_processed_at = datetime.utcnow()
                        pass
                else:
                    # ä¸è¨­ç½® dom_statusï¼Œå› ç‚º Playwright å°ˆç”¨è¡¨æ ¼æ²’æœ‰é€™äº›å­—æ®µ
                    # post.dom_status = "failed"
                    pass
            
            # ä¸åœ¨å¾Œç«¯ä¿å­˜åˆ°è³‡æ–™åº«ï¼Œè®“å‰ç«¯UIè™•ç†è³‡æ–™åº«ä¿å­˜
            # é€™æ¨£å¯ä»¥ä¿æŒ Playwright å’Œ Realtime çˆ¬èŸ²çš„æ•¸æ“šåˆ†é›¢
            # saved_count = await crawl_history.upsert_posts(final_posts)
            # logging.info(f"âœ… æˆåŠŸè™•ç† {saved_count}/{len(final_posts)} ç¯‡è²¼æ–‡")
            
            # ä¸æ›´æ–°çˆ¬å–ç‹€æ…‹ï¼Œè®“å‰ç«¯UIè™•ç†
            # if final_posts:
            #     latest_post_id = final_posts[0].post_id
            #     await crawl_history.update_crawl_state(username, latest_post_id, saved_count)
            #     logging.info(f"ğŸ“Š æ›´æ–° {username} ç‹€æ…‹: latest={latest_post_id}, +{saved_count}ç¯‡")
            
            logging.info(f"âœ… æˆåŠŸè™•ç† {len(final_posts)} ç¯‡è²¼æ–‡ï¼Œè³‡æ–™åº«ä¿å­˜å°‡ç”±å‰ç«¯UIè™•ç†")
            
            # æ­¥é©Ÿ10: ç”Ÿæˆç°¡åŒ–çš„ä»»å‹™æŒ‡æ¨™ï¼ˆä¸ä¾è³´è³‡æ–™åº«ï¼‰
            # task_metrics = await crawl_history.get_task_metrics(username, need_to_fetch, len(final_posts))
            task_metrics = {
                "total_processed": len(final_posts),
                "username": username,
                "need_to_fetch": need_to_fetch,
                "success": True
            }
            logging.info(f"ğŸ“Š ä»»å‹™å®Œæˆ: {task_metrics}")

            return PostMetricsBatch(
                posts=final_posts,
                batch_id=task_id,
                username=username,
                total_count=len(final_posts),
                processing_stage="playwright_incremental_completed"
            )

        except Exception as e:
            logging.error(f"âŒ [Task: {task_id}] çˆ¬å–éç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
            await publish_progress(task_id, "error", message=f"çˆ¬å–å¤±æ•—: {str(e)}")
            raise e
        finally:
            await self._cleanup(task_id)

    async def _setup_browser_and_auth(self, auth_json_content: Dict, task_id: str):
        """è¨­ç½®ç€è¦½å™¨å’Œèªè­‰"""
        # ä¿å­˜ playwright å¯¦ä¾‹ï¼Œä¾¿æ–¼å®Œæ•´æ¸…ç†ï¼Œé¿å… Windows Proactor æ®˜ç•™ç®¡é“
        self.playwright = await async_playwright().start()
        # ğŸ¬ 2025æ–°ç‰ˆThreadså½±ç‰‡æå–å„ªåŒ– - ç„¡æ‰‹å‹¢è‡ªå‹•æ’­æ”¾
        launch_args = [
            "--autoplay-policy=no-user-gesture-required",
            "--disable-background-media-suspend",
            "--disable-features=MediaSessionService",
            "--force-prefers-reduced-motion=0",
            "--disable-blink-features=AutomationControlled",
            "--disable-web-security",
            "--disable-features=VizDisplayCompositor",
        ]
        # åš´æ ¼é™ç´šé †åºï¼šchannel=chrome â†’ system å®‰è£ â†’ å…§å»º chromium
        try:
            self.browser = await self.playwright.chromium.launch(channel="chrome", headless=True, args=launch_args)
        except Exception as e1:
            logging.warning(f"âš ï¸ Chrome channel å•Ÿå‹•å¤±æ•—ï¼Œå˜—è©¦ system chromium: {e1}")
            try:
                self.browser = await self.playwright.chromium.launch(executable_path="/usr/bin/chromium" , headless=True, args=launch_args)
            except Exception as e2:
                logging.warning(f"âš ï¸ system chromium ä¸å¯ç”¨ï¼Œå›é€€å…§å»º chromium: {e2}")
                self.browser = await self.playwright.chromium.launch(headless=True, args=launch_args)
        # å‰µå»ºcontextï¼ˆè‡ªå‹•æ’­æ”¾é€šélaunch argsæ§åˆ¶ï¼‰
        self.context = await self.browser.new_context()
        
        # è¨­ç½®èªè­‰
        auth_file = Path(tempfile.gettempdir()) / f"{task_id}_auth.json"
        auth_file.write_text(json.dumps(auth_json_content))
        
        await self.context.add_cookies(auth_json_content.get('cookies', []))
        logging.info(f"ğŸ” [Task: {task_id}] èªè­‰è¨­ç½®å®Œæˆ")

    async def _cleanup(self, task_id: str):
        """æ¸…ç†è³‡æºï¼ˆåŠ å¼·ç‰ˆï¼šshield + timeoutï¼Œé¿å… Chromium æ®˜ç•™ï¼‰"""
        try:
            # å…ˆé—œé–‰ contextï¼Œå†é—œé–‰ browserï¼Œæœ€å¾Œåœæ­¢ playwright
            if self.context:
                try:
                    await asyncio.wait_for(asyncio.shield(self.context.close()), timeout=10)
                except Exception:
                    pass
                finally:
                    self.context = None

            if self.browser:
                try:
                    await asyncio.wait_for(asyncio.shield(self.browser.close()), timeout=10)
                except Exception:
                    pass
                finally:
                    self.browser = None
            
            # åˆªé™¤è‡¨æ™‚èªè­‰æª”æ¡ˆ
            auth_file = Path(tempfile.gettempdir()) / f"{task_id}_auth.json"
            if auth_file.exists():
                try:
                    auth_file.unlink()
                    logging.info(f"ğŸ—‘ï¸ [Task: {task_id}] å·²åˆªé™¤è‡¨æ™‚èªè­‰æª”æ¡ˆ: {auth_file}")
                except Exception:
                    pass

            if self.playwright:
                try:
                    await asyncio.wait_for(asyncio.shield(self.playwright.stop()), timeout=10)
                except Exception:
                    pass
                finally:
                    self.playwright = None
        except Exception as e:
            logging.warning(f"âš ï¸ [Task: {task_id}] æ¸…ç†è³‡æºæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}") 

    async def _save_debug_data(self, task_id: str, username: str, total_found: int, final_posts: List[PostMetrics]):
        """ä¿å­˜èª¿è©¦æ•¸æ“š"""
        try:
            raw_data = {
                "task_id": task_id,
                "username": username, 
                "timestamp": get_taipei_time().isoformat(),
                "total_found": total_found,
                "returned_count": len(final_posts),
                "posts": [
                    {
                        "url": post.url,
                        "post_id": post.post_id,
                        "likes_count": post.likes_count,
                        "comments_count": post.comments_count,
                        "reposts_count": post.reposts_count,
                        "shares_count": post.shares_count,
                        "views_count": post.views_count,
                        "calculated_score": post.calculate_score(),
                        "content": post.content,
                        "created_at": post.created_at.isoformat() if post.created_at else None,
                        "post_published_at": post.post_published_at.isoformat() if post.post_published_at else None,
                        "tags": post.tags,
                        "images": post.images,
                        "videos": post.videos
                    } for post in final_posts
                ]
            }
            
            timestamp = get_taipei_time().strftime("%Y%m%d_%H%M%S")
            raw_file = DEBUG_DIR / f"crawl_data_{timestamp}_{task_id[:8]}.json"
            raw_file.write_text(json.dumps(raw_data, indent=2, ensure_ascii=False), encoding="utf-8")
            logging.info(f"ğŸ’¾ [Task: {task_id}] å·²ä¿å­˜åŸå§‹æŠ“å–è³‡æ–™è‡³: {raw_file}")
            
        except Exception as e:
            logging.warning(f"âš ï¸ [Task: {task_id}] ä¿å­˜èª¿è©¦è³‡æ–™å¤±æ•—: {e}")
    
    async def _realtime_download_media(self, posts: List, task_id: str, username: str):
        """
        å³æ™‚ä¸‹è¼‰åª’é«”ï¼šåœ¨çˆ¬å–å®Œæˆå¾Œç«‹å³ä¸‹è¼‰æ¯å€‹è²¼æ–‡çš„åª’é«”æª”æ¡ˆ
        ç¢ºä¿åª’é«”URLçš„æ™‚æ•ˆæ€§ï¼Œç‰¹åˆ¥æ˜¯Instagramå½±ç‰‡
        """
        if not posts:
            return
            
        try:
            # å‹•æ…‹å°å…¥ä¸‹è¼‰æœå‹™ï¼Œé¿å…å¾ªç’°ä¾è³´
            from agents.vision.media_download_service import MediaDownloadService
            
            logging.info(f"â¬‡ï¸ [Task: {task_id}] é–‹å§‹å³æ™‚ä¸‹è¼‰ {len(posts)} å€‹è²¼æ–‡çš„åª’é«”...")
            
            # å»ºç«‹ä¸‹è¼‰è¨ˆåŠƒ
            download_plan = {}
            media_count = 0
            
            for post in posts:
                media_urls = []
                
                # æ”¶é›†åœ–ç‰‡URL
                if hasattr(post, 'images') and post.images:
                    media_urls.extend(post.images)
                    
                # æ”¶é›†å½±ç‰‡URL
                if hasattr(post, 'videos') and post.videos:
                    media_urls.extend(post.videos)
                
                if media_urls:
                    download_plan[post.url] = media_urls
                    media_count += len(media_urls)
            
            if not download_plan:
                logging.info(f"â„¹ï¸ [Task: {task_id}] æœ¬æ‰¹è²¼æ–‡æ²’æœ‰å¯ä¸‹è¼‰çš„åª’é«”")
                return
                
            logging.info(f"ğŸ“‹ [Task: {task_id}] æº–å‚™ä¸‹è¼‰ {media_count} å€‹åª’é«”æª”æ¡ˆ (ä¾†è‡ª {len(download_plan)} å€‹è²¼æ–‡)")
            
            # åŸ·è¡Œä¸‹è¼‰
            download_service = MediaDownloadService()
            result = await download_service.run_download(download_plan, concurrency_per_post=2)
            
            # è¨˜éŒ„çµæœ
            success_rate = (result['success'] / result['total'] * 100) if result['total'] > 0 else 0
            logging.info(f"âœ… [Task: {task_id}] å³æ™‚ä¸‹è¼‰å®Œæˆï¼šæˆåŠŸ {result['success']}/{result['total']} ({success_rate:.1f}%)")
            
            if result['failed'] > 0:
                logging.warning(f"âš ï¸ [Task: {task_id}] å¤±æ•— {result['failed']} å€‹åª’é«”ï¼Œç¨å¾Œå¯é€éUIé‡è©¦")
                
                # è¨˜éŒ„å¤±æ•—çš„è©³æƒ…ï¼ˆåƒ…å‰5å€‹ï¼‰
                failed_details = [d for d in result.get('details', []) if d.get('status') == 'failed']
                for detail in failed_details[:5]:
                    logging.warning(f"   âŒ {detail.get('original_url', 'N/A')}: {detail.get('error', 'Unknown error')}")
                    
        except Exception as e:
            logging.error(f"âŒ [Task: {task_id}] å³æ™‚ä¸‹è¼‰å¤±æ•—: {e}")
            # ä¸‹è¼‰å¤±æ•—ä¸å½±éŸ¿çˆ¬èŸ²ä¸»æµç¨‹ï¼Œåªè¨˜éŒ„éŒ¯èª¤
            
    async def _smart_scroll_new_mode(
        self, 
        page, 
        username: str, 
        target_count: int,
        existing_post_ids: set,
        anchor_post_id: str,
        max_scroll_rounds: int
    ) -> List[str]:
        """
        NEWæ¨¡å¼æ™ºèƒ½æ»¾å‹•ï¼šè£œè¶³æ–°è²¼æ–‡
        å¾æœ€æ–°é–‹å§‹ï¼Œç›´åˆ°é‡åˆ°éŒ¨é»æˆ–é”åˆ°ç›®æ¨™æ•¸é‡
        """
        collected_urls = []
        found_anchor = False
        scroll_round = 0
        
        logging.info(f"ğŸ”„ NEWæ¨¡å¼é–‹å§‹ï¼šç›®æ¨™ {target_count} ç¯‡ï¼ŒéŒ¨é» {anchor_post_id}")
        
        while scroll_round < max_scroll_rounds:
            # æ”¶é›†ç•¶å‰é é¢çš„æ–°URLs
            new_urls = await collect_urls_from_dom(page, existing_post_ids, username)
            
            # éæ¿¾é‡è¤‡ä¸¦æ·»åŠ 
            for url in new_urls:
                if url not in collected_urls:
                    collected_urls.append(url)
                    
            logging.debug(f"ğŸ”„ NEWæ¨¡å¼ç¬¬ {scroll_round+1} è¼ªï¼šç´¯è¨ˆ {len(collected_urls)}/{target_count}")
            
            # æª¢æŸ¥éŒ¨é»ï¼ˆæ¯3è¼ªæª¢æŸ¥ä¸€æ¬¡ä»¥æé«˜æ•ˆç‡ï¼‰
            if not found_anchor and scroll_round % 3 == 0:
                current_post_ids = await extract_current_post_ids(page)
                found_anchor, anchor_idx = is_anchor_visible(current_post_ids, anchor_post_id)
                
                if found_anchor and anchor_idx >= len(current_post_ids) * 0.5:
                    logging.info(f"ğŸ¯ NEWæ¨¡å¼æ‰¾åˆ°éŒ¨é»åœ¨å¾ŒåŠéƒ¨ï¼Œåœæ­¢æ»¾å‹•")
                    break
                    
            # æª¢æŸ¥åœæ­¢æ¢ä»¶
            if should_stop_new_mode(found_anchor, collected_urls, target_count):
                break
                
            # æª¢æŸ¥é é¢åº•éƒ¨
            if await check_page_bottom(page):
                logging.info(f"ğŸ“„ NEWæ¨¡å¼åˆ°é”é é¢åº•éƒ¨")
                break
                
            # æ»¾å‹•åˆ°ä¸‹ä¸€æ®µ
            await scroll_once(page)
            scroll_round += 1
            
        logging.info(f"âœ… NEWæ¨¡å¼å®Œæˆï¼š{len(collected_urls)}/{target_count}ï¼Œæ»¾å‹• {scroll_round} è¼ª")
        return collected_urls[:target_count]  # é™åˆ¶æ•¸é‡
        
    async def _smart_scroll_hist_mode(
        self,
        page,
        username: str,
        target_count: int, 
        existing_post_ids: set,
        anchor_post_id: str,
        max_scroll_rounds: int
    ) -> List[str]:
        """
        HISTæ¨¡å¼æ™ºèƒ½æ»¾å‹•ï¼šæ­·å²å›æº¯
        æ»¾å‹•åˆ°éŒ¨é»ä½ç½®ï¼Œç„¶å¾Œç¹¼çºŒå¾€ä¸‹æ”¶é›†æ›´èˆŠçš„è²¼æ–‡
        """
        collected_urls = []
        found_anchor = False
        passed_anchor = False
        scroll_round = 0
        
        logging.info(f"ğŸ”„ HISTæ¨¡å¼é–‹å§‹ï¼šç›®æ¨™ {target_count} ç¯‡ï¼ŒéŒ¨é» {anchor_post_id}")
        
        if not anchor_post_id:
            logging.warning("âš ï¸ HISTæ¨¡å¼éœ€è¦éŒ¨é»ï¼Œä½†æœªæä¾›ï¼Œé€€å›åˆ°æ™®é€šæ¨¡å¼")
            return await collect_urls_from_dom(page, existing_post_ids, username)
            
        while scroll_round < max_scroll_rounds:
            # æ»¾å‹•ä¸€æ¬¡
            await scroll_once(page)
            scroll_round += 1
            
            # æª¢æŸ¥æ˜¯å¦æ‰¾åˆ°éŒ¨é»ï¼ˆæ¯2è¼ªæª¢æŸ¥ä¸€æ¬¡ï¼‰
            if not found_anchor and scroll_round % 2 == 0:
                current_post_ids = await extract_current_post_ids(page)
                found_anchor, anchor_idx = is_anchor_visible(current_post_ids, anchor_post_id)
                
                if found_anchor:
                    logging.info(f"ğŸ¯ HISTæ¨¡å¼æ‰¾åˆ°éŒ¨é»åœ¨ä½ç½® {anchor_idx}")
                    if anchor_idx >= len(current_post_ids) * 0.6:
                        passed_anchor = True
                        logging.info(f"ğŸš€ HISTæ¨¡å¼è¶ŠééŒ¨é»ï¼Œé–‹å§‹æ”¶é›†æ­·å²è²¼æ–‡")
                        
            # åªæœ‰è¶ŠééŒ¨é»å¾Œæ‰é–‹å§‹æ”¶é›†
            if passed_anchor:
                new_urls = await collect_urls_from_dom(page, existing_post_ids, username)
                
                # éæ¿¾é‡è¤‡ä¸¦æ·»åŠ 
                for url in new_urls:
                    if url not in collected_urls:
                        collected_urls.append(url)
                        
                logging.debug(f"ğŸ”„ HISTæ¨¡å¼ç¬¬ {scroll_round} è¼ªï¼šæ­·å²è²¼æ–‡ {len(collected_urls)}/{target_count}")
                
            # æª¢æŸ¥åœæ­¢æ¢ä»¶
            if should_stop_hist_mode(found_anchor, passed_anchor, collected_urls, target_count, scroll_round, max_scroll_rounds):
                break
                
            # æª¢æŸ¥é é¢åº•éƒ¨
            if await check_page_bottom(page):
                logging.info(f"ğŸ“„ HISTæ¨¡å¼åˆ°é”é é¢åº•éƒ¨")
                break
                
        if not found_anchor:
            logging.warning(f"âš ï¸ HISTæ¨¡å¼æœªæ‰¾åˆ°éŒ¨é» {anchor_post_id}ï¼Œå¯èƒ½éŒ¨é»å¤ªèˆŠ")
            
        logging.info(f"âœ… HISTæ¨¡å¼å®Œæˆï¼š{len(collected_urls)}/{target_count}ï¼Œæ»¾å‹• {scroll_round} è¼ª")
        return collected_urls[:target_count]  # é™åˆ¶æ•¸é‡
    
    async def _collect_urls_realtime_style(
        self, 
        page, 
        username: str, 
        target_count: int, 
        existing_post_ids: set, 
        incremental: bool,
        max_scroll_rounds: int = 80
    ) -> List[str]:
        """
        æ¡ç”¨Realtime Crawleré¢¨æ ¼çš„URLæ”¶é›†
        è¶³é¡æ”¶é›†URLsï¼Œæ”¯æŒå¢é‡æª¢æ¸¬
        """
        urls = []
        scroll_rounds = 0
        no_new_content_rounds = 0
        max_no_new_rounds = 15
        consecutive_existing_rounds = 0
        max_consecutive_existing = 15
        
        logging.info(f"ğŸ”„ é–‹å§‹Realtimeé¢¨æ ¼URLæ”¶é›†ï¼ˆç›®æ¨™: {target_count} ç¯‡ï¼Œå¢é‡: {incremental}ï¼‰")
        
        while len(urls) < target_count and scroll_rounds < max_scroll_rounds:
            # æå–ç•¶å‰é é¢çš„URLsï¼ˆæ¡ç”¨Realtimeçš„JavaScripté‚è¼¯ï¼‰
            js_code = """
                function(targetUsername) {
                    const links = Array.from(document.querySelectorAll('a[href*="/post/"]'));
                    return [...new Set(links.map(link => link.href)
                        .filter(url => url.includes('/post/'))
                        .filter(url => {
                            // æª¢æŸ¥URLæ˜¯å¦å±¬æ–¼ç›®æ¨™ç”¨æˆ¶
                            const usernamePart = url.split('/@')[1];
                            if (!usernamePart) return false;
                            const extractedUsername = usernamePart.split('/')[0];
                            
                            const postId = url.split('/post/')[1];
                            // éæ¿¾æ‰ mediaã€ç„¡æ•ˆIDç­‰ï¼Œä¸¦ç¢ºä¿æ˜¯ç›®æ¨™ç”¨æˆ¶çš„è²¼æ–‡
                            return postId && 
                                   postId !== 'media' && 
                                   postId.length > 5 && 
                                   /^[A-Za-z0-9_-]+$/.test(postId) &&
                                   extractedUsername === targetUsername;
                        }))];
                }
            """
            current_urls = await page.evaluate(js_code, username)
            
            before_count = len(urls)
            new_urls_this_round = 0
            found_existing_this_round = False
            existing_skipped_this_round = 0
            
            # å»é‡ä¸¦æ·»åŠ æ–°URLsï¼ˆæ”¯æŒå¢é‡æª¢æ¸¬ï¼‰
            for url in current_urls:
                # å¾ URL æå–å®Œæ•´çš„ post_id (æ ¼å¼: username_postid)
                raw_post_id = url.split('/')[-1] if url else None
                post_id = f"{username}_{raw_post_id}" if raw_post_id else None
                
                # è·³éå·²æ”¶é›†çš„URL
                if url in urls:
                    continue
                    
                # å¢é‡æ¨¡å¼ï¼šæª¢æŸ¥æ˜¯å¦å·²å­˜åœ¨æ–¼è³‡æ–™åº«
                if incremental and post_id in existing_post_ids:
                    logging.info(f"   ğŸ” [{len(urls)+1}] ç™¼ç¾å·²çˆ¬å–è²¼æ–‡: {post_id} - è·³é (å¢é‡æ¨¡å¼)")
                    found_existing_this_round = True
                    existing_skipped_this_round += 1
                    continue
                elif incremental:
                    logging.info(f"   ğŸ†• [{len(urls)+1}] ç™¼ç¾æ–°è²¼æ–‡: {post_id} (å¢é‡æ¨¡å¼)")
                
                # æª¢æŸ¥æ˜¯å¦å·²é”åˆ°ç›®æ¨™æ•¸é‡
                if len(urls) >= target_count:
                    break
                    
                urls.append(url)
                new_urls_this_round += 1
                
                status_icon = "ğŸ†•" if incremental else "ğŸ“"
                logging.debug(f"   {status_icon} [{len(urls)}] ç™¼ç¾: {post_id}")
            
            # å¢é‡æ¨¡å¼ï¼šä½¿ç”¨ä¿®å¾©å¾Œçš„æ™ºèƒ½åœæ­¢æ¢ä»¶
            if incremental:
                # ä½¿ç”¨ä¿®å¾©å¾Œçš„æ™ºèƒ½åœæ­¢åˆ¤æ–·ï¼ˆåªæª¢æŸ¥æ˜¯å¦æ”¶é›†è¶³å¤ ï¼‰
                if should_stop_incremental_mode(
                    found_existing_this_round, consecutive_existing_rounds, 
                    len(urls), target_count, max_consecutive_existing
                ):
                    break
            
            # æª¢æŸ¥æ˜¯å¦æœ‰æ–°å…§å®¹
            new_urls_found = len(urls) - before_count
            
            # é—œéµä¿®å¾©ï¼šé‡æ–°è¨­è¨ˆåœæ­¢é‚è¼¯
            if new_urls_found > 0:
                # ç™¼ç¾æ–°URLï¼Œé‡ç½®ç„¡æ–°å…§å®¹è¨ˆæ•¸å™¨
                no_new_content_rounds = 0
                logging.debug(f"   âœ… ç¬¬{scroll_rounds+1}è¼ªç™¼ç¾ {new_urls_found} å€‹æ–°URLï¼Œé‡ç½®ç„¡æ–°å…§å®¹è¨ˆæ•¸å™¨")
            elif not found_existing_this_round:
                # æ—¢æ²’æœ‰æ–°URLä¹Ÿæ²’æœ‰å·²å­˜åœ¨è²¼æ–‡ï¼ŒçœŸæ­£çš„ç„¡å…§å®¹è¼ªæ¬¡
                no_new_content_rounds += 1
                logging.debug(f"   â³ ç¬¬{scroll_rounds+1}è¼ªå®Œå…¨æ²’æœ‰ç™¼ç¾å…§å®¹ ({no_new_content_rounds}/{max_no_new_rounds})")
                
                if no_new_content_rounds >= max_no_new_rounds:
                    # åŸ·è¡Œæœ€å¾Œå˜—è©¦æ©Ÿåˆ¶ - ä½¿ç”¨æ–°çš„ç­–ç•¥
                    attempt_result = await final_attempt_scroll(page)
                    
                    if attempt_result > 0:
                        # æª¢æŸ¥æœ€å¾Œå˜—è©¦æ˜¯å¦æœ‰æ–°å…§å®¹
                        final_urls = await page.evaluate(js_code, username)
                        final_new_count = 0
                        
                        for url in final_urls:
                            raw_post_id = url.split('/')[-1] if url else None
                            post_id = f"{username}_{raw_post_id}" if raw_post_id else None
                            
                            # æª¢æŸ¥æ˜¯å¦æ˜¯æ–°çš„URL
                            if url not in urls:
                                # å¢é‡æ¨¡å¼æª¢æŸ¥
                                if incremental and post_id in existing_post_ids:
                                    continue
                                # å°‡æ–°URLæ·»åŠ åˆ°æ”¶é›†åˆ—è¡¨
                                if len(urls) < target_count:
                                    urls.append(url)
                                    final_new_count += 1
                                    logging.info(f"   ğŸ“ [{len(urls)}] æœ€å¾Œç™¼ç¾: {raw_post_id}")
                        
                        if final_new_count == 0:
                            logging.info("   ğŸ›‘ æœ€å¾Œå˜—è©¦ç„¡æ–°å…§å®¹ï¼Œç¢ºèªåˆ°é”åº•éƒ¨")
                            break
                        else:
                            logging.info(f"   ğŸ¯ æœ€å¾Œå˜—è©¦ç™¼ç¾{final_new_count}å€‹æ–°URLï¼Œç¹¼çºŒæ”¶é›†...")
                            no_new_content_rounds = 0  # é‡ç½®è¨ˆæ•¸å™¨
                            continue
                    else:
                        logging.info("   ğŸ›‘ æœ€å¾Œå˜—è©¦åŸ·è¡Œå¤±æ•—ï¼Œåœæ­¢æ”¶é›†")
                        break
                        
                    # ä½¿ç”¨æ–°çš„éå¢ç­‰å¾…ç­–ç•¥
                    await progressive_wait(no_new_content_rounds)
            else:
                # æœ‰å·²å­˜åœ¨è²¼æ–‡ä½†æ²’æœ‰æ–°URLï¼Œä¸å¢åŠ ç„¡æ–°å…§å®¹è¨ˆæ•¸å™¨ï¼ˆç¹¼çºŒå°‹æ‰¾æ›´èˆŠçš„å…§å®¹ï¼‰
                logging.debug(f"   ğŸ” ç¬¬{scroll_rounds+1}è¼ªç™¼ç¾å·²å­˜åœ¨è²¼æ–‡ä½†ç„¡æ–°URLï¼Œç¹¼çºŒå°‹æ‰¾æ›´èˆŠå…§å®¹...")
            
            # ä½¿ç”¨å¢å¼·çš„æ»¾å‹•ç­–ç•¥
            await enhanced_scroll_with_strategy(page, scroll_rounds)
            scroll_rounds += 1
            
            # å®šæœŸé¡¯ç¤ºé€²åº¦
            if scroll_rounds % 5 == 0:
                logging.info(f"   ğŸ“Š æ»¾å‹•é€²åº¦: ç¬¬{scroll_rounds}è¼ªï¼Œå·²æ”¶é›†{len(urls)}å€‹URL")
        
        logging.info(f"âœ… URLæ”¶é›†å®Œæˆï¼š{len(urls)} å€‹URLï¼Œæ»¾å‹• {scroll_rounds} è¼ª")
        return urls
    
    async def _convert_urls_to_posts(self, urls: List[str], username: str, mode: str, task_id: str) -> List[PostMetrics]:
        """è½‰æ›URLsç‚ºPostMetricsä¸¦éæ¿¾éç›®æ¨™ç”¨æˆ¶"""
        valid_posts = []
        
        for url in urls:
            # é©—è­‰URLæ˜¯å¦ç¢ºå¯¦å±¬æ–¼ç›®æ¨™ç”¨æˆ¶
            if f"@{username}/post/" not in url:
                logging.debug(f"âš ï¸ è·³ééç›®æ¨™ç”¨æˆ¶çš„URL: {url}")
                continue
                
            post_id = url.split('/')[-1]
            post_metrics = PostMetrics(
                post_id=f"{username}_{post_id}",
                username=username,
                url=url,
                content="",
                created_at=get_taipei_time(),
                fetched_at=get_taipei_time(),
                source=f"playwright_{mode}",
                processing_stage="urls_extracted",
                is_complete=False,
                likes_count=0,
                comments_count=0,
                reposts_count=0,
                shares_count=0,
                views_count=None,
            )
            valid_posts.append(post_metrics)
            
        logging.info(f"âœ… [Task: {task_id}] URLè½‰æ›ï¼š{len(urls)} å€‹URL â†’ {len(valid_posts)} å€‹æœ‰æ•ˆPostMetrics")
        return valid_posts