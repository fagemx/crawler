import json
import asyncio
import logging
import random
import re
import tempfile
import uuid
from pathlib import Path
from typing import Dict, List, Optional, AsyncIterable, Callable, Any
from datetime import datetime
import httpx

from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeoutError, Browser, BrowserContext, Page, Route, APIResponse

from common.settings import get_settings
from common.a2a import stream_text, stream_data, stream_status, stream_error, TaskState
from common.models import PostMetrics, PostMetricsBatch
from common.nats_client import publish_progress
from common.utils import (
    get_best_video_url,
    get_best_image_url,
    generate_post_url,
    first_of,
    parse_thread_item
)

# Vision Agent URL - æŒ‡å‘æˆ‘å€‘å³å°‡å»ºç«‹çš„æ–°ç«¯é»
VISION_AGENT_URL = "http://vision-agent:8005/v1/vision/extract-views-from-image"

# èª¿è©¦æª”æ¡ˆè·¯å¾‘
DEBUG_DIR = Path(__file__).parent / "debug"
DEBUG_DIR.mkdir(exist_ok=True)
DEBUG_FAILED_ITEM_FILE = DEBUG_DIR / "failed_post_sample.json"
SAMPLE_THREAD_ITEM_FILE = DEBUG_DIR / "sample_thread_item.json"
RAW_CRAWL_DATA_FILE = DEBUG_DIR / "raw_crawl_data.json"

# è¨­å®šæ—¥èªŒ
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# --- GraphQL API è©³ç´°è³‡è¨Š ---
GRAPHQL_RE = re.compile(r"/graphql/query")
# ä¿ç•™ç”¨ä¾† debugï¼›å¯¦éš›é‚è¼¯æ”¹æˆã€Œçœ‹çµæ§‹ã€å³å¯
POST_QUERY_NAMES = set()

# --- å¼·å¥çš„æ¬„ä½å°ç…§è¡¨ ---
FIELD_MAP = {
    "like_count": [
        "like_count", "likeCount", 
        ["feedback_info", "aggregated_like_count"],
        ["like_info", "count"]
    ],
    "comment_count": [
        ["text_post_app_info", "direct_reply_count"],
        "comment_count", "commentCount",
        ["reply_info", "count"]
    ],
    "share_count": [
        ["text_post_app_info", "reshare_count"],  # çœŸæ­£çš„ shares æ¬„ä½
        "reshareCount", "share_count", "shareCount"
    ],
    "repost_count": [
        ["text_post_app_info", "repost_count"],  # reposts æ¬„ä½
        "repostCount", "repost_count"
    ],
    "content": [
        ["caption", "text"],
        "caption", "text", "content"
    ],
    "author": [
        ["user", "username"], 
        ["owner", "username"],
        ["user", "handle"]
    ],
    "created_at": [
        "taken_at", "taken_at_timestamp", 
        "publish_date", "created_time"
    ],
    "post_id": [
        "pk", "id", "post_id"
    ],
    "code": [
        "code", "shortcode", "media_code"
    ],
    # å¢åŠ ç›´æ¥å¾ API å˜—è©¦ç²å– views çš„è·¯å¾‘
    "view_count": [
        ["feedback_info", "view_count"],
        ["video_info", "play_count"],
        "view_count",
        "views",
        "impression_count"
    ],
}

def parse_post_data(thread_item: Dict[str, Any], username: str) -> Optional[PostMetrics]:
    """
    å¼·å¥çš„è²¼æ–‡è§£æå™¨ï¼Œä½¿ç”¨å¤šéµ fallback æ©Ÿåˆ¶è™•ç†æ¬„ä½è®Šå‹•ã€‚
    æ”¯æ´ Threads GraphQL API çš„ä¸åŒç‰ˆæœ¬å’Œæ¬„ä½å‘½åè®ŠåŒ–ã€‚
    """
    # ä½¿ç”¨æ™ºèƒ½æœå°‹æ‰¾åˆ°çœŸæ­£çš„è²¼æ–‡ç‰©ä»¶
    post = parse_thread_item(thread_item)
    
    if not post:
        logging.info(f"âŒ æ‰¾ä¸åˆ°æœ‰æ•ˆçš„ post ç‰©ä»¶ï¼Œæ”¶åˆ°çš„è³‡æ–™éµ: {list(thread_item.keys())}")
        logging.info(f"âŒ thread_item å…§å®¹ç¯„ä¾‹: {str(thread_item)[:300]}...")
        
        # è‡ªå‹•å„²å­˜ç¬¬ä¸€ç­† raw JSON ä¾›åˆ†æ
        try:
            if not DEBUG_FAILED_ITEM_FILE.exists():
                DEBUG_FAILED_ITEM_FILE.write_text(
                    json.dumps(thread_item, indent=2, ensure_ascii=False),
                    encoding="utf-8" # æ˜ç¢ºæŒ‡å®š UTF-8
                )
                logging.info(f"ğŸ“ å·²å„²å­˜å¤±æ•—ç¯„ä¾‹è‡³ {DEBUG_FAILED_ITEM_FILE}")
        except Exception:
            pass  # å¯«æª”å¤±æ•—ä¸å½±éŸ¿ä¸»è¦åŠŸèƒ½
            
        return None

    # ä½¿ç”¨å¼·å¥çš„æ¬„ä½è§£æ
    post_id = first_of(post, *FIELD_MAP["post_id"])
    code = first_of(post, *FIELD_MAP["code"])
    
    if not post_id:
        logging.info(f"âŒ æ‰¾ä¸åˆ°æœ‰æ•ˆçš„ post_idï¼Œå¯ç”¨æ¬„ä½: {list(post.keys())}")
        return None
        
    if not code:
        logging.info(f"âŒ æ‰¾ä¸åˆ°æœ‰æ•ˆçš„ codeï¼Œå¯ç”¨æ¬„ä½: {list(post.keys())}")
        return None

    # --- URL ä¿®å¾©ï¼šä½¿ç”¨æ­£ç¢ºçš„æ ¼å¼ ---
    # èˆŠæ ¼å¼ (éŒ¯èª¤): f"https://www.threads.net/t/{code}"
    # æ–°æ ¼å¼ (æ­£ç¢º): f"https://www.threads.com/@{username}/post/{code}"
    url = generate_post_url(username, code)

    # ä½¿ç”¨å¤šéµ fallback è§£ææ‰€æœ‰æ¬„ä½
    author = first_of(post, *FIELD_MAP["author"]) or username
    content = first_of(post, *FIELD_MAP["content"]) or ""
    like_count = first_of(post, *FIELD_MAP["like_count"]) or 0
    comment_count = first_of(post, *FIELD_MAP["comment_count"]) or 0
    share_count = first_of(post, *FIELD_MAP["share_count"]) or 0
    repost_count = first_of(post, *FIELD_MAP["repost_count"]) or 0
    created_at = first_of(post, *FIELD_MAP["created_at"])
    
    # åµæ¸¬æœªçŸ¥æ¬„ä½ï¼ˆç”¨æ–¼èª¿è©¦å’Œæ”¹é€²ï¼‰
    unknown_keys = set(post.keys()) - {
        'pk', 'id', 'post_id', 'code', 'shortcode', 'media_code',
        'user', 'owner', 'caption', 'text', 'content',
        'like_count', 'likeCount', 'feedback_info', 'like_info',
        'text_post_app_info', 'comment_count', 'commentCount', 'reply_info',
        'taken_at', 'taken_at_timestamp', 'publish_date', 'created_time',
        # å¸¸è¦‹ä½†ä¸é‡è¦çš„æ¬„ä½
        'media_type', 'has_liked', 'image_versions2', 'video_versions',
        'carousel_media', 'location', 'user_tags', 'usertags'
    }
    
    if unknown_keys:
        logging.debug(f"ğŸ” ç™¼ç¾æœªçŸ¥æ¬„ä½: {unknown_keys}")
        # å¯ä»¥é¸æ“‡å¯«å…¥æª”æ¡ˆä»¥ä¾›åˆ†æ
        try:
            with open("unknown_fields.log", "a", encoding="utf-8") as f:
                f.write(f"{json.dumps(list(unknown_keys))}\n")
        except Exception:
            pass  # å¯«æª”å¤±æ•—ä¸å½±éŸ¿ä¸»è¦åŠŸèƒ½

    # --- MEDIA -------------------------------------------------------------
    images, videos = [], []

    def append_image(url):
        if url and url not in images:
            images.append(url)

    def append_video(url):
        if url and url not in videos:
            videos.append(url)

    def best_candidate(candidates: list[dict], prefer_mp4=False):
        """
        å¾ image_versions2 æˆ– video_versions è£¡æŒ‘ã€Œå¯¬åº¦æœ€å¤§çš„é‚£å€‹ã€URL
        """
        if not candidates:
            return None
        key = "url"
        return max(candidates, key=lambda c: c.get("width", 0)).get(key)

    # 1) å–®åœ– / å–®ç‰‡
    if "image_versions2" in post:
        append_image(best_candidate(post["image_versions2"].get("candidates", [])))
    if "video_versions" in post:
        append_video(best_candidate(post.get("video_versions", []), prefer_mp4=True))

    # 2) è¼ªæ’­ carousel_media (åŠ å…¥ None çš„å®‰å…¨è™•ç†)
    for media in post.get("carousel_media") or []:
        if "image_versions2" in media:
            append_image(best_candidate(media["image_versions2"].get("candidates", [])))
        if "video_versions" in media:
            append_video(best_candidate(media.get("video_versions", []), prefer_mp4=True))
    # -----------------------------------------------------------------------


    # æˆåŠŸè§£æï¼Œè¨˜éŒ„éƒ¨åˆ†è³‡è¨Šä¾›é™¤éŒ¯
    logging.info(f"âœ… æˆåŠŸè§£æè²¼æ–‡ {post_id}: ä½œè€…={author}, è®šæ•¸={like_count}, åœ–ç‰‡={len(images)}, å½±ç‰‡={len(videos)}")
    
    return PostMetrics(
        url=url,
        post_id=str(post_id),
        username=username,
        source="playwright",
        processing_stage="playwright_crawled",
        likes_count=int(like_count) if isinstance(like_count, (int, str)) and str(like_count).isdigit() else 0,
        comments_count=int(comment_count) if isinstance(comment_count, (int, str)) and str(comment_count).isdigit() else 0,
        reposts_count=int(repost_count) if isinstance(repost_count, (int, str)) and str(repost_count).isdigit() else 0,
        shares_count=int(share_count) if isinstance(share_count, (int, str)) and str(share_count).isdigit() else 0,
        content=content,
        created_at=datetime.fromtimestamp(created_at) if created_at and isinstance(created_at, (int, float)) else datetime.utcnow(),
        images=images,
        videos=videos,
        # æ–°å¢ï¼šç›´æ¥å¾ API è§£æ views_countï¼ˆæŒ‰æŒ‡å¼•å„ªå…ˆå˜—è©¦ APIï¼‰
        views_count=first_of(post, *FIELD_MAP["view_count"]) if first_of(post, *FIELD_MAP["view_count"]) is not None else None,
    )

# +++ æ–°å¢ï¼šå¾å‰ç«¯è§£æç€è¦½æ•¸çš„è¼”åŠ©å‡½å¼ï¼ˆä¿®å¾©ç©ºæ ¼å•é¡Œï¼‰ +++
def parse_views_text(text: Optional[str]) -> Optional[int]:
    """å°‡ '161.9è¬æ¬¡ç€è¦½' æˆ– '4 è¬æ¬¡ç€è¦½' æˆ– '1.2M views' é€™é¡æ–‡å­—è½‰æ›ç‚ºæ•´æ•¸"""
    if not text:
        return None
    try:
        original_text = text
        
        # ç§»é™¤ä¸å¿…è¦çš„æ–‡å­—ï¼Œä¿ç•™æ•¸å­—å’Œå–®ä½
        text = re.sub(r'ä¸²æ–‡\s*', '', text)  # ç§»é™¤ "ä¸²æ–‡"
        
        # è™•ç†ä¸­æ–‡æ ¼å¼ï¼š1.2è¬ã€4 è¬æ¬¡ç€è¦½ã€5000æ¬¡ç€è¦½
        if 'è¬' in text:
            match = re.search(r'([\d.]+)\s*è¬', text)  # å…è¨±æ•¸å­—å’Œè¬ä¹‹é–“æœ‰ç©ºæ ¼
            if match:
                return int(float(match.group(1)) * 10000)
        elif 'å„„' in text:
            match = re.search(r'([\d.]+)\s*å„„', text)  # å…è¨±æ•¸å­—å’Œå„„ä¹‹é–“æœ‰ç©ºæ ¼
            if match:
                return int(float(match.group(1)) * 100000000)
        
        # è™•ç†è‹±æ–‡æ ¼å¼ï¼š1.2M views, 500K views
        text_upper = text.upper()
        if 'M' in text_upper:
            match = re.search(r'([\d.]+)M', text_upper)
            if match:
                return int(float(match.group(1)) * 1000000)
        elif 'K' in text_upper:
            match = re.search(r'([\d.]+)K', text_upper)
            if match:
                return int(float(match.group(1)) * 1000)
        
        # è™•ç†ç´”æ•¸å­—æ ¼å¼ï¼ˆå¯èƒ½åŒ…å«é€—è™Ÿï¼‰
        match = re.search(r'[\d,]+', text)
        if match:
            return int(match.group(0).replace(',', ''))
        
        logging.debug(f"ğŸ” ç„¡æ³•è§£æç€è¦½æ•¸æ–‡å­—: '{original_text}'")
        return None
        
    except (ValueError, IndexError) as e:
        logging.warning(f"âš ï¸ ç„¡æ³•è§£æç€è¦½æ•¸æ–‡å­—: '{text}' - {e}")
        return None

class PlaywrightLogic:
    """
    ä½¿ç”¨ Playwright é€²è¡Œçˆ¬èŸ²çš„æ ¸å¿ƒé‚è¼¯ã€‚
    """
    def __init__(self):
        self.settings = get_settings().playwright
        self.known_queries = set()  # è¿½è¹¤å·²è¦‹éçš„æŸ¥è©¢åç¨±
        self.context = None  # åˆå§‹åŒ– context

    def _build_response_handler(self, username: str, posts: dict, task_id: str, max_posts: int, stream_callback):
        """å»ºç«‹ GraphQL å›æ‡‰è™•ç†å™¨ - ä½¿ç”¨çµæ§‹åˆ¤æ–·è€Œéåç¨±ç™½åå–®"""
        async def _handle_response(res):
            if not GRAPHQL_RE.search(res.url):
                return
                
            qname = res.request.headers.get("x-fb-friendly-name", "UNKNOWN")
            try:
                data = await res.json()
            except Exception:
                return  # é JSON å›æ‡‰ï¼Œç›´æ¥è·³é
                
            # çµæ§‹åˆ¤æ–·ï¼šåªè¦æœ‰ data.mediaData.edges å°±æ˜¯è²¼æ–‡è³‡æ–™
            edges = data.get("data", {}).get("mediaData", {}).get("edges", [])
            if not edges:
                return  # è·Ÿè²¼æ–‡ç„¡é—œï¼Œå¿½ç•¥
                
            # è§£æè²¼æ–‡ - æ”¯æ´æ–°èˆŠçµæ§‹çš„ fallback
            new_count = 0
            for edge in edges:
                # æ”¯æ´ thread_items / items çš„ fallback
                node = edge.get("node", {})
                thread_items = node.get("thread_items") or node.get("items") or []
                
                # è‡ªå‹•å„²å­˜ç¬¬ä¸€ç­†æˆåŠŸçš„ thread_item ä¾›åˆ†æ
                if thread_items and new_count == 0:
                    try:
                        if not SAMPLE_THREAD_ITEM_FILE.exists():
                            SAMPLE_THREAD_ITEM_FILE.write_text(
                                json.dumps(thread_items[0], indent=2, ensure_ascii=False),
                                encoding="utf-8" # æ˜ç¢ºæŒ‡å®š UTF-8
                            )
                            logging.info(f"ğŸ“ å·²å„²å­˜æˆåŠŸç¯„ä¾‹è‡³ {SAMPLE_THREAD_ITEM_FILE}")
                    except Exception:
                        pass

                for item in thread_items:
                    parsed_post = parse_post_data(item, username)
                    if parsed_post and parsed_post.post_id not in posts:
                        posts[parsed_post.post_id] = parsed_post
                        new_count += 1
                        
                        # ğŸ”¥ æ–°å¢ï¼šæ¯è§£æä¸€å€‹è²¼æ–‡å°±ç™¼å¸ƒå³æ™‚é€²åº¦
                        from common.nats_client import publish_progress
                        await publish_progress(
                            task_id, 
                            "post_parsed",
                            username=username,
                            post_id=parsed_post.post_id,
                            current=len(posts),
                            total=max_posts,
                            progress=len(posts) / max_posts,
                            content_preview=parsed_post.content[:50] + "..." if parsed_post.content else "ç„¡å…§å®¹",
                            likes=parsed_post.likes_count
                        )
                        
            if new_count > 0:
                logging.info(f"âœ… [{qname}] +{new_count} (ç¸½ {len(posts)}/{max_posts})")
                # ä½¿ç”¨å›èª¿å‡½æ•¸ç™¼é€ä¸²æµè¨Šæ¯ (å¦‚æœæœ‰çš„è©±)
                if stream_callback:
                    stream_callback(f"âœ… å¾ {qname} è§£æåˆ° {new_count} å‰‡æ–°è²¼æ–‡ï¼Œç¸½æ•¸: {len(posts)}")
                    
                # ğŸ”¥ æ–°å¢ï¼šæ¯æ‰¹è§£æå®Œæˆå¾Œçš„é€²åº¦æ›´æ–°
                from common.nats_client import publish_progress
                await publish_progress(
                    task_id, 
                    "batch_parsed",
                    username=username,
                    batch_size=new_count,
                    current=len(posts),
                    total=max_posts,
                    progress=len(posts) / max_posts,
                    query_name=qname
                )
            
            # æª¢æŸ¥æ˜¯å¦å·²é”åˆ°ç›®æ¨™æ•¸é‡
            if len(posts) >= max_posts:
                logging.info(f"é”åˆ°ç›®æ¨™è²¼æ–‡æ•¸ {max_posts}ï¼Œå°‡ç›¡å¿«åœæ­¢æ»¾å‹•ã€‚")
                # é€™è£¡å¯ä»¥è§¸ç™¼ä¸€å€‹äº‹ä»¶ä¾†åœæ­¢æ»¾å‹•ï¼Œä½†ç›®å‰æ¶æ§‹ä¸‹æœƒè‡ªç„¶åœæ­¢
                pass
                
            # è¨˜éŒ„æ–°ç™¼ç¾çš„æŸ¥è©¢åç¨±ï¼ˆç”¨æ–¼é™¤éŒ¯ï¼‰
            if qname not in self.known_queries:
                self.known_queries.add(qname)
                # å¯é¸ï¼šå¯«å…¥æª”æ¡ˆä»¥ä¾›æ—¥å¾Œåˆ†æ
                try:
                    with open("seen_queries.txt", "a", encoding="utf-8") as f:
                        f.write(f"{qname}\n")
                except Exception:
                    pass  # å¯«æª”å¤±æ•—ä¸å½±éŸ¿ä¸»è¦åŠŸèƒ½
                    
        return _handle_response

    async def get_ordered_post_urls_from_page(self, page: Page, username: str, max_posts: int) -> List[str]:
        """
        å¾ç”¨æˆ¶é é¢ç›´æ¥æå–è²¼æ–‡ URLsï¼Œä¿æŒæ™‚é–“é †åº
        é€™æ˜¯è§£æ±ºè²¼æ–‡é †åºæ··äº‚å•é¡Œçš„é—œéµæ–¹æ³•
        """
        user_url = f"https://www.threads.com/@{username}"
        logging.info(f"ğŸ” æ­£åœ¨å¾ç”¨æˆ¶é é¢ç²å–æœ‰åºçš„è²¼æ–‡ URLs: {user_url}")
        
        await page.goto(user_url, wait_until="networkidle")
        
        # ç­‰å¾…é é¢è¼‰å…¥
        await asyncio.sleep(3)
        
        # æ»¾å‹•ä»¥è¼‰å…¥æ›´å¤šè²¼æ–‡ï¼ˆä½†ä¸è¦æ»¾å‹•å¤ªå¤šæ¬¡é¿å…è¼‰å…¥éèˆŠçš„è²¼æ–‡ï¼‰
        scroll_count = min(3, max(1, max_posts // 10))  # æ ¹æ“šéœ€æ±‚å‹•æ…‹èª¿æ•´æ»¾å‹•æ¬¡æ•¸
        for i in range(scroll_count):
            await page.mouse.wheel(0, 1000)
            await asyncio.sleep(2)
        
        # æå–è²¼æ–‡ URLsï¼Œä¿æŒåŸå§‹é †åº
        post_urls = await page.evaluate("""
            () => {
                // ç²å–æ‰€æœ‰è²¼æ–‡é€£çµï¼Œä¿æŒDOMä¸­çš„åŸå§‹é †åº
                const links = Array.from(document.querySelectorAll('a[href*="/post/"]'));
                const urls = [];
                const seen = new Set();
                
                // éæ­·æ™‚ä¿æŒé †åºï¼Œåªå»é‡ä½†ä¸é‡æ’
                for (const link of links) {
                    const url = link.href;
                    if (url.includes('/post/') && !seen.has(url)) {
                        seen.add(url);
                        urls.push(url);
                    }
                }
                
                return urls;
            }
        """)
        
        # é™åˆ¶æ•¸é‡ä½†ä¿æŒé †åº
        post_urls = post_urls[:max_posts]
        logging.info(f"   âœ… æŒ‰æ™‚é–“é †åºæ‰¾åˆ° {len(post_urls)} å€‹è²¼æ–‡ URLs")
        
        return post_urls

    async def fetch_posts(
        self,
        username: str,
        max_posts: int,
        auth_json_content: Dict,
        task_id: str = None
    ) -> PostMetricsBatch:
        """
        ä½¿ç”¨æŒ‡å®šçš„èªè­‰å…§å®¹çˆ¬å–è²¼æ–‡ã€‚
        æ­¤ç‰ˆæœ¬æœƒèšåˆæ‰€æœ‰çµæœï¼Œä¸¦ä¸€æ¬¡æ€§è¿”å› PostMetricsBatchã€‚
        """
        if task_id is None:
            task_id = str(uuid.uuid4())
            
        target_url = f"https://www.threads.com/@{username}"
        posts: Dict[str, PostMetrics] = {}
        
        # ç™¼å¸ƒé–‹å§‹çˆ¬å–çš„é€²åº¦
        await publish_progress(task_id, "fetch_start", username=username, max_posts=max_posts)
        
        # 1. å®‰å…¨åœ°å°‡ auth.json å…§å®¹å¯«å…¥è‡¨æ™‚æª”æ¡ˆ
        auth_file = Path(tempfile.gettempdir()) / f"{task_id or uuid.uuid4()}_auth.json"
        try:
            with open(auth_file, 'w', encoding='utf-8') as f:
                json.dump(auth_json_content, f)

            async with async_playwright() as p:
                browser = await p.chromium.launch(
                    headless=self.settings.headless,
                    timeout=self.settings.navigation_timeout,
                    args=["--no-sandbox", "--disable-dev-shm-usage", "--disable-blink-features=AutomationControlled"]
                )
                self.context = await browser.new_context(
                    storage_state=str(auth_file),
                    user_agent=self.settings.user_agent, # å¾è¨­å®šè®€å–
                    viewport={"width": 1920, "height": 1080},
                    locale="zh-TW",  # è¨­å®šç‚ºç¹é«”ä¸­æ–‡
                    has_touch=True,
                    accept_downloads=False,
                    bypass_csp=True  # æ–°å¢ï¼šç¹é CSP é™åˆ¶
                )
                # æ–°å¢ï¼šéš±è— webdriver å±¬æ€§
                await self.context.add_init_script(
                    "Object.defineProperty(navigator,'webdriver',{get:()=>undefined})"
                )
                
                page = await self.context.new_page()
                page.on("console", lambda m: logging.info(f"CONSOLE [{m.type}] {m.text}"))

                # --- æ–°æ–¹æ³•ï¼šå…ˆç²å–æœ‰åºçš„è²¼æ–‡ URLs ---
                logging.info(f"ğŸ¯ [Task: {task_id}] ä½¿ç”¨æ–°çš„æœ‰åºè²¼æ–‡ç²å–æ–¹æ³•")
                ordered_post_urls = await self.get_ordered_post_urls_from_page(page, username, max_posts)
                
                if not ordered_post_urls:
                    logging.warning(f"âš ï¸ [Task: {task_id}] ç„¡æ³•å¾ç”¨æˆ¶é é¢ç²å–è²¼æ–‡ URLsï¼Œå›é€€è‡³èˆŠæ–¹æ³•")
                    # å›é€€è‡³åŸå§‹çš„ GraphQL æ””æˆªæ–¹æ³•
                response_handler = self._build_response_handler(username, posts, task_id, max_posts, stream_callback=None)
                page.on("response", response_handler)

                logging.info(f"å°è¦½è‡³ç›®æ¨™é é¢: {target_url}")
                await page.goto(target_url, wait_until="networkidle", timeout=self.settings.navigation_timeout)

                # --- æ»¾å‹•èˆ‡å»¶é²é‚è¼¯ ---
                scroll_attempts_without_new_posts = 0
                max_retries = 5

                while len(posts) < max_posts:
                    posts_before_scroll = len(posts)
                    await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")

                    try:
                        async with page.expect_response(lambda res: GRAPHQL_RE.search(res.url), timeout=15000):
                            pass
                    except PlaywrightTimeoutError:
                        logging.warning(f"â³ [Task: {task_id}] æ»¾å‹•å¾Œç­‰å¾…ç¶²è·¯å›æ‡‰é€¾æ™‚ã€‚")

                    delay = random.uniform(self.settings.scroll_delay_min, self.settings.scroll_delay_max) # å¾è¨­å®šè®€å–
                    await asyncio.sleep(delay)

                    if len(posts) == posts_before_scroll:
                        scroll_attempts_without_new_posts += 1
                        logging.info(f"æ»¾å‹•å¾Œæœªç™¼ç¾æ–°è²¼æ–‡ (å˜—è©¦ {scroll_attempts_without_new_posts}/{max_retries})")
                        if scroll_attempts_without_new_posts >= max_retries:
                            logging.info("å·²é”é é¢æœ«ç«¯æˆ–ç„¡æ–°å…§å®¹ï¼Œåœæ­¢æ»¾å‹•ã€‚")
                            break
                    else:
                        scroll_attempts_without_new_posts = 0
                        # ç™¼å¸ƒé€²åº¦æ›´æ–°
                        await publish_progress(
                            task_id, "fetch_progress", 
                            username=username,
                            current=len(posts), 
                            total=max_posts,
                            progress=min(len(posts) / max_posts, 1.0)
                        )
                else:
                    # --- æ–°æ–¹æ³•ï¼šä½¿ç”¨æœ‰åºURLså‰µå»ºPostMetricsï¼Œç¨å¾Œè£œé½Šè©³ç´°æ•¸æ“š ---
                    logging.info(f"âœ… [Task: {task_id}] ä½¿ç”¨æœ‰åºURLså‰µå»ºåŸºç¤PostMetricsï¼Œä¿æŒæ™‚é–“é †åº")
                    ordered_posts = []  # ä¿æŒé †åºçš„é™£åˆ—
                    
                    for i, post_url in enumerate(ordered_post_urls):
                        # å¾ URL ä¸­æå– post_id å’Œ code
                        url_parts = post_url.split('/')
                        if len(url_parts) >= 2:
                            code = url_parts[-1] if url_parts[-1] != 'media' else url_parts[-2]  # è™•ç† /media çµå°¾
                            post_id = f"{username}_{code}"  # ç”Ÿæˆå”¯ä¸€çš„ post_id
                            
                            # å‰µå»ºåŸºæœ¬çš„ PostMetrics ç‰©ä»¶
                            post_metrics = PostMetrics(
                                url=post_url,
                                post_id=post_id,
                                username=username,
                                source="playwright_ordered",
                                processing_stage="url_extracted",
                                likes_count=0,  # å°‡é€šéé é¢è¨ªå•è£œé½Š
                                comments_count=0,  # å°‡é€šéé é¢è¨ªå•è£œé½Š
                                reposts_count=0,  # å°‡é€šéé é¢è¨ªå•è£œé½Š
                                shares_count=0,  # å°‡é€šéé é¢è¨ªå•è£œé½Š
                                content="",  # å°‡é€šéé é¢è¨ªå•è£œé½Š
                                created_at=datetime.utcnow(),  # ä½¿ç”¨ç•¶å‰æ™‚é–“ä½œç‚ºé è¨­å€¼
                                images=[],  # å°‡é€šéé é¢è¨ªå•è£œé½Š
                                videos=[],  # å°‡é€šéé é¢è¨ªå•è£œé½Š
                                views_count=None  # å°‡é€šé fill_views_from_page è£œé½Š
                            )
                            
                            ordered_posts.append(post_metrics)
                            posts[post_id] = post_metrics  # åŒæ™‚åŠ å…¥å­—å…¸ä¾›å¾ŒçºŒè™•ç†
                            
                            # ç™¼å¸ƒé€²åº¦
                            await publish_progress(
                                task_id, 
                                "post_url_extracted",
                                username=username,
                                post_id=post_id,
                                current=len(ordered_posts),
                                total=max_posts,
                                progress=len(ordered_posts) / max_posts,
                                url=post_url
                            )
                    
                    # ä½¿ç”¨ ordered_posts è€Œä¸æ˜¯ posts.values() ä¾†ä¿æŒé †åº
                    logging.info(f"âœ… [Task: {task_id}] å‰µå»ºäº† {len(ordered_posts)} å€‹æœ‰åºçš„åŸºç¤PostMetrics")
                
                # é—œé–‰ page ä½†ä¿ç•™ context ä¾› fill_views_from_page ä½¿ç”¨
                await page.close()

                # --- æ•´ç†ä¸¦å›å‚³çµæœ ---
                # ä½¿ç”¨æœ‰åºçš„ posts åˆ—è¡¨æˆ–å›é€€åˆ°åŸå§‹æ–¹æ³•
                if 'ordered_posts' in locals():
                    final_posts = ordered_posts[:max_posts]  # ä¿æŒåŸå§‹é †åºï¼Œåªé™åˆ¶æ•¸é‡
                    total_found = len(ordered_posts)
                    logging.info(f"ğŸ¯ [Task: {task_id}] ä½¿ç”¨æœ‰åºè²¼æ–‡åˆ—è¡¨ï¼Œä¿æŒDOMæå–çš„æ™‚é–“é †åº")
                else:
                    # å›é€€åˆ°åŸå§‹æ–¹æ³•ï¼ˆGraphQLæ””æˆªçš„æƒ…æ³ï¼‰
                final_posts = list(posts.values())
                total_found = len(final_posts)
                # æ ¹æ“š max_posts æˆªæ–·çµæœ
                if total_found > max_posts:
                    try:
                        final_posts.sort(key=lambda p: p.created_at or datetime.min, reverse=True)
                    except Exception:
                        pass 
                    final_posts = final_posts[:max_posts]
                    logging.info(f"ğŸ”„ [Task: {task_id}] ä½¿ç”¨GraphQLæ””æˆªæ–¹æ³•ï¼ŒæŒ‰created_atæ’åº")
                
                logging.info(f"ğŸ”„ [Task: {task_id}] æº–å‚™å›å‚³æœ€çµ‚è³‡æ–™ï¼šå…±ç™¼ç¾ {total_found} å‰‡è²¼æ–‡, å›å‚³ {len(final_posts)} å‰‡")
                
                # --- è£œé½Šè©³ç´°æ•¸æ“šå’Œè§€çœ‹æ•¸ (åœ¨ browser context é‚„å­˜åœ¨æ™‚åŸ·è¡Œ) ---
                if 'ordered_posts' in locals():
                    # é…ç½®é¸é …ï¼šæ˜¯å¦å•Ÿç”¨è©³ç´°æ•¸æ“šè£œé½Š
                    enable_details_filling = getattr(self.settings, 'enable_details_filling', False)
                    
                    if enable_details_filling:
                        # æ–°æ–¹æ³•ï¼šéœ€è¦è£œé½Šè©³ç´°æ•¸æ“š
                        logging.info(f"ğŸ” [Task: {task_id}] é–‹å§‹è£œé½Šè©³ç´°æ•¸æ“šï¼ˆlikes, contentç­‰ï¼‰...")
                        await publish_progress(task_id, "fill_details_start", username=username, posts_count=len(final_posts))
                        
                        try:
                            final_posts = await self.fill_post_details_from_page(final_posts, task_id=task_id, username=username)
                            logging.info(f"âœ… [Task: {task_id}] è©³ç´°æ•¸æ“šè£œé½Šå®Œæˆ")
                            await publish_progress(task_id, "fill_details_completed", username=username, posts_count=len(final_posts))
                        except Exception as e:
                            logging.warning(f"âš ï¸ [Task: {task_id}] è£œé½Šè©³ç´°æ•¸æ“šæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                            await publish_progress(task_id, "fill_details_error", username=username, error=str(e))
                    else:
                        logging.info(f"âš ï¸ [Task: {task_id}] è©³ç´°æ•¸æ“šè£œé½Šå·²ç¦ç”¨ï¼Œå°‡åªè£œé½Šç€è¦½æ•¸")
                
                # è£œé½Šè§€çœ‹æ•¸ï¼ˆå…©ç¨®æ–¹æ³•éƒ½éœ€è¦ï¼‰
                logging.info(f"ğŸ” [Task: {task_id}] é–‹å§‹è£œé½Šè§€çœ‹æ•¸...")
                await publish_progress(task_id, "fill_views_start", username=username, posts_count=len(final_posts))
                
                try:
                    final_posts = await self.fill_views_from_page(final_posts, task_id=task_id, username=username)
                    logging.info(f"âœ… [Task: {task_id}] è§€çœ‹æ•¸è£œé½Šå®Œæˆ")
                    await publish_progress(task_id, "fill_views_completed", username=username, posts_count=len(final_posts))
                except Exception as e:
                    logging.warning(f"âš ï¸ [Task: {task_id}] è£œé½Šè§€çœ‹æ•¸æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                    await publish_progress(task_id, "fill_views_error", username=username, error=str(e))
                    # å³ä½¿è£œé½Šå¤±æ•—ï¼Œä¹Ÿç¹¼çºŒè¿”å›åŸºæœ¬æ•¸æ“š

                # æ‰‹å‹•é—œé–‰ browser å’Œ context
                await self.context.close()
                await browser.close()
                self.context = None
            
            # ä¿å­˜åŸå§‹æŠ“å–è³‡æ–™ä¾›èª¿è©¦
            try:
                raw_data = {
                    "task_id": task_id,
                    "username": username, 
                    "timestamp": datetime.now().isoformat(),
                    "total_found": total_found,
                    "returned_count": len(final_posts),
                    "posts": [
                        {
                            "url": post.url,
                            "post_id": post.post_id,
                            "likes_count": post.likes_count,
                            "comments_count": post.comments_count,
                            "reposts_count": post.reposts_count,
                            "shares_count": post.shares_count,
                            "views_count": post.views_count,
                            "calculated_score": post.calculate_score(),  # ğŸ†• ç§»åˆ° views_count ä¸‹é¢
                            "content": post.content,
                            "created_at": post.created_at.isoformat() if post.created_at else None,
                            "images": post.images,  # æ·»åŠ åœ–ç‰‡ URL
                            "videos": post.videos   # æ·»åŠ å½±ç‰‡ URL
                        } for post in final_posts
                    ]
                }
                
                # ä½¿ç”¨æ™‚é–“æˆ³è¨˜é¿å…æª”æ¡ˆè¡çª
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                raw_file = DEBUG_DIR / f"crawl_data_{timestamp}_{task_id[:8]}.json"
                raw_file.write_text(
                    json.dumps(raw_data, indent=2, ensure_ascii=False),
                    encoding="utf-8" # æ˜ç¢ºæŒ‡å®š UTF-8
                )
                logging.info(f"ğŸ’¾ [Task: {task_id}] å·²ä¿å­˜åŸå§‹æŠ“å–è³‡æ–™è‡³: {raw_file}")
                
            except Exception as e:
                logging.warning(f"âš ï¸ [Task: {task_id}] ä¿å­˜èª¿è©¦è³‡æ–™å¤±æ•—: {e}")
            
            # ç™¼å¸ƒå®Œæˆé€²åº¦
            await publish_progress(
                task_id, "completed", 
                username=username,
                total_posts=len(final_posts),
                success=True
            )

            batch = PostMetricsBatch(
                posts=final_posts,
                username=username,
                total_count=total_found,
                processing_stage="playwright_completed"
            )
            return batch
            
        except Exception as e:
            error_message = f"Playwright æ ¸å¿ƒé‚è¼¯å‡ºéŒ¯: {e}"
            logging.error(error_message, exc_info=True)
            # ç™¼å¸ƒéŒ¯èª¤é€²åº¦
            await publish_progress(
                task_id, "error", 
                username=username,
                error=error_message,
                success=False
            )
            raise
        
        finally:
            # æ¸…ç†è‡¨æ™‚èªè­‰æª”æ¡ˆ
            if auth_file.exists():
                auth_file.unlink()
                logging.info(f"ğŸ—‘ï¸ [Task: {task_id}] å·²åˆªé™¤è‡¨æ™‚èªè­‰æª”æ¡ˆ: {auth_file}")
            
            # ç¢ºä¿ context è¢«é‡ç½®ï¼ˆbrowser å·²åœ¨ä¸Šé¢æ‰‹å‹•é—œé–‰ï¼‰
            self.context = None 

    # +++ æ–°å¢ï¼šå¾å‰ç«¯è£œé½Šç€è¦½æ•¸çš„æ ¸å¿ƒæ–¹æ³•ï¼ˆæ•´åˆ Gate é é¢è™•ç†ï¼‰ +++
    async def fill_views_from_page(self, posts_to_fill: List[PostMetrics], task_id: str = None, username: str = None) -> List[PostMetrics]:
        """
        éæ­·è²¼æ–‡åˆ—è¡¨ï¼Œå°èˆªåˆ°æ¯å€‹è²¼æ–‡çš„é é¢ä»¥è£œé½Š views_countã€‚
        æ•´åˆäº†æˆåŠŸçš„ Gate é é¢è™•ç†å’Œé›™ç­–ç•¥æå–æ–¹æ³•ã€‚
        """
        if not self.context:
            logging.error("âŒ Browser context æœªåˆå§‹åŒ–ï¼Œç„¡æ³•åŸ·è¡Œ fill_views_from_pageã€‚")
            return posts_to_fill

        # æ¸›å°‘ä¸¦ç™¼æ•¸ä»¥é¿å…è§¸ç™¼åçˆ¬èŸ²æ©Ÿåˆ¶
        semaphore = asyncio.Semaphore(2)
        
        async def fetch_single_view(post: PostMetrics):
            async with semaphore:
                page = None
                try:
                    page = await self.context.new_page()
                    # ç¦ç”¨åœ–ç‰‡å’Œå½±ç‰‡è¼‰å…¥ä»¥åŠ é€Ÿ
                    await page.route("**/*.{png,jpg,jpeg,gif,mp4,webp}", lambda r: r.abort())
                    
                    logging.debug(f"ğŸ“„ æ­£åœ¨è™•ç†: {post.url}")
                    
                    # å°èˆªåˆ°è²¼æ–‡é é¢
                    await page.goto(post.url, wait_until="networkidle", timeout=30000)
                    
                    # æª¢æŸ¥é é¢é¡å‹ï¼ˆå®Œæ•´é é¢ vs Gate é é¢ï¼‰
                    page_content = await page.content()
                    is_gate_page = "__NEXT_DATA__" not in page_content
                    
                    if is_gate_page:
                        logging.debug(f"   âš ï¸ æª¢æ¸¬åˆ° Gate é é¢ï¼Œç›´æ¥ä½¿ç”¨ DOM é¸æ“‡å™¨...")
                    
                    views_count = None
                    extraction_method = None
                    
                    # ç­–ç•¥ 1: GraphQL æ””æˆªï¼ˆåªåœ¨é Gate é é¢æ™‚ï¼‰
                    if not is_gate_page:
                        try:
                            response = await page.wait_for_response(
                                lambda r: "containing_thread" in r.url and r.status == 200, 
                                timeout=8000
                            )
                            data = await response.json()
                            
                            # è§£æç€è¦½æ•¸
                            thread_items = data["data"]["containing_thread"]["thread_items"]
                            post_data = thread_items[0]["post"]
                            views_count = (post_data.get("feedback_info", {}).get("view_count") or
                                          post_data.get("video_info", {}).get("play_count") or 0)
                            
                            if views_count > 0:
                                extraction_method = "graphql_api"
                                logging.debug(f"   âœ… GraphQL API ç²å–ç€è¦½æ•¸: {views_count:,}")
                        except Exception as e:
                            logging.debug(f"   âš ï¸ GraphQL æ””æˆªå¤±æ•—: {str(e)[:100]}")
                    
                    # ç­–ç•¥ 2: DOM é¸æ“‡å™¨ï¼ˆGate é é¢çš„ä¸»è¦æ–¹æ³•ï¼‰
                    if views_count is None or views_count == 0:
                        selectors = [
                            "a:has-text(' æ¬¡ç€è¦½'), a:has-text(' views')",    # ä¸»è¦é¸æ“‡å™¨
                            "*:has-text('æ¬¡ç€è¦½'), *:has-text('views')",      # é€šç”¨é¸æ“‡å™¨
                            "span:has-text('æ¬¡ç€è¦½'), span:has-text('views')", # span å…ƒç´ 
                            "text=/\\d+.*æ¬¡ç€è¦½/, text=/\\d+.*views?/",       # æ­£å‰‡è¡¨é”å¼
                        ]
                        
                        for i, selector in enumerate(selectors):
                            try:
                                element = await page.wait_for_selector(selector, timeout=3000)
                            if element:
                                view_text = await element.inner_text()
                                    parsed_views = parse_views_text(view_text)
                                    if parsed_views and parsed_views > 0:
                                        views_count = parsed_views
                                        extraction_method = f"dom_selector_{i+1}"
                                        logging.debug(f"   âœ… DOM é¸æ“‡å™¨ {i+1} ç²å–ç€è¦½æ•¸: {views_count:,}")
                                        break
                            except Exception:
                                continue
                    
                    # æ›´æ–°çµæœ
                    if views_count and views_count > 0:
                                    post.views_count = views_count
                                    post.views_fetched_at = datetime.utcnow()
                        logging.info(f"  âœ… æˆåŠŸç²å– {post.post_id} çš„ç€è¦½æ•¸: {views_count:,} (æ–¹æ³•: {extraction_method})")
                                    
                        # ç™¼å¸ƒé€²åº¦
                                    if task_id:
                                        from common.nats_client import publish_progress
                                        await publish_progress(
                                            task_id, 
                                            "views_fetched",
                                            username=username or "unknown",
                                            post_id=post.post_id,
                                            views_count=views_count,
                                extraction_method=extraction_method,
                                is_gate_page=is_gate_page
                            )
                            else:
                        logging.warning(f"  âŒ ç„¡æ³•ç²å– {post.post_id} çš„ç€è¦½æ•¸")
                        post.views_count = -1
                                            post.views_fetched_at = datetime.utcnow()
                    
                    # éš¨æ©Ÿå»¶é²é¿å…åçˆ¬èŸ²
                    delay = random.uniform(2, 4)
                    await asyncio.sleep(delay)
                    
                except Exception as e:
                    logging.error(f"  âŒ è™•ç† {post.post_id} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                    post.views_count = -1
                    post.views_fetched_at = datetime.utcnow()
                finally:
                    if page:
                        await page.close()

        # åºåˆ—è™•ç†é¿å…ä¸¦ç™¼å•é¡Œï¼ˆæ ¹æ“šæˆåŠŸç¶“é©—ï¼‰
        for post in posts_to_fill:
            await fetch_single_view(post)
        
        return posts_to_fill

    # +++ æ–°å¢ï¼šè£œé½Šè²¼æ–‡è©³ç´°æ•¸æ“šçš„æ–¹æ³• +++
    async def fill_post_details_from_page(self, posts_to_fill: List[PostMetrics], task_id: str = None, username: str = None) -> List[PostMetrics]:
        """
        éæ­·è²¼æ–‡åˆ—è¡¨ï¼Œå°èˆªåˆ°æ¯å€‹è²¼æ–‡é é¢ä»¥è£œé½Šè©³ç´°æ•¸æ“šï¼ˆlikes, content, imagesç­‰ï¼‰ã€‚
        ä¿æŒåŸå§‹é †åºä¸è®Šã€‚
        """
        if not self.context:
            logging.error("âŒ Browser context æœªåˆå§‹åŒ–ï¼Œç„¡æ³•åŸ·è¡Œ fill_post_details_from_pageã€‚")
            return posts_to_fill

        # æ¸›å°‘ä¸¦ç™¼æ•¸ä»¥é¿å…è§¸ç™¼åçˆ¬èŸ²æ©Ÿåˆ¶
        semaphore = asyncio.Semaphore(1)  # æ›´ä¿å®ˆçš„ä¸¦ç™¼æ•¸
        
        async def fetch_single_details(post: PostMetrics):
            async with semaphore:
                page = None
                try:
                    page = await self.context.new_page()
                    # ç¦ç”¨åœ–ç‰‡å’Œå½±ç‰‡è¼‰å…¥ä»¥åŠ é€Ÿ
                    await page.route("**/*.{png,jpg,jpeg,gif,mp4,webp}", lambda r: r.abort())
                    
                    logging.debug(f"ğŸ“„ æ­£åœ¨è£œé½Šè©³ç´°æ•¸æ“š: {post.url}")
                    
                    # è¨­ç½®GraphQLæ””æˆªå™¨ä¾†ç²å–å®Œæ•´æ•¸æ“š
                    captured_data = {}
                    
                    async def handle_graphql_response(response):
                        if GRAPHQL_RE.search(response.url):
                            try:
                                data = await response.json()
                                
                                # å¤šç¨®GraphQLçµæ§‹çš„æ”¯æ´
                                thread_items = None
                                
                                # çµæ§‹1: data.containing_thread.thread_items
                                if "data" in data and "containing_thread" in data.get("data", {}):
                                    thread_items = data["data"]["containing_thread"].get("thread_items", [])
                                
                                # çµæ§‹2: data.mediaData.edges[].node.thread_items  
                                elif "data" in data and "mediaData" in data.get("data", {}):
                                    edges = data["data"]["mediaData"].get("edges", [])
                                    for edge in edges:
                                        node_items = edge.get("node", {}).get("thread_items", [])
                                        if node_items:
                                            thread_items = node_items
                                            break
                                
                                # çµæ§‹3: ç›´æ¥åœ¨dataå±¤ç´š
                                elif "data" in data:
                                    for key, value in data["data"].items():
                                        if isinstance(value, dict) and "thread_items" in value:
                                            thread_items = value["thread_items"]
                                            break
                                
                                if thread_items and len(thread_items) > 0:
                                    captured_data["post_data"] = thread_items[0]
                                    logging.debug(f"   âœ… æˆåŠŸæ””æˆªåˆ°GraphQLæ•¸æ“š: {response.url}")
                                else:
                                    logging.debug(f"   âš ï¸ GraphQLéŸ¿æ‡‰ä¸­ç„¡thread_items: {list(data.get('data', {}).keys())}")
                                    
                            except Exception as e:
                                logging.debug(f"   âŒ è§£æGraphQLéŸ¿æ‡‰å¤±æ•—: {e}")
                    
                    page.on("response", handle_graphql_response)
                    
                    # å°èˆªåˆ°è²¼æ–‡é é¢
                    await page.goto(post.url, wait_until="networkidle", timeout=30000)
                    
                    # ç­‰å¾…GraphQLéŸ¿æ‡‰
                    await asyncio.sleep(2)
                    
                    # å¦‚æœç²å–åˆ°GraphQLæ•¸æ“šï¼Œè§£æä¸¦æ›´æ–°PostMetrics
                    if "post_data" in captured_data:
                        post_data = captured_data["post_data"]
                        parsed_post = parse_post_data(post_data, username)
                        
                        if parsed_post:
                            # æ›´æ–°è©³ç´°æ•¸æ“šï¼Œä½†ä¿æŒåŸå§‹çš„URLå’Œpost_id
                            post.likes_count = parsed_post.likes_count
                            post.comments_count = parsed_post.comments_count
                            post.reposts_count = parsed_post.reposts_count
                            post.shares_count = parsed_post.shares_count
                            post.content = parsed_post.content
                            post.images = parsed_post.images
                            post.videos = parsed_post.videos
                            post.processing_stage = "details_filled_graphql"
                            
                            logging.info(f"  âœ… GraphQLæˆåŠŸè£œé½Š {post.post_id}: è®š={post.likes_count}, å…§å®¹é•·åº¦={len(post.content)}")
                            
                            # ç™¼å¸ƒé€²åº¦
                            if task_id:
                                from common.nats_client import publish_progress
                                await publish_progress(
                                    task_id, 
                                    "details_fetched_graphql",
                                    username=username or "unknown",
                                    post_id=post.post_id,
                                    likes_count=post.likes_count,
                                    content_length=len(post.content)
                                )
                        else:
                            logging.warning(f"  âš ï¸ ç„¡æ³•è§£æ {post.post_id} çš„GraphQLæ•¸æ“š")
                    else:
                        # GraphQL å›é€€æ–¹æ¡ˆï¼šä½¿ç”¨ DOM æå–åŸºæœ¬è³‡è¨Š
                        logging.info(f"  ğŸ”„ GraphQLå¤±æ•—ï¼Œæ”¹ç”¨DOMæå– {post.post_id} çš„åŸºæœ¬è³‡è¨Š...")
                        
                        try:
                            # ç­‰å¾…é é¢è¼‰å…¥å®Œæˆ
                            await page.wait_for_load_state("networkidle", timeout=10000)
                            
                            # æå–æ–‡å­—å…§å®¹
                            try:
                                content_selectors = [
                                    '[data-testid="thread-text-content"]',
                                    'div[dir="auto"]',
                                    'span:has-text("...")',
                                    'div:has(span)'
                                ]
                                content = ""
                                for selector in content_selectors:
                                    elements = await page.query_selector_all(selector)
                                    for elem in elements:
                                        text = await elem.inner_text()
                                        if text and len(text) > len(content):
                                            content = text
                                    if content:
                                        break
                                
                                if content:
                                    post.content = content.strip()
                                    
                            except Exception as e:
                                logging.debug(f"    âš ï¸ DOMå…§å®¹æå–å¤±æ•—: {e}")
                            
                            # ç°¡å–®çš„æ•¸å­—æå–ï¼ˆè®šæ•¸ç­‰ï¼‰
                            try:
                                # é€™è£¡å¯ä»¥æ·»åŠ DOMé¸æ“‡å™¨ä¾†æå–likesç­‰æ•¸æ“š
                                # ç›®å‰å…ˆè·³éï¼Œå°ˆæ³¨æ–¼contentæå–
                                pass
                            except Exception as e:
                                logging.debug(f"    âš ï¸ DOMæ•¸å­—æå–å¤±æ•—: {e}")
                            
                            post.processing_stage = "details_filled_dom"
                            logging.info(f"  âœ… DOMæˆåŠŸè£œé½Š {post.post_id}: å…§å®¹é•·åº¦={len(post.content)}")
                            
                        except Exception as e:
                            logging.warning(f"  âš ï¸ DOMæå–ä¹Ÿå¤±æ•— {post.post_id}: {e}")
                            post.processing_stage = "details_failed"
                    
                    # éš¨æ©Ÿå»¶é²é¿å…åçˆ¬èŸ²
                    delay = random.uniform(2, 4)
                    await asyncio.sleep(delay)
                    
                except Exception as e:
                    logging.error(f"  âŒ è™•ç† {post.post_id} è©³ç´°æ•¸æ“šæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                finally:
                    if page:
                        await page.close()

        # åºåˆ—è™•ç†ä¿æŒé †åº
        for post in posts_to_fill:
            await fetch_single_details(post)
        
        return posts_to_fill


