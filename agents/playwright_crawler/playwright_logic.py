"""
Playwright çˆ¬èŸ²æ ¸å¿ƒé‚è¼¯ï¼ˆé‡æ§‹ç‰ˆï¼‰
"""
import json
import asyncio
import logging
import tempfile
from pathlib import Path
from typing import Dict, List, Optional, Any
from datetime import datetime

from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeoutError

from common.settings import get_settings
from common.models import PostMetrics, PostMetricsBatch
from common.nats_client import publish_progress
from common.utils import generate_post_url, first_of, parse_thread_item
from common.history import crawl_history

# å°å…¥é‡æ§‹å¾Œçš„æ¨¡çµ„
from .parsers.number_parser import parse_number
from .parsers.post_parser import parse_post_data
from .extractors.url_extractor import URLExtractor
from .extractors.views_extractor import ViewsExtractor
from .extractors.details_extractor import DetailsExtractor
from .config.field_mappings import FIELD_MAP
from .utils.post_deduplicator import apply_deduplication

# èª¿è©¦æª”æ¡ˆè·¯å¾‘
DEBUG_DIR = Path(__file__).parent / "debug"
DEBUG_DIR.mkdir(exist_ok=True)

# è¨­å®šæ—¥èªŒï¼ˆé¿å…é‡è¤‡é…ç½®ï¼‰
if not logging.getLogger().handlers:
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')


class PlaywrightLogic:
    """ä½¿ç”¨ Playwright é€²è¡Œçˆ¬èŸ²çš„æ ¸å¿ƒé‚è¼¯ï¼ˆé‡æ§‹ç‰ˆï¼‰"""
    
    def __init__(self):
        self.browser = None
        self.context = None
        self.settings = get_settings()
        
        # åˆå§‹åŒ–æå–å™¨
        self.url_extractor = URLExtractor()
        self.views_extractor = ViewsExtractor()
        self.details_extractor = DetailsExtractor()

    async def fetch_posts(
        self,
        username: str,
        extra_posts: int,  # æ”¹ç‚ºå¢é‡èªç¾©
        auth_json_content: Dict,
        task_id: str = None
    ) -> PostMetricsBatch:
        """
        å¢é‡çˆ¬å–è²¼æ–‡
        
        Args:
            username: ç›®æ¨™ç”¨æˆ¶å
            extra_posts: éœ€è¦é¡å¤–æŠ“å–çš„è²¼æ–‡æ•¸é‡
            auth_json_content: èªè­‰è³‡è¨Š
            task_id: ä»»å‹™ID
        """
        if task_id is None:
            task_id = f"task_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            
        logging.info(f"ğŸš€ [Task: {task_id}] é–‹å§‹å¢é‡çˆ¬å– @{username}ï¼Œç›®æ¨™: {extra_posts} ç¯‡æ–°è²¼æ–‡")
        
        try:
            # æ­¥é©Ÿ1: åˆå§‹åŒ–ç€è¦½å™¨å’Œèªè­‰
            await self._setup_browser_and_auth(auth_json_content, task_id)
            
            # æ­¥é©Ÿ2: ç²å–ç¾æœ‰è²¼æ–‡IDï¼Œè¨ˆç®—éœ€è¦æŠ“å–çš„æ•¸é‡
            existing_post_ids = await crawl_history.get_existing_post_ids(username)
            need_to_fetch = extra_posts
            
            logging.info(f"ğŸ“š {username} å·²æœ‰ {len(existing_post_ids)} ç¯‡è²¼æ–‡ï¼Œéœ€è¦æ–°å¢ {need_to_fetch} ç¯‡")
            
            # æ­¥é©Ÿ3: ä½¿ç”¨URLExtractorç²å–URLåˆ—è¡¨ï¼ˆå¸¶æ—©åœæ©Ÿåˆ¶ï¼‰
            page = await self.context.new_page()
            await page.goto(f"https://www.threads.com/@{username}")
            
            ordered_urls = await self.url_extractor.get_ordered_post_urls_from_page(
                page, username, max_posts=need_to_fetch + 10  # å¤šæŠ“ä¸€äº›ä»¥é˜²é‡è¤‡
            )
            
            # æ­¥é©Ÿ4: éæ¿¾å‡ºæ–°è²¼æ–‡URLsï¼ˆå¢é‡é‚è¼¯ï¼‰
            ordered_posts = []
            new_posts_found = 0
            
            for url in ordered_urls:
                if new_posts_found >= need_to_fetch:
                    break  # æ—©åœæ©Ÿåˆ¶
                    
                post_id = url.split('/')[-1]
                if post_id not in existing_post_ids:
                    # å‰µå»ºåŸºç¤PostMetrics
                    post_metrics = PostMetrics(
                        post_id=f"{username}_{post_id}",
                        username=username,
                        url=url,
                        content="",
                        created_at=datetime.utcnow(),
                        fetched_at=datetime.utcnow(),
                        source="playwright_incremental",
                        processing_stage="urls_extracted",
                        is_complete=False,
                        likes_count=0,
                        comments_count=0,
                        reposts_count=0,
                        shares_count=0,
                        views_count=None,
                    )
                    ordered_posts.append(post_metrics)
                    new_posts_found += 1
                    
                    logging.info(f"âœ… ç™¼ç¾æ–°è²¼æ–‡ {new_posts_found}/{need_to_fetch}: {post_id}")
            
            logging.info(f"âœ… [Task: {task_id}] å‰µå»ºäº† {len(ordered_posts)} å€‹æœ‰åºçš„åŸºç¤PostMetrics")
            await page.close()

            # æ­¥é©Ÿ5: ä½¿ç”¨DetailsExtractorè£œé½Šè©³ç´°æ•¸æ“š
            final_posts = ordered_posts
            logging.info(f"ğŸ” [Task: {task_id}] é–‹å§‹ DOM æ•¸æ“šè£œé½Š...")
            await publish_progress(task_id, "fill_details_start", username=username, posts_count=len(final_posts))
            
            try:
                final_posts = await self.details_extractor.fill_post_details_from_page(final_posts, self.context, task_id=task_id, username=username)
                logging.info(f"âœ… [Task: {task_id}] è©³ç´°æ•¸æ“šè£œé½Šå®Œæˆ")
                await publish_progress(task_id, "fill_details_completed", username=username, posts_count=len(final_posts))
            except Exception as e:
                logging.warning(f"âš ï¸ [Task: {task_id}] è£œé½Šè©³ç´°æ•¸æ“šæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            
            # æ­¥é©Ÿ6: ä½¿ç”¨ViewsExtractorè£œé½Šè§€çœ‹æ•¸
            logging.info(f"ğŸ” [Task: {task_id}] é–‹å§‹è£œé½Šè§€çœ‹æ•¸...")
            await publish_progress(task_id, "fill_views_start", username=username, posts_count=len(final_posts))
            
            try:
                final_posts = await self.views_extractor.fill_views_from_page(final_posts, self.context, task_id=task_id, username=username)
                logging.info(f"âœ… [Task: {task_id}] è§€çœ‹æ•¸è£œé½Šå®Œæˆ")
                await publish_progress(task_id, "fill_views_completed", username=username, posts_count=len(final_posts))
            except Exception as e:
                logging.warning(f"âš ï¸ [Task: {task_id}] è£œé½Šè§€çœ‹æ•¸æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

            # æ­¥é©Ÿ7: å»é‡è™•ç†ï¼ˆä¿ç•™ä¸»è²¼æ–‡ï¼Œéæ¿¾å›æ‡‰ï¼‰
            logging.info(f"ğŸ”„ [Task: {task_id}] é–‹å§‹å»é‡è™•ç†...")
            final_posts = apply_deduplication(final_posts)
            logging.info(f"âœ… [Task: {task_id}] å»é‡å®Œæˆï¼Œæœ€çµ‚è²¼æ–‡æ•¸: {len(final_posts)}")

            # æ­¥é©Ÿ7.5: è£œè¶³æ©Ÿåˆ¶ï¼ˆå¦‚æœå»é‡å¾Œä¸è¶³ç›®æ¨™æ•¸é‡ï¼‰
            if len(final_posts) < need_to_fetch:
                shortage = need_to_fetch - len(final_posts)
                logging.info(f"âš ï¸ [Task: {task_id}] å»é‡å¾Œè²¼æ–‡ä¸è¶³ï¼éœ€è¦: {need_to_fetch}ï¼Œå¯¦éš›: {len(final_posts)}ï¼Œç¼ºå°‘: {shortage}")
                
                # å¯¦æ–½è£œè¶³ç­–ç•¥
                max_supplement_rounds = 2  # æœ€å¤š2è¼ªè£œè¶³
                
                for supplement_round in range(1, max_supplement_rounds + 1):
                    current_shortage = need_to_fetch - len(final_posts)
                    if current_shortage <= 0:
                        break
                        
                    # å‹•æ…‹å¢åŠ çˆ¬å–æ•¸é‡ï¼šç¼ºå¹¾å‰‡å°±å¤šçˆ¬å¹¾å€
                    supplement_target = current_shortage * (2 + supplement_round)  # ç¬¬1è¼ªÃ—3ï¼Œç¬¬2è¼ªÃ—4
                    
                    logging.info(f"ğŸ”„ [Task: {task_id}] è£œè¶³ç¬¬ {supplement_round} è¼ªï¼šé‚„ç¼º {current_shortage} å‰‡ï¼Œå°‡çˆ¬ {supplement_target} å‰‡")
                    
                    try:
                        # é‡æ–°çˆ¬å–æ›´å¤šè²¼æ–‡
                        supplement_page = await self.context.new_page()
                        await supplement_page.goto(f"https://www.threads.com/@{username}")
                        
                        supplement_urls = await self.url_extractor.get_ordered_post_urls_from_page(
                            supplement_page, username, max_posts=supplement_target
                        )
                        await supplement_page.close()
                        
                        # éæ¿¾å‡ºæ–°çš„è²¼æ–‡URLs
                        existing_post_ids_expanded = existing_post_ids | {p.post_id for p in final_posts}
                        supplement_posts = []
                        
                        for url in supplement_urls:
                            post_id = url.split('/')[-1]
                            full_post_id = f"{username}_{post_id}"
                            
                            if full_post_id not in existing_post_ids_expanded:
                                supplement_posts.append(PostMetrics(
                                    post_id=full_post_id,
                                    username=username,
                                    url=url,
                                    content="",
                                    created_at=datetime.utcnow(),
                                    fetched_at=datetime.utcnow(),
                                    source=f"playwright_supplement_r{supplement_round}",
                                    processing_stage="urls_extracted",
                                    is_complete=False,
                                    likes_count=0, comments_count=0, reposts_count=0, shares_count=0, views_count=None,
                                ))
                        
                        if supplement_posts:
                            # è£œé½Šæ•¸æ“š
                            supplement_posts = await self.details_extractor.fill_post_details_from_page(supplement_posts, self.context, task_id=task_id, username=username)
                            supplement_posts = await self.views_extractor.fill_views_from_page(supplement_posts, self.context, task_id=task_id, username=username)
                            
                            # æœ¬è¼ªå»é‡
                            supplement_posts = apply_deduplication(supplement_posts)
                            
                            # èˆ‡ç¾æœ‰è²¼æ–‡åˆä½µå»é‡
                            combined_posts = final_posts + supplement_posts
                            combined_posts = apply_deduplication(combined_posts)
                            
                            added_count = len(combined_posts) - len(final_posts)
                            final_posts = combined_posts
                            
                            logging.info(f"âœ… [Task: {task_id}] è£œè¶³ç¬¬ {supplement_round} è¼ªå®Œæˆï¼šæ–°å¢ {added_count} å‰‡ï¼Œç´¯è¨ˆ {len(final_posts)} å‰‡")
                            
                        else:
                            logging.info(f"âš ï¸ [Task: {task_id}] è£œè¶³ç¬¬ {supplement_round} è¼ªï¼šç„¡æ–°è²¼æ–‡å¯è£œå……")
                            break
                            
                    except Exception as e:
                        logging.warning(f"âš ï¸ [Task: {task_id}] è£œè¶³ç¬¬ {supplement_round} è¼ªå¤±æ•—: {e}")
                        break
                
                final_count = len(final_posts)
                if final_count >= need_to_fetch:
                    logging.info(f"ğŸ¯ [Task: {task_id}] è£œè¶³æˆåŠŸï¼ç›®æ¨™: {need_to_fetch}ï¼Œæœ€çµ‚: {final_count}")
                else:
                    logging.warning(f"âš ï¸ [Task: {task_id}] è£œè¶³æœªé”æ¨™ï¼šç›®æ¨™: {need_to_fetch}ï¼Œæœ€çµ‚: {final_count}")

            # æ­¥é©Ÿ8: ä¿å­˜èª¿è©¦æ•¸æ“š
            await self._save_debug_data(task_id, username, len(ordered_urls), final_posts)
            await publish_progress(task_id, "completed", username=username, posts_count=len(final_posts))

            # æ­¥é©Ÿ9: ä¿å­˜åˆ°æ•¸æ“šåº«ä¸¦æ›´æ–°ç‹€æ…‹
            saved_count = await crawl_history.upsert_posts(final_posts)
            logging.info(f"âœ… æˆåŠŸè™•ç† {saved_count}/{len(final_posts)} ç¯‡è²¼æ–‡")
            
            # æ›´æ–°çˆ¬å–ç‹€æ…‹
            if final_posts:
                latest_post_id = final_posts[0].post_id
                await crawl_history.update_crawl_state(username, latest_post_id, saved_count)
                logging.info(f"ğŸ“Š æ›´æ–° {username} ç‹€æ…‹: latest={latest_post_id}, +{saved_count}ç¯‡")
            
            # æ­¥é©Ÿ10: ç”Ÿæˆä»»å‹™æŒ‡æ¨™
            task_metrics = await crawl_history.get_task_metrics(username, need_to_fetch, len(final_posts))
            logging.info(f"ğŸ“Š ä»»å‹™å®Œæˆ: {task_metrics}")

            return PostMetricsBatch(
                posts=final_posts,
                batch_id=task_id,
                username=username,
                total_count=len(final_posts),
                processing_stage="playwright_incremental_completed"
            )

        except Exception as e:
            logging.error(f"âŒ [Task: {task_id}] çˆ¬å–éç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
            await publish_progress(task_id, "error", message=f"çˆ¬å–å¤±æ•—: {str(e)}")
            raise e
        finally:
            await self._cleanup(task_id)

    async def _setup_browser_and_auth(self, auth_json_content: Dict, task_id: str):
        """è¨­ç½®ç€è¦½å™¨å’Œèªè­‰"""
        playwright = await async_playwright().start()
        self.browser = await playwright.chromium.launch(headless=True)
        self.context = await self.browser.new_context()
        
        # è¨­ç½®èªè­‰
        auth_file = Path(tempfile.gettempdir()) / f"{task_id}_auth.json"
        auth_file.write_text(json.dumps(auth_json_content))
        
        await self.context.add_cookies(auth_json_content.get('cookies', []))
        logging.info(f"ğŸ” [Task: {task_id}] èªè­‰è¨­ç½®å®Œæˆ")

    async def _cleanup(self, task_id: str):
        """æ¸…ç†è³‡æº"""
        try:
            if self.browser:
                await self.browser.close()
            
            # åˆªé™¤è‡¨æ™‚èªè­‰æª”æ¡ˆ
            auth_file = Path(tempfile.gettempdir()) / f"{task_id}_auth.json"
            if auth_file.exists():
                auth_file.unlink()
                logging.info(f"ğŸ—‘ï¸ [Task: {task_id}] å·²åˆªé™¤è‡¨æ™‚èªè­‰æª”æ¡ˆ: {auth_file}")
            self.context = None
        except Exception as e:
            logging.warning(f"âš ï¸ [Task: {task_id}] æ¸…ç†è³‡æºæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}") 

    async def _save_debug_data(self, task_id: str, username: str, total_found: int, final_posts: List[PostMetrics]):
        """ä¿å­˜èª¿è©¦æ•¸æ“š"""
        try:
            raw_data = {
                "task_id": task_id,
                "username": username, 
                "timestamp": datetime.now().isoformat(),
                "total_found": total_found,
                "returned_count": len(final_posts),
                "posts": [
                    {
                        "url": post.url,
                        "post_id": post.post_id,
                        "likes_count": post.likes_count,
                        "comments_count": post.comments_count,
                        "reposts_count": post.reposts_count,
                        "shares_count": post.shares_count,
                        "views_count": post.views_count,
                        "calculated_score": post.calculate_score(),
                        "content": post.content,
                        "created_at": post.created_at.isoformat() if post.created_at else None,
                        "images": post.images,
                        "videos": post.videos
                    } for post in final_posts
                ]
            }
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            raw_file = DEBUG_DIR / f"crawl_data_{timestamp}_{task_id[:8]}.json"
            raw_file.write_text(json.dumps(raw_data, indent=2, ensure_ascii=False), encoding="utf-8")
            logging.info(f"ğŸ’¾ [Task: {task_id}] å·²ä¿å­˜åŸå§‹æŠ“å–è³‡æ–™è‡³: {raw_file}")
            
        except Exception as e:
            logging.warning(f"âš ï¸ [Task: {task_id}] ä¿å­˜èª¿è©¦è³‡æ–™å¤±æ•—: {e}")