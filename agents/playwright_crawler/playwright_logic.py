import json
import asyncio
import logging
import random
import re
import tempfile
import uuid
from pathlib import Path
from typing import Dict, List, Optional, AsyncIterable, Callable
from datetime import datetime

from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeoutError, Browser, BrowserContext, Page

from common.settings import get_settings
from common.a2a import stream_text, stream_data, stream_status, stream_error, TaskState
from common.models import PostMetrics, PostMetricsBatch

# èª¿è©¦æª”æ¡ˆè·¯å¾‘
DEBUG_DIR = Path(__file__).parent / "debug"
DEBUG_DIR.mkdir(exist_ok=True)
DEBUG_FAILED_ITEM_FILE = DEBUG_DIR / "failed_post_sample.json"
SAMPLE_THREAD_ITEM_FILE = DEBUG_DIR / "sample_thread_item.json"
RAW_CRAWL_DATA_FILE = DEBUG_DIR / "raw_crawl_data.json"

# è¨­å®šæ—¥èªŒ
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# --- GraphQL API è©³ç´°è³‡è¨Š ---
GRAPHQL_RE = re.compile(r"/graphql/query")
# ä¿ç•™ç”¨ä¾† debugï¼›å¯¦éš›é‚è¼¯æ”¹æˆã€Œçœ‹çµæ§‹ã€å³å¯
POST_QUERY_NAMES = set()

# --- å¼·å¥çš„æ¬„ä½å°ç…§è¡¨ ---
FIELD_MAP = {
    "like_count": [
        "like_count", "likeCount", 
        ["feedback_info", "aggregated_like_count"],
        ["like_info", "count"]
    ],
    "comment_count": [
        ["text_post_app_info", "direct_reply_count"],
        "comment_count", "commentCount",
        ["reply_info", "count"]
    ],
    "share_count": [
        ["text_post_app_info", "repost_count"],
        "repostCount", "share_count", "shareCount"
    ],
    "content": [
        ["caption", "text"],
        "caption", "text", "content"
    ],
    "author": [
        ["user", "username"], 
        ["owner", "username"],
        ["user", "handle"]
    ],
    "created_at": [
        "taken_at", "taken_at_timestamp", 
        "publish_date", "created_time"
    ],
    "post_id": [
        "pk", "id", "post_id"
    ],
    "code": [
        "code", "shortcode", "media_code"
    ]
}

def first_of(obj, *keys):
    """
    å¤šéµ fallback æ©Ÿåˆ¶ï¼šä¾åºå˜—è©¦å¤šå€‹å¯èƒ½çš„éµåï¼Œå›å‚³ç¬¬ä¸€å€‹éç©ºå€¼ã€‚
    æ”¯æ´å·¢ç‹€éµï¼š["parent", "child"] æœƒå– obj["parent"]["child"]
    """
    for key in keys:
        try:
            if isinstance(key, (list, tuple)):
                # å·¢ç‹€éµè™•ç†
                value = obj
                for sub_key in key:
                    if not isinstance(value, dict) or sub_key not in value:
                        value = None
                        break
                    value = value[sub_key]
            else:
                # å–®ä¸€éµè™•ç†
                value = obj.get(key) if isinstance(obj, dict) else None
            
            # æª¢æŸ¥å€¼æ˜¯å¦æœ‰æ•ˆï¼ˆé Noneã€ç©ºå­—ä¸²ã€ç©ºåˆ—è¡¨ã€ç©ºå­—å…¸ï¼‰
            if value not in (None, "", [], {}):
                return value
        except (KeyError, TypeError, AttributeError):
            continue
    return None


def find_post_dict(item: dict) -> Optional[dict]:
    """
    åœ¨ thread_item è£¡è‡ªå‹•æ‰¾åˆ°çœŸæ­£çš„è²¼æ–‡ dictã€‚
    æ”¯æ´ä¸åŒç‰ˆæœ¬çš„ GraphQL çµæ§‹è®ŠåŒ–ã€‚
    å›å‚³å«æœ‰ pk/id çš„é‚£å±¤ã€‚
    """
    # 1) å‚³çµ±çµæ§‹
    if 'post' in item and isinstance(item['post'], dict):
        return item['post']
        
    # 2) æ–°ç‰ˆçµæ§‹ï¼špost_info / postInfo / postV2
    for key in ('post_info', 'postInfo', 'postV2', 'media_data', 'thread_data'):
        if key in item and isinstance(item[key], dict):
            return item[key]
    
    # 3) æ·±åº¦æœå°‹ï¼šæ‰¾ç¬¬ä¸€å€‹æœ‰ pk æˆ– id çš„å­ dict
    def search_for_post(obj, max_depth=3):
        if max_depth <= 0:
            return None
            
        if isinstance(obj, dict):
            # æª¢æŸ¥ç•¶å‰å±¤æ˜¯å¦ç‚ºè²¼æ–‡ç‰©ä»¶
            if ('pk' in obj or 'id' in obj) and 'user' in obj:
                return obj
            # éæ­¸æœå°‹å­ç‰©ä»¶
            for value in obj.values():
                result = search_for_post(value, max_depth - 1)
                if result:
                    return result
        elif isinstance(obj, list) and obj:
            # æœå°‹åˆ—è¡¨ä¸­çš„ç¬¬ä¸€å€‹å…ƒç´ 
            return search_for_post(obj[0], max_depth - 1)
            
        return None
    
    return search_for_post(item)


def parse_post_data(post_data: dict, username: str) -> Optional[PostMetrics]:
    """
    å¼·å¥çš„è²¼æ–‡è§£æå™¨ï¼Œä½¿ç”¨å¤šéµ fallback æ©Ÿåˆ¶è™•ç†æ¬„ä½è®Šå‹•ã€‚
    æ”¯æ´ Threads GraphQL API çš„ä¸åŒç‰ˆæœ¬å’Œæ¬„ä½å‘½åè®ŠåŒ–ã€‚
    """
    # ä½¿ç”¨æ™ºèƒ½æœå°‹æ‰¾åˆ°çœŸæ­£çš„è²¼æ–‡ç‰©ä»¶
    post = find_post_dict(post_data)
    
    if not post:
        logging.info(f"âŒ æ‰¾ä¸åˆ°æœ‰æ•ˆçš„ post ç‰©ä»¶ï¼Œæ”¶åˆ°çš„è³‡æ–™éµ: {list(post_data.keys())}")
        logging.info(f"âŒ post_data å…§å®¹ç¯„ä¾‹: {str(post_data)[:300]}...")
        
        # è‡ªå‹•å„²å­˜ç¬¬ä¸€ç­† raw JSON ä¾›åˆ†æ
        try:
            if not DEBUG_FAILED_ITEM_FILE.exists():
                DEBUG_FAILED_ITEM_FILE.write_text(json.dumps(post_data, indent=2, ensure_ascii=False))
                logging.info(f"ğŸ“ å·²å„²å­˜å¤±æ•—ç¯„ä¾‹è‡³ {DEBUG_FAILED_ITEM_FILE}")
        except Exception:
            pass
            
        return None

    # ä½¿ç”¨å¼·å¥çš„æ¬„ä½è§£æ
    post_id = first_of(post, *FIELD_MAP["post_id"])
    code = first_of(post, *FIELD_MAP["code"])
    
    if not post_id:
        logging.info(f"âŒ æ‰¾ä¸åˆ°æœ‰æ•ˆçš„ post_idï¼Œå¯ç”¨æ¬„ä½: {list(post.keys())}")
        return None
        
    if not code:
        logging.info(f"âŒ æ‰¾ä¸åˆ°æœ‰æ•ˆçš„ codeï¼Œå¯ç”¨æ¬„ä½: {list(post.keys())}")
        return None

    url = f"https://www.threads.net/t/{code}"

    # ä½¿ç”¨å¤šéµ fallback è§£ææ‰€æœ‰æ¬„ä½
    author = first_of(post, *FIELD_MAP["author"]) or username
    content = first_of(post, *FIELD_MAP["content"]) or ""
    like_count = first_of(post, *FIELD_MAP["like_count"]) or 0
    comment_count = first_of(post, *FIELD_MAP["comment_count"]) or 0
    share_count = first_of(post, *FIELD_MAP["share_count"]) or 0
    created_at = first_of(post, *FIELD_MAP["created_at"])
    
    # åµæ¸¬æœªçŸ¥æ¬„ä½ï¼ˆç”¨æ–¼èª¿è©¦å’Œæ”¹é€²ï¼‰
    unknown_keys = set(post.keys()) - {
        'pk', 'id', 'post_id', 'code', 'shortcode', 'media_code',
        'user', 'owner', 'caption', 'text', 'content',
        'like_count', 'likeCount', 'feedback_info', 'like_info',
        'text_post_app_info', 'comment_count', 'commentCount', 'reply_info',
        'taken_at', 'taken_at_timestamp', 'publish_date', 'created_time',
        # å¸¸è¦‹ä½†ä¸é‡è¦çš„æ¬„ä½
        'media_type', 'has_liked', 'image_versions2', 'video_versions',
        'carousel_media', 'location', 'user_tags', 'usertags'
    }
    
    if unknown_keys:
        logging.debug(f"ğŸ” ç™¼ç¾æœªçŸ¥æ¬„ä½: {unknown_keys}")
        # å¯ä»¥é¸æ“‡å¯«å…¥æª”æ¡ˆä»¥ä¾›åˆ†æ
        try:
            with open("unknown_fields.log", "a", encoding="utf-8") as f:
                f.write(f"{json.dumps(list(unknown_keys))}\n")
        except Exception:
            pass  # å¯«æª”å¤±æ•—ä¸å½±éŸ¿ä¸»è¦åŠŸèƒ½

    # æˆåŠŸè§£æï¼Œè¨˜éŒ„éƒ¨åˆ†è³‡è¨Šä¾›é™¤éŒ¯
    logging.info(f"âœ… æˆåŠŸè§£æè²¼æ–‡ {post_id}: ä½œè€…={author}, è®šæ•¸={like_count}, å…§å®¹å‰50å­—={content[:50]}...")
    
    return PostMetrics(
        url=url,
        post_id=str(post_id),
        username=username,
        source="playwright",
        processing_stage="playwright_crawled",
        likes_count=int(like_count) if isinstance(like_count, (int, str)) and str(like_count).isdigit() else 0,
        comments_count=int(comment_count) if isinstance(comment_count, (int, str)) and str(comment_count).isdigit() else 0,
        reposts_count=int(share_count) if isinstance(share_count, (int, str)) and str(share_count).isdigit() else 0,
        content=content,
        created_at=created_at,
    )


class PlaywrightLogic:
    """
    ä½¿ç”¨ Playwright é€²è¡Œçˆ¬èŸ²çš„æ ¸å¿ƒé‚è¼¯ã€‚
    """
    def __init__(self):
        self.settings = get_settings().playwright
        self.known_queries = set()  # è¿½è¹¤å·²è¦‹éçš„æŸ¥è©¢åç¨±

    def _build_response_handler(self, username: str, posts: dict, task_id: str, max_posts: int, stream_callback):
        """å»ºç«‹ GraphQL å›æ‡‰è™•ç†å™¨ - ä½¿ç”¨çµæ§‹åˆ¤æ–·è€Œéåç¨±ç™½åå–®"""
        async def _handle_response(res):
            if not GRAPHQL_RE.search(res.url):
                return
                
            qname = res.request.headers.get("x-fb-friendly-name", "UNKNOWN")
            try:
                data = await res.json()
            except Exception:
                return  # é JSON å›æ‡‰ï¼Œç›´æ¥è·³é
                
            # çµæ§‹åˆ¤æ–·ï¼šåªè¦æœ‰ data.mediaData.edges å°±æ˜¯è²¼æ–‡è³‡æ–™
            edges = data.get("data", {}).get("mediaData", {}).get("edges", [])
            if not edges:
                return  # è·Ÿè²¼æ–‡ç„¡é—œï¼Œå¿½ç•¥
                
            # è§£æè²¼æ–‡ - æ”¯æ´æ–°èˆŠçµæ§‹çš„ fallback
            new_count = 0
            for edge in edges:
                # æ”¯æ´ thread_items / items çš„ fallback
                node = edge.get("node", {})
                thread_items = node.get("thread_items") or node.get("items") or []
                
                # è‡ªå‹•å„²å­˜ç¬¬ä¸€ç­†æˆåŠŸçš„ thread_item ä¾›åˆ†æ
                if thread_items and new_count == 0:
                    try:
                        if not SAMPLE_THREAD_ITEM_FILE.exists():
                            SAMPLE_THREAD_ITEM_FILE.write_text(json.dumps(thread_items[0], indent=2, ensure_ascii=False))
                            logging.info(f"ğŸ“ å·²å„²å­˜æˆåŠŸç¯„ä¾‹è‡³ {SAMPLE_THREAD_ITEM_FILE}")
                    except Exception:
                        pass

                for item in thread_items:
                    parsed_post = parse_post_data(item, username)
                    if parsed_post and parsed_post.post_id not in posts:
                        posts[parsed_post.post_id] = parsed_post
                        new_count += 1
                        
            if new_count > 0:
                logging.info(f"âœ… [{qname}] +{new_count} (ç¸½ {len(posts)}/{max_posts})")
                # ä½¿ç”¨å›èª¿å‡½æ•¸ç™¼é€ä¸²æµè¨Šæ¯ (å¦‚æœæœ‰çš„è©±)
                if stream_callback:
                    stream_callback(f"âœ… å¾ {qname} è§£æåˆ° {new_count} å‰‡æ–°è²¼æ–‡ï¼Œç¸½æ•¸: {len(posts)}")
            
            # æª¢æŸ¥æ˜¯å¦å·²é”åˆ°ç›®æ¨™æ•¸é‡
            if len(posts) >= max_posts:
                logging.info(f"é”åˆ°ç›®æ¨™è²¼æ–‡æ•¸ {max_posts}ï¼Œå°‡ç›¡å¿«åœæ­¢æ»¾å‹•ã€‚")
                # é€™è£¡å¯ä»¥è§¸ç™¼ä¸€å€‹äº‹ä»¶ä¾†åœæ­¢æ»¾å‹•ï¼Œä½†ç›®å‰æ¶æ§‹ä¸‹æœƒè‡ªç„¶åœæ­¢
                pass
                
            # è¨˜éŒ„æ–°ç™¼ç¾çš„æŸ¥è©¢åç¨±ï¼ˆç”¨æ–¼é™¤éŒ¯ï¼‰
            if qname not in self.known_queries:
                self.known_queries.add(qname)
                # å¯é¸ï¼šå¯«å…¥æª”æ¡ˆä»¥ä¾›æ—¥å¾Œåˆ†æ
                try:
                    with open("seen_queries.txt", "a", encoding="utf-8") as f:
                        f.write(f"{qname}\n")
                except Exception:
                    pass  # å¯«æª”å¤±æ•—ä¸å½±éŸ¿ä¸»è¦åŠŸèƒ½
                    
        return _handle_response

    async def fetch_posts(
        self,
        username: str,
        max_posts: int,
        auth_json_content: Dict,
        task_id: str = None
    ) -> PostMetricsBatch:
        """
        ä½¿ç”¨æŒ‡å®šçš„èªè­‰å…§å®¹çˆ¬å–è²¼æ–‡ã€‚
        æ­¤ç‰ˆæœ¬æœƒèšåˆæ‰€æœ‰çµæœï¼Œä¸¦ä¸€æ¬¡æ€§è¿”å› PostMetricsBatchã€‚
        """
        target_url = f"https://www.threads.com/@{username}"
        posts: Dict[str, PostMetrics] = {}
        
        # 1. å®‰å…¨åœ°å°‡ auth.json å…§å®¹å¯«å…¥è‡¨æ™‚æª”æ¡ˆ
        auth_file = Path(tempfile.gettempdir()) / f"{task_id or uuid.uuid4()}_auth.json"
        try:
            with open(auth_file, 'w', encoding='utf-8') as f:
                json.dump(auth_json_content, f)

            async with async_playwright() as p:
                browser = await p.chromium.launch(
                    headless=self.settings.headless,
                    timeout=self.settings.navigation_timeout,
                    args=["--no-sandbox", "--disable-dev-shm-usage", "--disable-blink-features=AutomationControlled"]
                )
                ctx = await browser.new_context(
                    storage_state=str(auth_file),
                    user_agent=self.settings.user_agent, # å¾è¨­å®šè®€å–
                    viewport={"width": 1920, "height": 1080},
                    locale="en-US",
                    has_touch=True,
                    accept_downloads=False
                )
                page = await ctx.new_page()
                page.on("console", lambda m: logging.info(f"CONSOLE [{m.type}] {m.text}"))

                # Response handler
                response_handler = self._build_response_handler(username, posts, task_id, max_posts, stream_callback=None)
                page.on("response", response_handler)

                logging.info(f"å°è¦½è‡³ç›®æ¨™é é¢: {target_url}")
                await page.goto(target_url, wait_until="networkidle", timeout=self.settings.navigation_timeout)

                # --- æ»¾å‹•èˆ‡å»¶é²é‚è¼¯ ---
                scroll_attempts_without_new_posts = 0
                max_retries = 5

                while len(posts) < max_posts:
                    posts_before_scroll = len(posts)
                    await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")

                    try:
                        async with page.expect_response(lambda res: GRAPHQL_RE.search(res.url), timeout=15000):
                            pass
                    except PlaywrightTimeoutError:
                        logging.warning(f"â³ [Task: {task_id}] æ»¾å‹•å¾Œç­‰å¾…ç¶²è·¯å›æ‡‰é€¾æ™‚ã€‚")

                    delay = random.uniform(self.settings.scroll_delay_min, self.settings.scroll_delay_max) # å¾è¨­å®šè®€å–
                    await asyncio.sleep(delay)

                    if len(posts) == posts_before_scroll:
                        scroll_attempts_without_new_posts += 1
                        logging.info(f"æ»¾å‹•å¾Œæœªç™¼ç¾æ–°è²¼æ–‡ (å˜—è©¦ {scroll_attempts_without_new_posts}/{max_retries})")
                        if scroll_attempts_without_new_posts >= max_retries:
                            logging.info("å·²é”é é¢æœ«ç«¯æˆ–ç„¡æ–°å…§å®¹ï¼Œåœæ­¢æ»¾å‹•ã€‚")
                            break
                    else:
                        scroll_attempts_without_new_posts = 0
                
                await browser.close()

            # --- æ•´ç†ä¸¦å›å‚³çµæœ ---
            final_posts = list(posts.values())
            total_found = len(final_posts)
            
            # æ ¹æ“š max_posts æˆªæ–·çµæœ
            if total_found > max_posts:
                try:
                    final_posts.sort(key=lambda p: p.created_at or datetime.min, reverse=True)
                except Exception:
                    pass 
                final_posts = final_posts[:max_posts]
            
            logging.info(f"ğŸ”„ [Task: {task_id}] æº–å‚™å›å‚³æœ€çµ‚è³‡æ–™ï¼šå…±ç™¼ç¾ {total_found} å‰‡è²¼æ–‡, å›å‚³ {len(final_posts)} å‰‡")
            
            # ä¿å­˜åŸå§‹æŠ“å–è³‡æ–™ä¾›èª¿è©¦
            try:
                raw_data = {
                    "task_id": task_id,
                    "username": username, 
                    "timestamp": datetime.now().isoformat(),
                    "total_found": total_found,
                    "returned_count": len(final_posts),
                    "posts": [
                        {
                            "url": post.url,
                            "post_id": post.post_id,
                            "likes_count": post.likes_count,
                            "comments_count": post.comments_count,
                            "reposts_count": post.reposts_count,
                            "shares_count": post.shares_count,
                            "views_count": post.views_count,
                            "content": post.content,
                            "created_at": post.created_at.isoformat() if post.created_at else None
                        } for post in final_posts
                    ]
                }
                
                # ä½¿ç”¨æ™‚é–“æˆ³è¨˜é¿å…æª”æ¡ˆè¡çª
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                raw_file = DEBUG_DIR / f"crawl_data_{timestamp}_{task_id[:8]}.json"
                raw_file.write_text(json.dumps(raw_data, indent=2, ensure_ascii=False))
                logging.info(f"ğŸ’¾ [Task: {task_id}] å·²ä¿å­˜åŸå§‹æŠ“å–è³‡æ–™è‡³: {raw_file}")
                
            except Exception as e:
                logging.warning(f"âš ï¸ [Task: {task_id}] ä¿å­˜èª¿è©¦è³‡æ–™å¤±æ•—: {e}")
            
            batch = PostMetricsBatch(
                posts=final_posts,
                username=username,
                total_count=total_found,
                processing_stage="playwright_completed"
            )
            return batch
            
        except Exception as e:
            error_message = f"Playwright æ ¸å¿ƒé‚è¼¯å‡ºéŒ¯: {e}"
            logging.error(error_message, exc_info=True)
            raise
        
        finally:
            if auth_file.exists():
                auth_file.unlink()
                logging.info(f"ğŸ—‘ï¸ [Task: {task_id}] å·²åˆªé™¤è‡¨æ™‚èªè­‰æª”æ¡ˆ: {auth_file}") 