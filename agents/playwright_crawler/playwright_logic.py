import json
import asyncio
import logging
import random
import re
import tempfile
import uuid
from pathlib import Path
from typing import Dict, List, Optional, AsyncIterable, Callable, Any
from datetime import datetime
import httpx

from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeoutError, Browser, BrowserContext, Page, Route, APIResponse

from common.settings import get_settings
from common.a2a import stream_text, stream_data, stream_status, stream_error, TaskState
from common.models import PostMetrics, PostMetricsBatch
from common.nats_client import publish_progress
from common.utils import (
    get_best_video_url,
    get_best_image_url,
    generate_post_url,
    first_of,
    parse_thread_item
)

# Vision Agent URL - æŒ‡å‘æˆ‘å€‘å³å°‡å»ºç«‹çš„æ–°ç«¯é»
VISION_AGENT_URL = "http://vision-agent:8005/v1/vision/extract-views-from-image"

# èª¿è©¦æª”æ¡ˆè·¯å¾‘
DEBUG_DIR = Path(__file__).parent / "debug"
DEBUG_DIR.mkdir(exist_ok=True)
DEBUG_FAILED_ITEM_FILE = DEBUG_DIR / "failed_post_sample.json"
SAMPLE_THREAD_ITEM_FILE = DEBUG_DIR / "sample_thread_item.json"
RAW_CRAWL_DATA_FILE = DEBUG_DIR / "raw_crawl_data.json"

# è¨­å®šæ—¥èªŒ
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# --- GraphQL API è©³ç´°è³‡è¨Š ---
GRAPHQL_RE = re.compile(r"/graphql/query")
# ä¿ç•™ç”¨ä¾† debugï¼›å¯¦éš›é‚è¼¯æ”¹æˆã€Œçœ‹çµæ§‹ã€å³å¯
POST_QUERY_NAMES = set()

# --- å¼·å¥çš„æ¬„ä½å°ç…§è¡¨ ---
FIELD_MAP = {
    "like_count": [
        "like_count", "likeCount", 
        ["feedback_info", "aggregated_like_count"],
        ["like_info", "count"]
    ],
    "comment_count": [
        ["text_post_app_info", "direct_reply_count"],
        "comment_count", "commentCount",
        ["reply_info", "count"]
    ],
    "share_count": [
        ["text_post_app_info", "reshare_count"],  # çœŸæ­£çš„ shares æ¬„ä½
        "reshareCount", "share_count", "shareCount"
    ],
    "repost_count": [
        ["text_post_app_info", "repost_count"],  # reposts æ¬„ä½
        "repostCount", "repost_count"
    ],
    "content": [
        ["caption", "text"],
        "caption", "text", "content"
    ],
    "author": [
        ["user", "username"], 
        ["owner", "username"],
        ["user", "handle"]
    ],
    "created_at": [
        "taken_at", "taken_at_timestamp", 
        "publish_date", "created_time"
    ],
    "post_id": [
        "pk", "id", "post_id"
    ],
    "code": [
        "code", "shortcode", "media_code"
    ],
    # å¢åŠ ç›´æ¥å¾ API å˜—è©¦ç²å– views çš„è·¯å¾‘
    "view_count": [
        ["feedback_info", "view_count"],
        ["video_info", "play_count"],
        "view_count",
        "views",
        "impression_count"
    ],
}

def parse_post_data(thread_item: Dict[str, Any], username: str) -> Optional[PostMetrics]:
    """
    å¼·å¥çš„è²¼æ–‡è§£æå™¨ï¼Œä½¿ç”¨å¤šéµ fallback æ©Ÿåˆ¶è™•ç†æ¬„ä½è®Šå‹•ã€‚
    æ”¯æ´ Threads GraphQL API çš„ä¸åŒç‰ˆæœ¬å’Œæ¬„ä½å‘½åè®ŠåŒ–ã€‚
    """
    # ä½¿ç”¨æ™ºèƒ½æœå°‹æ‰¾åˆ°çœŸæ­£çš„è²¼æ–‡ç‰©ä»¶
    post = parse_thread_item(thread_item)
    
    if not post:
        logging.info(f"âŒ æ‰¾ä¸åˆ°æœ‰æ•ˆçš„ post ç‰©ä»¶ï¼Œæ”¶åˆ°çš„è³‡æ–™éµ: {list(thread_item.keys())}")
        logging.info(f"âŒ thread_item å…§å®¹ç¯„ä¾‹: {str(thread_item)[:300]}...")
        
        # è‡ªå‹•å„²å­˜ç¬¬ä¸€ç­† raw JSON ä¾›åˆ†æ
        try:
            if not DEBUG_FAILED_ITEM_FILE.exists():
                DEBUG_FAILED_ITEM_FILE.write_text(
                    json.dumps(thread_item, indent=2, ensure_ascii=False),
                    encoding="utf-8" # æ˜ç¢ºæŒ‡å®š UTF-8
                )
                logging.info(f"ğŸ“ å·²å„²å­˜å¤±æ•—ç¯„ä¾‹è‡³ {DEBUG_FAILED_ITEM_FILE}")
        except Exception:
            pass  # å¯«æª”å¤±æ•—ä¸å½±éŸ¿ä¸»è¦åŠŸèƒ½
            
        return None

    # ä½¿ç”¨å¼·å¥çš„æ¬„ä½è§£æ
    post_id = first_of(post, *FIELD_MAP["post_id"])
    code = first_of(post, *FIELD_MAP["code"])
    
    if not post_id:
        logging.info(f"âŒ æ‰¾ä¸åˆ°æœ‰æ•ˆçš„ post_idï¼Œå¯ç”¨æ¬„ä½: {list(post.keys())}")
        return None
        
    if not code:
        logging.info(f"âŒ æ‰¾ä¸åˆ°æœ‰æ•ˆçš„ codeï¼Œå¯ç”¨æ¬„ä½: {list(post.keys())}")
        return None

    # --- URL ä¿®å¾©ï¼šä½¿ç”¨æ­£ç¢ºçš„æ ¼å¼ ---
    # èˆŠæ ¼å¼ (éŒ¯èª¤): f"https://www.threads.net/t/{code}"
    # æ–°æ ¼å¼ (æ­£ç¢º): f"https://www.threads.com/@{username}/post/{code}"
    url = generate_post_url(username, code)

    # ä½¿ç”¨å¤šéµ fallback è§£ææ‰€æœ‰æ¬„ä½
    author = first_of(post, *FIELD_MAP["author"]) or username
    content = first_of(post, *FIELD_MAP["content"]) or ""
    like_count = first_of(post, *FIELD_MAP["like_count"]) or 0
    comment_count = first_of(post, *FIELD_MAP["comment_count"]) or 0
    share_count = first_of(post, *FIELD_MAP["share_count"]) or 0
    repost_count = first_of(post, *FIELD_MAP["repost_count"]) or 0
    created_at = first_of(post, *FIELD_MAP["created_at"])
    
    # åµæ¸¬æœªçŸ¥æ¬„ä½ï¼ˆç”¨æ–¼èª¿è©¦å’Œæ”¹é€²ï¼‰
    unknown_keys = set(post.keys()) - {
        'pk', 'id', 'post_id', 'code', 'shortcode', 'media_code',
        'user', 'owner', 'caption', 'text', 'content',
        'like_count', 'likeCount', 'feedback_info', 'like_info',
        'text_post_app_info', 'comment_count', 'commentCount', 'reply_info',
        'taken_at', 'taken_at_timestamp', 'publish_date', 'created_time',
        # å¸¸è¦‹ä½†ä¸é‡è¦çš„æ¬„ä½
        'media_type', 'has_liked', 'image_versions2', 'video_versions',
        'carousel_media', 'location', 'user_tags', 'usertags'
    }
    
    if unknown_keys:
        logging.debug(f"ğŸ” ç™¼ç¾æœªçŸ¥æ¬„ä½: {unknown_keys}")
        # å¯ä»¥é¸æ“‡å¯«å…¥æª”æ¡ˆä»¥ä¾›åˆ†æ
        try:
            with open("unknown_fields.log", "a", encoding="utf-8") as f:
                f.write(f"{json.dumps(list(unknown_keys))}\n")
        except Exception:
            pass  # å¯«æª”å¤±æ•—ä¸å½±éŸ¿ä¸»è¦åŠŸèƒ½

    # --- MEDIA -------------------------------------------------------------
    images, videos = [], []

    def append_image(url):
        if url and url not in images:
            images.append(url)

    def append_video(url):
        if url and url not in videos:
            videos.append(url)

    def best_candidate(candidates: list[dict], prefer_mp4=False):
        """
        å¾ image_versions2 æˆ– video_versions è£¡æŒ‘ã€Œå¯¬åº¦æœ€å¤§çš„é‚£å€‹ã€URL
        """
        if not candidates:
            return None
        key = "url"
        return max(candidates, key=lambda c: c.get("width", 0)).get(key)

    # 1) å–®åœ– / å–®ç‰‡
    if "image_versions2" in post:
        append_image(best_candidate(post["image_versions2"].get("candidates", [])))
    if "video_versions" in post:
        append_video(best_candidate(post.get("video_versions", []), prefer_mp4=True))

    # 2) è¼ªæ’­ carousel_media (åŠ å…¥ None çš„å®‰å…¨è™•ç†)
    for media in post.get("carousel_media") or []:
        if "image_versions2" in media:
            append_image(best_candidate(media["image_versions2"].get("candidates", [])))
        if "video_versions" in media:
            append_video(best_candidate(media.get("video_versions", []), prefer_mp4=True))
    # -----------------------------------------------------------------------


    # æˆåŠŸè§£æï¼Œè¨˜éŒ„éƒ¨åˆ†è³‡è¨Šä¾›é™¤éŒ¯
    logging.info(f"âœ… æˆåŠŸè§£æè²¼æ–‡ {post_id}: ä½œè€…={author}, è®šæ•¸={like_count}, åœ–ç‰‡={len(images)}, å½±ç‰‡={len(videos)}")
    
    return PostMetrics(
        url=url,
        post_id=str(post_id),
        username=username,
        source="playwright",
        processing_stage="playwright_crawled",
        likes_count=int(like_count) if isinstance(like_count, (int, str)) and str(like_count).isdigit() else 0,
        comments_count=int(comment_count) if isinstance(comment_count, (int, str)) and str(comment_count).isdigit() else 0,
        reposts_count=int(repost_count) if isinstance(repost_count, (int, str)) and str(repost_count).isdigit() else 0,
        shares_count=int(share_count) if isinstance(share_count, (int, str)) and str(share_count).isdigit() else 0,
        content=content,
        created_at=datetime.fromtimestamp(created_at) if created_at and isinstance(created_at, (int, float)) else datetime.utcnow(),
        images=images,
        videos=videos,
        # æ–°å¢ï¼šç›´æ¥å¾ API è§£æ views_countï¼ˆæŒ‰æŒ‡å¼•å„ªå…ˆå˜—è©¦ APIï¼‰
        views_count=first_of(post, *FIELD_MAP["view_count"]) if first_of(post, *FIELD_MAP["view_count"]) is not None else None,
    )

# +++ æ–°å¢ï¼šå¾å‰ç«¯è§£æç€è¦½æ•¸çš„è¼”åŠ©å‡½å¼ï¼ˆä¿®å¾©ç©ºæ ¼å•é¡Œï¼‰ +++
def parse_views_text(text: Optional[str]) -> Optional[int]:
    """å°‡ '161.9è¬æ¬¡ç€è¦½' æˆ– '4 è¬æ¬¡ç€è¦½' æˆ– '1.2M views' é€™é¡æ–‡å­—è½‰æ›ç‚ºæ•´æ•¸"""
    if not text:
        return None
    try:
        original_text = text
        
        # ç§»é™¤ä¸å¿…è¦çš„æ–‡å­—ï¼Œä¿ç•™æ•¸å­—å’Œå–®ä½
        text = re.sub(r'ä¸²æ–‡\s*', '', text)  # ç§»é™¤ "ä¸²æ–‡"
        
        # è™•ç†ä¸­æ–‡æ ¼å¼ï¼š1.2è¬ã€4 è¬æ¬¡ç€è¦½ã€5000æ¬¡ç€è¦½
        if 'è¬' in text:
            match = re.search(r'([\d.]+)\s*è¬', text)  # å…è¨±æ•¸å­—å’Œè¬ä¹‹é–“æœ‰ç©ºæ ¼
            if match:
                return int(float(match.group(1)) * 10000)
        elif 'å„„' in text:
            match = re.search(r'([\d.]+)\s*å„„', text)  # å…è¨±æ•¸å­—å’Œå„„ä¹‹é–“æœ‰ç©ºæ ¼
            if match:
                return int(float(match.group(1)) * 100000000)
        
        # è™•ç†è‹±æ–‡æ ¼å¼ï¼š1.2M views, 500K views
        text_upper = text.upper()
        if 'M' in text_upper:
            match = re.search(r'([\d.]+)M', text_upper)
            if match:
                return int(float(match.group(1)) * 1000000)
        elif 'K' in text_upper:
            match = re.search(r'([\d.]+)K', text_upper)
            if match:
                return int(float(match.group(1)) * 1000)
        
        # è™•ç†ç´”æ•¸å­—æ ¼å¼ï¼ˆå¯èƒ½åŒ…å«é€—è™Ÿï¼‰
        match = re.search(r'[\d,]+', text)
        if match:
            return int(match.group(0).replace(',', ''))
        
        logging.debug(f"ğŸ” ç„¡æ³•è§£æç€è¦½æ•¸æ–‡å­—: '{original_text}'")
        return None
        
    except (ValueError, IndexError) as e:
        logging.warning(f"âš ï¸ ç„¡æ³•è§£æç€è¦½æ•¸æ–‡å­—: '{text}' - {e}")
        return None

class PlaywrightLogic:
    """
    ä½¿ç”¨ Playwright é€²è¡Œçˆ¬èŸ²çš„æ ¸å¿ƒé‚è¼¯ã€‚
    """
    def __init__(self):
        self.settings = get_settings().playwright
        self.known_queries = set()  # è¿½è¹¤å·²è¦‹éçš„æŸ¥è©¢åç¨±
        self.context = None  # åˆå§‹åŒ– context

    def _build_response_handler(self, username: str, posts: dict, task_id: str, max_posts: int, stream_callback):
        """å»ºç«‹ GraphQL å›æ‡‰è™•ç†å™¨ - ä½¿ç”¨çµæ§‹åˆ¤æ–·è€Œéåç¨±ç™½åå–®"""
        async def _handle_response(res):
            if not GRAPHQL_RE.search(res.url):
                return
                
            qname = res.request.headers.get("x-fb-friendly-name", "UNKNOWN")
            try:
                data = await res.json()
            except Exception:
                return  # é JSON å›æ‡‰ï¼Œç›´æ¥è·³é
                
            # çµæ§‹åˆ¤æ–·ï¼šåªè¦æœ‰ data.mediaData.edges å°±æ˜¯è²¼æ–‡è³‡æ–™
            edges = data.get("data", {}).get("mediaData", {}).get("edges", [])
            if not edges:
                return  # è·Ÿè²¼æ–‡ç„¡é—œï¼Œå¿½ç•¥
                
            # è§£æè²¼æ–‡ - æ”¯æ´æ–°èˆŠçµæ§‹çš„ fallback
            new_count = 0
            for edge in edges:
                # æ”¯æ´ thread_items / items çš„ fallback
                node = edge.get("node", {})
                thread_items = node.get("thread_items") or node.get("items") or []
                
                # è‡ªå‹•å„²å­˜ç¬¬ä¸€ç­†æˆåŠŸçš„ thread_item ä¾›åˆ†æ
                if thread_items and new_count == 0:
                    try:
                        if not SAMPLE_THREAD_ITEM_FILE.exists():
                            SAMPLE_THREAD_ITEM_FILE.write_text(
                                json.dumps(thread_items[0], indent=2, ensure_ascii=False),
                                encoding="utf-8" # æ˜ç¢ºæŒ‡å®š UTF-8
                            )
                            logging.info(f"ğŸ“ å·²å„²å­˜æˆåŠŸç¯„ä¾‹è‡³ {SAMPLE_THREAD_ITEM_FILE}")
                    except Exception:
                        pass

                for item in thread_items:
                    parsed_post = parse_post_data(item, username)
                    if parsed_post and parsed_post.post_id not in posts:
                        posts[parsed_post.post_id] = parsed_post
                        new_count += 1
                        
                        # ğŸ”¥ æ–°å¢ï¼šæ¯è§£æä¸€å€‹è²¼æ–‡å°±ç™¼å¸ƒå³æ™‚é€²åº¦
                        from common.nats_client import publish_progress
                        await publish_progress(
                            task_id, 
                            "post_parsed",
                            username=username,
                            post_id=parsed_post.post_id,
                            current=len(posts),
                            total=max_posts,
                            progress=len(posts) / max_posts,
                            content_preview=parsed_post.content[:50] + "..." if parsed_post.content else "ç„¡å…§å®¹",
                            likes=parsed_post.likes_count
                        )
                        
            if new_count > 0:
                logging.info(f"âœ… [{qname}] +{new_count} (ç¸½ {len(posts)}/{max_posts})")
                # ä½¿ç”¨å›èª¿å‡½æ•¸ç™¼é€ä¸²æµè¨Šæ¯ (å¦‚æœæœ‰çš„è©±)
                if stream_callback:
                    stream_callback(f"âœ… å¾ {qname} è§£æåˆ° {new_count} å‰‡æ–°è²¼æ–‡ï¼Œç¸½æ•¸: {len(posts)}")
                    
                # ğŸ”¥ æ–°å¢ï¼šæ¯æ‰¹è§£æå®Œæˆå¾Œçš„é€²åº¦æ›´æ–°
                from common.nats_client import publish_progress
                await publish_progress(
                    task_id, 
                    "batch_parsed",
                    username=username,
                    batch_size=new_count,
                    current=len(posts),
                    total=max_posts,
                    progress=len(posts) / max_posts,
                    query_name=qname
                )
            
            # æª¢æŸ¥æ˜¯å¦å·²é”åˆ°ç›®æ¨™æ•¸é‡
            if len(posts) >= max_posts:
                logging.info(f"é”åˆ°ç›®æ¨™è²¼æ–‡æ•¸ {max_posts}ï¼Œå°‡ç›¡å¿«åœæ­¢æ»¾å‹•ã€‚")
                # é€™è£¡å¯ä»¥è§¸ç™¼ä¸€å€‹äº‹ä»¶ä¾†åœæ­¢æ»¾å‹•ï¼Œä½†ç›®å‰æ¶æ§‹ä¸‹æœƒè‡ªç„¶åœæ­¢
                pass
                
            # è¨˜éŒ„æ–°ç™¼ç¾çš„æŸ¥è©¢åç¨±ï¼ˆç”¨æ–¼é™¤éŒ¯ï¼‰
            if qname not in self.known_queries:
                self.known_queries.add(qname)
                # å¯é¸ï¼šå¯«å…¥æª”æ¡ˆä»¥ä¾›æ—¥å¾Œåˆ†æ
                try:
                    with open("seen_queries.txt", "a", encoding="utf-8") as f:
                        f.write(f"{qname}\n")
                except Exception:
                    pass  # å¯«æª”å¤±æ•—ä¸å½±éŸ¿ä¸»è¦åŠŸèƒ½
                    
        return _handle_response

    async def get_ordered_post_urls_from_page(self, page: Page, username: str, max_posts: int) -> List[str]:
        """
        å¾ç”¨æˆ¶é é¢ç›´æ¥æå–è²¼æ–‡ URLsï¼Œä¿æŒæ™‚é–“é †åº
        é€™æ˜¯è§£æ±ºè²¼æ–‡é †åºæ··äº‚å•é¡Œçš„é—œéµæ–¹æ³•
        """
        user_url = f"https://www.threads.com/@{username}"
        logging.info(f"ğŸ” æ­£åœ¨å¾ç”¨æˆ¶é é¢ç²å–æœ‰åºçš„è²¼æ–‡ URLs: {user_url}")
        
        await page.goto(user_url, wait_until="networkidle")
        
        # ç­‰å¾…é é¢è¼‰å…¥
        await asyncio.sleep(3)
        
        # æ»¾å‹•ä»¥è¼‰å…¥æ›´å¤šè²¼æ–‡ï¼ˆä½†ä¸è¦æ»¾å‹•å¤ªå¤šæ¬¡é¿å…è¼‰å…¥éèˆŠçš„è²¼æ–‡ï¼‰
        scroll_count = min(3, max(1, max_posts // 10))  # æ ¹æ“šéœ€æ±‚å‹•æ…‹èª¿æ•´æ»¾å‹•æ¬¡æ•¸
        for i in range(scroll_count):
            await page.mouse.wheel(0, 1000)
            await asyncio.sleep(2)
        
        # æå–è²¼æ–‡ URLsï¼Œä¿æŒåŸå§‹é †åº
        post_urls = await page.evaluate("""
            () => {
                // ç²å–æ‰€æœ‰è²¼æ–‡é€£çµï¼Œä¿æŒDOMä¸­çš„åŸå§‹é †åº
                const links = Array.from(document.querySelectorAll('a[href*="/post/"]'));
                const urls = [];
                const seen = new Set();
                
                // éæ­·æ™‚ä¿æŒé †åºï¼Œåªå»é‡ä½†ä¸é‡æ’
                for (const link of links) {
                    const url = link.href;
                    if (url.includes('/post/') && !seen.has(url)) {
                        seen.add(url);
                        urls.push(url);
                    }
                }
                
                return urls;
            }
        """)
        
        # é™åˆ¶æ•¸é‡ä½†ä¿æŒé †åº
        post_urls = post_urls[:max_posts]
        logging.info(f"   âœ… æŒ‰æ™‚é–“é †åºæ‰¾åˆ° {len(post_urls)} å€‹è²¼æ–‡ URLs")
        
        return post_urls

    async def fetch_posts(
        self,
        username: str,
        max_posts: int,
        auth_json_content: Dict,
        task_id: str = None
    ) -> PostMetricsBatch:
        """
        ä½¿ç”¨æŒ‡å®šçš„èªè­‰å…§å®¹çˆ¬å–è²¼æ–‡ã€‚
        æ­¤ç‰ˆæœ¬æœƒèšåˆæ‰€æœ‰çµæœï¼Œä¸¦ä¸€æ¬¡æ€§è¿”å› PostMetricsBatchã€‚
        """
        if task_id is None:
            task_id = str(uuid.uuid4())
            
        target_url = f"https://www.threads.com/@{username}"
        posts: Dict[str, PostMetrics] = {}
        
        # ç™¼å¸ƒé–‹å§‹çˆ¬å–çš„é€²åº¦
        await publish_progress(task_id, "fetch_start", username=username, max_posts=max_posts)
        
        # 1. å®‰å…¨åœ°å°‡ auth.json å…§å®¹å¯«å…¥è‡¨æ™‚æª”æ¡ˆ
        auth_file = Path(tempfile.gettempdir()) / f"{task_id or uuid.uuid4()}_auth.json"
        try:
            with open(auth_file, 'w', encoding='utf-8') as f:
                json.dump(auth_json_content, f)

            async with async_playwright() as p:
                browser = await p.chromium.launch(
                    headless=self.settings.headless,
                    timeout=self.settings.navigation_timeout,
                    args=["--no-sandbox", "--disable-dev-shm-usage", "--disable-blink-features=AutomationControlled"]
                )
                self.context = await browser.new_context(
                    storage_state=str(auth_file),
                    user_agent=self.settings.user_agent, # å¾è¨­å®šè®€å–
                    viewport={"width": 1920, "height": 1080},
                    locale="zh-TW",  # è¨­å®šç‚ºç¹é«”ä¸­æ–‡
                    has_touch=True,
                    accept_downloads=False,
                    bypass_csp=True  # æ–°å¢ï¼šç¹é CSP é™åˆ¶
                )
                # æ–°å¢ï¼šéš±è— webdriver å±¬æ€§
                await self.context.add_init_script(
                    "Object.defineProperty(navigator,'webdriver',{get:()=>undefined})"
                )
                
                page = await self.context.new_page()
                page.on("console", lambda m: logging.info(f"CONSOLE [{m.type}] {m.text}"))

                # --- æ–°æ–¹æ³•ï¼šå…ˆç²å–æœ‰åºçš„è²¼æ–‡ URLs ---
                logging.info(f"ğŸ¯ [Task: {task_id}] ä½¿ç”¨æ–°çš„æœ‰åºè²¼æ–‡ç²å–æ–¹æ³•")
                ordered_post_urls = await self.get_ordered_post_urls_from_page(page, username, max_posts)
                
                if not ordered_post_urls:
                    logging.warning(f"âš ï¸ [Task: {task_id}] ç„¡æ³•å¾ç”¨æˆ¶é é¢ç²å–è²¼æ–‡ URLsï¼Œå›é€€è‡³èˆŠæ–¹æ³•")
                    # å›é€€è‡³åŸå§‹çš„ GraphQL æ””æˆªæ–¹æ³•
                response_handler = self._build_response_handler(username, posts, task_id, max_posts, stream_callback=None)
                page.on("response", response_handler)

                logging.info(f"å°è¦½è‡³ç›®æ¨™é é¢: {target_url}")
                await page.goto(target_url, wait_until="networkidle", timeout=self.settings.navigation_timeout)

                # --- æ»¾å‹•èˆ‡å»¶é²é‚è¼¯ ---
                scroll_attempts_without_new_posts = 0
                max_retries = 5

                while len(posts) < max_posts:
                    posts_before_scroll = len(posts)
                    await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")

                    try:
                        async with page.expect_response(lambda res: GRAPHQL_RE.search(res.url), timeout=15000):
                            pass
                    except PlaywrightTimeoutError:
                        logging.warning(f"â³ [Task: {task_id}] æ»¾å‹•å¾Œç­‰å¾…ç¶²è·¯å›æ‡‰é€¾æ™‚ã€‚")

                    delay = random.uniform(self.settings.scroll_delay_min, self.settings.scroll_delay_max) # å¾è¨­å®šè®€å–
                    await asyncio.sleep(delay)

                    if len(posts) == posts_before_scroll:
                        scroll_attempts_without_new_posts += 1
                        logging.info(f"æ»¾å‹•å¾Œæœªç™¼ç¾æ–°è²¼æ–‡ (å˜—è©¦ {scroll_attempts_without_new_posts}/{max_retries})")
                        if scroll_attempts_without_new_posts >= max_retries:
                            logging.info("å·²é”é é¢æœ«ç«¯æˆ–ç„¡æ–°å…§å®¹ï¼Œåœæ­¢æ»¾å‹•ã€‚")
                            break
                    else:
                        scroll_attempts_without_new_posts = 0
                        # ç™¼å¸ƒé€²åº¦æ›´æ–°
                        await publish_progress(
                            task_id, "fetch_progress", 
                            username=username,
                            current=len(posts), 
                            total=max_posts,
                            progress=min(len(posts) / max_posts, 1.0)
                        )
                else:
                    # --- æ–°æ–¹æ³•ï¼šä½¿ç”¨æœ‰åºURLså‰µå»ºPostMetricsï¼Œç¨å¾Œè£œé½Šè©³ç´°æ•¸æ“š ---
                    logging.info(f"âœ… [Task: {task_id}] ä½¿ç”¨æœ‰åºURLså‰µå»ºåŸºç¤PostMetricsï¼Œä¿æŒæ™‚é–“é †åº")
                    ordered_posts = []  # ä¿æŒé †åºçš„é™£åˆ—
                    
                    for i, post_url in enumerate(ordered_post_urls):
                        # å¾ URL ä¸­æå– post_id å’Œ code
                        url_parts = post_url.split('/')
                        if len(url_parts) >= 2:
                            code = url_parts[-1] if url_parts[-1] != 'media' else url_parts[-2]  # è™•ç† /media çµå°¾
                            post_id = f"{username}_{code}"  # ç”Ÿæˆå”¯ä¸€çš„ post_id
                            
                            # å‰µå»ºåŸºæœ¬çš„ PostMetrics ç‰©ä»¶
                            post_metrics = PostMetrics(
                                url=post_url,
                                post_id=post_id,
                                username=username,
                                source="playwright_ordered",
                                processing_stage="url_extracted",
                                likes_count=0,  # å°‡é€šéé é¢è¨ªå•è£œé½Š
                                comments_count=0,  # å°‡é€šéé é¢è¨ªå•è£œé½Š
                                reposts_count=0,  # å°‡é€šéé é¢è¨ªå•è£œé½Š
                                shares_count=0,  # å°‡é€šéé é¢è¨ªå•è£œé½Š
                                content="",  # å°‡é€šéé é¢è¨ªå•è£œé½Š
                                created_at=datetime.utcnow(),  # ä½¿ç”¨ç•¶å‰æ™‚é–“ä½œç‚ºé è¨­å€¼
                                images=[],  # å°‡é€šéé é¢è¨ªå•è£œé½Š
                                videos=[],  # å°‡é€šéé é¢è¨ªå•è£œé½Š
                                views_count=None  # å°‡é€šé fill_views_from_page è£œé½Š
                            )
                            
                            ordered_posts.append(post_metrics)
                            posts[post_id] = post_metrics  # åŒæ™‚åŠ å…¥å­—å…¸ä¾›å¾ŒçºŒè™•ç†
                            
                            # ç™¼å¸ƒé€²åº¦
                            await publish_progress(
                                task_id, 
                                "post_url_extracted",
                                username=username,
                                post_id=post_id,
                                current=len(ordered_posts),
                                total=max_posts,
                                progress=len(ordered_posts) / max_posts,
                                url=post_url
                            )
                    
                    # ä½¿ç”¨ ordered_posts è€Œä¸æ˜¯ posts.values() ä¾†ä¿æŒé †åº
                    logging.info(f"âœ… [Task: {task_id}] å‰µå»ºäº† {len(ordered_posts)} å€‹æœ‰åºçš„åŸºç¤PostMetrics")
                
                # é—œé–‰ page ä½†ä¿ç•™ context ä¾› fill_views_from_page ä½¿ç”¨
                await page.close()

                # --- æ•´ç†ä¸¦å›å‚³çµæœ ---
                # ä½¿ç”¨æœ‰åºçš„ posts åˆ—è¡¨æˆ–å›é€€åˆ°åŸå§‹æ–¹æ³•
                if 'ordered_posts' in locals():
                    final_posts = ordered_posts[:max_posts]  # ä¿æŒåŸå§‹é †åºï¼Œåªé™åˆ¶æ•¸é‡
                    total_found = len(ordered_posts)
                    logging.info(f"ğŸ¯ [Task: {task_id}] ä½¿ç”¨æœ‰åºè²¼æ–‡åˆ—è¡¨ï¼Œä¿æŒDOMæå–çš„æ™‚é–“é †åº")
                else:
                    # å›é€€åˆ°åŸå§‹æ–¹æ³•ï¼ˆGraphQLæ””æˆªçš„æƒ…æ³ï¼‰
                    final_posts = list(posts.values())
                total_found = len(final_posts)
                # æ ¹æ“š max_posts æˆªæ–·çµæœ
                if total_found > max_posts:
                    try:
                        final_posts.sort(key=lambda p: p.created_at or datetime.min, reverse=True)
                    except Exception:
                        pass 
                    final_posts = final_posts[:max_posts]
                    logging.info(f"ğŸ”„ [Task: {task_id}] ä½¿ç”¨GraphQLæ””æˆªæ–¹æ³•ï¼ŒæŒ‰created_atæ’åº")
                
                logging.info(f"ğŸ”„ [Task: {task_id}] æº–å‚™å›å‚³æœ€çµ‚è³‡æ–™ï¼šå…±ç™¼ç¾ {total_found} å‰‡è²¼æ–‡, å›å‚³ {len(final_posts)} å‰‡")
                
                # --- è£œé½Šè©³ç´°æ•¸æ“šå’Œè§€çœ‹æ•¸ (åœ¨ browser context é‚„å­˜åœ¨æ™‚åŸ·è¡Œ) ---
                # é…ç½®é¸é …ï¼šæ˜¯å¦å•Ÿç”¨è©³ç´°æ•¸æ“šè£œé½Šï¼ˆä¿®å¾©é»˜èªå€¼ï¼‰
                enable_details_filling = getattr(self.settings, 'enable_details_filling', True)
                
                if enable_details_filling:
                    # æ–°æ–¹æ³•ï¼šéœ€è¦è£œé½Šè©³ç´°æ•¸æ“š
                    logging.info(f"ğŸ” [Task: {task_id}] é–‹å§‹è£œé½Šè©³ç´°æ•¸æ“šï¼ˆlikes, contentç­‰ï¼‰...")
                    await publish_progress(task_id, "fill_details_start", username=username, posts_count=len(final_posts))
                    
                    try:
                        final_posts = await self.fill_post_details_from_page(final_posts, task_id=task_id, username=username)
                        logging.info(f"âœ… [Task: {task_id}] è©³ç´°æ•¸æ“šè£œé½Šå®Œæˆ")
                        await publish_progress(task_id, "fill_details_completed", username=username, posts_count=len(final_posts))
                    except Exception as e:
                        logging.warning(f"âš ï¸ [Task: {task_id}] è£œé½Šè©³ç´°æ•¸æ“šæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                        await publish_progress(task_id, "fill_details_error", username=username, error=str(e))
                else:
                    logging.info(f"âš ï¸ [Task: {task_id}] è©³ç´°æ•¸æ“šè£œé½Šå·²ç¦ç”¨ï¼Œå°‡åªè£œé½Šç€è¦½æ•¸")
                
                # è£œé½Šè§€çœ‹æ•¸ï¼ˆå…©ç¨®æ–¹æ³•éƒ½éœ€è¦ï¼‰
                logging.info(f"ğŸ” [Task: {task_id}] é–‹å§‹è£œé½Šè§€çœ‹æ•¸...")
                await publish_progress(task_id, "fill_views_start", username=username, posts_count=len(final_posts))
                
                try:
                    final_posts = await self.fill_views_from_page(final_posts, task_id=task_id, username=username)
                    logging.info(f"âœ… [Task: {task_id}] è§€çœ‹æ•¸è£œé½Šå®Œæˆ")
                    await publish_progress(task_id, "fill_views_completed", username=username, posts_count=len(final_posts))
                except Exception as e:
                    logging.warning(f"âš ï¸ [Task: {task_id}] è£œé½Šè§€çœ‹æ•¸æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                    await publish_progress(task_id, "fill_views_error", username=username, error=str(e))
                    # å³ä½¿è£œé½Šå¤±æ•—ï¼Œä¹Ÿç¹¼çºŒè¿”å›åŸºæœ¬æ•¸æ“š

                # æ‰‹å‹•é—œé–‰ browser å’Œ context
                await self.context.close()
                await browser.close()
                self.context = None
            
            # ä¿å­˜åŸå§‹æŠ“å–è³‡æ–™ä¾›èª¿è©¦
            try:
                raw_data = {
                    "task_id": task_id,
                    "username": username, 
                    "timestamp": datetime.now().isoformat(),
                    "total_found": total_found,
                    "returned_count": len(final_posts),
                    "posts": [
                        {
                            "url": post.url,
                            "post_id": post.post_id,
                            "likes_count": post.likes_count,
                            "comments_count": post.comments_count,
                            "reposts_count": post.reposts_count,
                            "shares_count": post.shares_count,
                            "views_count": post.views_count,
                            "calculated_score": post.calculate_score(),  # ğŸ†• ç§»åˆ° views_count ä¸‹é¢
                            "content": post.content,
                            "created_at": post.created_at.isoformat() if post.created_at else None,
                            "images": post.images,  # æ·»åŠ åœ–ç‰‡ URL
                            "videos": post.videos   # æ·»åŠ å½±ç‰‡ URL
                        } for post in final_posts
                    ]
                }
                
                # ä½¿ç”¨æ™‚é–“æˆ³è¨˜é¿å…æª”æ¡ˆè¡çª
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                raw_file = DEBUG_DIR / f"crawl_data_{timestamp}_{task_id[:8]}.json"
                raw_file.write_text(
                    json.dumps(raw_data, indent=2, ensure_ascii=False),
                    encoding="utf-8" # æ˜ç¢ºæŒ‡å®š UTF-8
                )
                logging.info(f"ğŸ’¾ [Task: {task_id}] å·²ä¿å­˜åŸå§‹æŠ“å–è³‡æ–™è‡³: {raw_file}")
                
            except Exception as e:
                logging.warning(f"âš ï¸ [Task: {task_id}] ä¿å­˜èª¿è©¦è³‡æ–™å¤±æ•—: {e}")
            
            # ç™¼å¸ƒå®Œæˆé€²åº¦
            await publish_progress(
                task_id, "completed", 
                username=username,
                total_posts=len(final_posts),
                success=True
            )

            batch = PostMetricsBatch(
                posts=final_posts,
                username=username,
                total_count=total_found,
                processing_stage="playwright_completed"
            )
            return batch
            
        except Exception as e:
            error_message = f"Playwright æ ¸å¿ƒé‚è¼¯å‡ºéŒ¯: {e}"
            logging.error(error_message, exc_info=True)
            # ç™¼å¸ƒéŒ¯èª¤é€²åº¦
            await publish_progress(
                task_id, "error", 
                username=username,
                error=error_message,
                success=False
            )
            raise
        
        finally:
            # æ¸…ç†è‡¨æ™‚èªè­‰æª”æ¡ˆ
            if auth_file.exists():
                auth_file.unlink()
                logging.info(f"ğŸ—‘ï¸ [Task: {task_id}] å·²åˆªé™¤è‡¨æ™‚èªè­‰æª”æ¡ˆ: {auth_file}")
            
            # ç¢ºä¿ context è¢«é‡ç½®ï¼ˆbrowser å·²åœ¨ä¸Šé¢æ‰‹å‹•é—œé–‰ï¼‰
            self.context = None 

    # +++ æ–°å¢ï¼šå¾å‰ç«¯è£œé½Šç€è¦½æ•¸çš„æ ¸å¿ƒæ–¹æ³•ï¼ˆæ•´åˆ Gate é é¢è™•ç†ï¼‰ +++
    async def fill_views_from_page(self, posts_to_fill: List[PostMetrics], task_id: str = None, username: str = None) -> List[PostMetrics]:
        """
        éæ­·è²¼æ–‡åˆ—è¡¨ï¼Œå°èˆªåˆ°æ¯å€‹è²¼æ–‡çš„é é¢ä»¥è£œé½Š views_countã€‚
        æ•´åˆäº†æˆåŠŸçš„ Gate é é¢è™•ç†å’Œé›™ç­–ç•¥æå–æ–¹æ³•ã€‚
        """
        if not self.context:
            logging.error("âŒ Browser context æœªåˆå§‹åŒ–ï¼Œç„¡æ³•åŸ·è¡Œ fill_views_from_pageã€‚")
            return posts_to_fill

        # æ¸›å°‘ä¸¦ç™¼æ•¸ä»¥é¿å…è§¸ç™¼åçˆ¬èŸ²æ©Ÿåˆ¶
        semaphore = asyncio.Semaphore(2)
        
        async def fetch_single_view(post: PostMetrics):
            async with semaphore:
                page = None
                try:
                    page = await self.context.new_page()
                    # ç¦ç”¨åœ–ç‰‡å’Œå½±ç‰‡è¼‰å…¥ä»¥åŠ é€Ÿ
                    await page.route("**/*.{png,jpg,jpeg,gif,mp4,webp}", lambda r: r.abort())
                    
                    logging.debug(f"ğŸ“„ æ­£åœ¨è™•ç†: {post.url}")
                    
                    # å°èˆªåˆ°è²¼æ–‡é é¢
                    await page.goto(post.url, wait_until="networkidle", timeout=30000)
                    
                    # æª¢æŸ¥é é¢é¡å‹ï¼ˆå®Œæ•´é é¢ vs Gate é é¢ï¼‰
                    page_content = await page.content()
                    is_gate_page = "__NEXT_DATA__" not in page_content
                    
                    if is_gate_page:
                        logging.debug(f"   âš ï¸ æª¢æ¸¬åˆ° Gate é é¢ï¼Œç›´æ¥ä½¿ç”¨ DOM é¸æ“‡å™¨...")
                    
                    views_count = None
                    extraction_method = None
                    
                    # ç­–ç•¥ 1: GraphQL æ””æˆªï¼ˆåªåœ¨é Gate é é¢æ™‚ï¼‰
                    if not is_gate_page:
                        try:
                            response = await page.wait_for_response(
                                lambda r: "containing_thread" in r.url and r.status == 200, 
                                timeout=8000
                            )
                            data = await response.json()
                            
                            # è§£æç€è¦½æ•¸
                            thread_items = data["data"]["containing_thread"]["thread_items"]
                            post_data = thread_items[0]["post"]
                            views_count = (post_data.get("feedback_info", {}).get("view_count") or
                                          post_data.get("video_info", {}).get("play_count") or 0)
                            
                            if views_count > 0:
                                extraction_method = "graphql_api"
                                logging.debug(f"   âœ… GraphQL API ç²å–ç€è¦½æ•¸: {views_count:,}")
                        except Exception as e:
                            logging.debug(f"   âš ï¸ GraphQL æ””æˆªå¤±æ•—: {str(e)[:100]}")
                    
                    # ç­–ç•¥ 2: DOM é¸æ“‡å™¨ï¼ˆGate é é¢çš„ä¸»è¦æ–¹æ³•ï¼‰
                    if views_count is None or views_count == 0:
                        selectors = [
                            "a:has-text(' æ¬¡ç€è¦½'), a:has-text(' views')",    # ä¸»è¦é¸æ“‡å™¨
                            "*:has-text('æ¬¡ç€è¦½'), *:has-text('views')",      # é€šç”¨é¸æ“‡å™¨
                            "span:has-text('æ¬¡ç€è¦½'), span:has-text('views')", # span å…ƒç´ 
                            "text=/\\d+.*æ¬¡ç€è¦½/, text=/\\d+.*views?/",       # æ­£å‰‡è¡¨é”å¼
                        ]
                        
                        for i, selector in enumerate(selectors):
                            try:
                                element = await page.wait_for_selector(selector, timeout=3000)
                                if element:
                                    view_text = await element.inner_text()
                                    parsed_views = parse_views_text(view_text)
                                    if parsed_views and parsed_views > 0:
                                        views_count = parsed_views
                                        extraction_method = f"dom_selector_{i+1}"
                                        logging.debug(f"   âœ… DOM é¸æ“‡å™¨ {i+1} ç²å–ç€è¦½æ•¸: {views_count:,}")
                                        break
                            except Exception:
                                continue
                    
                    # æ›´æ–°çµæœ
                    if views_count and views_count > 0:
                        post.views_count = views_count
                        post.views_fetched_at = datetime.utcnow()
                        logging.info(f"  âœ… æˆåŠŸç²å– {post.post_id} çš„ç€è¦½æ•¸: {views_count:,} (æ–¹æ³•: {extraction_method})")
                        
                        # ç™¼å¸ƒé€²åº¦
                        if task_id:
                            from common.nats_client import publish_progress
                            await publish_progress(
                                task_id, 
                                "views_fetched",
                                username=username or "unknown",
                                post_id=post.post_id,
                                views_count=views_count,
                                extraction_method=extraction_method,
                                is_gate_page=is_gate_page
                            )
                    else:
                        logging.warning(f"  âŒ ç„¡æ³•ç²å– {post.post_id} çš„ç€è¦½æ•¸")
                        post.views_count = -1
                        post.views_fetched_at = datetime.utcnow()
                    
                    # éš¨æ©Ÿå»¶é²é¿å…åçˆ¬èŸ²
                    delay = random.uniform(2, 4)
                    await asyncio.sleep(delay)
                    
                except Exception as e:
                    logging.error(f"  âŒ è™•ç† {post.post_id} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                    post.views_count = -1
                    post.views_fetched_at = datetime.utcnow()
                finally:
                    if page:
                        await page.close()

        # åºåˆ—è™•ç†é¿å…ä¸¦ç™¼å•é¡Œï¼ˆæ ¹æ“šæˆåŠŸç¶“é©—ï¼‰
        for post in posts_to_fill:
            await fetch_single_view(post)
        
        return posts_to_fill

    # +++ æ–°å¢ï¼šæ··åˆæå–å™¨ - çµåˆè¨ˆæ•¸æŸ¥è©¢å’ŒDOMè§£æ +++
    async def fill_post_details_from_page(self, posts_to_fill: List[PostMetrics], task_id: str = None, username: str = None) -> List[PostMetrics]:
        """
        ä½¿ç”¨æ··åˆç­–ç•¥è£œé½Šè²¼æ–‡è©³ç´°æ•¸æ“šï¼š
        1. GraphQL è¨ˆæ•¸æŸ¥è©¢ç²å–æº–ç¢ºçš„æ•¸å­—æ•¸æ“š (likes, commentsç­‰)
        2. DOM è§£æç²å–å®Œæ•´çš„å…§å®¹å’Œåª’é«” (content, images, videos)
        é€™ç¨®æ–¹æ³•çµåˆäº†å…©ç¨®æŠ€è¡“çš„å„ªå‹¢ï¼Œæä¾›æœ€ç©©å®šå¯é çš„æ•¸æ“šæå–ã€‚
        """
        if not self.context:
            logging.error("âŒ Browser context æœªåˆå§‹åŒ–ï¼Œç„¡æ³•åŸ·è¡Œ fill_post_details_from_pageã€‚")
            return posts_to_fill

        # æ¸›å°‘ä¸¦ç™¼æ•¸ä»¥é¿å…è§¸ç™¼åçˆ¬èŸ²æ©Ÿåˆ¶
        semaphore = asyncio.Semaphore(1)  # æ›´ä¿å®ˆçš„ä¸¦ç™¼æ•¸
        
        async def fetch_single_details_hybrid(post: PostMetrics):
            async with semaphore:
                page = None
                try:
                    page = await self.context.new_page()
                    
                    logging.debug(f"ğŸ“„ ä½¿ç”¨æ··åˆç­–ç•¥è£œé½Šè©³ç´°æ•¸æ“š: {post.url}")
                    
                    # === æ­¥é©Ÿ 1: æ””æˆªè¨ˆæ•¸æŸ¥è©¢ ===
                    counts_data = {}
                    video_urls = set()
                    
                    async def handle_counts_response(response):
                        try:
                            url = response.url.lower()
                            headers = response.request.headers
                            query_name = headers.get("x-fb-friendly-name", "")
                            
                            # æ””æˆªè¨ˆæ•¸æŸ¥è©¢
                            if ("/graphql" in url and response.status == 200 and 
                                "useBarcelonaBatchedDynamicPostCountsSubscriptionQuery" in query_name):
                                
                                data = await response.json()
                                if "data" in data and "data" in data["data"] and "posts" in data["data"]["data"]:
                                    posts_list = data["data"]["data"]["posts"]
                                    
                                    # æ‰¾åˆ°ç›®æ¨™è²¼æ–‡ (é€šé URL ä¸­çš„ä»£ç¢¼åŒ¹é…)
                                    import re as regex
                                    url_match = regex.search(r'/post/([^/?]+)', post.url)
                                    target_code = url_match.group(1) if url_match else None
                                    
                                    logging.debug(f"   ğŸ” å°‹æ‰¾ç›®æ¨™è²¼æ–‡ä»£ç¢¼: {target_code} (å¾ URL: {post.url})")
                                    logging.debug(f"   ğŸ“Š éŸ¿æ‡‰åŒ…å« {len(posts_list)} å€‹è²¼æ–‡")
                                    
                                    for post_data in posts_list:
                                        if isinstance(post_data, dict):
                                            post_code = post_data.get("code")
                                            logging.debug(f"   ğŸ“ æª¢æŸ¥è²¼æ–‡ä»£ç¢¼: {post_code}")
                                            if target_code and post_code == target_code:  # ç²¾ç¢ºåŒ¹é…
                                                counts_data.update({
                                                    "likes": post_data.get("like_count", 0),
                                                    "comments": post_data.get("text_post_app_info", {}).get("direct_reply_count", 0),
                                                    "reposts": post_data.get("text_post_app_info", {}).get("repost_count", 0),
                                                    "shares": post_data.get("text_post_app_info", {}).get("reshare_count", 0)
                                                })
                                                logging.debug(f"   âœ… æ””æˆªåˆ°è¨ˆæ•¸æ•¸æ“š: è®š={counts_data['likes']}")
                                                break
                            
                            # æ””æˆªå½±ç‰‡è³‡æº
                            content_type = response.headers.get("content-type", "")
                            resource_type = response.request.resource_type
                            if (resource_type == "media" or 
                                content_type.startswith("video/") or
                                ".mp4" in response.url.lower() or
                                ".m3u8" in response.url.lower() or
                                ".mpd" in response.url.lower()):
                                video_urls.add(response.url)
                                logging.debug(f"   ğŸ¥ æ””æˆªåˆ°å½±ç‰‡: {response.url[:60]}...")
                                
                        except Exception as e:
                            logging.debug(f"   âš ï¸ éŸ¿æ‡‰è™•ç†å¤±æ•—: {e}")
                    
                    page.on("response", handle_counts_response)
                    
                    # === æ­¥é©Ÿ 2: å°èˆªå’Œè§¸ç™¼è¼‰å…¥ ===
                    await page.goto(post.url, wait_until="networkidle", timeout=60000)
                    await asyncio.sleep(3)
                    
                    # å˜—è©¦è§¸ç™¼å½±ç‰‡è¼‰å…¥
                    try:
                        trigger_selectors = [
                            'div[data-testid="media-viewer"]',
                            'video',
                            'div[role="button"][aria-label*="play"]',
                            'div[role="button"][aria-label*="æ’­æ”¾"]',
                            '[data-pressable-container] div[style*="video"]'
                        ]
                        
                        for selector in trigger_selectors:
                            try:
                                elements = page.locator(selector)
                                count = await elements.count()
                                if count > 0:
                                    await elements.first.click(timeout=3000)
                                    await asyncio.sleep(2)
                                    break
                            except:
                                continue
                    except:
                        pass
                    
                    # === æ­¥é©Ÿ 3: DOM å…§å®¹æå– ===
                    content_data = {}
                    
                    try:
                        # æå–ç”¨æˆ¶åï¼ˆå¾ URLï¼‰
                        import re as regex
                        url_match = regex.search(r'/@([^/]+)/', post.url)
                        content_data["username"] = url_match.group(1) if url_match else username or ""
                        
                        # æå–å…§å®¹æ–‡å­—
                        content = ""
                        content_selectors = [
                            'div[data-pressable-container] span',
                            '[data-testid="thread-text"]',
                            'article div[dir="auto"]',
                            'div[role="article"] div[dir="auto"]'
                        ]
                        
                        for selector in content_selectors:
                            try:
                                elements = page.locator(selector)
                                count = await elements.count()
                                
                                for i in range(min(count, 20)):
                                    try:
                                        text = await elements.nth(i).inner_text()
                                        if (text and len(text.strip()) > 10 and 
                                            not text.strip().isdigit() and
                                            "å°æ™‚" not in text and "åˆ†é˜" not in text and
                                            not text.startswith("@")):
                                            content = text.strip()
                                            break
                                    except:
                                        continue
                                
                                if content:
                                    break
                            except:
                                continue
                        
                        content_data["content"] = content
                        
                        # æå–åœ–ç‰‡ï¼ˆéæ¿¾é ­åƒï¼‰
                        images = []
                        img_elements = page.locator('img')
                        img_count = await img_elements.count()
                        
                        for i in range(min(img_count, 50)):
                            try:
                                img_elem = img_elements.nth(i)
                                img_src = await img_elem.get_attribute("src")
                                
                                if not img_src or not ("fbcdn" in img_src or "cdninstagram" in img_src):
                                    continue
                                
                                if ("rsrc.php" in img_src or "static.cdninstagram.com" in img_src):
                                    continue
                                
                                # æª¢æŸ¥å°ºå¯¸éæ¿¾é ­åƒ
                                try:
                                    width = int(await img_elem.get_attribute("width") or 0)
                                    height = int(await img_elem.get_attribute("height") or 0)
                                    max_size = max(width, height)
                                    
                                    if max_size > 150 and img_src not in images:
                                        images.append(img_src)
                                except:
                                    if ("t51.2885-15" in img_src or "scontent" in img_src) and img_src not in images:
                                        images.append(img_src)
                            except:
                                continue
                        
                        content_data["images"] = images
                        
                        # æå–å½±ç‰‡ï¼ˆçµåˆç¶²è·¯æ””æˆªå’ŒDOMï¼‰
                        videos = list(video_urls)
                        
                        # DOM ä¸­çš„ video æ¨™ç±¤
                        video_elements = page.locator('video')
                        video_count = await video_elements.count()
                        
                        for i in range(video_count):
                            try:
                                video_elem = video_elements.nth(i)
                                src = await video_elem.get_attribute("src")
                                data_src = await video_elem.get_attribute("data-src")
                                poster = await video_elem.get_attribute("poster")
                                
                                if src and src not in videos:
                                    videos.append(src)
                                if data_src and data_src not in videos:
                                    videos.append(data_src)
                                if poster and poster not in videos:
                                    videos.append(f"POSTER::{poster}")
                                
                                # source å­å…ƒç´ 
                                sources = video_elem.locator('source')
                                source_count = await sources.count()
                                for j in range(source_count):
                                    source_src = await sources.nth(j).get_attribute("src")
                                    if source_src and source_src not in videos:
                                        videos.append(source_src)
                            except:
                                continue
                        
                        content_data["videos"] = videos
                        
                    except Exception as e:
                        logging.debug(f"   âš ï¸ DOM å…§å®¹æå–å¤±æ•—: {e}")
                    
                    # === æ­¥é©Ÿ 4: æ›´æ–°è²¼æ–‡æ•¸æ“š ===
                    updated = False
                    
                    # æ›´æ–°è¨ˆæ•¸æ•¸æ“š
                    if counts_data:
                        post.likes_count = counts_data.get("likes", post.likes_count)
                        post.comments_count = counts_data.get("comments", post.comments_count)
                        post.reposts_count = counts_data.get("reposts", post.reposts_count)
                        post.shares_count = counts_data.get("shares", post.shares_count)
                        updated = True
                    
                    # æ›´æ–°å…§å®¹æ•¸æ“š
                    if content_data.get("content"):
                        post.content = content_data["content"]
                        updated = True
                    
                    if content_data.get("images"):
                        post.images = content_data["images"]
                        updated = True
                    
                    if content_data.get("videos"):
                        # éæ¿¾å¯¦éš›å½±ç‰‡ï¼ˆæ’é™¤ POSTERï¼‰
                        actual_videos = [v for v in content_data["videos"] if not v.startswith("POSTER::")]
                        if actual_videos:
                            post.videos = actual_videos
                            updated = True
                    
                    if updated:
                        post.processing_stage = "details_filled_hybrid"
                        logging.info(f"  âœ… æ··åˆç­–ç•¥æˆåŠŸè£œé½Š {post.post_id}: è®š={post.likes_count}, å…§å®¹={len(post.content)}å­—, åœ–ç‰‡={len(post.images)}å€‹, å½±ç‰‡={len(post.videos)}å€‹")
                        
                        # ç™¼å¸ƒé€²åº¦
                        if task_id:
                            from common.nats_client import publish_progress
                            await publish_progress(
                                task_id, 
                                "details_fetched_hybrid",
                                username=content_data.get("username", username or "unknown"),
                                post_id=post.post_id,
                                likes_count=post.likes_count,
                                content_length=len(post.content),
                                media_count=len(post.images) + len(post.videos)
                            )
                    else:
                        post.processing_stage = "details_failed"
                        logging.warning(f"  âš ï¸ æ··åˆç­–ç•¥ç„¡æ³•è£œé½Š {post.post_id} çš„æ•¸æ“š")
                    
                    # éš¨æ©Ÿå»¶é²é¿å…åçˆ¬èŸ²
                    delay = random.uniform(2, 4)
                    await asyncio.sleep(delay)
                    
                except Exception as e:
                    logging.error(f"  âŒ æ··åˆç­–ç•¥è™•ç† {post.post_id} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                    post.processing_stage = "details_failed"
                finally:
                    if page:
                        await page.close()

        # åºåˆ—è™•ç†ä¿æŒé †åº
        for post in posts_to_fill:
            await fetch_single_details_hybrid(post)
        
        return posts_to_fill


