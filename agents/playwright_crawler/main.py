import uuid
import asyncio
from contextlib import asynccontextmanager
from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field
from typing import Dict, Any, Optional, List
import json
import datetime
from pathlib import Path
import logging

from .playwright_logic import PlaywrightLogic
from common.models import PostMetricsBatch
from common.a2a import stream_error, TaskState
from common.mcp_client import agent_startup, agent_shutdown, get_mcp_client
from common.settings import get_settings
from common.history import CrawlHistoryDAO

@asynccontextmanager
async def lifespan(app: FastAPI):
    """æ‡‰ç”¨ç”Ÿå‘½é€±æœŸç®¡ç†"""
    # å•Ÿå‹•æ™‚è¨»å†Šåˆ° MCP Server
    settings = get_settings()
    
    capabilities = {
        "browser_automation": True,
        "dynamic_content": True,
        "threads_scraping": True,
        "auth_handling": True
    }
    
    metadata = {
        "version": "1.0.0",
        "author": "AI Assistant",
        "max_concurrent_crawls": 3,
        "supported_platforms": ["threads"],
        "requires_auth": True
    }
    
    success = await agent_startup(capabilities, metadata)
    if success:
        print("âœ… Playwright Crawler Agent registered to MCP Server")
    else:
        print("âŒ Failed to register Playwright Crawler Agent to MCP Server")
    
    yield
    
    # é—œé–‰æ™‚æ¸…ç†
    await agent_shutdown()
    print("ğŸ›‘ Playwright Crawler Agent shutdown completed")


app = FastAPI(
    title="Playwright Crawler Agent",
    version="1.0.0",
    description="ä½¿ç”¨ Playwright å’Œä½¿ç”¨è€…æä¾›çš„èªè­‰ç‹€æ…‹ä¾†çˆ¬å– Threads è²¼æ–‡ã€‚",
    lifespan=lifespan
)

def json_serializer(obj):
    """JSON serializer for objects not serializable by default json code"""
    if isinstance(obj, (datetime.datetime, datetime.date)):
        return obj.isoformat()
    raise TypeError ("Type %s not serializable" % type(obj))

class CrawlRequest(BaseModel):
    username: str = Field(..., description="è¦çˆ¬å–çš„ Threads ä½¿ç”¨è€…åç¨± (ä¸å« @)")
    max_posts: int = Field(default=100, gt=0, le=500, description="è¦çˆ¬å–çš„æœ€å¤§è²¼æ–‡æ•¸é‡")
    auth_json_content: Dict[str, Any] = Field(..., description="åŒ…å«èªè­‰ cookies å’Œç‹€æ…‹çš„ auth.json å…§å®¹")
    task_id: Optional[str] = Field(default=None, description="ä»»å‹™ IDï¼Œç”¨æ–¼é€²åº¦è¿½è¹¤")

class URLStatusItem(BaseModel):
    """å–®å€‹URLçš„ç‹€æ…‹ä¿¡æ¯"""
    url: str
    post_id: str
    reader_status: str = "pending"  # pending/success/failed
    dom_status: str = "pending"     # pending/success/failed
    reader_processed_at: Optional[datetime.datetime] = None
    dom_processed_at: Optional[datetime.datetime] = None
    needs_reader: bool = True
    needs_dom: bool = True
    has_content: bool = False
    has_metrics: bool = False
    has_media: bool = False

class URLStatusResponse(BaseModel):
    """URLç‹€æ…‹æŸ¥è©¢éŸ¿æ‡‰"""
    username: str
    urls: List[URLStatusItem]
    summary: Dict[str, Any]

# --- Agent Instance ---
playwright_logic = PlaywrightLogic()

# --- API Endpoints ---
@app.post("/v1/playwright/crawl", response_model=PostMetricsBatch, tags=["Plan F"])
async def crawl_and_get_batch(request: CrawlRequest):
    """
    æ¥æ”¶çˆ¬å–è«‹æ±‚ï¼ŒåŸ·è¡Œ Playwright çˆ¬èŸ²ï¼Œä¸¦ä¸€æ¬¡æ€§è¿”å›å®Œæ•´çš„ PostMetricsBatchã€‚
    èªè­‰å…§å®¹ç”±è«‹æ±‚æ–¹åœ¨ request body ä¸­æä¾›ã€‚
    """
    # ä½¿ç”¨å‚³å…¥çš„ task_idï¼Œå¦‚æœæ²’æœ‰å‰‡ç”Ÿæˆæ–°çš„
    task_id = request.task_id or str(uuid.uuid4())
    logic = PlaywrightLogic()

    try:
        batch = await logic.fetch_posts(
            username=request.username,
            extra_posts=request.max_posts,  # å‘å¾Œå…¼å®¹ï¼šmax_postsä½œç‚ºextra_postså‚³å…¥
            auth_json_content=request.auth_json_content, # ä½¿ç”¨å‚³å…¥çš„èªè­‰å…§å®¹
            task_id=task_id
        )
        return batch
    except Exception as e:
        logging.error(f"Crawling failed in main: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/urls/{username}", response_model=URLStatusResponse, tags=["URL Status"])
async def get_user_urls_status(
    username: str, 
    max_posts: int = 50,
    auth_json_content: Optional[Dict[str, Any]] = None
):
    """
    ç²å–ç”¨æˆ¶è²¼æ–‡URLsåŠå…¶è™•ç†ç‹€æ…‹
    åˆä½µURLæ”¶é›†å’Œç‹€æ…‹æŸ¥è©¢ï¼Œç”¨æ–¼å‰ç«¯UIé¡¯ç¤º
    """
    try:
        # åˆå§‹åŒ–æ­·å²è¨˜éŒ„DAO
        history = CrawlHistoryDAO()
        
        # ç¬¬1æ­¥ï¼šæ”¶é›†URLs (ä½¿ç”¨ç¾æœ‰çš„URLæ”¶é›†é‚è¼¯)
        logic = PlaywrightLogic()
        
        # æš«æ™‚ç°¡åŒ–ï¼šåªè¿”å›æ•¸æ“šåº«ä¸­å·²æœ‰çš„URLsç‹€æ…‹
        # TODO: å¾ŒçºŒå¯ä»¥æ·»åŠ å¯¦æ™‚URLæ”¶é›†åŠŸèƒ½
        collected_urls = []
        logging.info(f"ğŸ“‹ ç•¶å‰åƒ…é¡¯ç¤ºæ•¸æ“šåº«ä¸­å·²æœ‰çš„ {username} è²¼æ–‡ç‹€æ…‹")
        
        # ç¬¬2æ­¥ï¼šç²å–å·²å­˜åœ¨çš„ç‹€æ…‹ä¿¡æ¯
        existing_status = await history.get_posts_status(username)
        status_map = {item['url']: item for item in existing_status}
        
        # ç¬¬3æ­¥ï¼šåŸºæ–¼æ•¸æ“šåº«ç‹€æ…‹æ§‹å»ºéŸ¿æ‡‰
        url_status_list = []
        
        # åŸºæ–¼æ•¸æ“šåº«ä¸­å·²æœ‰çš„è²¼æ–‡æ§‹å»ºç‹€æ…‹åˆ—è¡¨
        for db_item in existing_status[:max_posts]:  # é™åˆ¶è¿”å›æ•¸é‡
            url = db_item['url']
            post_id = db_item['post_id']
            
            url_status_list.append(URLStatusItem(
                url=url,
                post_id=post_id,
                reader_status=db_item['reader_status'],
                dom_status=db_item['dom_status'],
                reader_processed_at=db_item['reader_processed_at'],
                dom_processed_at=db_item['dom_processed_at'],
                needs_reader=db_item['reader_status'] != 'success',
                needs_dom=db_item['dom_status'] != 'success',
                has_content=db_item['has_content'],
                has_metrics=db_item['has_metrics'],
                has_media=db_item['has_media']
            ))
        
        # ç¬¬4æ­¥ï¼šç²å–çµ±è¨ˆæ‘˜è¦
        summary = await history.get_processing_needs(username)
        summary['displayed_posts'] = len(url_status_list)
        summary['database_posts'] = len(existing_status)
        
        return URLStatusResponse(
            username=username,
            urls=url_status_list,
            summary=summary
        )
        
    except Exception as e:
        logging.error(f"âŒ ç²å– {username} URLç‹€æ…‹å¤±æ•—: {e}")
        raise HTTPException(status_code=500, detail=f"URLç‹€æ…‹æŸ¥è©¢å¤±æ•—: {str(e)}")

@app.get("/health", tags=["Monitoring"])
async def health_check():
    """
    åŸ·è¡Œå¥åº·æª¢æŸ¥ã€‚
    æœªä¾†å¯ä»¥æ“´å……æ­¤æª¢æŸ¥ä»¥é©—è­‰ Playwright ç’°å¢ƒæ˜¯å¦æ­£å¸¸ã€‚
    """
    return {"status": "healthy", "service": "Playwright Crawler Agent"}

# MCP æ•´åˆç«¯é»
@app.get("/mcp/capabilities", tags=["MCP"])
async def get_capabilities():
    """ç²å– Agent èƒ½åŠ›"""
    return {
        "browser_automation": True,
        "dynamic_content": True,
        "threads_scraping": True,
        "auth_handling": True,
        "max_concurrent": 3,
        "supported_formats": ["PostMetricsBatch"]
    }

@app.get("/mcp/discover", tags=["MCP"])
async def discover_other_agents():
    """ç™¼ç¾å…¶ä»– Agent"""
    try:
        mcp_client = get_mcp_client()
        
        # ç™¼ç¾ vision agent ç”¨æ–¼å¾ŒçºŒè™•ç†
        vision_agents = await mcp_client.discover_agents(role="vision", status="ONLINE")
        analysis_agents = await mcp_client.discover_agents(role="analysis", status="ONLINE")
        
        return {
            "vision_agents": vision_agents,
            "analysis_agents": analysis_agents,
            "total_discovered": len(vision_agents) + len(analysis_agents)
        }
    except Exception as e:
        logging.error(f"Failed to discover agents: {e}")
        return {"error": str(e), "vision_agents": [], "analysis_agents": []}

@app.post("/mcp/request-media-download", tags=["MCP"])
async def request_media_download(
    post_url: str,
    media_urls: list[str],
    background_tasks: BackgroundTasks
):
    """è«‹æ±‚ MCP Server ä¸‹è¼‰åª’é«”æª”æ¡ˆ"""
    try:
        mcp_client = get_mcp_client()
        result = await mcp_client.download_media(post_url, media_urls)
        
        logging.info(f"Media download requested for {post_url}: {len(media_urls)} files")
        return result
        
    except Exception as e:
        logging.error(f"Failed to request media download: {e}")
        raise HTTPException(status_code=500, detail=str(e)) 