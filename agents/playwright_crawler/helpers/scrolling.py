"""
æ»¾å‹•å’Œè²¼æ–‡å®šä½è¼”åŠ©å‡½å¼

å¯¦ç¾æ™ºèƒ½æ»¾å‹•é‚è¼¯ï¼Œæ”¯æŒ NEW-POST å’Œ HIST-BACKFILL å…©ç¨®æ¨¡å¼
"""

import asyncio
import logging
import random
from typing import List, Tuple, Set
from playwright.async_api import Page


async def extract_current_post_ids(page: Page) -> List[str]:
    """
    æå–ç•¶å‰è¦–çª—å…§çš„è²¼æ–‡IDsï¼Œä¿æŒDOMé †åº
    
    Returns:
        æŒ‰DOMå‡ºç¾é †åºçš„post_idåˆ—è¡¨ (ä¸å«usernameå‰ç¶´)
    """
    try:
        post_ids = await page.evaluate("""
            () => {
                const links = Array.from(document.querySelectorAll('a[href*="/post/"]'));
                const ids = [];
                const seen = new Set();
                
                for (const link of links) {
                    const match = link.href.match(/\/post\/([^\/\?]+)/);
                    if (match && !seen.has(match[1])) {
                        seen.add(match[1]);
                        ids.push(match[1]);
                    }
                }
                
                return ids;
            }
        """)
        
        logging.debug(f"ğŸ“‹ æå–åˆ° {len(post_ids)} å€‹è²¼æ–‡ID")
        return post_ids
        
    except Exception as e:
        logging.warning(f"âš ï¸ æå–è²¼æ–‡IDå¤±æ•—: {e}")
        return []


async def check_page_bottom(page: Page) -> bool:
    """
    æª¢æŸ¥æ˜¯å¦å·²æ»¾å‹•åˆ°é é¢åº•éƒ¨
    
    Returns:
        True å¦‚æœå·²åˆ°åº•éƒ¨
    """
    try:
        is_bottom = await page.evaluate("""
            () => {
                const threshold = 100;  // å®¹å¿100pxèª¤å·®
                return (window.scrollY + window.innerHeight) >= (document.body.scrollHeight - threshold);
            }
        """)
        
        if is_bottom:
            logging.debug("ğŸ“„ å·²åˆ°é”é é¢åº•éƒ¨")
        
        return is_bottom
        
    except Exception as e:
        logging.warning(f"âš ï¸ æª¢æŸ¥é é¢åº•éƒ¨å¤±æ•—: {e}")
        return False


async def scroll_once(page: Page, delta: int = 1000) -> None:
    """
    åŸ·è¡Œä¸€æ¬¡æ™ºèƒ½æ»¾å‹•
    
    Args:
        page: Playwrighté é¢å°è±¡
        delta: æ»¾å‹•è·é›¢ (åƒç´ )
    """
    try:
        # éš¨æ©ŸåŒ–æ»¾å‹•è·é›¢ Â±20%
        actual_delta = int(delta * random.uniform(0.8, 1.2))
        
        await page.mouse.wheel(0, actual_delta)
        
        # éš¨æ©ŸåŒ–ç­‰å¾…æ™‚é–“
        sleep_time = random.uniform(0.6, 1.2)
        await asyncio.sleep(sleep_time)
        
        logging.debug(f"ğŸ”„ æ»¾å‹• {actual_delta}pxï¼Œç­‰å¾… {sleep_time:.2f}s")
        
    except Exception as e:
        logging.warning(f"âš ï¸ æ»¾å‹•å¤±æ•—: {e}")


async def enhanced_scroll_with_strategy(page: Page, scroll_round: int) -> None:
    """
    å¢å¼·çš„äººæ€§åŒ–æ»¾å‹•ç­–ç•¥ - æ¡ç”¨Realtime Crawlerå„ªç§€ç­–ç•¥
    
    Args:
        page: Playwrighté é¢å°è±¡
        scroll_round: ç•¶å‰æ»¾å‹•è¼ªæ¬¡
    """
    try:
        if scroll_round % 6 == 5:  # æ¯6è¼ªé€²è¡Œä¸€æ¬¡æ¿€é€²æ»¾å‹•
            logging.debug("   ğŸš€ åŸ·è¡Œæ¿€é€²æ»¾å‹•æ¿€ç™¼è¼‰å…¥...")
            # æ¨¡æ“¬ç”¨æˆ¶å¿«é€Ÿæ»¾å‹•è¡Œç‚º
            await page.mouse.wheel(0, 1600)
            await asyncio.sleep(1.2)
            # ç¨å¾®å›æ»¾ï¼ˆåƒç”¨æˆ¶æ»¾éé ­äº†ï¼‰
            await page.mouse.wheel(0, -250)
            await asyncio.sleep(0.8)
            # å†ç¹¼çºŒå‘ä¸‹
            await page.mouse.wheel(0, 1400)
            await asyncio.sleep(3.5)
            
        elif scroll_round % 3 == 2:  # æ¯3è¼ªé€²è¡Œä¸€æ¬¡ä¸­åº¦æ»¾å‹•
            logging.debug("   ğŸ”„ åŸ·è¡Œä¸­åº¦æ»¾å‹•...")
            # åˆ†æ®µæ»¾å‹•ï¼Œæ›´åƒäººé¡è¡Œç‚º
            await page.mouse.wheel(0, 800)
            await asyncio.sleep(1)
            await page.mouse.wheel(0, 600)
            await asyncio.sleep(2.8)
            
        else:
            # æ­£å¸¸æ»¾å‹•ï¼ŒåŠ å…¥éš¨æ©Ÿæ€§å’Œäººæ€§åŒ–
            scroll_distance = 900 + (scroll_round % 3) * 100  # 900-1100pxéš¨æ©Ÿ
            await page.mouse.wheel(0, scroll_distance)
            
            # çŸ­æš«æš«åœï¼ˆæ¨¡æ“¬ç”¨æˆ¶é–±è®€ï¼‰
            await asyncio.sleep(1.8 + (scroll_round % 2) * 0.4)  # 1.8-2.2ç§’éš¨æ©Ÿ
        
        # çµ±ä¸€çš„è¼‰å…¥æª¢æ¸¬ï¼ˆæ‰€æœ‰æ»¾å‹•å¾Œéƒ½æª¢æŸ¥ï¼‰
        await wait_for_content_loading(page)
        
    except Exception as e:
        logging.warning(f"âš ï¸ å¢å¼·æ»¾å‹•å¤±æ•—: {e}")


async def wait_for_content_loading(page: Page) -> None:
    """
    ç­‰å¾…å…§å®¹è¼‰å…¥å®Œæˆ - æª¢æ¸¬è¼‰å…¥æŒ‡ç¤ºå™¨ï¼ˆå¢å¼·ç‰ˆï¼‰
    """
    try:
        has_loading = await page.evaluate("""
            () => {
                const indicators = document.querySelectorAll(
                    '[role="progressbar"], .loading, [aria-label*="loading"], [aria-label*="Loading"]'
                );
                return indicators.length > 0;
            }
        """)
        
        if has_loading:
            logging.debug("   â³ æª¢æ¸¬åˆ°è¼‰å…¥æŒ‡ç¤ºå™¨ï¼Œé¡å¤–ç­‰å¾…...")
            # éš¨æ©Ÿç­‰å¾…2-3.5ç§’
            loading_wait = random.uniform(2.0, 3.5)
            await asyncio.sleep(loading_wait)
        else:
            # å³ä½¿æ²’æœ‰è¼‰å…¥æŒ‡ç¤ºå™¨ï¼Œä¹Ÿçµ¦äºˆåŸºæœ¬ç­‰å¾…æ™‚é–“
            await asyncio.sleep(0.5)
            
    except Exception as e:
        logging.warning(f"âš ï¸ è¼‰å…¥æª¢æ¸¬å¤±æ•—: {e}")


async def final_attempt_scroll(page: Page) -> int:
    """
    æœ€å¾Œå˜—è©¦æ©Ÿåˆ¶ï¼šå¤šé‡æ¿€é€²æ»¾å‹•æ¿€ç™¼æ–°å…§å®¹è¼‰å…¥
    æ¡ç”¨ realtime_crawler çš„ç­–ç•¥
    
    Returns:
        int: ç™¼ç¾çš„æ–°URLæ•¸é‡ï¼ˆæ¦‚å¿µæ€§ï¼Œå¯¦éš›ç”±èª¿ç”¨æ–¹æª¢æŸ¥ï¼‰
    """
    try:
        logging.info("   ğŸš€ æœ€å¾Œå˜—è©¦ï¼šå¤šé‡æ¿€é€²æ»¾å‹•æ¿€ç™¼æ–°å…§å®¹...")
        
        # ç¬¬ä¸€æ¬¡ï¼šå¤§å¹…å‘ä¸‹
        await page.mouse.wheel(0, 2500)
        await asyncio.sleep(2)
        
        # ç¬¬äºŒæ¬¡ï¼šå‘ä¸Šå†å‘ä¸‹ï¼ˆæ¿€ç™¼è¼‰å…¥ï¼‰
        await page.mouse.wheel(0, -500)
        await asyncio.sleep(1)
        await page.mouse.wheel(0, 3000)
        await asyncio.sleep(3)
        
        # ç¬¬ä¸‰æ¬¡ï¼šæ»¾å‹•åˆ°æ›´åº•éƒ¨
        await page.mouse.wheel(0, 2000)
        await asyncio.sleep(2)
        
        logging.info("   â³ ç­‰å¾…æ‰€æœ‰å…§å®¹è¼‰å…¥å®Œæˆ...")
        await asyncio.sleep(3)
        
        # å¼·åˆ¶ç­‰å¾…è¼‰å…¥
        await wait_for_content_loading(page)
        
        return 1  # è¡¨ç¤ºå·²åŸ·è¡Œæœ€å¾Œå˜—è©¦
        
    except Exception as e:
        logging.warning(f"âš ï¸ æœ€å¾Œå˜—è©¦æ»¾å‹•å¤±æ•—: {e}")
        return 0


async def progressive_wait(no_new_content_rounds: int) -> None:
    """
    éå¢ç­‰å¾…æ™‚é–“ç­–ç•¥
    æ¡ç”¨ realtime_crawler çš„ç­–ç•¥
    """
    try:
        # åŠ å…¥éš¨æ©Ÿæ€§ï¼Œé™åˆ¶æœ€å¤§3.5ç§’
        base_wait = min(1.2 + (no_new_content_rounds - 1) * 0.3, 3.5)  # 1.2s -> 3.5s
        random_factor = random.uniform(0.8, 1.2)  # Â±20%éš¨æ©Ÿè®ŠåŒ–
        progressive_wait_time = base_wait * random_factor
        
        logging.debug(f"   â²ï¸ éå¢ç­‰å¾… {progressive_wait_time:.1f}s...")
        await asyncio.sleep(progressive_wait_time)
        
    except Exception as e:
        logging.warning(f"âš ï¸ éå¢ç­‰å¾…å¤±æ•—: {e}")


def should_stop_incremental_mode(
    found_existing_this_round: bool,
    consecutive_existing_rounds: int,
    collected_count: int,
    target_count: int,
    max_consecutive_existing: int = 15
) -> bool:
    """
    å¢é‡æ¨¡å¼çš„æ™ºèƒ½åœæ­¢æ¢ä»¶ï¼ˆä¿®å¾©ç‰ˆï¼‰
    åªæœ‰åœ¨æ”¶é›†åˆ°è¶³å¤ æ•¸é‡æ™‚æ‰åœæ­¢ï¼Œç™¼ç¾å·²å­˜åœ¨è²¼æ–‡ä¸æ‡‰è©²åœæ­¢
    
    Args:
        found_existing_this_round: æœ¬è¼ªæ˜¯å¦ç™¼ç¾å·²å­˜åœ¨è²¼æ–‡
        consecutive_existing_rounds: é€£çºŒç™¼ç¾å·²å­˜åœ¨è²¼æ–‡çš„è¼ªæ¬¡  
        collected_count: å·²æ”¶é›†çš„è²¼æ–‡æ•¸é‡
        target_count: ç›®æ¨™æ•¸é‡
        max_consecutive_existing: æœ€å¤§é€£çºŒå·²å­˜åœ¨è¼ªæ¬¡ï¼ˆæ­¤åƒæ•¸å·²å»¢æ£„ï¼‰
        
    Returns:
        True å¦‚æœæ‡‰è©²åœæ­¢
    """
    # åªæœ‰åœ¨æ”¶é›†åˆ°è¶³å¤ æ•¸é‡æ™‚æ‰åœæ­¢
    if collected_count >= target_count:
        logging.info(f"   âœ… å¢é‡æª¢æ¸¬: å·²æ”¶é›†è¶³å¤ æ–°è²¼æ–‡ ({collected_count} å€‹)")
        return True
    
    # ç™¼ç¾å·²å­˜åœ¨è²¼æ–‡æ˜¯æ­£å¸¸çš„ï¼Œç‰¹åˆ¥æ˜¯åœ¨å°‹æ‰¾é å¤è²¼æ–‡æ™‚ï¼Œä¸æ‡‰è©²åœæ­¢
    if found_existing_this_round:
        logging.debug(f"   ğŸ” å¢é‡æª¢æ¸¬: ç™¼ç¾å·²å­˜åœ¨è²¼æ–‡ï¼Œç¹¼çºŒå°‹æ‰¾æ›´èˆŠçš„å…§å®¹... ({collected_count}/{target_count})")
    
    return False


def is_anchor_visible(post_ids: List[str], anchor: str) -> Tuple[bool, int]:
    """
    æª¢æŸ¥éŒ¨é»æ˜¯å¦åœ¨ç•¶å‰è²¼æ–‡åˆ—è¡¨ä¸­
    
    Args:
        post_ids: ç•¶å‰è¦–çª—çš„è²¼æ–‡IDåˆ—è¡¨
        anchor: è¦æŸ¥æ‰¾çš„éŒ¨é»è²¼æ–‡ID (ä¸å«usernameå‰ç¶´)
        
    Returns:
        (æ˜¯å¦æ‰¾åˆ°, åœ¨åˆ—è¡¨ä¸­çš„ç´¢å¼•ä½ç½®) 
        æœªæ‰¾åˆ°æ™‚ç´¢å¼•ç‚º-1
    """
    if not anchor:
        return False, -1
        
    # ç§»é™¤å¯èƒ½çš„usernameå‰ç¶´
    anchor_clean = anchor.split('_')[-1] if '_' in anchor else anchor
    
    for i, post_id in enumerate(post_ids):
        if post_id == anchor_clean:
            logging.debug(f"ğŸ¯ æ‰¾åˆ°éŒ¨é» {anchor_clean} åœ¨ä½ç½® {i}/{len(post_ids)}")
            return True, i
            
    return False, -1


async def collect_urls_from_dom(page: Page, existing_set: Set[str], target_username: str = None) -> List[str]:
    """
    å¾DOMæ”¶é›†æ–°çš„è²¼æ–‡URLsï¼Œéæ¿¾å·²å­˜åœ¨çš„
    
    Args:
        page: Playwrighté é¢å°è±¡
        existing_set: å·²å­˜åœ¨çš„è²¼æ–‡IDé›†åˆ (ç”¨æ–¼å»é‡)
        
    Returns:
        æ–°ç™¼ç¾çš„è²¼æ–‡URLsåˆ—è¡¨
    """
    try:
        new_urls = await page.evaluate("""
            (existingIds, targetUsername) => {
                // ä½¿ç”¨èˆ‡åŸå§‹url_extractorå®Œå…¨ç›¸åŒçš„é‚è¼¯
                function normalizePostUrl(url) {
                    const match = url.match(/https:\\/\\/www\\.threads\\.com\\/@([^\\/]+)\\/post\\/([^\\/\\?]+)/);
                    if (match) {
                        const username = match[1];
                        const postId = match[2];
                        return `https://www.threads.com/@${username}/post/${postId}`;
                    }
                    return url;
                }
                
                // ç²å–æ‰€æœ‰è²¼æ–‡é€£çµï¼Œä¿æŒDOMä¸­çš„åŸå§‹é †åºï¼ˆèˆ‡url_extractorç›¸åŒï¼‰
                const links = Array.from(document.querySelectorAll('a[href*="/post/"]'));
                const urls = [];
                const seen = new Set();
                
                // éæ­·æ™‚ä¿æŒé †åºï¼Œæ¨™æº–åŒ–URLä¸¦å»é‡ï¼ˆèˆ‡url_extractorç›¸åŒï¼‰
                for (const link of links) {
                    const originalUrl = link.href;
                    const normalizedUrl = normalizePostUrl(originalUrl);
                    
                    if (originalUrl.includes('/post/') && !seen.has(normalizedUrl)) {
                        // æå–URLä¸­çš„å¯¦éš›ç”¨æˆ¶å
                        const match = normalizedUrl.match(/https:\/\/www\.threads\.com\/@([^\/]+)\/post\/([^\/\?]+)/);
                        if (match) {
                            const urlUsername = match[1];
                            const postId = match[2];
                            
                            // åªæ”¶é›†ç›®æ¨™ç”¨æˆ¶çš„è²¼æ–‡ï¼ˆéæ¿¾è½‰è²¼ï¼‰
                            if (!targetUsername || urlUsername === targetUsername) {
                                const fullId = `${urlUsername}_${postId}`;
                                
                                if (!existingIds.includes(fullId)) {
                                    seen.add(normalizedUrl);
                                    urls.push(normalizedUrl);
                                }
                            }
                        }
                    }
                }
                
                return urls;
            }
        """, list(existing_set), target_username)
        
        logging.debug(f"ğŸ”— æ”¶é›†åˆ° {len(new_urls)} å€‹æ–°URLs (ç›®æ¨™ç”¨æˆ¶: {target_username})")
        return new_urls
        
    except Exception as e:
        logging.warning(f"âš ï¸ æ”¶é›†URLså¤±æ•—: {e}")
        return []


def should_stop_new_mode(found_anchor: bool, collected: List[str], target_count: int) -> bool:
    """
    åˆ¤æ–·NEWæ¨¡å¼æ˜¯å¦æ‡‰è©²åœæ­¢æ»¾å‹•
    
    Args:
        found_anchor: æ˜¯å¦å·²æ‰¾åˆ°éŒ¨é»
        collected: å·²æ”¶é›†çš„è²¼æ–‡åˆ—è¡¨
        target_count: ç›®æ¨™æ”¶é›†æ•¸é‡
        
    Returns:
        True å¦‚æœæ‡‰è©²åœæ­¢
    """
    # æ¢ä»¶1: å·²é”åˆ°ç›®æ¨™æ•¸é‡
    if len(collected) >= target_count:
        logging.debug(f"âœ… NEWæ¨¡å¼é”æ¨™: {len(collected)}/{target_count}")
        return True
        
    # æ¢ä»¶2: æ‰¾åˆ°éŒ¨é» (æ„å‘³è‘—ä¸å†æœ‰æ–°è²¼æ–‡)
    if found_anchor:
        logging.debug(f"ğŸ¯ NEWæ¨¡å¼é‡åˆ°éŒ¨é»ï¼Œåœæ­¢æ»¾å‹•: {len(collected)}/{target_count}")
        return True
        
    return False


def should_stop_hist_mode(
    found_anchor: bool, 
    passed_anchor: bool,
    collected: List[str], 
    target_count: int,
    scroll_round: int,
    max_scroll_rounds: int
) -> bool:
    """
    åˆ¤æ–·HISTæ¨¡å¼æ˜¯å¦æ‡‰è©²åœæ­¢æ»¾å‹•
    
    Args:
        found_anchor: æ˜¯å¦å·²æ‰¾åˆ°éŒ¨é»
        passed_anchor: æ˜¯å¦å·²è¶ŠééŒ¨é»
        collected: å·²æ”¶é›†çš„è²¼æ–‡åˆ—è¡¨
        target_count: ç›®æ¨™æ”¶é›†æ•¸é‡  
        scroll_round: ç•¶å‰æ»¾å‹•è¼ªæ¬¡
        max_scroll_rounds: æœ€å¤§æ»¾å‹•è¼ªæ¬¡
        
    Returns:
        True å¦‚æœæ‡‰è©²åœæ­¢
    """
    # æ¢ä»¶1: å·²é”åˆ°ç›®æ¨™æ•¸é‡
    if passed_anchor and len(collected) >= target_count:
        logging.debug(f"âœ… HISTæ¨¡å¼é”æ¨™: {len(collected)}/{target_count}")
        return True
        
    # æ¢ä»¶2: é”åˆ°æœ€å¤§æ»¾å‹•æ¬¡æ•¸
    if scroll_round >= max_scroll_rounds:
        logging.warning(f"âš ï¸ HISTæ¨¡å¼é”åˆ°æ»¾å‹•ä¸Šé™: {scroll_round}/{max_scroll_rounds}")
        return True
        
    return False