import asyncio
import json
import re
import random
from typing import Dict, Any, List

from common.db_client import get_db_client
from common.settings import get_settings
from common.llm_manager import get_llm_manager, chat_completion
from common.llm_client import parse_llm_json_response
from datetime import datetime

class PostAnalyzerAgent:
    """
    Analyzes posts to extract success patterns based on different modes.
    """
    def __init__(self):
        self.settings = get_settings()
        self.llm_manager = get_llm_manager()
        
        # ğŸ² éš¨æ©Ÿè®ŠåŒ–è©å¥åº«
        self.analysis_variations = {
            "depth_words": ["æ·±å…¥", "ç´°ç·»", "å…¨é¢", "æ·±åº¦", "é€å¾¹", "è©³ç›¡"],
            "focus_words": ["ç²¾ç°¡é‡é»", "æ ¸å¿ƒè¦é»", "é—œéµç‰¹å¾µ", "é‡é»æ‘˜è¦", "ç²¾è¯æç…‰", "è¦é»æ­¸ç´"],
            "theme_words": ["é‡å°ä¸»é¡Œ", "ä¸»é¡Œå°å‘", "è­°é¡Œèšç„¦", "ä¸»æ—¨åˆ†æ", "è©±é¡Œä¸­å¿ƒ", "ä¸»é¡Œåˆ‡å…¥"],
            "style_words": ["é‡å°ä¸»é¡Œé¢¨æ ¼", "é¢¨æ ¼ç‰¹è‰²", "è¡¨é”é¢¨æ ¼", "æ–‡å­—é¢¨æ ¼", "æ•˜è¿°é¢¨æ ¼", "å‘ˆç¾é¢¨æ ¼"],
            "analysis_angles": ["çµæ§‹å±¤é¢", "å…§å®¹å±¤é¢", "é¢¨æ ¼å±¤é¢", "è¡¨é”å±¤é¢", "æ•˜äº‹å±¤é¢", "çµ„ç¹”å±¤é¢"],
            "pattern_descriptors": ["å…¸å‹æ¨¡å¼", "å¸¸è¦‹å½¢å¼", "ä¸»è¦é¡å‹", "æ ¸å¿ƒæ¨£å¼", "åŸºæœ¬æ¡†æ¶", "æ¨™æº–ç¯„å¼"]
        }

    def _get_random_variation(self, category: str) -> str:
        """ç²å–éš¨æ©Ÿè®ŠåŒ–è©å¥"""
        return random.choice(self.analysis_variations.get(category, [category]))
    
    def _get_random_pattern_count(self) -> int:
        """ç²å–éš¨æ©Ÿæ¨¡å¼æ•¸é‡ (3-5)"""
        return random.randint(3, 5)
    
    def _decide_min_groups(self, total_posts: int) -> int:
        """æ ¹æ“šç¸½è²¼æ–‡æ•¸æ±ºå®šæœ€ä½åˆ†çµ„æ•¸ï¼ˆå…è¨±é‡ç–Šè¦†è“‹ï¼‰"""
        if total_posts >= 100:
            return 10
        if total_posts >= 25:
            return 5
        return 3
    
    def _format_multiple_posts(self, posts_content: List[str]) -> str:
        """æ ¼å¼åŒ–å¤šç¯‡è²¼æ–‡å…§å®¹"""
        formatted_posts = []
        for i, content in enumerate(posts_content, 1):
            formatted_posts.append(f"ã€è²¼æ–‡ {i:02d}ã€‘\n{content}")
        return "\n\n" + "="*50 + "\n\n".join(formatted_posts)

    # ========= æ–°å¢ï¼šçŸ­æ–‡/é•·æ–‡è‡ªé©æ‡‰çš„å¿«é€Ÿçµ±è¨ˆè¼”åŠ© =========
    def _quick_text_stats(self, text: str) -> Dict[str, int]:
        """é‡å°è²¼æ–‡åšç²—ç•¥çµ±è¨ˆï¼Œä¾¿æ–¼åœ¨æç¤ºè©ä¸­æä¾›é•·åº¦è‡ªé©æ‡‰ç·šç´¢ã€‚
        - char_count: ä»¥å­—å…ƒæ•¸ç‚ºæº–ï¼ˆå«æ¨™é»ï¼‰
        - sentence_count: ä»¥æ¨™é»ç°¡æ˜“åˆ‡åˆ†ï¼ˆã€‚.!?ï¼ï¼Ÿï¼‰
        - paragraph_count: ä»¥ç©ºè¡Œåˆ‡åˆ†
        """
        cleaned = (text or "").strip()
        char_count = len(cleaned)
        # ä»¥å¸¸è¦‹çµ‚æ­¢ç¬¦åˆ‡åˆ†å¥å­ï¼Œéæ¿¾ç©ºç™½
        import re as _re
        sentences = [s for s in _re.split(r"[ã€‚\.\!\?ï¼ï¼Ÿ]+", cleaned) if s and s.strip()]
        sentence_count = len(sentences)
        # ä»¥ç©ºè¡Œåˆ‡åˆ†æ®µè½
        paragraphs = [p for p in _re.split(r"\n\s*\n+", cleaned) if p and p.strip()]
        paragraph_count = len(paragraphs)
        return {
            "char_count": char_count,
            "sentence_count": sentence_count,
            "paragraph_count": paragraph_count,
        }

    def _extract_main_post(self, markdown: str) -> str:
        """
        Extracts the main post content by splitting at the first long separator.
        """
        if not markdown:
            return ""
        
        # Split by the most likely separator first
        if '===============' in markdown:
            return markdown.split('===============', 1)[0].strip()
        
        # Fallback to the other common separator
        if '---' in markdown:
            return markdown.split('---', 1)[0].strip()
        
        # If no separator is found, try to find the "views" indicator as a last resort
        match = re.search(r"\n\n\d+\s*(views|æ¬¡æŸ¥çœ‹)", markdown)
        if match:
            return markdown[:match.start()].strip()

        # If absolutely no separator is found, return the original text, but log a warning.
        print(f"Warning: No clear separator found in markdown. Using full content for URL (this may be incorrect).")
        return markdown.strip()


    async def _fetch_posts_content(self, post_urls: List[str]) -> List[str]:
        """Fetches the markdown content for a list of post URLs."""
        db_client = await get_db_client()
        posts = await db_client.get_posts_with_metrics(post_urls)
        # Extract only the main post content from the full markdown
        return [self._extract_main_post(post['markdown']) for post in posts if post and 'markdown' in post]

    async def _analyze_mode_1(self, posts_content: List[str]) -> Dict[str, Any]:
        """Mode 1: Quick rewrite analysis from a single post."""
        if not posts_content:
            return {}
        
        post_text = posts_content[0] # Use the first post for quick analysis
        
        prompt = f"""
        Analyze the following social media post and generate a simple, lightweight template for a quick rewrite.
        Focus on the basic structure: introduction, emotional expression, and conclusion.

        Post:
        ---
        {post_text}
        ---

        Based on this post, provide a template with three steps.
        Example output format:
        {{
            "analysis_type": "Quick Rewrite Template",
            "template": {{
                "summary": "Generates a lightweight template from a single post.",
                "steps": [
                    "Step 1: Brief introduction related to an event or topic.",
                    "Step 2: Expression of personal emotion or reaction.",
                    "Step 3: A concluding sentence, possibly humorous or self-deprecating."
                ]
            }}
        }}
        """
        
        print("\n--- [Mode 1] Prompt to be sent ---\n")
        print(prompt)
        print("\n-------------------------------------\n")

        messages = [{"role": "user", "content": prompt}]
        response = await self.llm_client.chat_completion(messages, temperature=0.2, usage_scene="content-analysis")
        
        try:
            content = response["choices"][0]["message"]["content"]
            return parse_llm_json_response(content)
        except (json.JSONDecodeError, KeyError, IndexError) as e:
            print(f"Mode 1 - JSON Parsing Error: {e}")
            return {"error": "Failed to parse LLM response for mode 1."}


    async def _analyze_mode_2(self, posts_content: List[str]) -> Dict[str, Any]:
        """Mode 2: Style and structure analysis from multiple posts."""
        combined_posts = "\n\n---\n\n".join(posts_content)
        prompt = f"""
        Analyze the writing style and content structure from the following social media posts.
        Identify recurring patterns in tone, sentence structure, topics, and use of special elements like emojis or punctuation.
        Generate two distinct style templates based on your analysis.

        Posts:
        ---
        {combined_posts}
        ---

        Provide the output in a structured JSON format like this:
        {{
            "analysis_type": "Style & Structure Analysis",
            "templates": [
                {{
                    "name": "Style Template A",
                    "details": [
                        "Tone: Describe the primary tone.",
                        "Sentence Structure: Describe the sentence patterns.",
                        "Topics: List common topics.",
                        "Special Elements: Mention recurring emojis, symbols, or punctuation."
                    ]
                }},
                {{
                    "name": "Style Template B",
                    "details": [
                        "Tone: Describe a secondary or different tone.",
                        "Sentence Structure: Describe other sentence patterns observed.",
                        "Topics: List other common topics.",
                        "Special Elements: Mention other recurring elements."
                    ]
                }}
            ]
        }}
        """
        
        print("\n--- [Mode 2] Prompt to be sent ---\n")
        print(prompt)
        print("\n-------------------------------------\n")

        messages = [{"role": "user", "content": prompt}]
        response = await self.llm_client.chat_completion(messages, temperature=0.3, usage_scene="content-analysis")
        try:
            content = response["choices"][0]["message"]["content"]
            return parse_llm_json_response(content)
        except (json.JSONDecodeError, KeyError, IndexError) as e:
            print(f"Mode 2 - JSON Parsing Error: {e}")
            return {"error": "Failed to parse LLM response for mode 2."}

    async def _analyze_mode_3(self, posts_content: List[str]) -> Dict[str, Any]:
        """Mode 3: In-depth multi-LLM analysis report."""
        combined_posts = "\n\n---\n\n".join(posts_content)
        
        # Get the list of models directly from the LLM client
        models_to_use = self.llm_client.models

        if not models_to_use:
            return {"error": "No models configured for multi-LLM analysis."}

        findings = []
        
        for model in models_to_use:
            prompt = f"""
            As a professional content strategist, perform an in-depth analysis of the following posts.
            Your analysis should be from your unique perspective as the '{model}' model.
            Identify the core emotional tone, language style, and content themes.
            Summarize your findings in a few key points.

            Posts:
            ---
            {combined_posts}
            ---

            Present your findings as a list of strings.
            """
            print(f"\n--- [Mode 3 - Model: {model}] Prompt to be sent ---\n")
            print(prompt)
            print("\n------------------------------------------------\n")

            messages = [{"role": "user", "content": prompt}]
            try:
                response = await self.llm_client.chat_completion(messages, model=model, temperature=0.5, usage_scene="content-analysis")
                content = response["choices"][0]["message"]["content"]
                findings.append({
                    "model": model,
                    "report": content.strip().split('\n') # Simple parsing
                })
            except Exception as e:
                findings.append({
                    "model": model,
                    "report": f"Error during analysis: {str(e)}"
                })
        
        # Generate a final summary and high-level template based on the findings
        final_summary_prompt = f"""
        Based on the analysis from multiple AI models, synthesize a final report and a high-level creative template.
        
        Analysis Reports:
        ---
        {json.dumps(findings, indent=2, ensure_ascii=False)}
        ---

        Synthesize these findings into:
        1. A brief summary of the author's overall style.
        2. A high-level, three-paragraph template for creating new content in this style.
        
        Provide the output in a structured JSON format:
        {{
            "analysis_type": "In-depth Multi-LLM Report",
            "synthesis": {{
                "summary": "Your synthesized summary here.",
                "high_level_template": {{
                    "paragraph_1": "Guideline for the first paragraph.",
                    "paragraph_2": "Guideline for the second paragraph.",
                    "paragraph_3": "Guideline for the third paragraph."
                }}
            }},
            "individual_reports": {json.dumps(findings)}
        }}
        """
        
        print("\n--- [Mode 3 - Final Summary] Prompt to be sent ---\n")
        print(final_summary_prompt)
        print("\n----------------------------------------------------\n")

        summary_messages = [{"role": "user", "content": final_summary_prompt}]
        summary_response = await self.llm_client.chat_completion(summary_messages, temperature=0.3, usage_scene="content-analysis")
        
        try:
            content = summary_response["choices"][0]["message"]["content"]
            return parse_llm_json_response(content)
        except (json.JSONDecodeError, KeyError, IndexError) as e:
            print(f"Mode 3 - JSON Parsing Error: {e}")
            return {"error": "Failed to parse final summary response for mode 3."}


    async def analyze_posts(self, post_urls: List[str], analysis_mode: int) -> Dict[str, Any]:
        if not post_urls:
            return {"status": "error", "message": "No post URLs provided."}

        try:
            posts_content = await self._fetch_posts_content(post_urls)
            if not posts_content:
                return {"status": "error", "message": "Could not fetch content for the provided URLs."}

            result_data = {}
            if analysis_mode == 1:
                result_data = await self._analyze_mode_1(posts_content)
            elif analysis_mode == 2:
                result_data = await self._analyze_mode_2(posts_content)
            elif analysis_mode == 3:
                result_data = await self._analyze_mode_3(posts_content)
            else:
                return {"status": "error", "message": f"Invalid analysis mode: {analysis_mode}"}

            return {
                "status": "success",
                "analysis_mode": analysis_mode,
                "result": result_data,
                "message": f"Analysis for mode {analysis_mode} completed successfully."
            }

        except Exception as e:
            return {"status": "error", "message": f"An error occurred during analysis: {str(e)}"}
    
    async def analyze_post_structure(self, post_content: str, post_id: str, username: str) -> Dict[str, Any]:
        """åŸ·è¡Œå–®ç¯‡è²¼æ–‡çš„çµæ§‹åˆ†æ"""
        try:
            # ç¬¬ä¸€æ­¥ï¼šåˆ†æè²¼æ–‡çµæ§‹ç‰¹å¾µï¼Œå¡«å¯« post_structure_guide
            structure_guide = await self._analyze_post_structure_step1(post_content)
            
            # ç¬¬äºŒæ­¥ï¼šæ ¹æ“šè¡¨æ ¼ç”Ÿæˆç°¡çŸ­åˆ†æçµæœ
            analysis_summary = await self._analyze_post_structure_step2(post_content, structure_guide)
            
            return {
                "status": "success",
                "post_id": post_id,
                "username": username,
                "post_structure_guide": structure_guide,
                "analysis_summary": analysis_summary,
                "analyzed_at": datetime.utcnow().isoformat()
            }
            
        except Exception as e:
            return {"status": "error", "message": f"çµæ§‹åˆ†æå¤±æ•—: {str(e)}"}
    
    async def _analyze_post_structure_step1(self, post_content: str) -> Dict[str, Any]:
        """ç¬¬ä¸€æ­¥ï¼šåˆ†æè²¼æ–‡çµæ§‹ç‰¹å¾µ"""
        # å‹•æ…‹é•·åº¦çµ±è¨ˆï¼Œä¾›æç¤ºè©è‡ªé©æ‡‰
        length_stats = self._quick_text_stats(post_content)
        char_count = length_stats["char_count"]
        sentence_count = length_stats["sentence_count"]
        paragraph_count = length_stats["paragraph_count"]

        prompt = f"""ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„æ–‡æœ¬çµæ§‹åˆ†æå¸«ã€‚è«‹åˆ†æä»¥ä¸‹è²¼æ–‡çš„çµæ§‹ç‰¹å¾µï¼Œä¸¦å¡«å¯«çµæ§‹æŒ‡å—ã€‚

é‡è¦è¦æ±‚ï¼š
- åªè¨±æ ¹æ“šè²¼æ–‡ã€Œçµæ§‹ç‰¹å¾µã€åˆ†æï¼Œä¸å¾—å¸¶å…¥ä»»ä½•åŸæ–‡å…§å®¹
- å„æ•¸å€¼ç”¨åˆç†ç¯„åœè¡¨ç¤ºï¼ˆä¾‹å¦‚ã€Œ7-18å¥ã€ã€ã€Œ30-45%ã€ï¼‰
- åˆ†æé‡é»ç‚ºã€Œå¥å­é•·çŸ­ã€ã€ã€Œæ®µè½çµ„ç¹”ã€ã€ã€Œå‰å¾Œé€£è²«ã€ã€ã€Œæ·±åº¦æ®µè½ã€
- ç¯„ä¾‹åªç”¨ã€Œçµæ§‹éˆã€ï¼Œä¸å¯æ¶‰åŠå…·é«”å…§å®¹æˆ–ä¸»é¡Œ

ã€è‡ªé©æ‡‰è¦å‰‡ï¼ˆæ ¹æ“šé•·åº¦çµ±è¨ˆï¼‰ã€‘
- åƒè€ƒçµ±è¨ˆï¼šå­—å…ƒæ•¸={char_count}ã€å¥æ•¸â‰ˆ{sentence_count}ã€æ®µè½æ•¸â‰ˆ{paragraph_count}
- è‹¥å±¬æ–¼çŸ­æ–‡ï¼ˆä»»ä¸€æ¢ä»¶æˆç«‹ï¼šå­—å…ƒæ•¸â‰¤160 æˆ– å¥æ•¸â‰¤3 æˆ– æ®µè½æ•¸â‰¤1ï¼‰ï¼š
  1) å…è¨±ã€Œå–®æ®µ/æ¥µçŸ­å¤šæ®µã€çµæ§‹ï¼Œå¥æ•¸ç¯„åœç”¨å°ç¯„åœè¡¨ç¤ºï¼ˆå¦‚ 2-4 å¥ï¼‰
  2) ã€ŒçŸ­å¥æ¯”ä¾‹ã€å¯åé«˜ï¼›å…è¨±ç„¡æ˜ç¢ºã€Œé•·å¥å®šç¾©ã€ï¼Œä»¥ç°¡çŸ­æ‰¿æ¥èªæ›¿ä»£
  3) ã€Œé€£è²«èˆ‡éŠœæ¥è¦å‰‡ã€ä»¥è¼•é‡åŒ–è¦å‰‡è¡¨è¿°ï¼ˆå¦‚ï¼šæ¯æ®µé¦–å¥æ‰¿ä¸Šã€çµå°¾ç”¨çŸ­å¥å¼·åŒ–ï¼‰
  4) ã€Œæ•˜äº‹å¼§ç·šã€å¯ä½¿ç”¨ã€Œå¾®å¼§ç·šã€ï¼šå¦‚ã€Œè§¸ç™¼â†’åæ‡‰ã€æˆ–ã€Œæå•â†’è£œå……â†’æ”¶æŸã€
- è‹¥å±¬æ–¼é•·æ–‡ï¼ˆå­—å…ƒæ•¸â‰¥600 æˆ– å¥æ•¸â‰¥10 æˆ– æ®µè½æ•¸â‰¥3ï¼‰ï¼š
  1) å®Œæ•´è¼¸å‡ºæ®µè½é¡å‹åˆ†å¸ƒèˆ‡å¥å‹åˆ†é¡æ¯”ä¾‹
  2) è£œå……æ›´æ¸…æ™°çš„æ®µè½éŠœæ¥èˆ‡æ·±åº¦æ®µè½è¦å‰‡

ã€è¼¸å‡ºä¸€è‡´æ€§ã€‘
- è«‹ç›¡é‡ä¿ç•™ç›¸åŒéµåï¼Œæ•¸å€¼ç”¨ç¯„åœè¡¨ç¤ºï¼›çŸ­æ–‡å ´æ™¯ä¸‹ä»éœ€çµ¦å‡ºåˆä¹çŸ­æ–‡çš„åˆç†ç¯„åœã€‚

è²¼æ–‡å…§å®¹ï¼š
---
{post_content}
---

è«‹æŒ‰ç…§ä»¥ä¸‹æ ¼å¼åˆ†æä¸¦å›æ‡‰ï¼ˆJSONæ ¼å¼ï¼‰ï¼š

{{
  "post_structure_guide": {{
    "ç¸½å¥æ•¸ç¯„åœ": "X-Yå¥",
    "å¹³å‡æ¯å¥å­—æ•¸": "A-Bå­—",
    "çŸ­å¥å®šç¾©": "C-Då­—",
    "é•·å¥å®šç¾©": "E-Få­—",
    "çŸ­å¥æ¯”ä¾‹": "P1-P2%",
    "é•·å¥æ¯”ä¾‹": "Q1-Q2%",
    "æ®µè½æ•¸é‡": "N1-N2æ®µ",
    "æ¯æ®µå¥æ•¸": "S1-S2å¥",
    "æ®µè½é¡å‹åˆ†å¸ƒ": [
      "äº‹ä»¶æ•˜è¿°æ®µï¼ˆK1-K2æ®µï¼Œæ¯æ®µM1-M2å¥ï¼‰",
      "ç´°ç¯€æå¯«/æ’æ›²æ®µï¼ˆL1-L2æ®µï¼Œæ¯æ®µM3-M4å¥ï¼‰",
      "è©•è«–æ¨è«–æ®µï¼ˆ1æ®µï¼ŒT1-T2å¥ï¼‰",
      "æƒ…æ„Ÿæ”¶æŸ/æ„Ÿè¬æ®µï¼ˆ1æ®µï¼ŒT3-T4å¥ï¼‰"
    ],
    "å¥å‹åˆ†é¡æ¯”ä¾‹": {{
      "æ•˜äº‹å‹ï¼ˆæè¿°äº‹ä»¶ç¶“éï¼‰": "X1-X2%",
      "ç´°ç¯€æå¯«å‹ï¼ˆè£œå……ç’°å¢ƒ/äººç‰©ç´°ç¯€ï¼‰": "Y1-Y2%",
      "è©•è«–æ¨è«–å‹ï¼ˆè©•è«–ç¾è±¡æˆ–æ¨è«–çµæœï¼‰": "Z1-Z2%",
      "æƒ…æ„Ÿå‹ï¼ˆæŠ’ç™¼æ„Ÿå—ã€æ”¶å°¾å¼·èª¿ï¼‰": "W1-W2%"
    }},
    "é€£è²«èˆ‡éŠœæ¥è¦å‰‡": {{
      "é•·å¥æ‡‰ç”¨": "é•·å¥æ‡‰åŒ…å«æ‰¿æ¥å‰æ–‡çš„é€£æ¥è©ï¼ˆå¦‚ã€ä½†ã€ã€ã€æ–¼æ˜¯ã€ã€ã€æœ€å¾Œã€ç­‰ï¼‰",
      "çŸ­å¥æ‡‰ç”¨": "çŸ­å¥å¤šç”¨æ–¼æ”¶å°¾ã€å¼·èª¿ã€æƒ…ç·’è¡¨é”",
      "æ®µè½éŠœæ¥": "æ¯æ®µé–‹é ­ç›¡é‡ç”¨è½‰æŠ˜æˆ–æ‰¿ä¸Šå•Ÿä¸‹è©ï¼ˆå¦‚ã€æ¥è‘—ã€ã€ã€æ­¤å¤–ã€ã€ã€æœ€å¾Œã€ï¼‰",
      "æ·±åº¦æ®µè½": "æ¯å‰‡è²¼æ–‡è‡³å°‘æœ‰ä¸€æ®µåŒ…å«è©•è«–æˆ–æ¨è«–ï¼Œç”¨ä¾†å±•ç¾æ€è€ƒæ·±åº¦"
    }},
    "æ•˜äº‹å¼§ç·š": [
      "1. äº‹ä»¶èµ·å› /èƒŒæ™¯ï¼ˆé–‹é ­1æ®µï¼‰",
      "2. éç¨‹ç´°ç¯€/æ’æ›²ï¼ˆä¸­æ®µ1-3æ®µï¼‰",
      "3. è©•è«–/æ¨è«–ï¼ˆå€’æ•¸ç¬¬2æ®µï¼‰",
      "4. æƒ…æ„Ÿæ”¶æŸæˆ–æ„Ÿè¬ï¼ˆçµå°¾1æ®µï¼‰"
    ],
    "ç¯„ä¾‹çµæ§‹éˆ": [
      "äº‹ä»¶å•Ÿå‹•â†’ç´°ç¯€é‹ªé™³â†’æ’æ›²/æ’è©±â†’è©•è«–/æ¨è«–â†’æƒ…æ„Ÿæ”¶æŸ"
    ]
  }},
  "analysis_elements": {{
    "é•·å¥åŠŸèƒ½åˆ†é¡": ["æ•˜äº‹å‹", "è©•è«–å‹", "æ¨è«–å‹"],
    "é•·å¥çµ„ç¹”æ¨¡å¼": [
      "ä¸»å¥ï¼‹ç´°ç¯€ï¼‹æƒ…ç·’è£œå……",
      "æè¿°ï¼‹å€‹äººæ„Ÿå—",
      "å› æœ/è½‰æŠ˜/è£œè¿°"
    ],
    "çŸ­å¥æ‡‰ç”¨å ´æ™¯": [
      "æ–·é»æ”¶å°¾",
      "æƒ…æ„Ÿå¼·èª¿",
      "ç¯€å¥åˆ‡æ›"
    ],
    "é€£è²«ç­–ç•¥": [
      "æ¯3-5å¥æœ‰æ˜é¡¯æ‰¿æ¥è©",
      "æ¯æ®µé–‹é ­ä½¿ç”¨è½‰æŠ˜/æ‰¿æ¥èª",
      "çŸ­å¥éŠœæ¥é•·å¥æ”¶å°¾"
    ]
  }}
}}"""

        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„æ–‡æœ¬çµæ§‹åˆ†æå¸«ï¼Œæ“…é•·è­˜åˆ¥æ–‡æœ¬çš„çµæ§‹ç‰¹å¾µè€Œä¸æ¶‰åŠå…§å®¹ç´°ç¯€ã€‚"},
            {"role": "user", "content": prompt}
        ]
        
        try:
            content = await chat_completion(
                messages=messages,
                model="gemini-2.0-flash",
                temperature=0.1,
                max_tokens=1000,
                provider="gemini"
            )
            result = parse_llm_json_response(content)
            # å¦‚æœæœ‰é‡è¤‡çš„ post_structure_guide åµŒå¥—ï¼Œæå–å…§å±¤çš„
            if "post_structure_guide" in result and "post_structure_guide" in result["post_structure_guide"]:
                return result["post_structure_guide"]
            return result
        except (json.JSONDecodeError, Exception) as e:
            print(f"çµæ§‹åˆ†ææ­¥é©Ÿ1 - LLMèª¿ç”¨æˆ–JSONè§£æéŒ¯èª¤: {e}")
            return {"error": f"Failed to parse LLM response for structure analysis step 1: {str(e)}"}
    
    async def _analyze_post_structure_step2(self, post_content: str, structure_guide: Dict[str, Any]) -> str:
        """ç¬¬äºŒæ­¥ï¼šæ ¹æ“šçµæ§‹æŒ‡å—ç”Ÿæˆåˆ†ææ‘˜è¦"""
        # å‹•æ…‹é•·åº¦çµ±è¨ˆï¼Œä¾›æç¤ºè©è‡ªé©æ‡‰
        length_stats = self._quick_text_stats(post_content)
        char_count = length_stats["char_count"]
        sentence_count = length_stats["sentence_count"]
        paragraph_count = length_stats["paragraph_count"]

        prompt = f"""è«‹æ ¹æ“šã€ŒåŸè²¼æ–‡å…§å®¹ã€èˆ‡ã€Œç¬¬ä¸€éšæ®µçµæ§‹åˆ†æã€ï¼Œä»¥å¦‚ä¸‹æ ¼å¼æ¢åˆ—å›æ‡‰ï¼š

ã€è‡ªé©æ‡‰æ‘˜è¦è¦å‰‡ã€‘
- åƒè€ƒçµ±è¨ˆï¼šå­—å…ƒæ•¸={char_count}ã€å¥æ•¸â‰ˆ{sentence_count}ã€æ®µè½æ•¸â‰ˆ{paragraph_count}
- è‹¥çŸ­æ–‡ï¼ˆå­—å…ƒæ•¸â‰¤160 æˆ– å¥æ•¸â‰¤3 æˆ– æ®µè½æ•¸â‰¤1ï¼‰ï¼š
  â€¢ ç²¾ç…‰è¼¸å‡ºï¼Œæ•´é«”ä¸è¶…é 6 æ¢è¦é»ï¼›é¿å…å†—é•·è§£é‡‹
  â€¢ å¼·èª¿ã€Œç¯€å¥/åœé “ã€è½‰æŠ˜èªã€æ”¶å°¾å¥å‹ã€ç­‰å¾®çµæ§‹å»ºè­°
  â€¢ å…è¨±ä»¥ã€Œå¾®å¼§ç·šã€æè¿°ï¼ˆå¦‚ï¼šè§¸ç™¼â†’åæ‡‰ / æå•â†’è£œå……â†’æ”¶æŸï¼‰
- è‹¥é•·æ–‡ï¼š
  â€¢ å¯ä¿ç•™å®Œæ•´æ¢åˆ—ï¼Œä½†ä»ä»¥å¯æ“ä½œç‚ºåŸå‰‡

åŸè²¼æ–‡å…§å®¹ï¼š
---
{post_content}
---

ç¬¬ä¸€éšæ®µçµæ§‹åˆ†æçµæœï¼š
{json.dumps(structure_guide, ensure_ascii=False, indent=2)}

è«‹æ ¹æ“šã€ŒåŸè²¼æ–‡å…§å®¹ã€åŠã€Œç¬¬ä¸€éšæ®µçµæ§‹åˆ†æã€ï¼Œä¾ä»¥ä¸‹æ ¼å¼å›æ‡‰ï¼š

ã€åˆ†æçµæœã€‘

1.æ­¸ç´æœ¬è²¼æ–‡çš„ä¸»é¡Œã€æƒ…ç·’å¼·åº¦ã€æ®µè½å®‰æ’ã€é•·çŸ­å¥ç¯€å¥ã€ç”¨è©ç‰¹è‰²èˆ‡é€£è²«æ€§ï¼ŒæŒ‡å‡ºçµæ§‹äº®é»åŠå¯å„ªåŒ–ç©ºé–“ã€‚
2.è§€å¯Ÿä¸¦æè¿°ç™¼æ–‡è€…åœ¨æœ¬è²¼æ–‡æ‰€å‘ˆç¾çš„è§’è‰²ã€èº«ä»½æˆ–ç«‹å ´ï¼ˆå¦‚ç¶“é©—åˆ†äº«è€…ã€å°ˆæ¥­æœå‹™è€…ã€åƒèˆ‡è€…ã€è§€å¯Ÿè€…ã€å‰µä½œè€…ã€æå•è€…ç­‰ï¼‰ï¼Œç°¡è¿°å…¶èªå¢ƒã€å°ˆæ¥­æ„Ÿã€é—œæ³¨é‡é»æˆ–è©±èªè§’åº¦ã€‚
3.åˆ¤æ–·æœ¬è²¼æ–‡èƒŒå¾Œéš±å«çš„æ°›åœã€åƒ¹å€¼è§€æˆ–æ…‹åº¦ï¼ˆå¦‚æº«æš–çœŸèª ã€å°ˆæ¥­è‡ªä¿¡ã€å¹½é»˜é¢¨è¶£ã€ç†æ€§åˆ†æã€å‘¼ç±²è¡Œå‹•ç­‰ï¼‰ï¼Œè£œå……å…§å®¹ç‰¹è‰²èˆ‡å¯å†åŠ å¼·ä¹‹è™•ã€‚
4.è‹¥æœ‰ä¸»é¡Œåˆ†æ•£ã€æƒ…ç·’ä¸è¶³ã€æ®µè½å¤±è¡¡ã€å¥å‹å–®èª¿ã€æ‰¿æ¥ä¸è¶³ç­‰ç¾è±¡ï¼Œè«‹å…·é«”æŒ‡å‡ºã€‚

ã€æ”¹å¯«å»ºè­°ã€‘

1.ä¾æ“šåˆ†æï¼Œæå‡º 3â€“5 é»å…·é«”å„ªåŒ–å»ºè­°ï¼Œå¯åŒ…å«ä¸»é¡Œèšç„¦ã€æ®µè½åˆ†ç•Œã€æƒ…ç·’è£œå¼·ã€é•·çŸ­å¥èª¿æ•´ã€è½‰æŠ˜èªè£œå……ã€è©•è«–æˆ–æ”¶å°¾æ®µå¢å¼·ç­‰ã€‚æ¯é»ç°¡æ½”æ˜ç¢ºã€å¯ç›´æ¥æ“ä½œã€‚
2.å¦‚è§’è‰²åå°ˆæ¥­æˆ–çµ„ç¹”ï¼Œå»ºè­°å¼·åŒ–å°ˆæ¥­è¡“èªã€æµç¨‹ç´°ç¯€ã€æˆå°±å±•ç¾ï¼›å¦‚è§’è‰²åç¶“é©—åˆ†äº«æˆ–è‡ªæˆ‘è¡¨é”ï¼Œå‰‡å¯å¢å¼·å€‹äººè§€é»ã€æƒ…æ„Ÿç´°ç¯€ã€äº’å‹•å¼•å°ç­‰ã€‚

ã€ç™¼å±•æ–¹å‘ã€‘

1.åˆ¤æ–·æœ¬è²¼æ–‡æœ€æ¥è¿‘å“ªä¸€ç¨®é¢¨æ ¼æ–¹å‘ï¼ˆæ•…äº‹å‹ï¼è©•è«–å‹ï¼äº’å‹•å‹ï¼å…¶ä»–ï¼Œå¦‚é©ç”¨ï¼‰ï¼Œåƒ…é¸ä¸€ç¨®ï¼Œç°¡è¿°ç†ç”±ã€‚
2.æ ¹æ“šè©²æ–¹å‘ï¼Œè£œå……å…§å®¹è¡ŒéŠ·ã€å“ç‰Œç¶“ç‡Ÿã€SEO/åˆ†ç™¼ç­‰ç­–ç•¥å»ºè­°ï¼ˆå¦‚è§’è‰²å½¢è±¡å»ºç«‹ã€å“ç‰Œä¿¡ä»»æ„Ÿã€è©±é¡Œï¼é—œéµè©ä½ˆå±€ã€è™Ÿå¬äº’å‹•ã€å°ˆæ¥­è­‰ç…§éœ²å‡ºã€ç†±é–€æ¨™ç±¤æ‡‰ç”¨ç­‰ï¼‰ã€‚
3.æè¿°æœè©²æ–¹å‘å»¶ä¼¸å…§å®¹çš„å¯«ä½œé‡é»ï¼ˆ1â€“2 å¥ï¼‰ã€‚
4.èˆ‰ä¸€å€‹ç¬¦åˆè©²è§’è‰²å®šä½ï¼‹é¢¨æ ¼æ–¹å‘ï¼‹å…§å®¹ç¶“ç‡Ÿæ„åœ–çš„ç¯„ä¾‹å¥å­ï¼ˆå¯å‡¸é¡¯è§€é»ã€æ°›åœã€é—œéµè©ã€å“ç‰Œèª¿æ€§ã€äº’å‹•è™Ÿå¬ç­‰ï¼‰ã€‚

è«‹ä»¥ä¸Šè¿°æ ¼å¼å›æ‡‰ï¼Œæ¯å€å¡Šæ¨™é¡Œæ˜ç¢ºï¼Œæ¢åˆ—ç°¡æ˜ï¼Œæ‰€æœ‰è§€å¯Ÿçš†é ˆåŸºæ–¼å¯¦éš›è²¼æ–‡å…§å®¹è‡ªå‹•åˆ¤æ–·ï¼Œä¸é è¨­ç‰¹å®šè·æ¥­æˆ–èªå¢ƒï¼Œç¢ºä¿é©ç”¨æ–¼å„é¡ Threads è²¼æ–‡å„ªåŒ–ã€å…§å®¹ç­–ç•¥èˆ‡å“ç‰Œç¶“ç‡Ÿã€‚
"""

        messages = [
            {"role": "system", "content": "ä½ æ˜¯ Threads è²¼æ–‡çµæ§‹å„ªåŒ–åŠ©æ‰‹ï¼Œå°ˆé–€åˆ†æè²¼æ–‡çµæ§‹ä¸¦æä¾›å…·é«”çš„æ”¹å¯«å»ºè­°ã€‚"},
            {"role": "user", "content": prompt}
        ]
        
        try:
            content = await chat_completion(
                messages=messages,
                model="gemini-2.0-flash",
                temperature=0.2,
                max_tokens=8192,
                provider="gemini"
            )
            return content.strip()
        except Exception as e:
            print(f"çµæ§‹åˆ†ææ­¥é©Ÿ2 - LLMèª¿ç”¨éŒ¯èª¤: {e}")
            return f"ç„¡æ³•ç”Ÿæˆåˆ†ææ‘˜è¦: {str(e)}"
    
    # ğŸš€ æ™ºèƒ½æ‰¹é‡çµæ§‹åˆ†ææ–¹æ³•
    async def analyze_batch_structure(self, posts_content: List[str], username: str = "unknown") -> Dict[str, Any]:
        """åŸ·è¡Œæ™ºèƒ½æ‰¹é‡çµæ§‹åˆ†æ - èªæ–™åº«é©…å‹•çš„æ¨¡å¼ç™¼ç¾"""
        try:
            # ä½¿ç”¨æ–°çš„æ™ºèƒ½åˆ†æå™¨
            from .batch_analyzer import BatchAnalyzer
            batch_analyzer = BatchAnalyzer()
            
            return await batch_analyzer.analyze_batch_structure(posts_content, username)
            
        except ImportError:
            # å‘å¾Œå…¼å®¹ï¼šå¦‚æœæ–°æ¨¡çµ„ä¸å¯ç”¨ï¼Œä½¿ç”¨èˆŠæ–¹æ³•
            return await self._legacy_batch_analysis(posts_content, username)
        except Exception as e:
            return {"status": "error", "message": f"æ™ºèƒ½æ‰¹é‡çµæ§‹åˆ†æå¤±æ•—: {str(e)}"}
    
    async def _legacy_batch_analysis(self, posts_content: List[str], username: str = "unknown") -> Dict[str, Any]:
        """èˆŠç‰ˆæ‰¹é‡åˆ†ææ–¹æ³•ï¼ˆå‘å¾Œå…¼å®¹ï¼‰"""
        try:
            min_groups = self._decide_min_groups(len(posts_content))
            
            pattern_analysis = await self._batch_pattern_recognition(posts_content, min_groups)
            structure_templates = await self._generate_structure_templates(posts_content, pattern_analysis)
            
            identified_patterns = pattern_analysis.get("identified_patterns", []) if isinstance(pattern_analysis, dict) else []
            actual_pattern_count = len(identified_patterns)

            return {
                "status": "success",
                "username": username,
                "analysis_type": "legacy_batch_structure_analysis",
                "pattern_count": actual_pattern_count or min_groups,
                "total_posts": len(posts_content),
                "pattern_analysis": pattern_analysis,
                "structure_templates": structure_templates,
                "analyzed_at": datetime.utcnow().isoformat()
            }
            
        except Exception as e:
            return {"status": "error", "message": f"æ‰¹é‡çµæ§‹åˆ†æå¤±æ•—: {str(e)}"}
    
    async def _batch_pattern_recognition(self, posts_content: List[str], min_groups: int) -> Dict[str, Any]:
        """ç¬¬ä¸€å±¤ï¼šç´”çµæ§‹æ¨¡å¼è­˜åˆ¥ï¼ˆä¸æ¶‰åŠå…§å®¹ä¸»é¡Œï¼‰"""
        formatted_posts = self._format_multiple_posts(posts_content)
        
        # å‹•æ…‹å»ºè­°æ¯çµ„è‡³å°‘æ¨£æœ¬ï¼ˆå…è¨±é‡ç–Šï¼Œæ”¾å¯¬é–€æª»ï¼‰
        suggested_min_samples = max(3, len(posts_content) // max(min_groups, 5))
        target_groups_high = min_groups + 2
        
        prompt = f"""åˆ†æ{len(posts_content)}ç¯‡è²¼æ–‡çš„çµæ§‹ç‰¹å¾µï¼Œè«‹è¼¸å‡ºè‡³å°‘{min_groups}çµ„ï¼ˆå¯é”åˆ°{target_groups_high}çµ„ï¼‰çš„çµæ§‹æ¨¡å¼åˆ†ç¾¤ï¼Œå…è¨±é‡ç–Šè¦†è“‹ã€‚

{formatted_posts}

è¦æ±‚ï¼š
1. åƒ…æ ¹æ“šå¥æ•¸ã€å­—æ•¸åˆ†å¸ƒã€æ®µè½çµ„ç¹”ã€æ¨™é»å¯†åº¦ã€ç¯€å¥/åœé “ç­‰çµæ§‹ç‰¹å¾µåˆ†çµ„
2. å…è¨±åŒä¸€ç¯‡è²¼æ–‡å‡ºç¾åœ¨å¤šå€‹æ¨¡å¼ä¸­ï¼ˆé‡ç–Šè¦†è“‹ï¼‰ï¼Œä½†å¿…é ˆè¦†è“‹æ‰€æœ‰è²¼æ–‡ï¼ˆ1..{len(posts_content)}ï¼‰ï¼Œä¸å¯éºæ¼
3. å‘½ååªå¯åŸºæ–¼è§€å¯Ÿåˆ°çš„çµæ§‹ç‰¹å¾µï¼Œä¸å¾—å«ä»»ä½•ä¸»é¡Œ/æ–‡é«”å­—çœ¼
4. å»ºè­°æ¯çµ„è‡³å°‘{suggested_min_samples}ç¯‡ï¼›è‹¥æŸçµ„æ¨£æœ¬åå°‘ä»å¯ä¿ç•™ï¼Œä½†éœ€åœ¨ notes ä¸­èªªæ˜ä»£è¡¨æ€§é™åˆ¶

å›æ‡‰æ ¼å¼ï¼š
{{
  "analysis_summary": {{
    "total_posts": {len(posts_content)},
    "min_required_groups": {min_groups},
    "target_groups": "{min_groups}-{target_groups_high}",
    "overlap_allowed": true,
    "coverage": "å¿…é ˆè¦†è“‹å…¨éƒ¨è²¼æ–‡ï¼ˆå…è¨±é‡ç–Šï¼‰",
    "suggested_min_samples_per_group": {suggested_min_samples}
  }},
  "identified_patterns": [
    {{
      "pattern_id": "A",
      "pattern_name": "æ ¹æ“šè§€å¯Ÿåˆ°çš„çµæ§‹ç‰¹å¾µå‘½å",
      "post_indices": [1, 3, 7, 12],  
      "post_count": 4,
      "structure_characteristics": {{
        "å¥æ•¸ç¯„åœ": "X-Yå¥",
        "å­—æ•¸ç¯„åœ": "A-Bå­—", 
        "æ®µè½ç‰¹å¾µ": "å–®æ®µ/å¤šæ®µ/åˆ—é»",
        "å¥å‹åˆ†å¸ƒ": {{"é™³è¿°": "X%", "ç–‘å•": "Y%", "æ„Ÿå˜†": "Z%", "ç¥ˆä½¿": "W%"}},
        "æ¨™é»ç‰¹å¾µ": "æ„Ÿå˜†è™Ÿå¯†åº¦ã€é€—è™Ÿä½¿ç”¨ã€çœç•¥è™Ÿç­‰",
        "ç¯€å¥ç‰¹å¾µ": "æ€¥ä¿ƒ/èˆ’ç·©/åœé “",
        "ç¬¦è™Ÿä½¿ç”¨": "emoji/hashtagä½¿ç”¨é »ç‡"
      }},
      "sample_indices": [3, 12],
      "notes": "è‹¥æ¨£æœ¬åå°‘ï¼Œè«‹èªªæ˜ä»£è¡¨æ€§é™åˆ¶èˆ‡ä¸ç¢ºå®šæ€§"
    }}
  ]
}}"""

        messages = [
            {"role": "system", "content": "ä½ æ˜¯å°ˆæ¥­çš„çµæ§‹æ¨¡å¼è­˜åˆ¥å°ˆå®¶ï¼Œå°ˆæ³¨æ–¼å¾å®¢è§€çµæ§‹ç‰¹å¾µä¸­ç™¼ç¾è²¼æ–‡æ¨¡å¼ï¼Œä¸æ¶‰åŠå…§å®¹ä¸»é¡Œåˆ†æã€‚"},
            {"role": "user", "content": prompt}
        ]
        
        try:
            content = await chat_completion(
                messages=messages,
                model="gemini-2.0-flash",
                temperature=0.3,  # é©ä¸­çš„å‰µæ„æ€§ï¼Œä¿æŒä¸€è‡´æ€§
                max_tokens=2000,
                provider="gemini"
            )
            return parse_llm_json_response(content)
        except Exception as e:
            print(f"æ‰¹é‡æ¨¡å¼è­˜åˆ¥ - LLMèª¿ç”¨éŒ¯èª¤: {e}")
            return {"error": f"æ¨¡å¼è­˜åˆ¥å¤±æ•—: {str(e)}"}
    
    async def _representative_deep_analysis(self, posts_content: List[str], pattern_analysis: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ç¬¬äºŒå±¤ï¼šå°ä»£è¡¨æ€§è²¼æ–‡é€²è¡Œæ·±åº¦çµæ§‹åˆ†æ"""
        analyses = []
        patterns = pattern_analysis.get("identified_patterns", [])
        
        for pattern in patterns:
            pattern_name = pattern.get("pattern_name", "æœªçŸ¥æ¨¡å¼")
            representative_indices = pattern.get("representative_indices", [])
            
            # ç¢ºä¿è‡³å°‘æœ‰ä»£è¡¨æ€§è²¼æ–‡
            if not representative_indices:
                continue
            
            # ğŸ”§ ä¿®æ­£ï¼šæ¯å€‹æ¨¡å¼åˆ†æå¤šç¯‡è²¼æ–‡ï¼ˆ2-3ç¯‡ï¼‰
            # ç¢ºä¿æ¯å€‹æ¨¡å¼è‡³å°‘åˆ†æ2ç¯‡ï¼Œæœ€å¤š3ç¯‡
            analysis_count = min(len(representative_indices), random.randint(2, 3))
            selected_indices = random.sample(representative_indices, analysis_count)
            
            pattern_analyses = []
            
            for selected_index in selected_indices:
                if selected_index <= len(posts_content):
                    post_content = posts_content[selected_index - 1]  # è½‰æ›ç‚º0åŸºç´¢å¼•
                    
                    # ä½¿ç”¨è®ŠåŒ–çš„åˆ†æé¢¨æ ¼
                    style_word = self._get_random_variation("style_words")
                    focus_word = self._get_random_variation("focus_words")
                    
                    try:
                        structure_guide = await self._analyze_pattern_structure(
                            post_content, pattern_name, style_word, focus_word
                        )
                        
                        pattern_analyses.append({
                            "post_index": selected_index,
                            "post_content_preview": post_content[:100] + "..." if len(post_content) > 100 else post_content,
                            "structure_guide": structure_guide,
                            "analysis_variations": {
                                "style_focus": style_word,
                                "analysis_focus": focus_word
                            }
                        })
                        
                    except Exception as e:
                        print(f"ä»£è¡¨æ€§åˆ†æéŒ¯èª¤ - æ¨¡å¼ {pattern_name}, è²¼æ–‡ {selected_index}: {e}")
                        continue
            
            # å°‡è©²æ¨¡å¼çš„å¤šç¯‡åˆ†æçµæœæ•´åˆ
            if pattern_analyses:
                analyses.append({
                    "pattern_id": pattern.get("pattern_id"),
                    "pattern_name": pattern_name,
                    "analysis_count": len(pattern_analyses),
                    "analyzed_posts": pattern_analyses,
                    "representative_indices": selected_indices
                })
        
        return analyses
    
    async def _generate_structure_templates(self, posts_content: List[str], pattern_analysis: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ç¬¬äºŒå±¤ï¼šç‚ºæ¯å€‹çµæ§‹æ¨¡å¼ç”Ÿæˆå‰µä½œæ¨¡æ¿"""
        templates = []
        patterns = pattern_analysis.get("identified_patterns", [])
        
        for pattern in patterns:
            pattern_name = pattern.get("pattern_name", "æœªçŸ¥æ¨¡å¼")
            structure_chars = pattern.get("structure_characteristics", {})
            sample_indices = pattern.get("sample_indices", [])
            
            try:
                template = await self._generate_universal_structure_template(
                    pattern_name, structure_chars, posts_content, sample_indices
                )
                
                templates.append({
                    "pattern_id": pattern.get("pattern_id"),
                    "pattern_name": pattern_name,
                    "template_type": "universal",
                    "structure_template": template
                })
                
            except Exception as e:
                print(f"çµæ§‹æ¨¡æ¿ç”ŸæˆéŒ¯èª¤ - æ¨¡å¼ {pattern_name}: {e}")
                continue
        
        return templates
    
    def _estimate_avg_length(self, posts_content: List[str], post_indices: List[int]) -> int:
        """ä¼°ç®—æ¨¡å¼çš„å¹³å‡å­—æ•¸"""
        if not post_indices:
            return 0
        
        total_length = 0
        valid_count = 0
        
        for idx in post_indices:
            if 0 <= idx - 1 < len(posts_content):  # è½‰æ›ç‚º0åŸºç´¢å¼•
                total_length += len(posts_content[idx - 1])
                valid_count += 1
        
        return total_length // valid_count if valid_count > 0 else 0
    
    async def _generate_universal_structure_template(self, pattern_name: str, structure_chars: Dict[str, Any], 
                                                   posts_content: List[str], sample_indices: List[int]) -> Dict[str, Any]:
        """ç”Ÿæˆé€šç”¨çµæ§‹æ¨¡æ¿"""
        # ç²å–æ¨£æœ¬è²¼æ–‡
        sample_posts = []
        for idx in sample_indices:
            if 0 <= idx - 1 < len(posts_content):
                sample_posts.append(posts_content[idx - 1])
        
        samples_text = "\n".join([f"æ¨£æœ¬{i+1}: {post}" for i, post in enumerate(sample_posts)])
        
        prompt = f"""æ ¹æ“šçµæ§‹æ¨¡å¼ã€Œ{pattern_name}ã€çš„ç‰¹å¾µï¼Œåˆ†ææ¨£æœ¬ä¸¦ç”Ÿæˆå‰µä½œæŒ‡å¼•ã€‚

æ¨¡å¼ç‰¹å¾µï¼š{json.dumps(structure_chars, ensure_ascii=False)}

æ¨£æœ¬è²¼æ–‡ï¼š
{samples_text}

è«‹åˆ†æé€™å€‹æ¨¡å¼çš„çµæ§‹ç‰¹é»ï¼Œè‡ªè¡Œæ±ºå®šæœ€åˆé©çš„åˆ†æç¶­åº¦ï¼Œåƒ…è¼¸å‡ºã€Œçµæ§‹æŒ‡å¼•ã€ï¼Œä¸å¾—æ¶‰åŠå…§å®¹ä¸»é¡Œæˆ–èªç¾©ã€‚

è¼¸å‡ºJSONï¼ˆéµåå›ºå®šä½†å­é …é–‹æ”¾ï¼Œåƒ…åœ¨é©ç”¨æ™‚è¼¸å‡ºï¼‰ï¼š
{{
  "structure_guide": {{
    "length_profile": {{ "ç¸½å¥æ•¸ç¯„åœ": "X-Yå¥", "å¹³å‡æ¯å¥å­—æ•¸": "A-Bå­—" }},
    "organization": {{ "æ®µè½æ•¸é‡": "N1-N2æ®µ", "æ¯æ®µå¥æ•¸": "S1-S2å¥", "åˆ—é»/å°è©±/å¼•ç”¨": "æœ‰/ç„¡/æ¯”ä¾‹" }},
    "rhythm": {{ "ç¯€å¥æè¿°": "...", "æ¨™é»/emojiä½¿ç”¨": "..." }},
    "coherence": {{ "é€£è²«ç­–ç•¥": ["...","..."] }},
    "micro_arc": "è‹¥ç‚ºçŸ­ç¯‡ä¸”é©ç”¨æ™‚è¼¸å‡ºï¼šå¦‚ è§¸ç™¼â†’åæ‡‰ / æƒ…ç·’â†’å…·é«”åŒ–",
    "density": "è‹¥é©ç”¨ï¼šæ¯å¥éœ€åŒ…å«çš„è³‡è¨Š/æƒ…ç·’å…ƒç´ ",
    "tension": "è‹¥é©ç”¨ï¼šå°æ¯”/åå·®/å¼·èª¿è©/åœé “çš„é‹ç”¨",
    "completeness": "è‹¥é©ç”¨ï¼šå³ä½¿çŸ­æ–‡ä¹Ÿéœ€å…·å‚™çš„è¦ç´ ",
    "sentence_types": {{ "å¾æ¨£æœ¬ç™¼ç¾çš„å¥å‹é¡åˆ¥1": "çµ±è¨ˆæ¯”ä¾‹", "å¾æ¨£æœ¬ç™¼ç¾çš„å¥å‹é¡åˆ¥2": "çµ±è¨ˆæ¯”ä¾‹" }},
    "structure_chain": ["å¾æ¨£æœ¬æŠ½è±¡å‡ºçš„çµæ§‹æµç¨‹"]
  }},
  "creation_guidance": {{
    "writing_steps": ["å…·é«”å‰µä½œæ­¥é©Ÿ1", "å…·é«”å‰µä½œæ­¥é©Ÿ2", "å…·é«”å‰µä½œæ­¥é©Ÿ3"],
    "style_constraints": ["é¢¨æ ¼ç´„æŸ1", "é¢¨æ ¼ç´„æŸ2"],
    "common_pitfalls": ["å¸¸è¦‹éŒ¯èª¤1", "å¸¸è¦‹éŒ¯èª¤2"]
  }},
  "notes": "å¯è£œå……é™åˆ¶æˆ–æ³¨æ„äº‹é …"
}}

è¦å‰‡ï¼š
- åƒ…åœ¨ã€Œé©ç”¨æ™‚ã€æ‰è¼¸å‡º micro_arc / density / tension / completeness ç­‰ç¶­åº¦ï¼›ä¸å¼·è¡Œå¥—ç”¨
- æ‰€æœ‰æ•¸å€¼ç”¨ç¯„åœè¡¨ç¤ºï¼ˆå¦‚ 1-2 å¥ã€8-15 å­—ï¼‰
- åƒ…æ ¹æ“šçµæ§‹ç‰¹å¾µæ­¸ç´ï¼Œä¸å¾—å¸¶å…¥ä»»ä½•å…§å®¹ç´°ç¯€
- çµæ§‹éˆåªç”¨é‚è¼¯é—œä¿‚ï¼Œä¸æ¶‰åŠå…·é«”ä¸»é¡Œ
"""

        messages = [
            {"role": "system", "content": "ä½ æ˜¯å°ˆæ¥­çš„çµæ§‹åˆ†æå¸«ï¼Œå°ˆæ³¨æ–¼å¾çµæ§‹ç‰¹å¾µä¸­æå–å¯æŒ‡å°AIå‰µä½œçš„æ¨¡æ¿ã€‚"},
            {"role": "user", "content": prompt}
        ]
        
        try:
            content = await chat_completion(
                messages=messages,
                model="gemini-2.0-flash",
                temperature=0.2,
                max_tokens=1500,
                provider="gemini"
            )
            return parse_llm_json_response(content)
        except Exception as e:
            print(f"çµæ§‹æ¨¡æ¿ç”ŸæˆéŒ¯èª¤: {e}")
            return {"error": f"ç„¡æ³•ç”Ÿæˆçµæ§‹æ¨¡æ¿: {str(e)}"}
    
    async def _generate_long_form_template_backup(self, pattern_name: str, structure_chars: Dict[str, Any],
                                         posts_content: List[str], sample_indices: List[int]) -> Dict[str, Any]:
        """å‚™ç”¨å‡½æ•¸ï¼ˆæš«æ™‚ä¿ç•™ï¼‰"""
        # ç²å–æ¨£æœ¬è²¼æ–‡
        sample_posts = []
        for idx in sample_indices:
            if 0 <= idx - 1 < len(posts_content):
                sample_posts.append(posts_content[idx - 1])
        
        samples_text = "\n".join([f"æ¨£æœ¬{i+1}: {post}" for i, post in enumerate(sample_posts)])
        
        prompt = f"""è«‹æ‰®æ¼”çµæ§‹åˆ†æå°ˆå®¶ï¼Œåƒ…æ ¹æ“šä»¥ä¸‹è²¼æ–‡çš„å¥å‹ã€é•·çŸ­ã€æ®µè½åˆ†ä½ˆå’Œé€£è²«æ€§ï¼Œ
ç”¢ç”Ÿä¸€ä»½ã€Œè²¼æ–‡å‰µä½œçµæ§‹æŒ‡å¼•æ¨¡æ¿ã€ï¼Œç”¨ä»¥æŒ‡å°å¾ŒçºŒ AI è²¼æ–‡å‰µä½œï¼Œä¸å¾—åˆ†ææˆ–å›è¦†ä»»ä½•å…§å®¹ç´°ç¯€ã€‚

æ¨¡å¼åç¨±ï¼š{pattern_name}
çµæ§‹ç‰¹å¾µï¼š{json.dumps(structure_chars, ensure_ascii=False)}

æ¨£æœ¬è²¼æ–‡ï¼š
{samples_text}

è«‹åš´æ ¼æŒ‰ç…§ä¸‹è¿°æ ¼å¼è¼¸å‡ºï¼Œæ‰€æœ‰æ•¸æ“šéœ€æ¨ä¼°ç¯„åœï¼Œä¸éœ€å®Œå…¨ç²¾ç¢ºï¼š

{{
  "post_structure_guide": {{
    "ç¸½å¥æ•¸ç¯„åœ": "X-Yå¥",
    "å¹³å‡æ¯å¥å­—æ•¸": "A-Bå­—",
    "çŸ­å¥å®šç¾©": "C-Då­—",
    "é•·å¥å®šç¾©": "E-Få­—",
    "çŸ­å¥æ¯”ä¾‹": "P1-P2%",
    "é•·å¥æ¯”ä¾‹": "Q1-Q2%",
    "æ®µè½æ•¸é‡": "N1-N2æ®µ",
    "æ¯æ®µå¥æ•¸": "S1-S2å¥",
    "æ®µè½é¡å‹åˆ†å¸ƒ": [
      "å¾æ¨£æœ¬ç™¼ç¾çš„æ®µè½é¡å‹1ï¼ˆK1-K2æ®µï¼Œæ¯æ®µM1-M2å¥ï¼‰",
      "å¾æ¨£æœ¬ç™¼ç¾çš„æ®µè½é¡å‹2ï¼ˆL1-L2æ®µï¼Œæ¯æ®µM3-M4å¥ï¼‰"
    ],
    "å¥å‹åˆ†é¡æ¯”ä¾‹": {{
      "å¾æ¨£æœ¬ç™¼ç¾çš„å¥å‹é¡åˆ¥1": "X1-X2%",
      "å¾æ¨£æœ¬ç™¼ç¾çš„å¥å‹é¡åˆ¥2": "Y1-Y2%"
    }},
    "é€£è²«èˆ‡éŠœæ¥è¦å‰‡": {{
      "é•·å¥æ‡‰ç”¨": "å¾æ¨£æœ¬ç¸½çµçš„é•·å¥ä½¿ç”¨è¦å‰‡",
      "çŸ­å¥æ‡‰ç”¨": "å¾æ¨£æœ¬ç¸½çµçš„çŸ­å¥ä½¿ç”¨è¦å‰‡",
      "æ®µè½éŠœæ¥": "å¾æ¨£æœ¬ç¸½çµçš„æ®µè½é€£æ¥æ–¹å¼"
    }},
    "æ•˜äº‹å¼§ç·š": [
      "å¾æ¨£æœ¬æŠ½è±¡å‡ºçš„çµæ§‹æµç¨‹1",
      "å¾æ¨£æœ¬æŠ½è±¡å‡ºçš„çµæ§‹æµç¨‹2"
    ],
    "ç¯„ä¾‹çµæ§‹éˆ": [
      "å¾æ¨£æœ¬æŠ½è±¡å‡ºçš„çµæ§‹éˆ"
    ]
  }},
  "analysis_elements": {{
    "é•·å¥åŠŸèƒ½åˆ†é¡": ["å¾æ¨£æœ¬ç™¼ç¾çš„é•·å¥åŠŸèƒ½1", "å¾æ¨£æœ¬ç™¼ç¾çš„é•·å¥åŠŸèƒ½2"],
    "é•·å¥çµ„ç¹”æ¨¡å¼": ["å¾æ¨£æœ¬ç¸½çµçš„çµ„ç¹”æ¨¡å¼1", "å¾æ¨£æœ¬ç¸½çµçš„çµ„ç¹”æ¨¡å¼2"],
    "çŸ­å¥æ‡‰ç”¨å ´æ™¯": ["å¾æ¨£æœ¬ç¸½çµçš„æ‡‰ç”¨å ´æ™¯1", "å¾æ¨£æœ¬ç¸½çµçš„æ‡‰ç”¨å ´æ™¯2"],
    "é€£è²«ç­–ç•¥": ["å¾æ¨£æœ¬ç¸½çµçš„é€£è²«ç­–ç•¥1", "å¾æ¨£æœ¬ç¸½çµçš„é€£è²«ç­–ç•¥2"]
  }}
}}

è¼¸å‡ºé‡é»ï¼š
- åªè¨±æ ¹æ“šè²¼æ–‡ã€Œçµæ§‹ç‰¹å¾µã€åˆ†æï¼Œä¸å¾—å¸¶å…¥ä»»ä½•åŸæ–‡å…§å®¹
- å„æ•¸å€¼ç”¨åˆç†ç¯„åœè¡¨ç¤ºï¼ˆä¾‹å¦‚ã€Œ7-18å¥ã€ã€ã€Œ30-45%ã€ï¼‰
- åˆ†æé‡é»ç‚ºã€Œå¥å­é•·çŸ­ã€ã€ã€Œæ®µè½çµ„ç¹”ã€ã€ã€Œå‰å¾Œé€£è²«ã€ã€ã€Œæ·±åº¦æ®µè½ã€
- ç¯„ä¾‹åªç”¨ã€Œçµæ§‹éˆã€ï¼Œä¸å¯æ¶‰åŠå…·é«”å…§å®¹æˆ–ä¸»é¡Œ
"""

        messages = [
            {"role": "system", "content": "ä½ æ˜¯å°ˆæ¥­çš„æ–‡æœ¬çµæ§‹åˆ†æå¸«ï¼Œå°ˆæ³¨æ–¼å¾çµæ§‹ç‰¹å¾µä¸­æå–å¯æŒ‡å°AIå‰µä½œçš„æ¨¡æ¿ã€‚"},
            {"role": "user", "content": prompt}
        ]
        
        try:
            content = await chat_completion(
                messages=messages,
                model="gemini-2.0-flash",
                temperature=0.2,
                max_tokens=2000,
                provider="gemini"
            )
            return parse_llm_json_response(content)
        except Exception as e:
            print(f"é•·æ–‡æ¨¡æ¿ç”ŸæˆéŒ¯èª¤: {e}")
            return {"error": f"ç„¡æ³•ç”Ÿæˆé•·æ–‡æ¨¡æ¿: {str(e)}"}

    async def _analyze_pattern_structure(self, post_content: str, pattern_name: str, style_word: str, focus_word: str) -> Dict[str, Any]:
        """ç‚ºç‰¹å®šæ¨¡å¼åˆ†æçµæ§‹ï¼ˆåŸºæ–¼ç¾æœ‰å–®ç¯‡åˆ†æï¼Œä½†èª¿æ•´ç‚ºæ¨¡å¼å°å‘ï¼‰"""
        theme_word = self._get_random_variation("theme_words")
        
        prompt = f"""ä½ æ˜¯å°ˆæ¥­çš„æ–‡æœ¬çµæ§‹åˆ†æå¸«ã€‚è«‹{theme_word}åˆ†æä»¥ä¸‹ä»£è¡¨ã€Œ{pattern_name}ã€çš„è²¼æ–‡çµæ§‹ç‰¹å¾µï¼Œ{focus_word}ä¸¦å¾{style_word}è§’åº¦å¡«å¯«çµæ§‹æŒ‡å—ã€‚

é‡è¦è¦æ±‚ï¼š
- é‡é»åˆ†æç¬¦åˆã€Œ{pattern_name}ã€çš„çµæ§‹ç‰¹å¾µ
- å„æ•¸å€¼ç”¨åˆç†ç¯„åœè¡¨ç¤ºï¼ˆä¾‹å¦‚ã€Œ7-18å¥ã€ã€ã€Œ30-45%ã€ï¼‰
- åˆ†æé‡é»ç‚ºã€Œå¥å­é•·çŸ­ã€ã€ã€Œæ®µè½çµ„ç¹”ã€ã€ã€Œå‰å¾Œé€£è²«ã€ã€ã€Œæ·±åº¦æ®µè½ã€
- ç¯„ä¾‹åªç”¨ã€Œçµæ§‹éˆã€ï¼Œä¸å¯æ¶‰åŠå…·é«”å…§å®¹æˆ–ä¸»é¡Œ

ä»£è¡¨æ€§è²¼æ–‡å…§å®¹ï¼š
---
{post_content}
---

è«‹æŒ‰ç…§ä»¥ä¸‹æ ¼å¼åˆ†æä¸¦å›æ‡‰ï¼ˆJSONæ ¼å¼ï¼‰ï¼š

{{
  "pattern_specific_guide": {{
    "æ¨¡å¼åç¨±": "{pattern_name}",
    "ç¸½å¥æ•¸ç¯„åœ": "X-Yå¥",
    "å¹³å‡æ¯å¥å­—æ•¸": "A-Bå­—",
    "çŸ­å¥å®šç¾©": "C-Då­—",
    "é•·å¥å®šç¾©": "E-Få­—",
    "çŸ­å¥æ¯”ä¾‹": "P1-P2%",
    "é•·å¥æ¯”ä¾‹": "Q1-Q2%",
    "æ®µè½æ•¸é‡": "N1-N2æ®µ",
    "æ¯æ®µå¥æ•¸": "S1-S2å¥",
    "æ¨¡å¼ç‰¹è‰²": ["ç‰¹è‰²1", "ç‰¹è‰²2", "ç‰¹è‰²3"],
    "é©ç”¨æƒ…å¢ƒ": "æ­¤æ¨¡å¼æœ€é©åˆçš„ä½¿ç”¨å ´æ™¯"
  }},
  "analysis_metadata": {{
    "style_focus": "{style_word}",
    "analysis_focus": "{focus_word}",
    "theme_approach": "{theme_word}"
  }}
}}"""

        messages = [
            {"role": "system", "content": f"ä½ æ˜¯å°ˆæ¥­çš„æ–‡æœ¬çµæ§‹åˆ†æå¸«ï¼Œç‰¹åˆ¥æ“…é•·{style_word}åˆ†æï¼Œèƒ½å¤ {focus_word}é€²è¡Œ{theme_word}çš„çµæ§‹åˆ†æã€‚"},
            {"role": "user", "content": prompt}
        ]
        
        try:
            content = await chat_completion(
                messages=messages,
                model="gemini-2.0-flash",
                temperature=0.2,
                max_tokens=1000,
                provider="gemini"
            )
            return parse_llm_json_response(content)
        except Exception as e:
            print(f"æ¨¡å¼çµæ§‹åˆ†æéŒ¯èª¤: {e}")
            return {"error": f"ç„¡æ³•åˆ†ææ¨¡å¼çµæ§‹: {str(e)}"}
    
    async def _generate_unified_guide(self, pattern_analysis: Dict[str, Any], structure_templates: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ç¬¬ä¸‰å±¤ï¼šç”Ÿæˆçµ±åˆçš„çµæ§‹å‰µä½œæŒ‡å—"""
        prompt = f"""åŸºæ–¼å¤šç¨®çµæ§‹æ¨¡å¼çš„åˆ†æçµæœï¼Œç”Ÿæˆçµ±åˆçš„å‰µä½œç­–ç•¥æŒ‡å—ï¼š

ğŸ” æ¨¡å¼è­˜åˆ¥çµæœï¼š
{json.dumps(pattern_analysis, ensure_ascii=False, indent=2)}

ğŸ“‹ çµæ§‹æ¨¡æ¿é›†ï¼š
{json.dumps(structure_templates, ensure_ascii=False, indent=2)}

è«‹ç”ŸæˆåŒ…å«ä»¥ä¸‹å…§å®¹çš„çµ±åˆæŒ‡å—ï¼š

1. **æ¨¡å¼é¸æ“‡ç­–ç•¥**ï¼šæ ¹æ“šå‰µä½œéœ€æ±‚ï¼ˆå­—æ•¸é™åˆ¶ã€å…§å®¹é¡å‹ã€è¡¨é”ç›®æ¨™ï¼‰é¸æ“‡åˆé©çš„çµæ§‹æ¨¡å¼
2. **è·¨æ¨¡å¼æ‡‰ç”¨æŠ€å·§**ï¼šå¦‚ä½•åœ¨ä¸åŒæ¨¡å¼é–“åˆ‡æ›æˆ–çµ„åˆä½¿ç”¨
3. **çµæ§‹å„ªåŒ–å»ºè­°**ï¼šå¦‚ä½•é‡å°æ¯ç¨®æ¨¡å¼é€²è¡Œæ•ˆæœæå‡
4. **å¯¦æˆ°æ‡‰ç”¨æŒ‡å—**ï¼šé‡å°å¸¸è¦‹å‰µä½œå ´æ™¯çš„æ¨¡å¼æ¨è–¦

æ ¼å¼è¦æ±‚ï¼š
- åŸºæ–¼çµæ§‹æ¨¡æ¿çš„å¯¦ç”¨æŒ‡å°
- å¯ç›´æ¥ç”¨æ–¼AIå‰µä½œç³»çµ±çš„ç­–ç•¥å»ºè­°
- é‡é»åœ¨çµæ§‹ç‰¹å¾µï¼Œä¸æ¶‰åŠå…§å®¹ä¸»é¡Œ

{{
  "unified_creation_guide": {{
    "pattern_selection_strategy": {{
      "selection_criteria": ["é¸æ“‡æ¨™æº–1", "é¸æ“‡æ¨™æº–2"],
      "scenario_mapping": {{"æƒ…å¢ƒ1": "æ¨è–¦æ¨¡å¼", "æƒ…å¢ƒ2": "æ¨è–¦æ¨¡å¼"}}
    }},
    "structure_templates": [
      {{
        "pattern_name": "æ¨¡å¼åç¨±",
        "writing_steps": ["æ­¥é©Ÿ1", "æ­¥é©Ÿ2", "æ­¥é©Ÿ3"],
        "key_techniques": ["æŠ€å·§1", "æŠ€å·§2"],
        "example_structure": "ç¯„ä¾‹çµæ§‹æ¡†æ¶"
      }}
    ],
    "optimization_tips": {{
      "general_principles": ["é€šç”¨åŸå‰‡1", "é€šç”¨åŸå‰‡2"],
      "pattern_specific_tips": {{"æ¨¡å¼A": ["tip1", "tip2"]}}
    }},
    "hybrid_strategies": [
      "æ··åˆç­–ç•¥1ï¼šä½•æ™‚ä½•åœ°å¦‚ä½•çµåˆ",
      "æ··åˆç­–ç•¥2ï¼šé€²éšæ‡‰ç”¨æŠ€å·§"
    ],
    "practical_application": {{
      "content_type_recommendations": {{"å…§å®¹é¡å‹1": "å»ºè­°æ¨¡å¼åŠåŸå› "}},
      "common_pitfalls": ["å¸¸è¦‹éŒ¯èª¤1", "å¸¸è¦‹éŒ¯èª¤2"],
      "success_indicators": ["æˆåŠŸæŒ‡æ¨™1", "æˆåŠŸæŒ‡æ¨™2"]
    }}
  }},
  "meta_analysis": {{
    "analysis_approach": "çµæ§‹æ¨¡æ¿æ•´åˆåˆ†æ",
    "total_patterns_analyzed": {len(structure_templates)},
    "template_types": ["é•·æ–‡æ¨¡æ¿", "çŸ­æ–‡æ¨¡æ¿"]
  }}
}}"""

        messages = [
            {"role": "system", "content": "ä½ æ˜¯è³‡æ·±çš„çµæ§‹å‰µä½œç­–ç•¥é¡§å•ï¼Œå°ˆç²¾æ–¼æ•´åˆå¤šç¨®çµæ§‹æ¨¡æ¿ï¼Œåˆ¶å®šå¯¦ç”¨çš„å‰µä½œæŒ‡å—ã€‚"},
            {"role": "user", "content": prompt}
        ]
        
        try:
            content = await chat_completion(
                messages=messages,
                model="gemini-2.0-flash",
                temperature=0.4,  # ç¨é«˜çš„å‰µæ„æ€§ç”¨æ–¼ç”ŸæˆæŒ‡å—
                max_tokens=4096,
                provider="gemini"
            )
            return parse_llm_json_response(content)
        except Exception as e:
            print(f"çµ±åˆæŒ‡å—ç”ŸæˆéŒ¯èª¤: {e}")
            return {"error": f"ç„¡æ³•ç”Ÿæˆçµ±åˆæŒ‡å—: {str(e)}"}

    # ========= æ–°å¢ï¼šå¤šç¯‡æ‘˜è¦ï¼ˆæ‰¹é‡æ¨¡æ¿ç”¨ï¼‰ =========
    async def summarize_multi_posts(self, pattern_name: str, structure_guide: Dict[str, Any], sample_posts: List[str]) -> str:
        """æ‰¹é‡ç‰ˆæœ¬çš„ã€ç¬¬äºŒæ­¥ï¼šæ ¹æ“šçµæ§‹æŒ‡å—ç”Ÿæˆåˆ†ææ‘˜è¦ã€ã€‚
        èšç„¦æ–¼ï¼šè©²æ¨¡æ¿é¡å‹çš„ä¸»é¡Œå‚¾å‘ã€æƒ…ç·’/èªæ°£ã€ç¯€å¥èˆ‡æ®µè½å®‰æ’ã€ç”¨è©èˆ‡è¡¨é”ã€äº’å‹•ç­–ç•¥ï¼Œ
        ä¸¦ç”¢å‡ºå¯ç›´æ¥ç”¨æ–¼å¾ŒçºŒå¯«ä½œçš„ã€Œè¡€èˆ‡è‚‰ã€ç´šæ‘˜è¦èˆ‡å»ºè­°ã€‚
        """
        merged_samples = []
        for i, p in enumerate(sample_posts or [], 1):
            merged_samples.append(f"æ¨£æœ¬{i:02d}:\n{p}")
        merged_posts_text = "\n\n".join(merged_samples)
        guide_json = json.dumps(structure_guide or {}, ensure_ascii=False, indent=2)

        prompt = f"""è«‹æ ¹æ“šã€Œçµæ§‹æŒ‡å—ã€èˆ‡ã€Œå¤šç¯‡æ¨£æœ¬å…§å®¹ã€ï¼Œç‚ºã€{pattern_name}ã€é€™ä¸€æ¨¡æ¿é¡å‹ç”Ÿæˆå¯ç”¨æ–¼å¾ŒçºŒå¯«ä½œçš„ã€åˆ†ææ‘˜è¦ã€‘ï¼ˆè¡€èˆ‡è‚‰ï¼‰ã€‚

é‡è¦è¦æ±‚ï¼š
- å¾å¤šç¯‡æ¨£æœ¬æ­¸ç´ã€Œå…±é€šçš„ä¸»é¡Œå‚¾å‘ã€æƒ…ç·’å¼·åº¦èˆ‡èªæ°£åŸºèª¿ã€ç¯€å¥èˆ‡æ®µè½å®‰æ’ã€ç”¨è©èˆ‡è¡¨é”ã€äº’å‹•ç­–ç•¥ã€
- ä¸å¯ç›´æ¥æŠ„å¯«æ¨£æœ¬åŸå¥ï¼›å¦‚éœ€æŒ‡ç¨±å…·é«”å…ƒç´ ï¼Œè«‹ç”¨å ä½ç¬¦ï¼ˆå¦‚ã€Šä½œå“åã€‹ã€SxEyã€YYYY/MM/DDã€#æ¨™ç±¤ï¼‰
- å…è¨±æè¿°ã€Œæ­¤æ¨¡æ¿é¡å‹ã€çš„å¸¸è¦‹å…ƒç´ èˆ‡èªå¢ƒï¼ˆä¾‹å¦‚å½±è¦–ä½œå“æåŠå‹å¯æåŠï¼šä½œå“/é›†æ•¸/æ’­å‡ºæ™‚é–“ï¼‰ï¼Œä½†ä¸å¡«å…¥çœŸå¯¦å°ˆæœ‰åè©
- ç²¾ç…‰ã€å¯æ“ä½œï¼Œæ¯é»ä¸è¶…éä¸€è¡Œï¼›å¯è¢«ç›´æ¥æ‹¿å»æŒ‡å° AI å¯«ä½œ

ã€çµæ§‹æŒ‡å—ã€‘
{guide_json}

ã€å¤šç¯‡æ¨£æœ¬å…§å®¹ã€‘ï¼ˆä¾›ä½ è§€å¯Ÿå…±é€šè¦å¾‹ï¼Œä¸å¾—é€å­—å¼•ç”¨ï¼‰
{merged_posts_text}

è«‹ç”¨ Markdown æ¢åˆ—å›æ‡‰ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
ã€åˆ†æçµæœã€‘
1. ä¸»é¡Œå‚¾å‘ï¼ç´ æå…ƒç´ ï¼šæ­¤é¡è²¼æ–‡å¸¸åœç¹å“ªäº›é¡Œæèˆ‡è³‡è¨Šå…ƒç´ ï¼ˆç”¨å ä½ç¬¦ï¼Œä¸å¡«çœŸåï¼‰
2. æƒ…ç·’èˆ‡èªæ°£ï¼šå¼·åº¦ç¯„åœã€ä¸»å°èªæ°£ï¼ˆå¦‚ç†±è¡€ï¼å†·éœï¼å¹½é»˜ï¼æ­£å¼ï¼‰
3. ç¯€å¥èˆ‡æ®µè½ï¼šçŸ­é•·å¥ç¯€å¥ã€æ®µè½åˆ†å·¥ã€å¸¸ç”¨æ‰¿æ¥/è½‰æŠ˜èªå‹ã€æ”¶å°¾å¥å‹
4. ç”¨è©èˆ‡è¡¨é”ï¼šå£èªåº¦ã€å°ˆæ¥­è©å½™ã€emojiï¼hashtag æ¨¡å¼ã€å…¸å‹å¥å¼
5. äº’å‹•ç­–ç•¥ï¼šå¸¸è¦‹æå•ã€CTAã€äº’å‹•èª˜å› 

ã€æ”¹å¯«å»ºè­°ã€‘ï¼ˆ3â€“5é»ï¼Œå¯ç›´æ¥æ“ä½œï¼‰
- é‡å°æ­¤æ¨¡æ¿é¡å‹ï¼Œæå‡ºå…·é«”å¯åŸ·è¡Œçš„å„ªåŒ–ï¼æ“´å¯«æ–¹å¼

ã€ç™¼å±•æ–¹å‘ã€‘
1. æœ€æ¥è¿‘çš„é¢¨æ ¼æ–¹å‘ï¼ˆæ•…äº‹å‹ï¼è©•è«–å‹ï¼äº’å‹•å‹ï¼è³‡è¨Šå‹ï¼‰ï¼Œä¸¦ç°¡è¿°ç†ç”±
2. å…§å®¹è¡ŒéŠ·ï¼å“ç‰Œï¼SEOï¼åˆ†ç™¼å»ºè­°ï¼ˆå¦‚ #æ¨™ç±¤ï¼æ¬„ä½éœ²å‡ºï¼CTAï¼‰
3. å¯«ä½œé‡é»ï¼ˆ1â€“2 å¥ï¼‰
4. ç¯„ä¾‹å¥ï¼ˆç¬¦åˆæ­¤æ¨¡æ¿èˆ‡å®šä½ï¼›ä¸å¾—æŠ„æ¨£æœ¬åŸå¥ï¼Œå¯ç”¨å ä½ç¬¦ï¼‰
"""

        messages = [
            {"role": "system", "content": "ä½ æ˜¯ Threads è²¼æ–‡ç­–ç•¥èˆ‡å¯«ä½œé¡§å•ã€‚è«‹ä»¥ç¹é«”ä¸­æ–‡è¼¸å‡ºï¼Œå¾å¤šç¯‡æ¨£æœ¬æ­¸ç´å¯ç›´æ¥ç”¨æ–¼å‰µä½œçš„æ‘˜è¦èˆ‡å»ºè­°ï¼›ä¸å¾—é€å­—å¼•ç”¨æ¨£æœ¬åŸå¥ï¼Œå¯ç”¨å ä½ç¬¦ä»£è¡¨å°ˆæœ‰åè©ã€‚"},
            {"role": "user", "content": prompt}
        ]

        try:
            content = await chat_completion(
                messages=messages,
                model="gemini-2.0-flash",
                temperature=0.2,
                max_tokens=1500,
                provider="gemini"
            )
            return (content or "").strip()
        except Exception as e:
            return f"ç„¡æ³•ç”Ÿæˆæ‘˜è¦ï¼š{str(e)}"

# Example Usage (requires .env configuration)
async def main():
    try:
        from agents.ranker.ranker_logic import RankerAgent
        ranker = RankerAgent()
        ranked_result = await ranker.rank_posts(author_id="@victor31429", top_n=5)
        
        if ranked_result['status'] == 'error':
            print("Could not get ranked posts for testing.")
            print(f"RankerAgent Error: {ranked_result.get('message', 'No error message returned.')}")
            return

        top_post_urls = [p['url'] for p in ranked_result['ranked_posts']]
        analyzer = PostAnalyzerAgent()
        
        print("--- Testing Mode 1 (Live LLM) ---")
        mode1_result = await analyzer.analyze_posts(post_urls=top_post_urls[:1], analysis_mode=1)
        print(json.dumps(mode1_result, indent=2, ensure_ascii=False))

        print("\n--- Testing Mode 2 (Live LLM) ---")
        mode2_result = await analyzer.analyze_posts(post_urls=top_post_urls, analysis_mode=2)
        print(json.dumps(mode2_result, indent=2, ensure_ascii=False))

        print("\n--- Testing Mode 3 (Live LLM) ---")
        mode3_result = await analyzer.analyze_posts(post_urls=top_post_urls, analysis_mode=3)
        print(json.dumps(mode3_result, indent=2, ensure_ascii=False))

    except (ValueError, FileNotFoundError) as e:
        print(f"Configuration Error: {e}")
        print("Please ensure your .env file is correctly configured with LLM provider settings.")
    finally:
        db_client = await get_db_client()
        if db_client:
            await db_client.close_pool()

if __name__ == '__main__':
    asyncio.run(main()) 