"""
æ‰¹é‡åˆ†æå”èª¿å™¨
çµ±ç±Œèªæ–™åº«åˆ†æã€æ¨¡å¼ç”Ÿæˆå’Œçµæ§‹æ¨¡æ¿ç”Ÿæˆçš„å®Œæ•´æµç¨‹
"""

import json
from typing import List, Dict, Any
from datetime import datetime

from .corpus_analyzer import CorpusAnalyzer
from .pattern_generator import PatternGenerator
from .structure_templater import StructureTemplater
from common.llm_manager import chat_completion
from common.llm_client import parse_llm_json_response


class BatchAnalyzer:
    """æ‰¹é‡çµæ§‹åˆ†æå”èª¿å™¨"""
    
    def __init__(self):
        self.corpus_analyzer = CorpusAnalyzer()
        self.pattern_generator = PatternGenerator()
        self.structure_templater = StructureTemplater()
    
    async def analyze_batch_structure(self, posts_content: List[str], 
                                    username: str = "unknown") -> Dict[str, Any]:
        """åŸ·è¡Œå®Œæ•´çš„æ‰¹é‡çµæ§‹åˆ†ææµç¨‹"""
        try:
            # ç¬¬ä¸€éšæ®µï¼šèªæ–™åº«ç‰¹å¾µåˆ†æ
            corpus_features = self.corpus_analyzer.analyze_corpus_features(posts_content)
            
            # ç¬¬äºŒéšæ®µï¼šå‹•æ…‹æ¨¡å¼ç”Ÿæˆ
            min_groups = self._decide_min_groups(len(posts_content))
            applicable_patterns = self.pattern_generator.generate_applicable_patterns(
                corpus_features, min_groups
            )
            
            # ç¬¬ä¸‰éšæ®µï¼šLLMæ™ºèƒ½åˆ†çµ„
            pattern_analysis = await self._intelligent_pattern_recognition(
                posts_content, applicable_patterns, corpus_features
            )
            
            # ç¬¬å››éšæ®µï¼šçµæ§‹æ¨¡æ¿ç”Ÿæˆ
            structure_templates = await self.structure_templater.generate_structure_templates(
                posts_content, pattern_analysis, corpus_features
            )
            
            return {
                "status": "success",
                "username": username,
                "analysis_type": "intelligent_batch_structure_analysis",
                "pattern_count": len(pattern_analysis.get("identified_patterns", [])),
                "total_posts": len(posts_content),
                "corpus_features": corpus_features,
                "applicable_patterns": applicable_patterns,
                "pattern_analysis": pattern_analysis,
                "structure_templates": structure_templates,
                "analyzed_at": datetime.utcnow().isoformat()
            }
            
        except Exception as e:
            return {
                "status": "error", 
                "message": f"æ™ºèƒ½æ‰¹é‡çµæ§‹åˆ†æå¤±æ•—: {str(e)}",
                "error_details": str(e)
            }
    
    def _decide_min_groups(self, total_posts: int) -> int:
        """æ ¹æ“šç¸½è²¼æ–‡æ•¸æ±ºå®šæœ€ä½åˆ†çµ„æ•¸"""
        if total_posts >= 100:
            return 10
        elif total_posts >= 50:
            return 8
        elif total_posts >= 25:
            return 5
        else:
            return 3
    
    async def _intelligent_pattern_recognition(self, posts_content: List[str], 
                                             applicable_patterns: List[Dict[str, Any]], 
                                             corpus_features: Dict[str, Any]) -> Dict[str, Any]:
        """åŸºæ–¼èªæ–™åº«ç‰¹å¾µçš„æ™ºèƒ½æ¨¡å¼è­˜åˆ¥"""
        
        formatted_posts = self._format_multiple_posts(posts_content)
        
        # æ§‹å»ºç‰¹å¾µæè¿°
        features_desc = self._build_features_description(corpus_features)
        patterns_desc = self._build_patterns_description(applicable_patterns)
        
        prompt = f"""æ ¹æ“šèªæ–™åº«ç‰¹å¾µåˆ†æçµæœï¼Œå°‡{len(posts_content)}ç¯‡è²¼æ–‡æ™ºèƒ½åˆ†çµ„åˆ°åˆé©çš„çµæ§‹æ¨¡å¼ä¸­ã€‚

{formatted_posts}

ğŸ” èªæ–™åº«ç‰¹å¾µåˆ†æçµæœï¼š
{features_desc}

ğŸ“‹ å»ºè­°çš„é©ç”¨æ¨¡å¼ï¼š
{patterns_desc}

ğŸ¯ åˆ†çµ„è¦æ±‚ï¼š
1. åƒ…å°‡è²¼æ–‡åˆ†é…åˆ°ç¢ºå¯¦é©ç”¨çš„æ¨¡å¼ä¸­ï¼ˆæŸäº›æ¨¡å¼å¯èƒ½æ²’æœ‰å°æ‡‰è²¼æ–‡ï¼‰
2. å…è¨±åŒä¸€ç¯‡è²¼æ–‡å±¬æ–¼å¤šå€‹æ¨¡å¼ï¼ˆé‡ç–Šè¦†è“‹ï¼‰
3. æ¨¡å¼å‘½åé ˆåŸºæ–¼å¯¦éš›è§€å¯Ÿåˆ°çš„çµæ§‹ç‰¹å¾µ
4. æ¯å€‹æœ‰æ•ˆæ¨¡å¼è‡³å°‘åŒ…å«3ç¯‡è²¼æ–‡ï¼Œå¦å‰‡èªªæ˜åŸå› 
5. å¿…é ˆè¦†è“‹æ‰€æœ‰è²¼æ–‡ï¼Œä¸å¯éºæ¼

å›æ‡‰æ ¼å¼ï¼š
{{
  "analysis_summary": {{
    "total_posts": {len(posts_content)},
    "applicable_patterns_count": {len(applicable_patterns)},
    "feature_based_grouping": true,
    "overlap_allowed": true
  }},
  "identified_patterns": [
    {{
      "pattern_id": "A",
      "pattern_name": "åŸºæ–¼å¯¦éš›ç‰¹å¾µçš„å‘½å",
      "post_indices": [1, 3, 7, 12],
      "post_count": 4,
      "structure_characteristics": {{
        "dominant_feature": "ä¸»å°ç‰¹å¾µæè¿°",
        "å¥æ•¸ç¯„åœ": "X-Yå¥",
        "å­—æ•¸ç¯„åœ": "A-Bå­—",
        "æ®µè½ç‰¹å¾µ": "å¯¦éš›è§€å¯Ÿçµæœ",
        "ç‰¹æ®Šæ¨™è¨˜": "å°è©±/åˆ—é»/å¼•ç”¨ç­‰ï¼ˆå¦‚æœ‰ï¼‰",
        "ç¯€å¥ç‰¹å¾µ": "åŸºæ–¼å¯¦éš›åˆ†æ"
      }},
      "confidence": 0.85,
      "sample_indices": [3, 7],
      "notes": "åŸºæ–¼èªæ–™åº«ç‰¹å¾µçš„åˆ†çµ„ç†ç”±"
    }}
  ],
  "unused_patterns": [
    {{
      "pattern_name": "æœªä½¿ç”¨çš„æ¨¡å¼å",
      "reason": "èªæ–™åº«ä¸­ç„¡å°æ‡‰ç‰¹å¾µ"
    }}
  ]
}}"""

        messages = [
            {"role": "system", "content": "ä½ æ˜¯å°ˆæ¥­çš„çµæ§‹æ¨¡å¼è­˜åˆ¥å°ˆå®¶ï¼Œæ“…é•·æ ¹æ“šèªæ–™åº«ç‰¹å¾µé€²è¡Œæ™ºèƒ½åˆ†çµ„ï¼Œé¿å…ç”Ÿæˆä¸å­˜åœ¨çš„æ¨¡å¼ã€‚"},
            {"role": "user", "content": prompt}
        ]
        
        try:
            content = await chat_completion(
                messages=messages,
                model="gemini-2.0-flash",
                temperature=0.3,
                max_tokens=3000,
                provider="gemini"
            )
            result = parse_llm_json_response(content)
            
            # é©—è­‰çµæœçš„åˆç†æ€§
            self._validate_pattern_analysis(result, applicable_patterns, posts_content)
            
            return result
            
        except Exception as e:
            print(f"æ™ºèƒ½æ¨¡å¼è­˜åˆ¥éŒ¯èª¤: {e}")
            return {
                "error": f"æ™ºèƒ½æ¨¡å¼è­˜åˆ¥å¤±æ•—: {str(e)}",
                "fallback": "ä½¿ç”¨åŸºæœ¬åˆ†çµ„ç­–ç•¥"
            }
    
    def _format_multiple_posts(self, posts_content: List[str]) -> str:
        """æ ¼å¼åŒ–å¤šç¯‡è²¼æ–‡å…§å®¹"""
        formatted_posts = []
        for i, content in enumerate(posts_content, 1):
            formatted_posts.append(f"ã€è²¼æ–‡ {i:02d}ã€‘\n{content}")
        return "\n\n" + "="*50 + "\n\n".join(formatted_posts)
    
    def _build_features_description(self, corpus_features: Dict[str, Any]) -> str:
        """æ§‹å»ºç‰¹å¾µæè¿°"""
        features_list = []
        
        if corpus_features.get("has_dialogue"):
            rate = corpus_features["feature_coverage"]["dialogue_rate"]
            features_list.append(f"âœ… å°è©±ç‰¹å¾µï¼š{len(corpus_features['dialogue_posts'])}ç¯‡è²¼æ–‡å«å°è©± ({rate:.1%})")
        else:
            features_list.append("âŒ å°è©±ç‰¹å¾µï¼šèªæ–™åº«ä¸­ç„¡æ˜é¡¯å°è©±çµæ§‹")
        
        if corpus_features.get("has_bullet_points"):
            rate = corpus_features["feature_coverage"]["bullet_rate"]
            features_list.append(f"âœ… åˆ—é»ç‰¹å¾µï¼š{len(corpus_features['bullet_posts'])}ç¯‡è²¼æ–‡å«åˆ—é» ({rate:.1%})")
        else:
            features_list.append("âŒ åˆ—é»ç‰¹å¾µï¼šèªæ–™åº«ä¸­ç„¡åˆ—é»çµæ§‹")
        
        if corpus_features.get("has_quotes"):
            rate = corpus_features["feature_coverage"]["quote_rate"]
            features_list.append(f"âœ… å¼•ç”¨ç‰¹å¾µï¼š{len(corpus_features['quote_posts'])}ç¯‡è²¼æ–‡å«å¼•ç”¨ ({rate:.1%})")
        else:
            features_list.append("âŒ å¼•ç”¨ç‰¹å¾µï¼šèªæ–™åº«ä¸­ç„¡å¼•ç”¨çµæ§‹")
        
        if corpus_features.get("has_multi_paragraph"):
            rate = corpus_features["feature_coverage"]["multi_paragraph_rate"]
            features_list.append(f"âœ… å¤šæ®µç‰¹å¾µï¼š{len(corpus_features['multi_paragraph_posts'])}ç¯‡è²¼æ–‡ç‚ºå¤šæ®µ ({rate:.1%})")
        else:
            features_list.append("âŒ å¤šæ®µç‰¹å¾µï¼šèªæ–™åº«ä¸»è¦ç‚ºå–®æ®µçµæ§‹")
        
        # é•·åº¦åˆ†å¸ƒ
        length_dist = corpus_features.get("length_distribution", {})
        features_list.append(f"ğŸ“ é•·åº¦åˆ†å¸ƒï¼š{dict(length_dist)}")
        
        return "\n".join(features_list)
    
    def _build_patterns_description(self, applicable_patterns: List[Dict[str, Any]]) -> str:
        """æ§‹å»ºæ¨¡å¼æè¿°"""
        patterns_list = []
        
        for pattern in applicable_patterns:
            confidence = pattern.get("confidence", 0.5)
            estimated_posts = pattern.get("estimated_posts", 0)
            patterns_list.append(
                f"â€¢ {pattern['pattern_name']} (ä¿¡å¿ƒåº¦: {confidence:.1%}, é ä¼°: {estimated_posts}ç¯‡)"
            )
        
        return "\n".join(patterns_list)
    
    def _validate_pattern_analysis(self, result: Dict[str, Any], 
                                 applicable_patterns: List[Dict[str, Any]], 
                                 posts_content: List[str]):
        """é©—è­‰æ¨¡å¼åˆ†æçµæœçš„åˆç†æ€§"""
        if "identified_patterns" not in result:
            raise ValueError("LLMæœªè¿”å›identified_patterns")
        
        patterns = result["identified_patterns"]
        if len(patterns) == 0:
            raise ValueError("LLMæœªè­˜åˆ¥å‡ºä»»ä½•æ¨¡å¼")
        
        # æª¢æŸ¥æ˜¯å¦æ‰€æœ‰è²¼æ–‡éƒ½è¢«è¦†è“‹
        covered_posts = set()
        for pattern in patterns:
            post_indices = pattern.get("post_indices", [])
            covered_posts.update(post_indices)
        
        all_posts = set(range(1, len(posts_content) + 1))
        if not all_posts.issubset(covered_posts):
            missing = all_posts - covered_posts
            print(f"è­¦å‘Šï¼šè²¼æ–‡ {missing} æœªè¢«ä»»ä½•æ¨¡å¼è¦†è“‹")
        
        print(f"âœ… æ¨¡å¼é©—è­‰å®Œæˆï¼š{len(patterns)}å€‹æ¨¡å¼ï¼Œè¦†è“‹{len(covered_posts)}ç¯‡è²¼æ–‡")
