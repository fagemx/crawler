from typing import List, Dict, Any, Optional
import asyncio
import json
import time

from common.db_client import get_db_client
from services.rustfs_client import get_rustfs_client
from agents.vision.gemini_vision import GeminiVisionAnalyzer
from common.image_primary_filter import compute_rule_score, decide_is_primary


class MediaDescribeService:
    """æä¾›åª’é«”æè¿°çš„æ¸…å–®ç”Ÿæˆèˆ‡åŸ·è¡Œï¼ˆå¯«å› media_descriptionsï¼‰ã€‚"""

    def __init__(self):
        self.analyzer = GeminiVisionAnalyzer()
        
    async def _analyze_media_with_retry(self, media_bytes: bytes, mime_type: str, extra_text: str = None, max_retries: int = 3) -> Dict[str, Any]:
        """
        å¸¶é‡è©¦æ©Ÿåˆ¶çš„ Gemini API èª¿ç”¨
        
        Args:
            media_bytes: åª’é«”äºŒé€²åˆ¶è³‡æ–™
            mime_type: MIME é¡å‹
            extra_text: é¡å¤–æ–‡å­—ä¸Šä¸‹æ–‡
            max_retries: æœ€å¤§é‡è©¦æ¬¡æ•¸
            
        Returns:
            åˆ†æçµæœå­—å…¸
        """
        last_error = None
        
        for attempt in range(max_retries + 1):
            try:
                result = await self.analyzer.analyze_media(media_bytes, mime_type, extra_text)
                return result
                
            except Exception as e:
                error_str = str(e)
                error_lower = error_str.lower()
                last_error = e
                
                # æª¢æŸ¥æ˜¯å¦æ˜¯å¯é‡è©¦çš„éŒ¯èª¤
                retryable_errors = [
                    "500 an internal error has occurred",
                    "internal error has occurred",
                    "503 service unavailable", 
                    "502 bad gateway",
                    "429 too many requests",
                    "timeout",
                    "timed out",
                    "connection",
                    "network",
                    "resource has been exhausted"
                ]
                
                is_retryable = any(phrase in error_lower for phrase in retryable_errors)
                
                if not is_retryable or attempt == max_retries:
                    # ä¸å¯é‡è©¦çš„éŒ¯èª¤æˆ–å·²é”æœ€å¤§é‡è©¦æ¬¡æ•¸
                    raise e
                
                # å¯é‡è©¦éŒ¯èª¤ï¼Œç­‰å¾…å¾Œé‡è©¦
                wait_time = (2 ** attempt) + (attempt * 0.5)  # æŒ‡æ•¸é€€é¿ + æŠ–å‹•
                print(f"âš ï¸ Gemini API éŒ¯èª¤ (ç¬¬ {attempt + 1}/{max_retries + 1} æ¬¡)ï¼š{error_str}")
                print(f"ğŸ”„ ç­‰å¾… {wait_time:.1f} ç§’å¾Œé‡è©¦...")
                await asyncio.sleep(wait_time)
        
        # æ‡‰è©²ä¸æœƒåˆ°é€™è£¡ï¼Œä½†ä»¥é˜²è¬ä¸€
        raise last_error

    async def get_account_describe_stats(self, limit: int = 50) -> List[Dict[str, Any]]:
        """çµ±è¨ˆå„å¸³è™Ÿåª’é«”æè¿°ç¾æ³ï¼ˆå¾…æè¿°/å·²æè¿°ï¼‰ã€‚"""
        db = await get_db_client()
        query = f"""
        WITH mf AS (
            SELECT id, post_url, media_type
            FROM media_files
            WHERE download_status = 'completed'
        ),
        pwm AS (
            SELECT url, username FROM playwright_post_metrics
        ),
        joined AS (
            SELECT p.username, m.id AS media_id, m.media_type
            FROM mf m
            JOIN pwm p ON p.url = m.post_url
        ),
        joined_d AS (
            SELECT j.username, j.media_id, j.media_type,
                   CASE WHEN d.media_id IS NULL THEN 0 ELSE 1 END AS is_desc
            FROM joined j
            LEFT JOIN media_descriptions d ON d.media_id = j.media_id
        )
        SELECT username,
               COUNT(*) FILTER (WHERE media_type='image' AND is_desc=0) AS pending_images,
               COUNT(*) FILTER (WHERE media_type='video' AND is_desc=0) AS pending_videos,
               COUNT(*) FILTER (WHERE media_type='image' AND is_desc=1) AS completed_images,
               COUNT(*) FILTER (WHERE media_type='video' AND is_desc=1) AS completed_videos,
               COUNT(*) FILTER (WHERE is_desc=0) AS pending_total,
               COUNT(*) FILTER (WHERE is_desc=1) AS completed_total
        FROM joined_d
        GROUP BY username
        ORDER BY pending_total DESC, completed_total DESC
        LIMIT {limit}
        """
        return await db.fetch_all(query)

    async def build_describe_plan(
        self,
        username: str,
        media_types: List[str],
        sort_by: str = "none",
        top_k: Optional[int] = None,
        only_undesc: bool = True,
        only_primary: bool = True,
        primary_threshold: float = 0.7,
    ) -> List[Dict[str, Any]]:
        """å»ºç«‹æè¿°æ¸…å–®ï¼Œå›å‚³å« media_id/post_url/original_url/media_type çš„åˆ—è¡¨ã€‚"""
        db = await get_db_client()

        # å…ˆå–è²¼æ–‡é›†åˆï¼ˆæ’åº Top-Nï¼‰
        post_filter_cte = ""
        if top_k and isinstance(top_k, int) and top_k > 0:
            sort_expr_map = {
                "views": "COALESCE(views_count, 0)",
                "likes": "COALESCE(likes_count, 0)",
                "comments": "COALESCE(comments_count, 0)",
                "reposts": "COALESCE(reposts_count, 0)",
            }
            sort_expr = sort_expr_map.get(sort_by) if sort_by and sort_by != "none" else None
            order_clause = f"ORDER BY {sort_expr} DESC NULLS LAST" if sort_expr else "ORDER BY COALESCE(created_at, fetched_at, NOW()) DESC"
            post_filter_cte = f"""
                , top_posts AS (
                    SELECT url
                    FROM playwright_post_metrics
                    WHERE replace(lower(username),'@','') = replace(lower($1),'@','')
                    {order_clause}
                    LIMIT {top_k}
                )
                """

        base_posts = "WHERE replace(lower(username),'@','') = replace(lower($1),'@','')" if not post_filter_cte else "WHERE url IN (SELECT url FROM top_posts)"
        query = f"""
        WITH
        {post_filter_cte}
        base AS (
            SELECT url, username FROM playwright_post_metrics {base_posts}
        )
        , completed AS (
            SELECT mf.*
            FROM media_files mf
            WHERE mf.download_status = 'completed' AND mf.rustfs_url IS NOT NULL AND mf.rustfs_url != ''
        )
        , described AS (
            SELECT d.media_id FROM media_descriptions d
        )
        SELECT mf.id AS media_id, mf.post_url, mf.original_url, mf.media_type, mf.rustfs_url,
               mf.width, mf.height, mf.file_size,
               COALESCE((mf.metadata->>'primary_score')::float, NULL) AS primary_score,
               COALESCE((mf.metadata->>'is_primary')::bool, NULL) AS is_primary
        FROM completed mf
        JOIN base b ON b.url = mf.post_url
        WHERE mf.media_type = ANY($2)
        """

        mt_array = media_types
        rows = await db.fetch_all(query, username, mt_array)

        if only_undesc and rows:
            # ç›´æ¥éæ¿¾æ‰å·²å­˜åœ¨æè¿°çš„ media_idï¼ˆç”¨ IN å­æŸ¥è©¢é¿å…ä½”ä½ï¼‰
            ids = [r["media_id"] for r in rows]
            if ids:
                described_set = set([r['media_id'] for r in await db.fetch_all(f"SELECT media_id FROM media_descriptions WHERE media_id = ANY($1)", ids)])
                rows = [r for r in rows if r['media_id'] not in described_set]

        # è¦å‰‡ç¯©é¸ï¼ˆç¬¬ä¸€æ®µï¼‰ï¼šè‹¥ only_primaryï¼ˆåƒ…å°åœ–ç‰‡ç”Ÿæ•ˆï¼Œå½±ç‰‡ä¸éæ¿¾ï¼‰
        if only_primary and rows:
            # å°ç¼ºå°‘æ¨™è¨˜çš„é …ç›®é€²è¡Œè¦å‰‡æ‰“åˆ†ï¼Œä¸¦å³æ™‚å¯«å› metadataï¼ˆåƒ… imageï¼‰
            to_update = []
            for r in rows:
                if r.get("is_primary") is None and r.get("media_type") == 'image':
                    score, reason = compute_rule_score(r)
                    r["primary_score"] = score
                    r["primary_reason"] = reason
                    r["is_primary"] = decide_is_primary(score, primary_threshold)
                    to_update.append((r["media_id"], score, r["is_primary"], reason))

            if to_update:
                async with (await get_db_client()).get_connection() as conn:
                    for media_id, score, is_primary, reason in to_update:
                        # åˆä½µå¯«å…¥ metadata æ¬„ä½
                        await conn.execute(
                            """
                            UPDATE media_files
                            SET metadata = COALESCE(metadata, '{}'::jsonb) ||
                                           jsonb_build_object('primary_score', $2::text, 'is_primary', $3::text, 'primary_reason', $4::text)
                            WHERE id = $1
                            """,
                            media_id, str(score), 'true' if is_primary else 'false', reason
                        )

            # æœ€çµ‚æŒ‰ç…§æ¨™è¨˜/åˆ†æ•¸éæ¿¾ï¼šå½±ç‰‡ç›´æ¥ä¿ç•™ï¼›åœ–ç‰‡æ‰ä¾é–€æª»
            filtered = []
            for r in rows:
                if r.get("media_type") != 'image':
                    filtered.append(r)
                    continue
                score = r.get("primary_score")
                is_primary = r.get("is_primary")
                if is_primary is True:
                    filtered.append(r)
                elif is_primary is False:
                    continue
                elif score is not None and score >= primary_threshold:
                    filtered.append(r)
            rows = filtered

        # æœ€çµ‚ç¸½é‡ä¸Šé™ï¼šç¢ºä¿ Top-N ä¸æœƒè¶…éä½¿ç”¨è€…è¨­å®šçš„æ•¸é‡ï¼ˆä»¥åª’é«”æ•¸é‡ç‚ºå–®ä½ï¼‰
        if top_k and isinstance(top_k, int) and top_k > 0 and rows:
            rows = rows[:top_k]

        return rows

    async def run_describe(self, items: List[Dict[str, Any]], overwrite: bool = True, attach_post_text: bool = True) -> Dict[str, Any]:
        """åŸ·è¡Œåª’é«”æè¿°ï¼Œå¯«å…¥ media_descriptionsã€‚"""
        db = await get_db_client()
        client = await get_rustfs_client()  # è‹¥éœ€è¦å¾ RustFS è®€å›æª”æ¡ˆå¯æ“´å……

        success, failed = 0, 0
        details: List[Dict[str, Any]] = []

        # é è®€ post å…§å®¹ï¼ˆåœ–ç‰‡æè¿°è¦é™„ä¸»è²¼æ–‡å…§æ–‡ï¼‰
        post_text_cache: Dict[str, str] = {}

        async with db.get_connection() as conn:
            for item in items:
                media_id = item["media_id"]
                post_url = item["post_url"]
                original_url = item["original_url"]
                media_type = item["media_type"]
                try:
                    # 0. å¯¦æ™‚é‡è¤‡æª¢æŸ¥ï¼ˆé˜²æ­¢ä¸¦ç™¼ç«¶çˆ­å°è‡´é‡è¤‡æè¿°ï¼‰
                    if not overwrite:
                        existing = await conn.fetchrow("SELECT id FROM media_descriptions WHERE media_id = $1", media_id)
                        if existing:
                            details.append({"media_id": media_id, "status": "skipped", "reason": "already_described"})
                            continue

                    # 1. è‹¥ç‚ºåœ–ç‰‡ä¸”æ¨™è¨˜ç‚ºéä¸»è²¼åœ–ï¼Œå‰‡è·³é
                    if item.get("media_type") == 'image':
                        is_primary = item.get("is_primary")
                        if is_primary is False:
                            details.append({"media_id": media_id, "status": "skipped", "reason": "not_primary"})
                            continue

                    # 1. ä¸‹è¼‰åª’é«”ï¼ˆåƒ… RustFSï¼Œä¸å†å›é€€åŸå§‹ URLï¼‰
                    import httpx
                    media_bytes = None
                    mime = ""

                    rustfs_url = item.get("rustfs_url")
                    if rustfs_url:
                        try:
                            # åƒ…å…è¨±å¾ RustFS bucket è®€å–ï¼Œé¿å…èª¤ç”¨åŸå§‹ Instagram/FBCDN é€£çµ
                            prefix = f"{client.base_url}/{client.bucket_name}/"
                            if not rustfs_url.startswith(prefix):
                                details.append({"media_id": media_id, "status": "skipped", "error": "invalid_rustfs_url_not_in_bucket"})
                                continue
                            key = rustfs_url[len(prefix):]
                            presigned = client.get_public_or_presigned_url(key, prefer_presigned=True)
                            async with httpx.AsyncClient(timeout=60.0) as http:
                                resp = await http.get(presigned, follow_redirects=True)
                                resp.raise_for_status()
                                media_bytes = resp.content
                                mime = resp.headers.get("content-type", "") or mime
                        except Exception as e:
                            # ç„¡æ³•å¾ RustFS è®€å– â†’ è·³é
                            details.append({"media_id": media_id, "status": "skipped", "error": f"rustfs_unavailable: {str(e)}"})
                            continue
                    else:
                        # æ²’æœ‰ rustfs_urlï¼ˆç†è«–ä¸Šä¸æœƒå‡ºç¾ï¼Œå› ç‚ºä¸Šé¢å·²éæ¿¾ï¼‰ï¼Œä¿éšªèµ·è¦‹ä¹Ÿè·³é
                        details.append({"media_id": media_id, "status": "skipped", "error": "no_rustfs_url"})
                        continue

                    # 2. æº–å‚™æç¤ºï¼ˆåœ–ç‰‡é™„ä¸»è²¼æ–‡å…§æ–‡ï¼‰
                    extra_text = ""
                    if attach_post_text and media_type == 'image':
                        if post_url not in post_text_cache:
                            row = await conn.fetchrow("SELECT content FROM playwright_post_metrics WHERE url = $1", post_url)
                            post_text_cache[post_url] = (row["content"] if row and row["content"] else "")
                        if post_text_cache[post_url]:
                            extra_text = f"è²¼æ–‡å…§æ–‡ï¼š\n{post_text_cache[post_url]}"

                    # 3. å‘¼å« Gemini (å¸¶é‡è©¦æ©Ÿåˆ¶)
                    if media_type == 'image':
                        # å°‡è²¼æ–‡åŸæ–‡ä½œç‚º extra_text æä¾›çµ¦æ¨¡å‹ï¼Œæ”¹å–„æƒ…å¢ƒåˆ¤è®€
                        result = await self._analyze_media_with_retry(media_bytes, mime or 'image/jpeg', extra_text=extra_text)
                        prompt_text = self.analyzer.image_prompt + ("\n\n" + (f"è²¼æ–‡åŸæ–‡ï¼š\n{extra_text}" if extra_text else ""))
                    else:
                        result = await self._analyze_media_with_retry(media_bytes, mime or 'video/mp4')
                        prompt_text = self.analyzer.video_prompt

                    # 4. è¦æ•´è¼¸å‡ºç‚º JSONï¼ˆå…è¨±æ¨¡å‹å›å‚³æ–‡å­—æ™‚åŒ…è£ï¼‰
                    if not isinstance(result, (dict, list)):
                        try:
                            result = json.loads(str(result))
                        except Exception:
                            result = {"raw": str(result)}

                    # 5. åŸå­æ€§è¦†è“‹æˆ–æ–°å¢ï¼ˆä½¿ç”¨ transaction + advisory lock é˜²æ­¢ä¸¦ç™¼é‡è¤‡ï¼‰
                    async with conn.transaction():
                        # æ¯å€‹ media_id æ‹¿ä¸€æŠŠäº¤æ˜“ç´šåˆ¥é–ï¼Œåºåˆ—åŒ–åŒä¸€è³‡æºçš„ä¸¦ç™¼å¯«å…¥
                        # æ˜ç¢ºè½‰æ›ç‚º bigint é¿å…é¡å‹æ¨æ–·è¡çª
                        await conn.execute("SELECT pg_advisory_xact_lock($1::bigint)", int(media_id))

                        if overwrite:
                            # è¦†è“‹æ¨¡å¼ï¼šå…ˆåˆªé™¤ï¼Œå†æ’å…¥
                            await conn.execute(
                                """
                                DELETE FROM media_descriptions WHERE media_id = $1
                                """,
                                int(media_id)
                            )
                            await conn.execute(
                                """
                                INSERT INTO media_descriptions
                                (media_id, post_url, username, media_type, model, prompt, response_json, language, status, created_at)
                                SELECT $1, mf.post_url, pwm.username, $2, $3, $4, $5, 'zh-TW', 'completed', NOW()
                                FROM media_files mf
                                JOIN playwright_post_metrics pwm ON pwm.url = mf.post_url
                                WHERE mf.id = $1
                                """,
                                int(media_id), media_type, "gemini-2.5-pro", prompt_text, json.dumps(result, ensure_ascii=False)
                            )
                        else:
                            # éè¦†è“‹ï¼šåƒ…åœ¨ä¸å­˜åœ¨æ™‚æ’å…¥
                            await conn.execute(
                                """
                                INSERT INTO media_descriptions
                                (media_id, post_url, username, media_type, model, prompt, response_json, language, status, created_at)
                                SELECT $1, mf.post_url, pwm.username, $2, $3, $4, $5, 'zh-TW', 'completed', NOW()
                                FROM media_files mf
                                JOIN playwright_post_metrics pwm ON pwm.url = mf.post_url
                                WHERE mf.id = $1
                                  AND NOT EXISTS (SELECT 1 FROM media_descriptions d WHERE d.media_id = $1)
                                """,
                                int(media_id), media_type, "gemini-2.5-pro", prompt_text, json.dumps(result, ensure_ascii=False)
                            )

                    success += 1
                    details.append({"media_id": media_id, "status": "completed"})
                except Exception as e:
                    failed += 1
                    details.append({"media_id": media_id, "status": "failed", "error": str(e)})

        return {"total": len(items), "success": success, "failed": failed, "details": details}


    async def get_undesc_summary_by_user(self, username: str, media_types: List[str], limit: int = 20) -> List[Dict[str, Any]]:
        """æŸ¥è©¢æŸå¸³è™Ÿå°šæœªæè¿°çš„è²¼æ–‡æ‘˜è¦ï¼ˆæ¯è²¼æ–‡æœªæè¿°åª’é«”æ•¸ï¼‰ã€‚"""
        db = await get_db_client()
        query = """
        SELECT mf.post_url,
               COUNT(*) FILTER (WHERE mf.media_type='image') AS pending_images,
               COUNT(*) FILTER (WHERE mf.media_type='video') AS pending_videos,
               COUNT(*) AS pending_total
        FROM media_files mf
        JOIN playwright_post_metrics pwm ON pwm.url = mf.post_url
        WHERE replace(lower(pwm.username),'@','') = replace(lower($1),'@','')
          AND mf.download_status = 'completed'
          AND mf.media_type = ANY($2)
          AND NOT EXISTS (
            SELECT 1 FROM media_descriptions d WHERE d.media_id = mf.id
          )
        GROUP BY mf.post_url
        ORDER BY pending_total DESC
        LIMIT $3
        """
        return await db.fetch_all(query, username, media_types, limit)

    async def get_descriptions_by_post(self, post_url: str) -> List[Dict[str, Any]]:
        """å–å¾—å–®ç¯‡è²¼æ–‡çš„æè¿°çµæœåˆ—è¡¨ã€‚"""
        db = await get_db_client()
        query = """
        SELECT d.media_id, d.post_url, d.media_type, d.model, d.prompt, d.response_json, d.status, d.created_at,
               mf.original_url, mf.rustfs_url
        FROM media_descriptions d
        JOIN media_files mf ON mf.id = d.media_id
        WHERE d.post_url = $1
        ORDER BY d.created_at DESC
        """
        return await db.fetch_all(query, post_url)

    async def get_recent_descriptions_by_user(self, username: str, media_types: List[str], limit: int = 50) -> List[Dict[str, Any]]:
        """å–å¾—æŸå¸³è™Ÿæœ€è¿‘çš„æè¿°çµæœï¼ˆç€è¦½æˆæœç”¨ï¼‰ã€‚"""
        db = await get_db_client()
        query = """
        SELECT d.media_id, d.post_url, d.media_type, d.model, d.response_json, d.status, d.created_at,
               mf.original_url, mf.rustfs_url
        FROM media_descriptions d
        JOIN media_files mf ON mf.id = d.media_id
        JOIN playwright_post_metrics pwm ON pwm.url = d.post_url
        WHERE replace(lower(pwm.username),'@','') = replace(lower($1),'@','')
          AND d.media_type = ANY($2)
        ORDER BY d.created_at DESC
        LIMIT $3
        """
        return await db.fetch_all(query, username, media_types, limit)

    async def get_pending_media_by_user(self, username: str, media_types: List[str], limit: int = 50) -> List[Dict[str, Any]]:
        """å–å¾—æŸå¸³è™Ÿå°šæœªæè¿°çš„åª’é«”ï¼ˆç€è¦½å…§å®¹ç”¨ï¼‰ã€‚"""
        db = await get_db_client()
        query = """
        SELECT mf.id AS media_id, mf.post_url, mf.media_type, mf.original_url, mf.rustfs_url,
               mf.width, mf.height, mf.file_size
        FROM media_files mf
        JOIN playwright_post_metrics pwm ON pwm.url = mf.post_url
        WHERE replace(lower(pwm.username),'@','') = replace(lower($1),'@','')
          AND mf.download_status = 'completed'
          AND mf.media_type = ANY($2)
          AND NOT EXISTS (SELECT 1 FROM media_descriptions d WHERE d.media_id = mf.id)
        ORDER BY mf.id DESC
        LIMIT $3
        """
        return await db.fetch_all(query, username, media_types, limit)

    async def describe_single_post(self,
        post_url: str,
        media_types: List[str],
        only_primary: bool = True,
        primary_threshold: float = 0.7,
        overwrite: bool = True,
    ) -> Dict[str, Any]:
        """é‡å°å–®ç¯‡è²¼æ–‡å»ºç«‹æ¸…å–®ä¸¦åŸ·è¡Œæè¿°ã€‚"""
        db = await get_db_client()

        def _normalize_threads_url(u: str) -> str:
            try:
                u = u.strip()
                # åŸŸå threads.com â†’ threads.net
                u = u.replace("threads.com", "threads.net")
                # å»æ‰å°¾ç«¯å¤šé¤˜æ–œç·š
                if u.endswith('/'):
                    u = u[:-1]
            except Exception:
                pass
            return u

        norm_url = _normalize_threads_url(post_url)

        async def _fetch_by_exact(url: str):
            # å–®ç¯‡ç«‹å³æè¿°ï¼šæ”¾å¯¬ç‚ºä¸å¼·åˆ¶å·²å®Œæˆä¸‹è¼‰ï¼Œå…è¨±ç›´æ¥ç”¨ original_url æè¿°
            return await db.fetch_all(
                """
                SELECT id AS media_id, post_url, original_url, media_type, rustfs_url,
                       width, height, file_size,
                       COALESCE((metadata->>'primary_score')::float, NULL) AS primary_score,
                       COALESCE((metadata->>'is_primary')::bool, NULL) AS is_primary
                FROM media_files
                WHERE post_url = $1 AND media_type = ANY($2)
                """,
                url, media_types
            )

        rows = await _fetch_by_exact(norm_url)

        # è‹¥æ‰¾ä¸åˆ°ï¼Œå˜—è©¦ä»¥ shortcode æ¨¡ç³ŠæŸ¥æ‰¾
        if not rows:
            try:
                from urllib.parse import urlparse
                p = urlparse(norm_url)
                parts = [seg for seg in p.path.split('/') if seg]
                shortcode = parts[-1] if parts else None
            except Exception:
                shortcode = None

            if shortcode:
                rows = await db.fetch_all(
                    """
                    SELECT id AS media_id, post_url, original_url, media_type, rustfs_url,
                           width, height, file_size,
                           COALESCE((metadata->>'primary_score')::float, NULL) AS primary_score,
                           COALESCE((metadata->>'is_primary')::bool, NULL) AS is_primary
                    FROM media_files
                    WHERE media_type = ANY($1)
                      AND (post_url ILIKE '%'||$2||'%' OR original_url ILIKE '%'||$2||'%')
                    ORDER BY id DESC
                    LIMIT 50
                    """,
                    media_types, shortcode
                )

        # éæ¿¾æœªæè¿°çš„ï¼šåƒ…åœ¨éè¦†è“‹æ¨¡å¼æ™‚æ‰æ’é™¤å·²æè¿°
        if rows and not overwrite:
            ids = [r["media_id"] for r in rows]
            placeholders = ",".join([str(int(i)) for i in ids])
            desc_rows = await db.fetch_all(
                f"SELECT media_id FROM media_descriptions WHERE media_id IN ({placeholders})"
            )
            described = {d["media_id"] for d in desc_rows}
            rows = [r for r in rows if r["media_id"] not in described]

        # ä¸»è²¼åœ–ç¯©é¸ï¼ˆåƒ…å½±éŸ¿åœ–ç‰‡ï¼Œå½±ç‰‡ä¸å—é™ï¼‰
        if only_primary and rows:
            filtered = []
            for r in rows:
                if r.get("media_type") != 'image':
                    filtered.append(r)
                    continue
                score = r.get("primary_score")
                is_primary = r.get("is_primary")
                if is_primary is True or (score is not None and score >= primary_threshold):
                    filtered.append(r)
            rows = filtered

        # å–®ç¯‡æ¨¡å¼ï¼šå¼·åˆ¶ä»¥ä¸¦ç™¼æ•¸ 1 çš„èªç¾©åŸ·è¡Œï¼ˆä¸Šå±¤ UI å·²å›ºå®šï¼Œé€™è£¡ç¶­æŒé€é …é †åºè™•ç†ï¼‰
        return await self.run_describe(rows, overwrite=overwrite)

