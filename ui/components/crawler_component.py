"""
Threads çˆ¬èŸ²çµ„ä»¶
åŸºæ–¼ test_playwright_agent.py çš„çœŸå¯¦åŠŸèƒ½
"""

import streamlit as st
import httpx
import json
import os
import asyncio
import time
from pathlib import Path
from typing import Dict, Any, Optional


class ThreadsCrawlerComponent:
    def __init__(self):
        self.agent_url = "http://localhost:8006/v1/playwright/crawl"
        self.orchestrator_url = "http://localhost:8000"  # ğŸ”¥ æ·»åŠ SSEæœå‹™å™¨URL
        # ä½¿ç”¨çµ±ä¸€çš„é…ç½®ç®¡ç†
        from common.config import get_auth_file_path
        self.auth_file_path = get_auth_file_path(from_project_root=True)
    
    def render(self):
        """æ¸²æŸ“çˆ¬èŸ²ç•Œé¢"""
        st.header("ğŸ•·ï¸ Threads å…§å®¹çˆ¬èŸ²")
        st.markdown("åŸºæ–¼ Playwright Agent çš„çœŸå¯¦ Threads çˆ¬èŸ²ï¼Œæ”¯æŒ SSE å¯¦æ™‚é€²åº¦é¡¯ç¤ºã€‚")
        
        # æª¢æŸ¥èªè­‰æ–‡ä»¶
        if not self._check_auth_file():
            st.error("âŒ æ‰¾ä¸åˆ°èªè­‰æª”æ¡ˆ")
            st.info("è«‹å…ˆåŸ·è¡Œ: `python tests/threads_fetch/save_auth.py` ä¾†ç”¢ç”Ÿèªè­‰æª”æ¡ˆ")
            return
        
        st.success("âœ… èªè­‰æª”æ¡ˆå·²å°±ç·’")
        
        # çˆ¬èŸ²é…ç½®
        self._render_crawler_config()
        
        # æ ¹æ“šç‹€æ…‹æ¸²æŸ“ä¸åŒç•Œé¢
        status = st.session_state.get('crawler_status', 'idle')
        
        if status == 'running':
            target = st.session_state.get('crawler_target', {})
            username = target.get('username', 'unknown')
            max_posts = target.get('max_posts', 0)
            current_progress = st.session_state.get('crawler_progress', 0)
            
            # é¡¯ç¤ºç•¶å‰é€²åº¦æ¦‚è¦½
            col1, col2 = st.columns([3, 1])
            with col1:
                st.info(f"ğŸ”„ æ­£åœ¨çˆ¬å– @{username} çš„è²¼æ–‡...")
                if current_progress > 0 and max_posts > 0:
                    estimated = int(current_progress * max_posts)
                    st.write(f"ğŸ“Š é€²åº¦: {estimated}/{max_posts} ç¯‡ ({current_progress:.1%})")
                else:
                    st.write("ğŸ“Š æº–å‚™ä¸­...")
            
            with col2:
                st.metric("é€²åº¦", f"{current_progress:.1%}")
            
            st.info("ğŸ’¡ **å³æ™‚é€²åº¦åé¥‹**ï¼šè«‹æŸ¥çœ‹å·¦å´é‚Šæ¬„ä¸‹æ–¹çš„ã€ŒğŸ“Š çˆ¬èŸ²é€²åº¦ã€å€åŸŸï¼Œæ¯2ç§’è‡ªå‹•æ›´æ–°ï¼")
            
            # é¡¯ç¤ºæœ€è¿‘çš„å¹¾æ¢æ—¥èªŒ
            logs = st.session_state.get('crawler_logs', [])
            if logs:
                st.subheader("ğŸ“ æœ€è¿‘æ´»å‹•")
                for log in logs[-3:]:  # æœ€è¿‘3æ¢
                    st.write(f"â€¢ {log}")
            
            # ğŸ”¥ å³æ™‚è²¼æ–‡é è¦½ï¼ˆä¸»å…§å®¹å€åŸŸï¼‰
            posts = st.session_state.get('crawler_posts', [])
            if posts:
                st.markdown("---")
                st.subheader("ğŸ“ å³æ™‚è²¼æ–‡é è¦½")
                
                # é¡¯ç¤ºæœ€æ–°çš„3å€‹è²¼æ–‡
                recent_posts = posts[-3:]
                
                for post in recent_posts:
                    # ä½¿ç”¨å¡ç‰‡æ¨£å¼é¡¯ç¤ºè²¼æ–‡
                    with st.container():
                        st.markdown(f"**ğŸ†” {post.get('summary', 'N/A')}** `{post.get('timestamp', 'N/A')}`")
                        
                        # é¡¯ç¤ºè²¼æ–‡è©³æƒ…
                        col1, col2 = st.columns([2, 1])
                        with col1:
                            # å…§å®¹é è¦½
                            content_preview = post.get('content_preview', post.get('content', ''))
                            if content_preview:
                                st.write(f"ğŸ’¬ {content_preview}")
                            else:
                                st.write("ğŸ’¬ ç„¡å…§å®¹")
                            
                            # åª’é«”ä¿¡æ¯
                            images_count = post.get('images_count', 0)
                            videos_count = post.get('videos_count', 0)
                            if images_count > 0 or videos_count > 0:
                                st.write(f"ğŸ“¸ åœ–ç‰‡: {images_count} | ğŸ¥ å½±ç‰‡: {videos_count}")
                        
                        with col2:
                            # ğŸ”¥ å®Œæ•´çš„çµ±è¨ˆæ•¸æ“š
                            likes_count = post.get('likes_count', 0)
                            comments_count = post.get('comments_count', 0)
                            reposts_count = post.get('reposts_count', 0)
                            shares_count = post.get('shares_count', 0)
                            views_count = post.get('views_count', 0)
                            calculated_score = post.get('calculated_score', 0)
                            
                            st.write(f"â¤ï¸ è®š: {likes_count:,}")
                            st.write(f"ğŸ’¬ ç•™è¨€: {comments_count:,}")
                            st.write(f"ğŸ”„ è½‰ç™¼: {reposts_count:,}")
                            st.write(f"ğŸ“¤ åˆ†äº«: {shares_count:,}")
                            st.write(f"ğŸ‘ï¸ ç€è¦½: {views_count:,}")
                            st.write(f"â­ åˆ†æ•¸: {calculated_score:.1f}")
                        
                        st.markdown("---")
        elif status == 'completed':
            self._render_crawler_results()
        elif status == 'error':
            st.error("âŒ çˆ¬èŸ²åŸ·è¡Œå¤±æ•—ï¼Œè«‹æª¢æŸ¥æ—¥èªŒ")
            self._render_crawler_logs()
    
    def _check_auth_file(self) -> bool:
        """æª¢æŸ¥èªè­‰æ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
        return self.auth_file_path.exists()
    
    def _render_crawler_config(self):
        """æ¸²æŸ“çˆ¬èŸ²é…ç½®ç•Œé¢"""
        col1, col2 = st.columns([2, 1])
        
        with col1:
            username = st.text_input(
                "Threads ç”¨æˆ¶åç¨±ï¼š",
                placeholder="ä¾‹å¦‚ï¼šnatgeo",
                help="è¼¸å…¥ä¸å« @ ç¬¦è™Ÿçš„ç”¨æˆ¶åç¨±",
                key="crawler_username"
            )
        
        with col2:
            max_posts = st.number_input(
                "çˆ¬å–è²¼æ–‡æ•¸é‡ï¼š",
                min_value=1,
                max_value=200,
                value=10,
                help="å»ºè­°ï¼š10ç¯‡ç©©å®šï¼Œ50ç¯‡å¯èƒ½è¼ƒæ…¢ï¼Œæœ€å¤š200ç¯‡",
                key="crawler_max_posts"
            )
        
        # æ§åˆ¶æŒ‰éˆ•
        col1, col2, col3 = st.columns([1, 1, 2])
        
        with col1:
            if st.button(
                "ğŸš€ é–‹å§‹çˆ¬å–", 
                type="primary",
                disabled=st.session_state.get('crawler_status') == 'running',
                use_container_width=True
            ):
                if username.strip():
                    self._start_crawler(username, max_posts)
                else:
                    st.warning("è«‹è¼¸å…¥ç”¨æˆ¶åç¨±")
        
        with col2:
            if st.button("ğŸ”„ é‡ç½®", use_container_width=True):
                self._reset_crawler()
                st.rerun()
    
    def _start_crawler(self, username: str, max_posts: int):
        """å•Ÿå‹•çˆ¬èŸ²"""
        # åˆå§‹åŒ–ç‹€æ…‹
        st.session_state.crawler_status = 'running'
        st.session_state.crawler_target = {"username": username, "max_posts": max_posts}
        st.session_state.crawler_logs = []
        st.session_state.crawler_posts = []  # è²¼æ–‡åˆ—è¡¨
        st.session_state.crawler_events = []
        st.session_state.final_data = None
        st.session_state.crawler_progress = 0
        st.session_state.crawler_current_work = "æ­£åœ¨åˆå§‹åŒ–çˆ¬èŸ²..."
        
        # ğŸ”¥ ç«‹å³è¨˜éŒ„å•Ÿå‹•æ—¥èªŒï¼Œç¢ºä¿å´é‚Šæ¬„æœ‰å…§å®¹é¡¯ç¤º
        st.session_state.crawler_logs.append("ğŸš€ çˆ¬èŸ²å·²å•Ÿå‹•ï¼Œæ­£åœ¨åˆå§‹åŒ–...")
        st.session_state.crawler_logs.append(f"ğŸ¯ ç›®æ¨™: @{username} ({max_posts} ç¯‡)")
        
        # ğŸ”¥ ç”Ÿæˆä»»å‹™IDï¼Œç¢ºä¿ has_task æ¢ä»¶æ»¿è¶³
        import uuid
        task_id = str(uuid.uuid4())
        st.session_state.crawler_task_id = task_id
        
        # ğŸ”¥ åˆå§‹åŒ–èª¿è©¦ä¿¡æ¯
        if 'debug_messages' not in st.session_state:
            st.session_state.debug_messages = []
        st.session_state.debug_messages.append(f"ğŸš€ çˆ¬èŸ²å•Ÿå‹•: task_id={task_id[:8]}")
        
        # ğŸ”¥ ç«‹å³å‰µå»ºé€²åº¦æ–‡ä»¶ï¼Œç¢ºä¿å´é‚Šæ¬„æ¢ä»¶æª¢æŸ¥ç”Ÿæ•ˆ
        import tempfile
        import json
        import time
        import os
        
        progress_file = tempfile.NamedTemporaryFile(mode='w+', delete=False, suffix=f'_{task_id}.json')
        progress_file_path = progress_file.name
        progress_file.close()
        st.session_state.crawler_progress_file = progress_file_path
        
        # å¯«å…¥åˆå§‹é€²åº¦
        initial_progress = {
            "stage": "initialization",
            "progress": 0.0,
            "status": "running",
            "current_work": "æ­£åœ¨åˆå§‹åŒ–çˆ¬èŸ²...",
            "task_id": task_id,
            "timestamp": time.time()
        }
        with open(progress_file_path, 'w') as f:
            json.dump(initial_progress, f)
            f.flush()
            os.fsync(f.fileno())
        
        print(f"ğŸ”¥ çˆ¬èŸ²å•Ÿå‹•: status={st.session_state.crawler_status}, task_id={task_id[:8]}, target={username}")
        print(f"ğŸ”¥ é€²åº¦æ–‡ä»¶å·²å‰µå»º: {progress_file_path}")
        
        # è®€å–èªè­‰æ–‡ä»¶
        try:
            with open(self.auth_file_path, "r", encoding="utf-8") as f:
                auth_content = json.load(f)
        except Exception as e:
            st.error(f"âŒ ç„¡æ³•è®€å–èªè­‰æª”æ¡ˆ: {e}")
            st.session_state.crawler_status = 'error'
            st.rerun()
            return
        
        # å•Ÿå‹•çœŸå¯¦çš„çˆ¬èŸ²ä»»å‹™
        st.success("ğŸš€ çˆ¬èŸ²å·²å•Ÿå‹•ï¼å³å°‡é–‹å§‹çˆ¬å–...")
        
        # ğŸ”¥ ä¿®å¾©ï¼šå‚³éå¿…è¦åƒæ•¸åˆ°å¾Œå°ç·šç¨‹ï¼Œé¿å…session_stateè·¨ç·šç¨‹å•é¡Œ
        task_id = st.session_state.crawler_task_id
        progress_file = st.session_state.crawler_progress_file
        
        def crawler_worker():
            """å¾Œå°çˆ¬èŸ²å·¥ä½œç·šç¨‹"""
            try:
                import asyncio
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                loop.run_until_complete(self._execute_crawler_safe(username, max_posts, auth_content, task_id, progress_file))
            except Exception as e:
                # ç›´æ¥å¯«å…¥é€²åº¦æ–‡ä»¶ï¼Œé¿å…session_stateå•é¡Œ
                import json
                import time
                error_data = {
                    "stage": "error",
                    "error": str(e),
                    "timestamp": time.time()
                }
                try:
                    with open(progress_file, 'w', encoding='utf-8') as f:
                        json.dump(error_data, f, ensure_ascii=False)
                        f.flush()
                except:
                    pass
                print(f"âŒ çˆ¬èŸ²åŸ·è¡Œå¤±æ•—: {e}")
        
        # åœ¨å¾Œå°ç·šç¨‹å•Ÿå‹•çˆ¬èŸ²ï¼Œé¿å…é˜»å¡UI
        import threading
        threading.Thread(target=crawler_worker, daemon=True).start()
        st.info("âš¡ çˆ¬èŸ²å·²åœ¨å¾Œå°å•Ÿå‹•ï¼Œè«‹æŸ¥çœ‹å·¦å´é‚Šæ¬„çš„é€²åº¦æ›´æ–°ï¼")
    
    async def _execute_crawler_safe(self, username: str, max_posts: int, auth_content: dict, task_id: str, progress_file: str):
        """åŸ·è¡ŒçœŸå¯¦çš„çˆ¬èŸ²ä»»å‹™ï¼Œç·šç¨‹å®‰å…¨ç‰ˆæœ¬ï¼Œä¸ä¾è³´session_state"""
        import requests
        import json
        import time
        
        print(f"ğŸ”¥ ä½¿ç”¨å·²è¨­ç½®çš„ task_id: {task_id[:8]}")
        
        # å¯«å…¥åˆå§‹ç‹€æ…‹åˆ°é€²åº¦æ–‡ä»¶
        def write_progress(stage, **kwargs):
            data = {"stage": stage, "timestamp": time.time(), **kwargs}
            try:
                with open(progress_file, 'w', encoding='utf-8') as f:
                    json.dump(data, f, ensure_ascii=False)
                    f.flush()
            except Exception as e:
                print(f"âŒ å¯«å…¥é€²åº¦æ–‡ä»¶å¤±æ•—: {e}")
        
        payload = {
            "username": username,
            "max_posts": max_posts,
            "auth_json_content": auth_content,
            "task_id": task_id  # ç¢ºä¿ Playwright Agent ä½¿ç”¨é€™å€‹ task_id
        }
        
        try:
            timeout = httpx.Timeout(600.0)  # 10åˆ†é˜è¶…æ™‚ï¼ˆæ”¯æŒæ›´å¤šè²¼æ–‡çˆ¬å–ï¼‰
            async with httpx.AsyncClient(timeout=timeout) as client:
                write_progress("crawler_start", message="ğŸš€ å•Ÿå‹•çˆ¬èŸ²ä¸¦é–‹å§‹ SSE é€²åº¦ç›£è½...")
                
                # å•Ÿå‹• SSE ç›£è½ï¼ˆåœ¨èƒŒæ™¯åŸ·è¡Œï¼‰
                self._start_sse_listener_safe(task_id, progress_file)
                
                # è§¸ç™¼çˆ¬èŸ²ï¼ˆåŒæ­¥èª¿ç”¨ï¼‰
                response = await client.post(self.agent_url, json=payload)
                
                if response.status_code != 200:
                    error_msg = f"âŒ API è«‹æ±‚å¤±æ•—ï¼Œç‹€æ…‹ç¢¼: {response.status_code}"
                    write_progress("error", error=error_msg, response_text=response.text)
                    print(error_msg)
                    return

                # è§£ææœ€çµ‚çµæœ
                try:
                    final_data = response.json()
                    write_progress("api_success", message="âœ… æˆåŠŸæ”¶åˆ°æœ€çµ‚çˆ¬å–çµæœï¼")
                    
                    # ğŸ”¥ èª¿è©¦ï¼šæª¢æŸ¥æ”¶åˆ°çš„æ•¸æ“šæ ¼å¼
                    data_type = type(final_data).__name__
                    data_keys = list(final_data.keys()) if isinstance(final_data, dict) else "éå­—å…¸"
                    posts_exists = "posts" in final_data if isinstance(final_data, dict) else False
                    posts_count = len(final_data.get("posts", [])) if isinstance(final_data, dict) else 0
                    
                    write_progress("api_debug", 
                        data_type=data_type, 
                        data_keys=str(data_keys), 
                        posts_exists=posts_exists, 
                        posts_count=posts_count
                    )
                    
                    print(f"ğŸ”¥ èª¿è©¦final_data: type={data_type}, keys={data_keys}, posts={posts_exists}, count={posts_count}")
                    
                    # å°‡æœ€çµ‚æ•¸æ“šå¯«å…¥é€²åº¦æ–‡ä»¶ï¼Œä¾›UIè®€å–
                    write_progress("final_data_received", 
                        final_data=final_data,
                        username=username,
                        task_id=task_id
                    )
                    
                except json.JSONDecodeError as e:
                    write_progress("json_error", error=f"ç„¡æ³•è§£æéŸ¿æ‡‰ JSON: {e}")
                    
        except Exception as e:
            write_progress("general_error", error=str(e))
            print(f"âŒ çˆ¬èŸ²åŸ·è¡Œå¤±æ•—: {e}")
    
    def _start_sse_listener_safe(self, task_id: str, progress_file: str):
        """ç·šç¨‹å®‰å…¨çš„SSEç›£è½å™¨ï¼Œä¸ä¾è³´session_state"""
        import threading
        
        def sse_worker():
            """SSEç›£è½å·¥ä½œç·šç¨‹"""
            self._run_sse_listener_safe(task_id, progress_file)
        
        # å•Ÿå‹•SSEç›£è½ç·šç¨‹
        threading.Thread(target=sse_worker, daemon=True).start()
        print(f"ğŸ”¥ SSEç›£è½å™¨å·²å•Ÿå‹•: {task_id[:8]}")
    
    def _run_sse_listener_safe(self, task_id: str, progress_file: str):
        """é‹è¡ŒSSEç›£è½å™¨ï¼Œç·šç¨‹å®‰å…¨ç‰ˆæœ¬"""
        import requests
        import json
        
        sse_url = f"{self.orchestrator_url.rstrip('/')}/stream/{task_id}"
        
        try:
            with requests.get(sse_url, stream=True, timeout=600) as response:
                if response.status_code == 200:
                    print(f"ğŸ“¡ SSE é€£æ¥æˆåŠŸ: {response.status_code}")
                    
                    for line in response.iter_lines():
                        if line:
                            line_str = line.decode('utf-8')
                            print(f"ğŸ” æ”¶åˆ° SSE è¡Œ: {line_str}")
                            
                            if line_str.startswith('data: '):
                                try:
                                    data_str = line_str[6:]  # ç§»é™¤ "data: " å‰ç¶´
                                    data = json.loads(data_str)
                                    print(f"ğŸ“Š è§£ææˆåŠŸ: {data}")
                                    
                                    # å¯«å…¥é€²åº¦æ–‡ä»¶
                                    with open(progress_file, 'w', encoding='utf-8') as f:
                                        json.dump(data, f, ensure_ascii=False)
                                        f.flush()
                                    print("ğŸ’¾ é€²åº¦å·²å¯«å…¥æ–‡ä»¶")
                                    
                                except json.JSONDecodeError as e:
                                    print(f"âŒ SSE JSONè§£æå¤±æ•—: {e}")
                else:
                    print(f"âŒ SSE é€£æ¥å¤±æ•—: {response.status_code}")
                    
        except Exception as e:
            print(f"âŒ SSE ç›£è½éŒ¯èª¤: {e}")

    def _start_sse_listener(self, task_id: str):
        """å•Ÿå‹• SSE ç›£è½å™¨ï¼ˆåœ¨èƒŒæ™¯åŸ·è¡Œï¼‰"""
        import tempfile
        import json
        import os
        
        # ğŸ”¥ ä½¿ç”¨å·²å‰µå»ºçš„é€²åº¦æ–‡ä»¶è·¯å¾‘
        progress_file_path = st.session_state.crawler_progress_file
        print(f"ğŸ”¥ ä½¿ç”¨å·²å‰µå»ºçš„é€²åº¦æ–‡ä»¶: {progress_file_path}")
        
        def sse_worker():
            try:
                import requests
                import json
                
                orchestrator_url = f"http://localhost:8000/stream/{task_id}"
                print(f"ğŸ“¡ é€£æ¥ SSE: {orchestrator_url}")
                
                with requests.get(orchestrator_url, stream=True, timeout=600) as response:
                    if response.status_code == 200:
                        print(f"ğŸ“¡ SSE é€£æ¥æˆåŠŸ: {response.status_code}")
                        
                        # ğŸ”¥ ä¿®å¾© #3: ä¸€é€£ä¸Šå°±å…ˆå¯« "connected"ï¼Œé¿å…ç«¶é€Ÿ
                        try:
                            with open(progress_file_path, 'w') as f:
                                json.dump({"stage": "connected", "message": "SSE é€£æ¥å·²å»ºç«‹"}, f)
                                f.flush()  # ğŸ”¥ ä¿®å¾© #2: å¼·åˆ¶åˆ·æ–°ç·©è¡å€
                                os.fsync(f.fileno())  # ğŸ”¥ ä¿®å¾© #2: å¼·åˆ¶å¯«å…¥ç£ç¢Ÿ
                            print(f"âœ… åˆå§‹é€£æ¥æ–‡ä»¶å·²å¯«å…¥: {progress_file_path}")
                        except Exception as e:
                            print(f"âš ï¸ å¯«å…¥åˆå§‹é€£æ¥æ–‡ä»¶éŒ¯èª¤: {e}")
                        
                        for line in response.iter_lines():
                            if line:
                                line_str = line.decode('utf-8').strip()
                                print(f"ğŸ” æ”¶åˆ° SSE è¡Œ: {line_str}")  # èª¿è©¦è¼¸å‡º
                                
                                # ğŸ”¥ ä¿®å¾© #1: æ”¹é€² SSE è³‡æ–™è¡Œæ ¼å¼è§£æ
                                if line_str.startswith('data:'):
                                    payload_txt = line_str.split(':', 1)[1].strip()
                                    if payload_txt:
                                        try:
                                            data = json.loads(payload_txt)
                                            print(f"ğŸ“Š è§£ææˆåŠŸ: {data}")  # èª¿è©¦è¼¸å‡º
                                            
                                            # ğŸ”¥ ä¿®å¾© #2: æ”¹é€²æª”æ¡ˆå¯«å…¥åŒæ­¥
                                            try:
                                                with open(progress_file_path, 'w') as f:
                                                    json.dump(data, f)
                                                    f.flush()  # å¼·åˆ¶åˆ·æ–°ç·©è¡å€
                                                    os.fsync(f.fileno())  # å¼·åˆ¶å¯«å…¥ç£ç¢Ÿ
                                                print(f"ğŸ’¾ é€²åº¦å·²å¯«å…¥æ–‡ä»¶")  # èª¿è©¦è¼¸å‡º
                                            except Exception as e:
                                                print(f"âš ï¸ å¯«å…¥é€²åº¦æ–‡ä»¶éŒ¯èª¤: {e}")
                                            
                                            # å¦‚æœæ”¶åˆ°å®Œæˆæˆ–éŒ¯èª¤äº‹ä»¶ï¼ŒçµæŸç›£è½
                                            if data.get('stage') in ['completed', 'error']:
                                                break
                                                
                                        except json.JSONDecodeError as e:
                                            print(f"âŒ JSON è§£æå¤±æ•—: {e}, åŸå§‹æ–‡æœ¬: {payload_txt}")
                                            continue
                    else:
                        print(f"âŒ SSE é€£æ¥å¤±æ•—: {response.status_code}")
                        
            except Exception as e:
                print(f"âŒ SSE ç›£è½éŒ¯èª¤: {e}")
            finally:
                # æ¸…ç†ï¼šå‰µå»ºå®Œæˆæ¨™è¨˜æ–‡ä»¶
                try:
                    completion_file = progress_file_path.replace('.json', '_completed.json')
                    with open(completion_file, 'w') as f:
                        json.dump({'completed': True}, f)
                except Exception:
                    pass
        
        # åœ¨èƒŒæ™¯åŸ·è¡Œç·’ä¸­å•Ÿå‹• SSE ç›£è½
        import threading
        threading.Thread(target=sse_worker, daemon=True).start()
    
    def _check_and_update_progress(self):
        """æª¢æŸ¥é€²åº¦æ–‡ä»¶ä¸¦æ›´æ–° UI ç‹€æ…‹ï¼Œè¿”å›æ˜¯å¦æœ‰æ›´æ–°"""
        import json
        import os
        
        progress_file_path = st.session_state.get('crawler_progress_file')
        if not progress_file_path or not os.path.exists(progress_file_path):
            # ğŸ”¥ æ›´è©³ç´°çš„èª¿è©¦ä¿¡æ¯
            if not progress_file_path:
                print("ğŸ” æ²’æœ‰é€²åº¦æ–‡ä»¶è·¯å¾‘")
            else:
                print(f"ğŸ” é€²åº¦æ–‡ä»¶ä¸å­˜åœ¨: {progress_file_path}")
            return False
            
        try:
            # ğŸ”¥ ä¿®å¾©ï¼šç°¡åŒ–æª¢æŸ¥é‚è¼¯ï¼Œæ¯æ¬¡éƒ½è®€å–æ–‡ä»¶
            # æª¢æŸ¥æ–‡ä»¶ä¿®æ”¹æ™‚é–“
            current_mtime = os.path.getmtime(progress_file_path)
            last_mtime = st.session_state.get('crawler_progress_mtime', 0)
            
            print(f"ğŸ” æª”æ¡ˆæ™‚é–“æª¢æŸ¥: current={current_mtime}, last={last_mtime}")
            
            # ğŸ”¥ ä¿®å¾©ï¼šæ›´å¯¬é¬†çš„æ›´æ–°æª¢æŸ¥ï¼Œé¿å…éŒ¯éæ›´æ–°
            if current_mtime > last_mtime:
                st.session_state.crawler_progress_mtime = current_mtime
                print(f"ğŸ”¥ æª¢æ¸¬åˆ°é€²åº¦æ–‡ä»¶æ›´æ–°: {current_mtime}")
                file_updated = True
            else:
                # å³ä½¿æ™‚é–“ç›¸åŒï¼Œä¹Ÿå˜—è©¦è®€å–ä¸€æ¬¡ï¼ˆç”¨æ–¼èª¿è©¦ï¼‰
                print(f"ğŸ” æª”æ¡ˆæ™‚é–“æœªè®Šï¼Œä½†ä»å˜—è©¦è®€å–")
                file_updated = False
            
            # è®€å–é€²åº¦æ•¸æ“š
            with open(progress_file_path, 'r') as f:
                data = json.load(f)
            
            print(f"ğŸ”¥ è®€å–åˆ°é€²åº¦æ•¸æ“š: stage={data.get('stage')}, progress={data.get('progress')}")
            
            # æ›´æ–° UI ç‹€æ…‹
            self._update_ui_from_progress(data)
            
            # ğŸ”¥ ä¿®å¾©ï¼šåªè¦æˆåŠŸè®€å–å°±è¿”å›Trueï¼Œç¢ºä¿UIæ›´æ–°
            return True
            
        except Exception as e:
            print(f"âš ï¸ æª¢æŸ¥é€²åº¦æ–‡ä»¶éŒ¯èª¤: {e}")
            return False
    
    def _update_ui_from_progress(self, data: dict):
        """æ ¹æ“šé€²åº¦æ•¸æ“šæ›´æ–° UI ç‹€æ…‹"""
        stage = data.get('stage', '')
        
        # ç¢ºä¿æ—¥èªŒåˆ—è¡¨å­˜åœ¨
        if 'crawler_logs' not in st.session_state:
            st.session_state.crawler_logs = []
        
        if stage == 'connected':
            st.session_state.crawler_logs.append("ğŸ“¡ SSE é€£æ¥å·²å»ºç«‹")
            if not hasattr(st.session_state, 'debug_messages'):
                st.session_state.debug_messages = []
            st.session_state.debug_messages.append(f"ğŸ“¡ SSE é€£æ¥å·²å»ºç«‹ (connected)")
            
        elif stage == 'fetch_start':
            username = data.get('username', '')
            st.session_state.crawler_logs.append(f"ğŸ” é–‹å§‹çˆ¬å– @{username} çš„è²¼æ–‡...")
            st.session_state.crawler_current_work = f"æ­£åœ¨é€£æ¥ Threads API..."
            
        elif stage == 'post_parsed':
            current = data.get('current', 0)
            total = data.get('total', 1)
            progress = data.get('progress', 0)
            post_id = data.get('post_id', '')
            content_preview = data.get('content_preview', '')
            
            # ğŸ”¥ æå–å®Œæ•´çš„çµ±è¨ˆæ•¸æ“š
            likes_count = data.get('likes_count', 0)
            comments_count = data.get('comments_count', 0)
            reposts_count = data.get('reposts_count', 0)
            shares_count = data.get('shares_count', 0)
            views_count = data.get('views_count', 0)
            calculated_score = data.get('calculated_score', 0)
            content = data.get('content', '')
            url = data.get('url', '')
            created_at = data.get('created_at', '')
            images_count = data.get('images_count', 0)
            videos_count = data.get('videos_count', 0)
            media_urls = data.get('media_urls', {})
            
            # ğŸ”¥ å¼·åˆ¶è¨­ç½®é€²åº¦ä¸¦æ·»åŠ èª¿è©¦ä¿¡æ¯
            st.session_state.crawler_progress = progress
            if not hasattr(st.session_state, 'debug_messages'):
                st.session_state.debug_messages = []
            st.session_state.debug_messages.append(f"ğŸ”¥ è¨­ç½®é€²åº¦: {progress:.1%} (post_parsed)")
            
            # ğŸ”¥ æ›´æ–°ç•¶å‰å·¥ä½œç‹€æ…‹
            st.session_state.crawler_current_work = f"å·²è§£æ {current}/{total} ç¯‡è²¼æ–‡ - æ­£åœ¨è§£æä¸‹ä¸€ç¯‡..."
            st.session_state.crawler_logs.append(f"âœ… è§£æè²¼æ–‡ {post_id[-8:]}: {likes_count}è®š - {content_preview}")
            
            # ğŸ”¥ å‰µå»ºå®Œæ•´çš„è²¼æ–‡å°è±¡ä¸¦æ·»åŠ åˆ°åˆ—è¡¨
            post_data = {
                'post_id': post_id,
                'summary': f"è²¼æ–‡ {post_id[-8:]}",
                'timestamp': created_at[:19] if created_at else "æœªçŸ¥æ™‚é–“",
                'content': content,
                'content_preview': content_preview,
                'url': url,
                'likes_count': likes_count,
                'comments_count': comments_count,
                'reposts_count': reposts_count,
                'shares_count': shares_count,
                'views_count': views_count,
                'calculated_score': calculated_score,
                'images_count': images_count,
                'videos_count': videos_count,
                'media_urls': media_urls
            }
            
            # ç¢ºä¿ crawler_posts åˆ—è¡¨å­˜åœ¨
            if 'crawler_posts' not in st.session_state:
                st.session_state.crawler_posts = []
            
            # æ·»åŠ åˆ°è²¼æ–‡åˆ—è¡¨ï¼ˆé¿å…é‡è¤‡ï¼‰
            existing_ids = [p.get('post_id') for p in st.session_state.crawler_posts]
            if post_id not in existing_ids:
                st.session_state.crawler_posts.append(post_data)
            
        elif stage == 'batch_parsed':
            batch_size = data.get('batch_size', 0)
            current = data.get('current', 0)
            total = data.get('total', 1)
            query_name = data.get('query_name', '')
            st.session_state.crawler_current_work = f"å·²è™•ç† {query_name} æ‰¹æ¬¡ï¼Œç²å¾— {batch_size} ç¯‡æ–°è²¼æ–‡..."
            st.session_state.crawler_logs.append(f"ğŸ“¦ å¾ {query_name} è§£æäº† {batch_size} å‰‡è²¼æ–‡ï¼Œç¸½è¨ˆ: {current}/{total}")
            
        elif stage == 'fill_views_start':
            st.session_state.crawler_current_work = "æ­£åœ¨è£œé½Šç€è¦½æ•¸æ•¸æ“š..."
            st.session_state.crawler_logs.append("ğŸ‘ï¸ é–‹å§‹è£œé½Šç€è¦½æ•¸...")
            
        elif stage == 'views_fetched':
            post_id = data.get('post_id', '')
            views_formatted = data.get('views_formatted', '0')
            st.session_state.crawler_current_work = f"æ­£åœ¨ç²å–è²¼æ–‡ {post_id[-8:]} çš„ç€è¦½æ•¸..."
            st.session_state.crawler_logs.append(f"ğŸ‘ï¸ è²¼æ–‡ {post_id[-8:]}: {views_formatted} æ¬¡ç€è¦½")
            
        elif stage == 'fill_views_completed':
            st.session_state.crawler_current_work = "ç€è¦½æ•¸è£œé½Šå®Œæˆï¼Œæº–å‚™æœ€çµ‚è™•ç†..."
            st.session_state.crawler_logs.append("âœ… ç€è¦½æ•¸è£œé½Šå®Œæˆ")
            
        elif stage == 'completed':
            st.session_state.crawler_current_work = "ğŸ‰ çˆ¬å–ä»»å‹™å·²å®Œæˆï¼"
            st.session_state.crawler_logs.append("ğŸ‰ çˆ¬å–ä»»å‹™å®Œæˆï¼")
            st.session_state.crawler_progress = 1.0
            
            # ğŸ”¥ ä¿®å¾©ï¼šå¾crawler_postså‰µå»ºfinal_data
            crawler_posts = st.session_state.get('crawler_posts', [])
            if crawler_posts and not st.session_state.get('final_data'):
                # ç²å–ç›®æ¨™ä¿¡æ¯
                target = st.session_state.get('crawler_target', {})
                username = target.get('username', 'unknown')
                task_id = st.session_state.get('crawler_task_id', 'unknown')
                
                # è½‰æ›crawler_postsç‚ºfinal_dataæ ¼å¼
                ui_posts = []
                for post in crawler_posts:
                    ui_post = {
                        "post_id": post.get("post_id", ""),
                        "username": post.get("username", username),
                        "content": post.get("content", ""),
                        "created_at": post.get("timestamp", ""),
                        "likes_count": post.get("likes_count", 0),
                        "comments_count": post.get("comments_count", 0),
                        "reposts_count": post.get("reposts_count", 0),
                        "shares_count": post.get("shares_count", 0),
                        "views_count": post.get("views_count", 0),
                        "calculated_score": post.get("calculated_score", 0),
                        "url": post.get("url", ""),
                        "source": "threads",
                        "processing_stage": "completed",
                        "media_urls": post.get("media_urls", [])
                    }
                    ui_posts.append(ui_post)
                
                # å‰µå»ºfinal_data
                import time
                final_data = {
                    "batch_id": task_id,
                    "username": username,
                    "processing_stage": "completed",
                    "total_count": len(ui_posts),
                    "posts": ui_posts,
                    "crawl_timestamp": time.time(),
                    "agent_version": "1.0.0"
                }
                
                st.session_state.final_data = final_data
                st.session_state.crawler_logs.append(f"ğŸ”§ å¾SSEæ•¸æ“šå‰µå»ºfinal_data ({len(ui_posts)} ç¯‡è²¼æ–‡)")
                print(f"ğŸ”¥ SSEå®Œæˆï¼šå‰µå»ºfinal_dataï¼ŒåŒ…å« {len(ui_posts)} ç¯‡è²¼æ–‡")
            
            st.session_state.crawler_status = 'completed'
            
        elif stage == 'error':
            error_msg = data.get('error', 'æœªçŸ¥éŒ¯èª¤')
            st.session_state.crawler_logs.append(f"âŒ çˆ¬å–éŒ¯èª¤: {error_msg}")
            st.session_state.crawler_status = 'error'
    
    def _handle_sse_event(self, data: dict):
        """è™•ç† SSE äº‹ä»¶ï¼ˆç·šç¨‹å®‰å…¨ç‰ˆæœ¬ï¼‰"""
        stage = data.get('stage', '')
        
        def safe_log(message: str):
            """å®‰å…¨åœ°è¨˜éŒ„æ—¥èªŒåˆ° session state"""
            try:
                if hasattr(st.session_state, 'crawler_logs') and st.session_state.crawler_logs is not None:
                    st.session_state.crawler_logs.append(message)
                else:
                    print(message)  # å‚™ç”¨æ—¥èªŒ
            except Exception:
                print(message)  # å‚™ç”¨æ—¥èªŒ
        
        def safe_set_progress(progress: float):
            """å®‰å…¨åœ°è¨­ç½®é€²åº¦"""
            try:
                if hasattr(st.session_state, 'crawler_progress'):
                    st.session_state.crawler_progress = progress
            except Exception:
                print(f"ğŸ“Š é€²åº¦: {progress:.1%}")
        
        def safe_set_status(status: str):
            """å®‰å…¨åœ°è¨­ç½®ç‹€æ…‹"""
            try:
                if hasattr(st.session_state, 'crawler_status'):
                    st.session_state.crawler_status = status
            except Exception:
                print(f"ç‹€æ…‹: {status}")
        
        if stage == 'connected':
            safe_log("ğŸ“¡ SSE é€£æ¥å·²å»ºç«‹")
        elif stage == 'fetch_start':
            safe_log(f"ğŸ” é–‹å§‹çˆ¬å– @{data.get('username')} çš„è²¼æ–‡...")
        elif stage == 'fetch_progress':
            current = data.get('current', 0)
            total = data.get('total', 1)
            progress = data.get('progress', 0)
            safe_set_progress(progress)
            safe_log(f"ğŸ“Š é€²åº¦: {current}/{total} ç¯‡è²¼æ–‡ ({progress:.1%})")
        elif stage == 'post_parsed':
            # ğŸ”¥ æ–°å¢ï¼šæ¯è§£æä¸€å€‹è²¼æ–‡çš„è©³ç´°é€²åº¦
            current = data.get('current', 0)
            total = data.get('total', 1)
            progress = data.get('progress', 0)
            post_id = data.get('post_id', '')
            content_preview = data.get('content_preview', '')
            likes = data.get('likes', 0)
            safe_set_progress(progress)
            safe_log(f"âœ… è§£æè²¼æ–‡ {post_id[-8:]}: {likes}è®š - {content_preview}")
        elif stage == 'batch_parsed':
            # ğŸ”¥ æ–°å¢ï¼šæ¯æ‰¹è§£æå®Œæˆçš„é€²åº¦
            batch_size = data.get('batch_size', 0)
            current = data.get('current', 0)
            total = data.get('total', 1)
            query_name = data.get('query_name', '')
            safe_log(f"ğŸ“¦ å¾ {query_name} è§£æäº† {batch_size} å‰‡è²¼æ–‡ï¼Œç¸½è¨ˆ: {current}/{total}")
        elif stage == 'fill_views_start':
            safe_log("ğŸ‘ï¸ é–‹å§‹è£œé½Šç€è¦½æ•¸...")
        elif stage == 'views_fetched':
            # ğŸ”¥ æ–°å¢ï¼šæ¯ç²å–ä¸€å€‹ç€è¦½æ•¸çš„è©³ç´°é€²åº¦
            post_id = data.get('post_id', '')
            views_formatted = data.get('views_formatted', '0')
            safe_log(f"ğŸ‘ï¸ è²¼æ–‡ {post_id[-8:]}: {views_formatted} æ¬¡ç€è¦½")
        elif stage == 'fill_views_completed':
            safe_log("âœ… ç€è¦½æ•¸è£œé½Šå®Œæˆ")
        elif stage == 'completed':
            safe_log("ğŸ‰ çˆ¬å–ä»»å‹™å®Œæˆï¼")
            safe_set_progress(1.0)
        elif stage == 'error':
            error_msg = data.get('error', 'æœªçŸ¥éŒ¯èª¤')
            safe_log(f"âŒ çˆ¬å–éŒ¯èª¤: {error_msg}")
            safe_set_status('error')
        elif stage == 'heartbeat':
            # å¿ƒè·³äº‹ä»¶ï¼Œä¸é¡¯ç¤º
            pass
    
    def _render_crawler_progress(self):
        """æ¸²æŸ“çˆ¬èŸ²é€²åº¦ï¼ˆé©åˆå´é‚Šæ¬„ï¼ŒåŒ…å«æ¨™é¡Œï¼‰"""
        self._render_crawler_progress_content_only()
    
    def _render_crawler_progress_content_only(self):
        """æ¸²æŸ“çˆ¬èŸ²é€²åº¦å…§å®¹ï¼ˆä¸åŒ…å«æ¨™é¡Œï¼Œé¿å…é‡è¤‡ï¼‰"""
        target = st.session_state.get('crawler_target', {})
        username = target.get("username", "N/A")
        max_posts = target.get("max_posts", 0)
        
        # æª¢æŸ¥ä¸¦æ›´æ–°é€²åº¦
        progress_updated = self._check_and_update_progress()
        progress = st.session_state.get('crawler_progress', 0)
        status = st.session_state.get('crawler_status', 'idle')
        current_work = st.session_state.get('crawler_current_work', '')
        
        # ç·Šæ¹Šé¡¯ç¤º
        st.write(f"ğŸ‘¤ @{username}")
        
        # é€²åº¦æ¢
        if progress > 0:
            # ğŸ”¥ ç¢ºä¿é€²åº¦å€¼åœ¨æœ‰æ•ˆç¯„åœå…§ [0.0, 1.0]
            safe_progress = max(0.0, min(1.0, progress))
            st.progress(safe_progress)
            if max_posts > 0:
                estimated = int(progress * max_posts)
                st.write(f"ğŸ“Š {estimated}/{max_posts} ({progress:.1%})")
            else:
                st.write(f"ğŸ“Š {progress:.1%}")
        else:
            st.write("ğŸ“Š æº–å‚™ä¸­...")
        
        # ç‹€æ…‹
        status_emoji = {"idle": "âšª", "running": "ğŸŸ¡", "completed": "ğŸŸ¢", "error": "ğŸ”´"}
        st.write(f"{status_emoji.get(status, 'âšª')} {status}")
        
        # ç•¶å‰å·¥ä½œ
        if current_work:
            st.write(f"ğŸ”„ {current_work}")
        else:
            # å¦‚æœæ²’æœ‰ç•¶å‰å·¥ä½œä½†ç‹€æ…‹æ˜¯runningï¼Œé¡¯ç¤ºé»˜èªä¿¡æ¯
            if status == 'running':
                st.write("ğŸ”„ æ­£åœ¨è™•ç†ä¸­...")
        
        # æœ€è¿‘æ—¥èªŒï¼ˆç·Šæ¹Šé¡¯ç¤ºï¼‰
        logs = st.session_state.get('crawler_logs', [])
        if logs:
            with st.expander("ğŸ“ é€²åº¦æ—¥èªŒ", expanded=True):
                for log in logs[-5:]:  # æœ€è¿‘5æ¢
                    st.write(f"â€¢ {log}")
        
        # ğŸ”¥ å¯¦æ™‚æ›´æ–°æç¤º
        if status == 'running':
            st.info("â±ï¸ æ¯2ç§’è‡ªå‹•æ›´æ–°é€²åº¦")
        
        # èª¿è©¦ä¿¡æ¯ï¼ˆå¯é¸ï¼‰
        if st.session_state.get('show_debug_sidebar', True):  # ğŸ”¥ ä¿®æ­£keyåç¨±ï¼Œé»˜èªç‚ºTrue
            with st.expander("ğŸ”§ èª¿è©¦ä¿¡æ¯", expanded=True):  # ğŸ”¥ é»˜èªå±•é–‹
                st.write(f"ğŸ†” ä»»å‹™: {st.session_state.get('crawler_task_id', 'N/A')[-8:]}")
                st.write(f"ğŸ”„ æ›´æ–°: {progress_updated}")
                
                # é€²åº¦æ–‡ä»¶ç‹€æ…‹
                progress_file = st.session_state.get('crawler_progress_file')
                if progress_file and os.path.exists(progress_file):
                    st.write("âœ… é€²åº¦æ–‡ä»¶å­˜åœ¨")
                else:
                    st.write("âŒ é€²åº¦æ–‡ä»¶ä¸å­˜åœ¨")
            st.write(f"ğŸ“Š é€²åº¦å€¼: {progress:.1%}")
            st.write(f"ğŸ”„ å·²æ›´æ–°: {progress_updated}")
            st.write(f"ğŸ†” Task ID: {st.session_state.get('crawler_task_id', 'N/A')}")
            st.write(f"ğŸ“ é€²åº¦æ–‡ä»¶: {st.session_state.get('crawler_progress_file', 'N/A')}")
            st.write(f"â° æœ€å¾Œä¿®æ”¹: {st.session_state.get('crawler_progress_mtime', 'N/A')}")
            st.write(f"ğŸ“‹ ç‹€æ…‹: {st.session_state.get('crawler_status', 'N/A')}")
            st.write(f"ğŸ“ ç•¶å‰å·¥ä½œ: {st.session_state.get('crawler_current_work', 'N/A')}")
            
            # é¡¯ç¤ºèª¿è©¦æ¶ˆæ¯
            debug_messages = st.session_state.get('debug_messages', [])
            if debug_messages:
                st.write("ğŸ” æœ€æ–°èª¿è©¦æ¶ˆæ¯:")
                for msg in debug_messages[-10:]:  # é¡¯ç¤ºæœ€è¿‘10æ¢
                    st.write(f"  {msg}")
            
            # é¡¯ç¤ºé€²åº¦æ–‡ä»¶å…§å®¹
            progress_file = st.session_state.get('crawler_progress_file')
            if progress_file and os.path.exists(progress_file):
                try:
                    with open(progress_file, 'r') as f:
                        file_content = json.load(f)
                    st.write("ğŸ“„ é€²åº¦æ–‡ä»¶å…§å®¹:")
                    st.json(file_content)
                except Exception as e:
                    st.write(f"âŒ ç„¡æ³•è®€å–é€²åº¦æ–‡ä»¶: {e}")
            else:
                st.write("âŒ é€²åº¦æ–‡ä»¶ä¸å­˜åœ¨")
        
        # å´é‚Šæ¬„ä¸é¡¯ç¤ºå³æ™‚è²¼æ–‡é è¦½ï¼ˆå·²ç§»åˆ°ä¸»å…§å®¹å€åŸŸï¼‰
        
        # ğŸ”¥ ç‹€æ…‹ç›¸é—œé¡¯ç¤º
        if status == 'running':
            st.markdown("---")
            st.subheader("ğŸ“Š çˆ¬å–ç‹€æ…‹")
            
            # é¡¯ç¤ºç•¶å‰é€²åº¦è©³æƒ…
            
            if progress > 0:
                estimated_posts = int(progress * max_posts)
                st.success(f"ğŸ“Š é€²åº¦: {estimated_posts}/{max_posts} ç¯‡è²¼æ–‡ ({progress:.1%})")
            else:
                st.info(f"ğŸ”„ åˆå§‹åŒ–ä¸­... ç•¶å‰é€²åº¦: {progress:.1%}")
            
            # ğŸ“Š å³æ™‚å·¥ä½œç‹€æ…‹å ±å‘Š
            task_id = st.session_state.get('crawler_task_id', 'N/A')
            current_work = st.session_state.get('crawler_current_work', 'æ­£åœ¨åˆå§‹åŒ–...')
            
            col1, col2 = st.columns([2, 1])
            with col1:
                st.write(f"ğŸ” **æ­£åœ¨çˆ¬å– @{username}**")
                st.info(f"âš¡ **ç•¶å‰å·¥ä½œ**: {current_work}")
            with col2:
                st.write(f"ğŸ†” Task: `{task_id[:8]}...`")
            
            # å·²ç§»åˆ°ä¸Šé¢ç¸½æ˜¯é¡¯ç¤ºçš„å€åŸŸ
            
            # è‡ªå‹•åˆ·æ–°
            if progress_updated:
                st.success(f"ğŸ”„ æœ‰é€²åº¦æ›´æ–°ï¼Œç«‹å³åˆ·æ–°")
            st.rerun()
        else:
                st.info(f"â³ ç„¡é€²åº¦æ›´æ–°ï¼Œst.fragmentæœƒè‡ªå‹•åˆ·æ–°")
                # ğŸ”¥ ç§»é™¤ time.sleep() - ä¸èƒ½åœ¨UIç·šç¨‹ä¸­é˜»å¡ï¼
                # ä¾è³´ st.fragment(run_every=2) è‡ªå‹•åˆ·æ–°
        
        # å®Œæ•´æ—¥èªŒï¼ˆå¯é¸å±•é–‹æŸ¥çœ‹ï¼‰
        if st.session_state.get('crawler_logs'):
            with st.expander("ğŸ“‹ æŸ¥çœ‹å®Œæ•´æ—¥èªŒ", expanded=False):
                for log in st.session_state.crawler_logs[-10:]:  # æœ€å¤šé¡¯ç¤º10æ¢
                    st.text(log)
    
    def _render_crawler_results(self):
        """æ¸²æŸ“çˆ¬èŸ²çµæœ"""
        st.subheader("ğŸ“‹ çˆ¬å–çµæœ")
        
        final_data = st.session_state.get('final_data')
        if not final_data:
            st.warning("æ²’æœ‰çˆ¬å–åˆ°æ•¸æ“š")
            return
        
        # çµæœæ‘˜è¦
        posts = final_data.get("posts", [])
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("æ‰¹æ¬¡ ID", final_data.get('batch_id', 'N/A'))
        with col2:
            st.metric("ç”¨æˆ¶", final_data.get('username', 'N/A'))
        with col3:
            st.metric("ç¸½æ•¸é‡", final_data.get('total_count', 0))
        with col4:
            st.metric("æˆåŠŸçˆ¬å–", len(posts))
        
        # ä¸‹è¼‰æŒ‰éˆ•
        self._render_download_button(final_data)
        
        # è²¼æ–‡é è¦½
        if posts:
            st.subheader("ğŸ“ è²¼æ–‡é è¦½")
            
            # ğŸ”¥ æ’åºæ–¹å¼é¸æ“‡
            col1, col2 = st.columns([3, 1])
            with col1:
                st.write(f"**å…± {len(posts)} ç¯‡è²¼æ–‡ï¼Œé¡¯ç¤ºå‰ 10 ç¯‡**")
            with col2:
                sort_method = st.selectbox(
                    "æ’åºæ–¹å¼",
                    options=["score", "date", "views", "likes"],
                    format_func=lambda x: {
                        "score": "ğŸ† æ¬Šé‡æ’åº",
                        "date": "ğŸ“… æ—¥æœŸæ’åº", 
                        "views": "ğŸ‘ï¸ è§€çœ‹æ’åº",
                        "likes": "â¤ï¸ æŒ‰è®šæ’åº"
                    }[x],
                    key="post_sort_method"
                )
            
            # ğŸ”¥ æ ¹æ“šé¸æ“‡çš„æ’åºæ–¹å¼å°æ‰€æœ‰è²¼æ–‡é€²è¡Œæ’åº
            if sort_method == "score":
                # æ¬Šé‡æ’åº (calculated_score é«˜åˆ°ä½)
                sorted_posts = sorted(posts, 
                                     key=lambda p: p.get('calculated_score', 0), 
                                     reverse=True)
            elif sort_method == "date":
                # æ—¥æœŸæ’åº (æœ€æ–°åœ¨å‰)
                sorted_posts = sorted(posts, 
                                     key=lambda p: p.get('created_at', '') or '', 
                                     reverse=True)
            elif sort_method == "views":
                # è§€çœ‹æ’åº (views_count é«˜åˆ°ä½)
                sorted_posts = sorted(posts, 
                                     key=lambda p: p.get('views_count', 0), 
                                     reverse=True)
            elif sort_method == "likes":
                # æŒ‰è®šæ’åº (likes_count é«˜åˆ°ä½)
                sorted_posts = sorted(posts, 
                                     key=lambda p: p.get('likes_count', 0), 
                                     reverse=True)
            else:
                # é»˜èªæ¬Šé‡æ’åº
                sorted_posts = sorted(posts, 
                                     key=lambda p: p.get('calculated_score', 0), 
                                     reverse=True)
            
            # é¡¯ç¤ºæ’åºèªªæ˜
            sort_descriptions = {
                "score": f"ğŸ“Š æŒ‰æ¬Šé‡åˆ†æ•¸æ’åº (æœ€é«˜: {sorted_posts[0].get('calculated_score', 0):.1f})",
                "date": f"ğŸ“… æŒ‰ç™¼å¸ƒæ™‚é–“æ’åº (æœ€æ–°: {sorted_posts[0].get('created_at', 'N/A')[:10]})",
                "views": f"ğŸ‘ï¸ æŒ‰è§€çœ‹æ•¸æ’åº (æœ€å¤š: {sorted_posts[0].get('views_count', 0):,} æ¬¡)",
                "likes": f"â¤ï¸ æŒ‰æŒ‰è®šæ•¸æ’åº (æœ€å¤š: {sorted_posts[0].get('likes_count', 0):,} è®š)"
            }
            st.info(sort_descriptions.get(sort_method, ""))
            
            for i, post in enumerate(sorted_posts[:10]):  # é¡¯ç¤ºå‰10ç¯‡
                with st.expander(f"è²¼æ–‡ {i+1} - {post.get('post_id', 'N/A')}", expanded=i < 2):
                    col1, col2 = st.columns([3, 1])
                    
                    with col1:
                        # å…§å®¹
                        content = post.get('content', '')
                        if content:
                            st.write("**å…§å®¹:**")
                            st.write(content[:200] + "..." if len(content) > 200 else content)
                        
                        # åª’é«” URL
                        if post.get('media_urls'):
                            st.write("**åª’é«”:**")
                            for media_url in post['media_urls'][:3]:  # æœ€å¤šé¡¯ç¤º3å€‹
                                st.write(f"ğŸ”— {media_url}")
                    
                    with col2:
                        st.write("**çµ±è¨ˆ:**")
                        st.write(f"â¤ï¸ è®š: {post.get('likes_count', 0):,}")
                        st.write(f"ğŸ’¬ ç•™è¨€: {post.get('comments_count', 0):,}")
                        st.write(f"ğŸ”„ è½‰ç™¼: {post.get('reposts_count', 0):,}")
                        st.write(f"ğŸ“¤ åˆ†äº«: {post.get('shares_count', 0):,}")
                        st.write(f"ğŸ‘ï¸ ç€è¦½: {post.get('views_count', 0):,}")
                        st.write(f"â­ åˆ†æ•¸: {post.get('calculated_score', 0):.1f}")
                        
                        st.write("**è©³æƒ…:**")
                        st.write(f"ğŸ”— [åŸæ–‡]({post.get('url', '#')})")
                        st.write(f"ğŸ“… {post.get('created_at', 'N/A')}")
    
    def _render_download_button(self, final_data: Dict[str, Any]):
        """æ¸²æŸ“ä¸‹è¼‰æŒ‰éˆ•"""
        st.subheader("ğŸ’¾ ä¸‹è¼‰çµæœ")
        
        # æº–å‚™ä¸‹è¼‰æ•¸æ“š
        json_str = json.dumps(final_data, indent=2, ensure_ascii=False)
        filename = f"threads_crawl_{final_data.get('username', 'unknown')}_{int(time.time())}.json"
        
        col1, col2, col3 = st.columns([1, 1, 2])
        
        with col1:
            st.download_button(
                label="ğŸ“¥ ä¸‹è¼‰ JSON",
                data=json_str,
                file_name=filename,
                mime="application/json",
                use_container_width=True
            )
        
        with col2:
            # é¡¯ç¤ºæ–‡ä»¶å¤§å°
            file_size = len(json_str.encode('utf-8'))
            st.metric("æ–‡ä»¶å¤§å°", f"{file_size / 1024:.1f} KB")
        
        with col3:
            st.info(f"ğŸ“ æ–‡ä»¶å: {filename}")
    
    def _render_crawler_logs(self):
        """æ¸²æŸ“çˆ¬èŸ²æ—¥èªŒ"""
        if st.session_state.get('crawler_logs'):
            with st.expander("ğŸ“‹ çˆ¬å–æ—¥èªŒ", expanded=False):
                for log in st.session_state.crawler_logs[-10:]:  # æœ€å¤šé¡¯ç¤º10æ¢
                    st.text(log)
    

    
    def _reset_crawler(self):
        """é‡ç½®çˆ¬èŸ²ç‹€æ…‹"""
        keys_to_reset = [
            'crawler_status', 'crawler_target', 'crawler_logs', 'crawler_posts',
            'crawler_events', 'final_data', 'crawler_step', 'crawler_progress',
            'crawler_task_id', 'crawler_progress_file', 'crawler_progress_mtime',
            'crawler_current_work', 'debug_messages'
        ]
        for key in keys_to_reset:
            if key in st.session_state:
                del st.session_state[key]
        
        # ğŸ”¥ å¼·åˆ¶é‡ç½®è¼¸å…¥æ¡†ï¼ˆè§£æ±ºå¡ä½å•é¡Œï¼‰
        st.session_state.crawler_username = ""
        st.session_state.crawler_max_posts = 10