"""
Threads çˆ¬èŸ²çµ„ä»¶
åŸºæ–¼ test_playwright_agent.py çš„çœŸå¯¦åŠŸèƒ½
"""

import streamlit as st
import httpx
import json
import asyncio
import time
from pathlib import Path
from typing import Dict, Any, Optional


class ThreadsCrawlerComponent:
    def __init__(self):
        self.agent_url = "http://localhost:8006/v1/playwright/crawl"
        # ä½¿ç”¨çµ±ä¸€çš„é…ç½®ç®¡ç†
        from common.config import get_auth_file_path
        self.auth_file_path = get_auth_file_path(from_project_root=True)
    
    def render(self):
        """æ¸²æŸ“çˆ¬èŸ²ç•Œé¢"""
        st.header("ğŸ•·ï¸ Threads å…§å®¹çˆ¬èŸ²")
        st.markdown("åŸºæ–¼ Playwright Agent çš„çœŸå¯¦ Threads çˆ¬èŸ²ï¼Œæ”¯æŒ SSE å¯¦æ™‚é€²åº¦é¡¯ç¤ºã€‚")
        
        # æª¢æŸ¥èªè­‰æ–‡ä»¶
        if not self._check_auth_file():
            st.error("âŒ æ‰¾ä¸åˆ°èªè­‰æª”æ¡ˆ")
            st.info("è«‹å…ˆåŸ·è¡Œ: `python tests/threads_fetch/save_auth.py` ä¾†ç”¢ç”Ÿèªè­‰æª”æ¡ˆ")
            return
        
        st.success("âœ… èªè­‰æª”æ¡ˆå·²å°±ç·’")
        
        # çˆ¬èŸ²é…ç½®
        self._render_crawler_config()
        
        # æ ¹æ“šç‹€æ…‹æ¸²æŸ“ä¸åŒç•Œé¢
        status = st.session_state.get('crawler_status', 'idle')
        
        if status == 'running':
            self._render_crawler_progress()
        elif status == 'completed':
            self._render_crawler_results()
        elif status == 'error':
            st.error("âŒ çˆ¬èŸ²åŸ·è¡Œå¤±æ•—ï¼Œè«‹æª¢æŸ¥æ—¥èªŒ")
            self._render_crawler_logs()
    
    def _check_auth_file(self) -> bool:
        """æª¢æŸ¥èªè­‰æ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
        return self.auth_file_path.exists()
    
    def _render_crawler_config(self):
        """æ¸²æŸ“çˆ¬èŸ²é…ç½®ç•Œé¢"""
        col1, col2 = st.columns([2, 1])
        
        with col1:
            username = st.text_input(
                "Threads ç”¨æˆ¶åç¨±ï¼š",
                placeholder="ä¾‹å¦‚ï¼šnatgeo",
                help="è¼¸å…¥ä¸å« @ ç¬¦è™Ÿçš„ç”¨æˆ¶åç¨±",
                key="crawler_username"
            )
        
        with col2:
            max_posts = st.number_input(
                "çˆ¬å–è²¼æ–‡æ•¸é‡ï¼š",
                min_value=1,
                max_value=50,
                value=10,
                help="å»ºè­°ä¸è¶…é 20 ç¯‡ä»¥é¿å…éé•·ç­‰å¾…æ™‚é–“",
                key="crawler_max_posts"
            )
        
        # æ§åˆ¶æŒ‰éˆ•
        col1, col2, col3 = st.columns([1, 1, 2])
        
        with col1:
            if st.button(
                "ğŸš€ é–‹å§‹çˆ¬å–", 
                type="primary",
                disabled=st.session_state.get('crawler_status') == 'running',
                use_container_width=True
            ):
                if username.strip():
                    self._start_crawler(username, max_posts)
                else:
                    st.warning("è«‹è¼¸å…¥ç”¨æˆ¶åç¨±")
        
        with col2:
            if st.button("ğŸ”„ é‡ç½®", use_container_width=True):
                self._reset_crawler()
                st.rerun()
    
    def _start_crawler(self, username: str, max_posts: int):
        """å•Ÿå‹•çˆ¬èŸ²"""
        # åˆå§‹åŒ–ç‹€æ…‹
        st.session_state.crawler_status = 'running'
        st.session_state.crawler_target = {"username": username, "max_posts": max_posts}
        st.session_state.crawler_logs = []
        st.session_state.crawler_events = []
        st.session_state.final_data = None
        st.session_state.crawler_progress = 0
        
        # è®€å–èªè­‰æ–‡ä»¶
        try:
            with open(self.auth_file_path, "r", encoding="utf-8") as f:
                auth_content = json.load(f)
        except Exception as e:
            st.error(f"âŒ ç„¡æ³•è®€å–èªè­‰æª”æ¡ˆ: {e}")
            st.session_state.crawler_status = 'error'
            st.rerun()
            return
        
        # å•Ÿå‹•çœŸå¯¦çš„çˆ¬èŸ²ä»»å‹™
        st.info("ğŸš€ çˆ¬èŸ²å·²å•Ÿå‹•ï¼Œæ­£åœ¨é€£æ¥ Playwright Agent...")
        
        # ä½¿ç”¨ asyncio ä¾†åŸ·è¡Œç•°æ­¥ä»»å‹™
        import asyncio
        try:
            # åœ¨ Streamlit ä¸­é‹è¡Œç•°æ­¥ä»»å‹™
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            loop.run_until_complete(self._execute_crawler(username, max_posts, auth_content))
        except Exception as e:
            st.error(f"âŒ çˆ¬èŸ²åŸ·è¡Œå¤±æ•—: {e}")
            st.session_state.crawler_status = 'error'
            st.session_state.crawler_logs.append(f"éŒ¯èª¤: {e}")
        finally:
            st.rerun()
    
    async def _execute_crawler(self, username: str, max_posts: int, auth_content: dict):
        """åŸ·è¡ŒçœŸå¯¦çš„çˆ¬èŸ²ä»»å‹™ï¼Œä½¿ç”¨æ··åˆæ¨¡å¼ï¼šè§¸ç™¼çˆ¬èŸ² + SSE é€²åº¦"""
        import uuid
        import threading
        import requests
        
        # ç”Ÿæˆ task_id ç”¨æ–¼è¿½è¹¤é€²åº¦
        task_id = str(uuid.uuid4())
        
        payload = {
            "username": username,
            "max_posts": max_posts,
            "auth_json_content": auth_content,
            "task_id": task_id  # ç¢ºä¿ Playwright Agent ä½¿ç”¨é€™å€‹ task_id
        }
        
        # å„²å­˜ task_id ä¾› SSE ä½¿ç”¨
        st.session_state.crawler_task_id = task_id
        
        try:
            timeout = httpx.Timeout(300.0)  # 5åˆ†é˜è¶…æ™‚
            async with httpx.AsyncClient(timeout=timeout) as client:
                st.session_state.crawler_logs.append("ğŸš€ å•Ÿå‹•çˆ¬èŸ²ä¸¦é–‹å§‹ SSE é€²åº¦ç›£è½...")
                
                # å•Ÿå‹• SSE ç›£è½ï¼ˆåœ¨èƒŒæ™¯åŸ·è¡Œï¼‰
                self._start_sse_listener(task_id)
                
                # è§¸ç™¼çˆ¬èŸ²ï¼ˆåŒæ­¥èª¿ç”¨ï¼‰
                response = await client.post(self.agent_url, json=payload)
                
                if response.status_code != 200:
                    error_msg = f"âŒ API è«‹æ±‚å¤±æ•—ï¼Œç‹€æ…‹ç¢¼: {response.status_code}"
                    st.session_state.crawler_logs.append(error_msg)
                    st.session_state.crawler_logs.append(f"éŒ¯èª¤å…§å®¹: {response.text}")
                    st.session_state.crawler_status = 'error'
                    return

                # è§£ææœ€çµ‚çµæœ
                try:
                    final_data = response.json()
                    st.session_state.crawler_logs.append("âœ… æˆåŠŸæ”¶åˆ°æœ€çµ‚çˆ¬å–çµæœï¼")
                    
                    # è½‰æ›ç‚ºUIæœŸæœ›çš„æ ¼å¼
                    if isinstance(final_data, dict) and "posts" in final_data:
                        ui_data = {
                            "batch_id": final_data.get("batch_id", task_id),
                            "username": final_data.get("username", username),
                            "processing_stage": "completed",
                            "total_count": final_data.get("total_count", len(final_data.get("posts", []))),
                            "posts": [],
                            "crawl_timestamp": time.time(),
                            "agent_version": "1.0.0"
                        }
                        
                        # è½‰æ›è²¼æ–‡æ ¼å¼
                        for post in final_data.get("posts", []):
                            ui_post = {
                                "post_id": post.get("post_id", ""),
                                "username": post.get("username", username),
                                "content": post.get("content", ""),
                                "created_at": post.get("created_at", ""),
                                "likes_count": post.get("likes_count", 0),
                                "comments_count": post.get("comments_count", 0),
                                "reposts_count": post.get("reposts_count", 0),
                                "url": post.get("url", ""),
                                "source": "threads",
                                "processing_stage": "completed",
                                "media_urls": post.get("images", []) + post.get("videos", [])
                            }
                            ui_data["posts"].append(ui_post)
                        
                        st.session_state.final_data = ui_data
                    else:
                        st.session_state.final_data = final_data
                    
                    st.session_state.crawler_status = 'completed'
                    posts_count = len(st.session_state.final_data.get("posts", []))
                    st.session_state.crawler_logs.append(f"âœ… çˆ¬å–å®Œæˆï¼æˆåŠŸç²å– {posts_count} ç¯‡è²¼æ–‡")
                    
                except json.JSONDecodeError as e:
                    st.session_state.crawler_logs.append(f"âŒ ç„¡æ³•è§£æéŸ¿æ‡‰ JSON: {e}")
                    st.session_state.crawler_status = 'error'
                
        except httpx.ConnectError as e:
            error_msg = f"é€£ç·šéŒ¯èª¤: ç„¡æ³•é€£ç·šè‡³ {self.agent_url}ã€‚è«‹ç¢ºèª Docker å®¹å™¨æ˜¯å¦æ­£åœ¨é‹è¡Œã€‚"
            st.session_state.crawler_logs.append(error_msg)
            st.session_state.crawler_status = 'error'
        except Exception as e:
            st.session_state.crawler_logs.append(f"åŸ·è¡Œæ™‚ç™¼ç”Ÿæœªé æœŸçš„éŒ¯èª¤: {e}")
            st.session_state.crawler_status = 'error'

    def _start_sse_listener(self, task_id: str):
        """å•Ÿå‹• SSE ç›£è½å™¨ï¼ˆåœ¨èƒŒæ™¯åŸ·è¡Œï¼‰"""
        def sse_worker():
            try:
                import requests
                import json
                
                orchestrator_url = f"http://localhost:8000/stream/{task_id}"
                st.session_state.crawler_logs.append(f"ğŸ“¡ é€£æ¥ SSE: {orchestrator_url}")
                
                with requests.get(orchestrator_url, stream=True, timeout=300) as response:
                    if response.status_code == 200:
                        for line in response.iter_lines():
                            if line:
                                line_str = line.decode('utf-8')
                                if line_str.startswith('data: '):
                                    try:
                                        data = json.loads(line_str[6:])  # ç§»é™¤ 'data: ' å‰ç¶´
                                        self._handle_sse_event(data)
                                        
                                        # å¦‚æœæ”¶åˆ°å®Œæˆæˆ–éŒ¯èª¤äº‹ä»¶ï¼ŒçµæŸç›£è½
                                        if data.get('stage') in ['completed', 'error']:
                                            break
                                            
                                    except json.JSONDecodeError:
                                        continue
                    else:
                        st.session_state.crawler_logs.append(f"âŒ SSE é€£æ¥å¤±æ•—: {response.status_code}")
                        
            except Exception as e:
                st.session_state.crawler_logs.append(f"âŒ SSE ç›£è½éŒ¯èª¤: {e}")
        
        # åœ¨èƒŒæ™¯åŸ·è¡Œç·’ä¸­å•Ÿå‹• SSE ç›£è½
        import threading
        threading.Thread(target=sse_worker, daemon=True).start()
    
    def _handle_sse_event(self, data: dict):
        """è™•ç† SSE äº‹ä»¶"""
        stage = data.get('stage', '')
        
        if stage == 'connected':
            st.session_state.crawler_logs.append("ğŸ“¡ SSE é€£æ¥å·²å»ºç«‹")
        elif stage == 'fetch_start':
            st.session_state.crawler_logs.append(f"ğŸ” é–‹å§‹çˆ¬å– @{data.get('username')} çš„è²¼æ–‡...")
        elif stage == 'fetch_progress':
            current = data.get('current', 0)
            total = data.get('total', 1)
            progress = data.get('progress', 0)
            st.session_state.crawler_progress = progress
            st.session_state.crawler_logs.append(f"ğŸ“Š é€²åº¦: {current}/{total} ç¯‡è²¼æ–‡ ({progress:.1%})")
        elif stage == 'post_parsed':
            # ğŸ”¥ æ–°å¢ï¼šæ¯è§£æä¸€å€‹è²¼æ–‡çš„è©³ç´°é€²åº¦
            current = data.get('current', 0)
            total = data.get('total', 1)
            progress = data.get('progress', 0)
            post_id = data.get('post_id', '')
            content_preview = data.get('content_preview', '')
            likes = data.get('likes', 0)
            st.session_state.crawler_progress = progress
            st.session_state.crawler_logs.append(f"âœ… è§£æè²¼æ–‡ {post_id[-8:]}: {likes}è®š - {content_preview}")
        elif stage == 'batch_parsed':
            # ğŸ”¥ æ–°å¢ï¼šæ¯æ‰¹è§£æå®Œæˆçš„é€²åº¦
            batch_size = data.get('batch_size', 0)
            current = data.get('current', 0)
            total = data.get('total', 1)
            query_name = data.get('query_name', '')
            st.session_state.crawler_logs.append(f"ğŸ“¦ å¾ {query_name} è§£æäº† {batch_size} å‰‡è²¼æ–‡ï¼Œç¸½è¨ˆ: {current}/{total}")
        elif stage == 'fill_views_start':
            st.session_state.crawler_logs.append("ğŸ‘ï¸ é–‹å§‹è£œé½Šç€è¦½æ•¸...")
        elif stage == 'views_fetched':
            # ğŸ”¥ æ–°å¢ï¼šæ¯ç²å–ä¸€å€‹ç€è¦½æ•¸çš„è©³ç´°é€²åº¦
            post_id = data.get('post_id', '')
            views_formatted = data.get('views_formatted', '0')
            st.session_state.crawler_logs.append(f"ğŸ‘ï¸ è²¼æ–‡ {post_id[-8:]}: {views_formatted} æ¬¡ç€è¦½")
        elif stage == 'fill_views_completed':
            st.session_state.crawler_logs.append("âœ… ç€è¦½æ•¸è£œé½Šå®Œæˆ")
        elif stage == 'completed':
            st.session_state.crawler_logs.append("ğŸ‰ çˆ¬å–ä»»å‹™å®Œæˆï¼")
            st.session_state.crawler_progress = 1.0
        elif stage == 'error':
            error_msg = data.get('error', 'æœªçŸ¥éŒ¯èª¤')
            st.session_state.crawler_logs.append(f"âŒ çˆ¬å–éŒ¯èª¤: {error_msg}")
            st.session_state.crawler_status = 'error'
        elif stage == 'heartbeat':
            # å¿ƒè·³äº‹ä»¶ï¼Œä¸é¡¯ç¤º
            pass

    def _render_crawler_progress(self):
        """æ¸²æŸ“çˆ¬èŸ²é€²åº¦"""
        st.subheader("ğŸ“Š çˆ¬å–ç‹€æ…‹")
        
        target = st.session_state.crawler_target
        username = target["username"]
        max_posts = target["max_posts"]
        
        # çœŸå¯¦é€²åº¦ï¼ˆä¾†è‡ª SSEï¼‰
        progress = st.session_state.get('crawler_progress', 0)
        status = st.session_state.get('crawler_status', 'running')
        
        # é¡¯ç¤ºé€²åº¦æ¢
        if status == 'running':
            if progress > 0:
                st.progress(progress)
                estimated_posts = int(progress * max_posts)
                st.text(f"é€²åº¦: ~{estimated_posts}/{max_posts} ç¯‡è²¼æ–‡ ({progress:.1%})")
            else:
                st.progress(0.0)
                st.text("åˆå§‹åŒ–ä¸­...")
            
            # é¡¯ç¤ºç•¶å‰ç‹€æ…‹
            task_id = st.session_state.get('crawler_task_id', 'N/A')
            st.info(f"ğŸ” æ­£åœ¨çˆ¬å– @{username} çš„è²¼æ–‡... (Task ID: {task_id[:8]})")
            st.info("ğŸ“¡ ä½¿ç”¨ SSE å³æ™‚æ›´æ–°é€²åº¦ï¼Œè«‹æŸ¥çœ‹ä¸‹æ–¹æ—¥èªŒäº†è§£è©³ç´°ç‹€æ…‹ã€‚")
            
            # è‡ªå‹•åˆ·æ–°æ¯3ç§’
            time.sleep(3)
            st.rerun()
            
        elif status == 'completed':
            st.progress(1.0)
            st.success("âœ… çˆ¬å–å®Œæˆï¼")
            final_data = st.session_state.get('final_data')
            if final_data:
                posts_count = len(final_data.get("posts", []))
                st.text(f"æˆåŠŸç²å– {posts_count} ç¯‡è²¼æ–‡")
                
        elif status == 'error':
            st.progress(0.0)
            st.error("âŒ çˆ¬å–éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤")
        
        # é¡¯ç¤ºæ—¥èªŒ
        self._render_crawler_logs()
    
    def _render_crawler_results(self):
        """æ¸²æŸ“çˆ¬èŸ²çµæœ"""
        st.subheader("ğŸ“‹ çˆ¬å–çµæœ")
        
        final_data = st.session_state.get('final_data')
        if not final_data:
            st.warning("æ²’æœ‰çˆ¬å–åˆ°æ•¸æ“š")
            return
        
        # çµæœæ‘˜è¦
        posts = final_data.get("posts", [])
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("æ‰¹æ¬¡ ID", final_data.get('batch_id', 'N/A'))
        with col2:
            st.metric("ç”¨æˆ¶", final_data.get('username', 'N/A'))
        with col3:
            st.metric("ç¸½æ•¸é‡", final_data.get('total_count', 0))
        with col4:
            st.metric("æˆåŠŸçˆ¬å–", len(posts))
        
        # ä¸‹è¼‰æŒ‰éˆ•
        self._render_download_button(final_data)
        
        # è²¼æ–‡é è¦½
        if posts:
            st.subheader("ğŸ“ è²¼æ–‡é è¦½")
            
            for i, post in enumerate(posts[:5]):  # åªé¡¯ç¤ºå‰5ç¯‡
                with st.expander(f"è²¼æ–‡ {i+1} - {post.get('post_id', 'N/A')}", expanded=i < 2):
                    col1, col2 = st.columns([3, 1])
                    
                    with col1:
                        # å…§å®¹
                        content = post.get('content', '')
                        if content:
                            st.write("**å…§å®¹:**")
                            st.write(content[:200] + "..." if len(content) > 200 else content)
                        
                        # åª’é«” URL
                        if post.get('media_urls'):
                            st.write("**åª’é«”:**")
                            for media_url in post['media_urls'][:3]:  # æœ€å¤šé¡¯ç¤º3å€‹
                                st.write(f"ğŸ”— {media_url}")
                    
                    with col2:
                        st.write("**çµ±è¨ˆ:**")
                        st.write(f"ğŸ‘ {post.get('likes_count', 0)}")
                        st.write(f"ğŸ’¬ {post.get('comments_count', 0)}")
                        st.write(f"ğŸ”„ {post.get('reposts_count', 0)}")
                        
                        st.write("**è©³æƒ…:**")
                        st.write(f"ğŸ”— [åŸæ–‡]({post.get('url', '#')})")
                        st.write(f"ğŸ“… {post.get('created_at', 'N/A')}")
    
    def _render_download_button(self, final_data: Dict[str, Any]):
        """æ¸²æŸ“ä¸‹è¼‰æŒ‰éˆ•"""
        st.subheader("ğŸ’¾ ä¸‹è¼‰çµæœ")
        
        # æº–å‚™ä¸‹è¼‰æ•¸æ“š
        json_str = json.dumps(final_data, indent=2, ensure_ascii=False)
        filename = f"threads_crawl_{final_data.get('username', 'unknown')}_{int(time.time())}.json"
        
        col1, col2, col3 = st.columns([1, 1, 2])
        
        with col1:
            st.download_button(
                label="ğŸ“¥ ä¸‹è¼‰ JSON",
                data=json_str,
                file_name=filename,
                mime="application/json",
                use_container_width=True
            )
        
        with col2:
            # é¡¯ç¤ºæ–‡ä»¶å¤§å°
            file_size = len(json_str.encode('utf-8'))
            st.metric("æ–‡ä»¶å¤§å°", f"{file_size / 1024:.1f} KB")
        
        with col3:
            st.info(f"ğŸ“ æ–‡ä»¶å: {filename}")
    
    def _render_crawler_logs(self):
        """æ¸²æŸ“çˆ¬èŸ²æ—¥èªŒ"""
        if st.session_state.get('crawler_logs'):
            with st.expander("ğŸ“‹ çˆ¬å–æ—¥èªŒ", expanded=False):
                for log in st.session_state.crawler_logs[-10:]:  # æœ€å¤šé¡¯ç¤º10æ¢
                    st.text(log)
    

    
    def _reset_crawler(self):
        """é‡ç½®çˆ¬èŸ²ç‹€æ…‹"""
        keys_to_reset = [
            'crawler_status', 'crawler_target', 'crawler_logs', 
            'crawler_events', 'final_data', 'crawler_step', 'crawler_progress'
        ]
        for key in keys_to_reset:
            if key in st.session_state:
                del st.session_state[key]