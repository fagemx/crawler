"""
Threads çˆ¬èŸ²çµ„ä»¶
åŸºæ–¼ test_playwright_agent.py çš„çœŸå¯¦åŠŸèƒ½
"""

import streamlit as st
import httpx
import json
import os
import asyncio
import time
from pathlib import Path
from typing import Dict, Any, Optional


class ThreadsCrawlerComponent:
    def __init__(self):
        self.agent_url = "http://localhost:8006/v1/playwright/crawl"
        # ä½¿ç”¨çµ±ä¸€çš„é…ç½®ç®¡ç†
        from common.config import get_auth_file_path
        self.auth_file_path = get_auth_file_path(from_project_root=True)
    
    def render(self):
        """æ¸²æŸ“çˆ¬èŸ²ç•Œé¢"""
        st.header("ğŸ•·ï¸ Threads å…§å®¹çˆ¬èŸ²")
        st.markdown("åŸºæ–¼ Playwright Agent çš„çœŸå¯¦ Threads çˆ¬èŸ²ï¼Œæ”¯æŒ SSE å¯¦æ™‚é€²åº¦é¡¯ç¤ºã€‚")
        
        # æª¢æŸ¥èªè­‰æ–‡ä»¶
        if not self._check_auth_file():
            st.error("âŒ æ‰¾ä¸åˆ°èªè­‰æª”æ¡ˆ")
            st.info("è«‹å…ˆåŸ·è¡Œ: `python tests/threads_fetch/save_auth.py` ä¾†ç”¢ç”Ÿèªè­‰æª”æ¡ˆ")
            return
        
        st.success("âœ… èªè­‰æª”æ¡ˆå·²å°±ç·’")
        
        # çˆ¬èŸ²é…ç½®
        self._render_crawler_config()
        
        # æ ¹æ“šç‹€æ…‹æ¸²æŸ“ä¸åŒç•Œé¢
        status = st.session_state.get('crawler_status', 'idle')
        
        if status == 'running':
            target = st.session_state.get('crawler_target', {})
            username = target.get('username', 'unknown')
            max_posts = target.get('max_posts', 0)
            current_progress = st.session_state.get('crawler_progress', 0)
            
            # é¡¯ç¤ºç•¶å‰é€²åº¦æ¦‚è¦½
            col1, col2 = st.columns([3, 1])
            with col1:
                st.info(f"ğŸ”„ æ­£åœ¨çˆ¬å– @{username} çš„è²¼æ–‡...")
                if current_progress > 0 and max_posts > 0:
                    estimated = int(current_progress * max_posts)
                    st.write(f"ğŸ“Š é€²åº¦: {estimated}/{max_posts} ç¯‡ ({current_progress:.1%})")
                else:
                    st.write("ğŸ“Š æº–å‚™ä¸­...")
            
            with col2:
                st.metric("é€²åº¦", f"{current_progress:.1%}")
            
            st.info("ğŸ’¡ **å³æ™‚é€²åº¦åé¥‹**ï¼šè«‹æŸ¥çœ‹å·¦å´é‚Šæ¬„ä¸‹æ–¹çš„ã€ŒğŸ“Š çˆ¬èŸ²é€²åº¦ã€å€åŸŸï¼Œæ¯2ç§’è‡ªå‹•æ›´æ–°ï¼")
            
            # é¡¯ç¤ºæœ€è¿‘çš„å¹¾æ¢æ—¥èªŒ
            logs = st.session_state.get('crawler_logs', [])
            if logs:
                st.subheader("ğŸ“ æœ€è¿‘æ´»å‹•")
                for log in logs[-3:]:  # æœ€è¿‘3æ¢
                    st.write(f"â€¢ {log}")
        elif status == 'completed':
            self._render_crawler_results()
        elif status == 'error':
            st.error("âŒ çˆ¬èŸ²åŸ·è¡Œå¤±æ•—ï¼Œè«‹æª¢æŸ¥æ—¥èªŒ")
            self._render_crawler_logs()
    
    def _check_auth_file(self) -> bool:
        """æª¢æŸ¥èªè­‰æ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
        return self.auth_file_path.exists()
    
    def _render_crawler_config(self):
        """æ¸²æŸ“çˆ¬èŸ²é…ç½®ç•Œé¢"""
        col1, col2 = st.columns([2, 1])
        
        with col1:
            username = st.text_input(
                "Threads ç”¨æˆ¶åç¨±ï¼š",
                placeholder="ä¾‹å¦‚ï¼šnatgeo",
                help="è¼¸å…¥ä¸å« @ ç¬¦è™Ÿçš„ç”¨æˆ¶åç¨±",
                key="crawler_username"
            )
        
        with col2:
            max_posts = st.number_input(
                "çˆ¬å–è²¼æ–‡æ•¸é‡ï¼š",
                min_value=1,
                max_value=200,
                value=10,
                help="å»ºè­°ï¼š10ç¯‡ç©©å®šï¼Œ50ç¯‡å¯èƒ½è¼ƒæ…¢ï¼Œæœ€å¤š200ç¯‡",
                key="crawler_max_posts"
            )
        
        # æ§åˆ¶æŒ‰éˆ•
        col1, col2, col3 = st.columns([1, 1, 2])
        
        with col1:
            if st.button(
                "ğŸš€ é–‹å§‹çˆ¬å–", 
                type="primary",
                disabled=st.session_state.get('crawler_status') == 'running',
                use_container_width=True
            ):
                if username.strip():
                    self._start_crawler(username, max_posts)
                else:
                    st.warning("è«‹è¼¸å…¥ç”¨æˆ¶åç¨±")
        
        with col2:
            if st.button("ğŸ”„ é‡ç½®", use_container_width=True):
                self._reset_crawler()
                st.rerun()
    
    def _start_crawler(self, username: str, max_posts: int):
        """å•Ÿå‹•çˆ¬èŸ²"""
        # åˆå§‹åŒ–ç‹€æ…‹
        st.session_state.crawler_status = 'running'
        st.session_state.crawler_target = {"username": username, "max_posts": max_posts}
        st.session_state.crawler_logs = []
        st.session_state.crawler_posts = []  # è²¼æ–‡åˆ—è¡¨
        st.session_state.crawler_events = []
        st.session_state.final_data = None
        st.session_state.crawler_progress = 0
        st.session_state.crawler_current_work = "æ­£åœ¨åˆå§‹åŒ–çˆ¬èŸ²..."
        
        # è®€å–èªè­‰æ–‡ä»¶
        try:
            with open(self.auth_file_path, "r", encoding="utf-8") as f:
                auth_content = json.load(f)
        except Exception as e:
            st.error(f"âŒ ç„¡æ³•è®€å–èªè­‰æª”æ¡ˆ: {e}")
            st.session_state.crawler_status = 'error'
            st.rerun()
            return
        
        # å•Ÿå‹•çœŸå¯¦çš„çˆ¬èŸ²ä»»å‹™
        st.success("ğŸš€ çˆ¬èŸ²å·²å•Ÿå‹•ï¼å³å°‡é–‹å§‹çˆ¬å–...")
        
        # ä½¿ç”¨ asyncio ä¾†åŸ·è¡Œç•°æ­¥ä»»å‹™
        import asyncio
        try:
            # åœ¨ Streamlit ä¸­é‹è¡Œç•°æ­¥ä»»å‹™
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            loop.run_until_complete(self._execute_crawler(username, max_posts, auth_content))
        except Exception as e:
            st.error(f"âŒ çˆ¬èŸ²åŸ·è¡Œå¤±æ•—: {e}")
            st.session_state.crawler_status = 'error'
            st.session_state.crawler_logs.append(f"éŒ¯èª¤: {e}")
        finally:
            st.rerun()
    
    async def _execute_crawler(self, username: str, max_posts: int, auth_content: dict):
        """åŸ·è¡ŒçœŸå¯¦çš„çˆ¬èŸ²ä»»å‹™ï¼Œä½¿ç”¨æ··åˆæ¨¡å¼ï¼šè§¸ç™¼çˆ¬èŸ² + SSE é€²åº¦"""
        import uuid
        import threading
        import requests
        
        # ç”Ÿæˆ task_id ç”¨æ–¼è¿½è¹¤é€²åº¦
        task_id = str(uuid.uuid4())
        
        payload = {
            "username": username,
            "max_posts": max_posts,
            "auth_json_content": auth_content,
            "task_id": task_id  # ç¢ºä¿ Playwright Agent ä½¿ç”¨é€™å€‹ task_id
        }
        
        # å„²å­˜ task_id ä¾› SSE ä½¿ç”¨
        st.session_state.crawler_task_id = task_id
        
        try:
            timeout = httpx.Timeout(600.0)  # 10åˆ†é˜è¶…æ™‚ï¼ˆæ”¯æŒæ›´å¤šè²¼æ–‡çˆ¬å–ï¼‰
            async with httpx.AsyncClient(timeout=timeout) as client:
                st.session_state.crawler_logs.append("ğŸš€ å•Ÿå‹•çˆ¬èŸ²ä¸¦é–‹å§‹ SSE é€²åº¦ç›£è½...")
                
                # å•Ÿå‹• SSE ç›£è½ï¼ˆåœ¨èƒŒæ™¯åŸ·è¡Œï¼‰
                self._start_sse_listener(task_id)
                
                # è§¸ç™¼çˆ¬èŸ²ï¼ˆåŒæ­¥èª¿ç”¨ï¼‰
                response = await client.post(self.agent_url, json=payload)
                
                if response.status_code != 200:
                    error_msg = f"âŒ API è«‹æ±‚å¤±æ•—ï¼Œç‹€æ…‹ç¢¼: {response.status_code}"
                    st.session_state.crawler_logs.append(error_msg)
                    st.session_state.crawler_logs.append(f"éŒ¯èª¤å…§å®¹: {response.text}")
                    st.session_state.crawler_status = 'error'
                    return

                # è§£ææœ€çµ‚çµæœ
                try:
                    final_data = response.json()
                    st.session_state.crawler_logs.append("âœ… æˆåŠŸæ”¶åˆ°æœ€çµ‚çˆ¬å–çµæœï¼")
                    
                    # è½‰æ›ç‚ºUIæœŸæœ›çš„æ ¼å¼
                    if isinstance(final_data, dict) and "posts" in final_data:
                        ui_data = {
                            "batch_id": final_data.get("batch_id", task_id),
                            "username": final_data.get("username", username),
                            "processing_stage": "completed",
                            "total_count": final_data.get("total_count", len(final_data.get("posts", []))),
                            "posts": [],
                            "crawl_timestamp": time.time(),
                            "agent_version": "1.0.0"
                        }
                        
                        # è½‰æ›è²¼æ–‡æ ¼å¼ï¼ˆæ·»åŠ èˆ‡Playwright Agentç›¸åŒçš„æ’åºé‚è¼¯ï¼‰
                        posts_data = final_data.get("posts", [])
                        # ğŸ”¥ ä¿®å¾©æ’åºå•é¡Œï¼šæŒ‰created_atæ™‚é–“é™åºæ’åˆ—ï¼ˆæœ€æ–°åœ¨å‰ï¼‰
                        if posts_data:
                            posts_data = sorted(posts_data, 
                                               key=lambda p: p.get('created_at', '') or '', 
                                               reverse=True)
                        
                        for post in posts_data:
                            ui_post = {
                                "post_id": post.get("post_id", ""),
                                "username": post.get("username", username),
                                "content": post.get("content", ""),
                                "created_at": post.get("created_at", ""),
                                "likes_count": post.get("likes_count", 0),
                                "comments_count": post.get("comments_count", 0),
                                "reposts_count": post.get("reposts_count", 0),
                                "shares_count": post.get("shares_count", 0),
                                "views_count": post.get("views_count", 0),
                                "calculated_score": post.get("calculated_score", 0),
                                "url": post.get("url", ""),
                                "source": "threads",
                                "processing_stage": "completed",
                                "media_urls": post.get("images", []) + post.get("videos", [])
                            }
                            ui_data["posts"].append(ui_post)
                        
                        st.session_state.final_data = ui_data
                        
                        # ğŸ”¥ æ–°å¢ï¼šæŒä¹…åŒ–å­˜å„²çˆ¬èŸ²çµæœ
                        try:
                            from common.crawler_storage import get_crawler_storage
                            storage = get_crawler_storage()
                            
                            # æº–å‚™è¦å­˜å„²çš„æ•¸æ“š
                            posts_to_store = []
                            # ğŸ”¥ å°ç¬¬äºŒè™•postsè™•ç†ä¹Ÿæ·»åŠ ç›¸åŒçš„æ’åºé‚è¼¯
                            posts_data_2 = final_data.get("posts", [])
                            if posts_data_2:
                                posts_data_2 = sorted(posts_data_2, 
                                                      key=lambda p: p.get('created_at', '') or '', 
                                                      reverse=True)
                            
                            for post in posts_data_2:
                                post_data = {
                                    "post_id": post.get("post_id", ""),
                                    "username": post.get("username", username),
                                    "content": post.get("content", ""),
                                    "url": post.get("url", ""),
                                    "created_at": post.get("created_at", ""),
                                    "likes_count": post.get("likes_count", 0),
                                    "comments_count": post.get("comments_count", 0),
                                    "reposts_count": post.get("reposts_count", 0),
                                    "shares_count": post.get("shares_count", 0),
                                    "views_count": post.get("views_count", 0),
                                    "calculated_score": post.get("calculated_score", 0),
                                    "images": post.get("images", []),
                                    "videos": post.get("videos", []),
                                    "images_count": len(post.get("images", [])),
                                    "videos_count": len(post.get("videos", []))
                                }
                                posts_to_store.append(post_data)
                            
                            # å­˜å„²çµæœ
                            batch_id = ui_data.get("batch_id", task_id)
                            metadata = {
                                "crawl_method": "playwright_agent",
                                "max_posts_requested": max_posts,
                                "task_id": task_id,
                                "agent_version": ui_data.get("agent_version", "1.0.0")
                            }
                            
                            storage.save_crawler_result(
                                username=username,
                                posts_data=posts_to_store,
                                batch_id=batch_id,
                                metadata=metadata
                            )
                            
                            st.session_state.crawler_logs.append(f"ğŸ’¾ çˆ¬èŸ²çµæœå·²ä¿å­˜ (æ‰¹æ¬¡ID: {batch_id[:8]}...)")
                            
                        except Exception as e:
                            st.session_state.crawler_logs.append(f"âš ï¸ ä¿å­˜çˆ¬èŸ²çµæœå¤±æ•—: {e}")
                        
                    else:
                        st.session_state.final_data = final_data
                    
                    st.session_state.crawler_status = 'completed'
                    posts_count = len(st.session_state.final_data.get("posts", []))
                    st.session_state.crawler_logs.append(f"âœ… çˆ¬å–å®Œæˆï¼æˆåŠŸç²å– {posts_count} ç¯‡è²¼æ–‡")
                    
                except json.JSONDecodeError as e:
                    st.session_state.crawler_logs.append(f"âŒ ç„¡æ³•è§£æéŸ¿æ‡‰ JSON: {e}")
                    st.session_state.crawler_status = 'error'
                
        except httpx.ConnectError as e:
            error_msg = f"é€£ç·šéŒ¯èª¤: ç„¡æ³•é€£ç·šè‡³ {self.agent_url}ã€‚è«‹ç¢ºèª Docker å®¹å™¨æ˜¯å¦æ­£åœ¨é‹è¡Œã€‚"
            st.session_state.crawler_logs.append(error_msg)
            st.session_state.crawler_status = 'error'
        except Exception as e:
            st.session_state.crawler_logs.append(f"åŸ·è¡Œæ™‚ç™¼ç”Ÿæœªé æœŸçš„éŒ¯èª¤: {e}")
            st.session_state.crawler_status = 'error'

    def _start_sse_listener(self, task_id: str):
        """å•Ÿå‹• SSE ç›£è½å™¨ï¼ˆåœ¨èƒŒæ™¯åŸ·è¡Œï¼‰"""
        import tempfile
        import json
        import os
        
        # å‰µå»ºå…±äº«çš„é€²åº¦æ–‡ä»¶
        progress_file = tempfile.NamedTemporaryFile(mode='w+', delete=False, suffix=f'_{task_id}.json')
        progress_file_path = progress_file.name
        progress_file.close()
        
        # å­˜å„²é€²åº¦æ–‡ä»¶è·¯å¾‘
        st.session_state.crawler_progress_file = progress_file_path
        
        def sse_worker():
            try:
                import requests
                import json
                
                orchestrator_url = f"http://localhost:8000/stream/{task_id}"
                print(f"ğŸ“¡ é€£æ¥ SSE: {orchestrator_url}")
                
                with requests.get(orchestrator_url, stream=True, timeout=600) as response:
                    if response.status_code == 200:
                        print(f"ğŸ“¡ SSE é€£æ¥æˆåŠŸ: {response.status_code}")
                        
                        # ğŸ”¥ ä¿®å¾© #3: ä¸€é€£ä¸Šå°±å…ˆå¯« "connected"ï¼Œé¿å…ç«¶é€Ÿ
                        try:
                            with open(progress_file_path, 'w') as f:
                                json.dump({"stage": "connected", "message": "SSE é€£æ¥å·²å»ºç«‹"}, f)
                                f.flush()  # ğŸ”¥ ä¿®å¾© #2: å¼·åˆ¶åˆ·æ–°ç·©è¡å€
                                os.fsync(f.fileno())  # ğŸ”¥ ä¿®å¾© #2: å¼·åˆ¶å¯«å…¥ç£ç¢Ÿ
                            print(f"âœ… åˆå§‹é€£æ¥æ–‡ä»¶å·²å¯«å…¥: {progress_file_path}")
                        except Exception as e:
                            print(f"âš ï¸ å¯«å…¥åˆå§‹é€£æ¥æ–‡ä»¶éŒ¯èª¤: {e}")
                        
                        for line in response.iter_lines():
                            if line:
                                line_str = line.decode('utf-8').strip()
                                print(f"ğŸ” æ”¶åˆ° SSE è¡Œ: {line_str}")  # èª¿è©¦è¼¸å‡º
                                
                                # ğŸ”¥ ä¿®å¾© #1: æ”¹é€² SSE è³‡æ–™è¡Œæ ¼å¼è§£æ
                                if line_str.startswith('data:'):
                                    payload_txt = line_str.split(':', 1)[1].strip()
                                    if payload_txt:
                                        try:
                                            data = json.loads(payload_txt)
                                            print(f"ğŸ“Š è§£ææˆåŠŸ: {data}")  # èª¿è©¦è¼¸å‡º
                                            
                                            # ğŸ”¥ ä¿®å¾© #2: æ”¹é€²æª”æ¡ˆå¯«å…¥åŒæ­¥
                                            try:
                                                with open(progress_file_path, 'w') as f:
                                                    json.dump(data, f)
                                                    f.flush()  # å¼·åˆ¶åˆ·æ–°ç·©è¡å€
                                                    os.fsync(f.fileno())  # å¼·åˆ¶å¯«å…¥ç£ç¢Ÿ
                                                print(f"ğŸ’¾ é€²åº¦å·²å¯«å…¥æ–‡ä»¶")  # èª¿è©¦è¼¸å‡º
                                            except Exception as e:
                                                print(f"âš ï¸ å¯«å…¥é€²åº¦æ–‡ä»¶éŒ¯èª¤: {e}")
                                            
                                            # å¦‚æœæ”¶åˆ°å®Œæˆæˆ–éŒ¯èª¤äº‹ä»¶ï¼ŒçµæŸç›£è½
                                            if data.get('stage') in ['completed', 'error']:
                                                break
                                                
                                        except json.JSONDecodeError as e:
                                            print(f"âŒ JSON è§£æå¤±æ•—: {e}, åŸå§‹æ–‡æœ¬: {payload_txt}")
                                            continue
                    else:
                        print(f"âŒ SSE é€£æ¥å¤±æ•—: {response.status_code}")
                        
            except Exception as e:
                print(f"âŒ SSE ç›£è½éŒ¯èª¤: {e}")
            finally:
                # æ¸…ç†ï¼šå‰µå»ºå®Œæˆæ¨™è¨˜æ–‡ä»¶
                try:
                    completion_file = progress_file_path.replace('.json', '_completed.json')
                    with open(completion_file, 'w') as f:
                        json.dump({'completed': True}, f)
                except Exception:
                    pass
        
        # åœ¨èƒŒæ™¯åŸ·è¡Œç·’ä¸­å•Ÿå‹• SSE ç›£è½
        import threading
        threading.Thread(target=sse_worker, daemon=True).start()
    
    def _check_and_update_progress(self):
        """æª¢æŸ¥é€²åº¦æ–‡ä»¶ä¸¦æ›´æ–° UI ç‹€æ…‹ï¼Œè¿”å›æ˜¯å¦æœ‰æ›´æ–°"""
        import json
        import os
        
        progress_file_path = st.session_state.get('crawler_progress_file')
        if not progress_file_path or not os.path.exists(progress_file_path):
            # ğŸ”¥ æ›´è©³ç´°çš„èª¿è©¦ä¿¡æ¯
            if not progress_file_path:
                print("ğŸ” æ²’æœ‰é€²åº¦æ–‡ä»¶è·¯å¾‘")
            else:
                print(f"ğŸ” é€²åº¦æ–‡ä»¶ä¸å­˜åœ¨: {progress_file_path}")
            return False
            
        try:
            # æª¢æŸ¥æ–‡ä»¶ä¿®æ”¹æ™‚é–“
            current_mtime = os.path.getmtime(progress_file_path)
            last_mtime = st.session_state.get('crawler_progress_mtime', 0)
            
            if current_mtime <= last_mtime:
                # print(f"ğŸ” æª”æ¡ˆæ²’æœ‰æ›´æ–°: {current_mtime} <= {last_mtime}")
                return False  # æ²’æœ‰æ›´æ–°
                
            # æ›´æ–°ä¿®æ”¹æ™‚é–“
            st.session_state.crawler_progress_mtime = current_mtime
            print(f"ğŸ”¥ æª¢æ¸¬åˆ°é€²åº¦æ–‡ä»¶æ›´æ–°: {current_mtime}")
            
            # è®€å–é€²åº¦æ•¸æ“š
            with open(progress_file_path, 'r') as f:
                data = json.load(f)
            
            print(f"ğŸ”¥ è®€å–åˆ°é€²åº¦æ•¸æ“š: stage={data.get('stage')}, progress={data.get('progress')}")
            
            # æ›´æ–° UI ç‹€æ…‹
            self._update_ui_from_progress(data)
            return True
            
        except Exception as e:
            print(f"âš ï¸ æª¢æŸ¥é€²åº¦æ–‡ä»¶éŒ¯èª¤: {e}")
            return False
    
    def _update_ui_from_progress(self, data: dict):
        """æ ¹æ“šé€²åº¦æ•¸æ“šæ›´æ–° UI ç‹€æ…‹"""
        stage = data.get('stage', '')
        
        # ç¢ºä¿æ—¥èªŒåˆ—è¡¨å­˜åœ¨
        if 'crawler_logs' not in st.session_state:
            st.session_state.crawler_logs = []
        
        if stage == 'connected':
            st.session_state.crawler_logs.append("ğŸ“¡ SSE é€£æ¥å·²å»ºç«‹")
            if not hasattr(st.session_state, 'debug_messages'):
                st.session_state.debug_messages = []
            st.session_state.debug_messages.append(f"ğŸ“¡ SSE é€£æ¥å·²å»ºç«‹ (connected)")
            
        elif stage == 'fetch_start':
            username = data.get('username', '')
            st.session_state.crawler_logs.append(f"ğŸ” é–‹å§‹çˆ¬å– @{username} çš„è²¼æ–‡...")
            st.session_state.crawler_current_work = f"æ­£åœ¨é€£æ¥ Threads API..."
            
        elif stage == 'post_parsed':
            current = data.get('current', 0)
            total = data.get('total', 1)
            progress = data.get('progress', 0)
            post_id = data.get('post_id', '')
            content_preview = data.get('content_preview', '')
            
            # ğŸ”¥ æå–å®Œæ•´çš„çµ±è¨ˆæ•¸æ“š
            likes_count = data.get('likes_count', 0)
            comments_count = data.get('comments_count', 0)
            reposts_count = data.get('reposts_count', 0)
            shares_count = data.get('shares_count', 0)
            views_count = data.get('views_count', 0)
            calculated_score = data.get('calculated_score', 0)
            content = data.get('content', '')
            url = data.get('url', '')
            created_at = data.get('created_at', '')
            images_count = data.get('images_count', 0)
            videos_count = data.get('videos_count', 0)
            media_urls = data.get('media_urls', {})
            
            # ğŸ”¥ å¼·åˆ¶è¨­ç½®é€²åº¦ä¸¦æ·»åŠ èª¿è©¦ä¿¡æ¯
            st.session_state.crawler_progress = progress
            if not hasattr(st.session_state, 'debug_messages'):
                st.session_state.debug_messages = []
            st.session_state.debug_messages.append(f"ğŸ”¥ è¨­ç½®é€²åº¦: {progress:.1%} (post_parsed)")
            
            # ğŸ”¥ æ›´æ–°ç•¶å‰å·¥ä½œç‹€æ…‹
            st.session_state.crawler_current_work = f"å·²è§£æ {current}/{total} ç¯‡è²¼æ–‡ - æ­£åœ¨è§£æä¸‹ä¸€ç¯‡..."
            st.session_state.crawler_logs.append(f"âœ… è§£æè²¼æ–‡ {post_id[-8:]}: {likes_count}è®š - {content_preview}")
            
            # ğŸ”¥ å‰µå»ºå®Œæ•´çš„è²¼æ–‡å°è±¡ä¸¦æ·»åŠ åˆ°åˆ—è¡¨
            post_data = {
                'post_id': post_id,
                'summary': f"è²¼æ–‡ {post_id[-8:]}",
                'timestamp': created_at[:19] if created_at else "æœªçŸ¥æ™‚é–“",
                'content': content,
                'content_preview': content_preview,
                'url': url,
                'likes_count': likes_count,
                'comments_count': comments_count,
                'reposts_count': reposts_count,
                'shares_count': shares_count,
                'views_count': views_count,
                'calculated_score': calculated_score,
                'images_count': images_count,
                'videos_count': videos_count,
                'media_urls': media_urls
            }
            
            # ç¢ºä¿ crawler_posts åˆ—è¡¨å­˜åœ¨
            if 'crawler_posts' not in st.session_state:
                st.session_state.crawler_posts = []
            
            # æ·»åŠ åˆ°è²¼æ–‡åˆ—è¡¨ï¼ˆé¿å…é‡è¤‡ï¼‰
            existing_ids = [p.get('post_id') for p in st.session_state.crawler_posts]
            if post_id not in existing_ids:
                st.session_state.crawler_posts.append(post_data)
            
        elif stage == 'batch_parsed':
            batch_size = data.get('batch_size', 0)
            current = data.get('current', 0)
            total = data.get('total', 1)
            query_name = data.get('query_name', '')
            st.session_state.crawler_current_work = f"å·²è™•ç† {query_name} æ‰¹æ¬¡ï¼Œç²å¾— {batch_size} ç¯‡æ–°è²¼æ–‡..."
            st.session_state.crawler_logs.append(f"ğŸ“¦ å¾ {query_name} è§£æäº† {batch_size} å‰‡è²¼æ–‡ï¼Œç¸½è¨ˆ: {current}/{total}")
            
        elif stage == 'fill_views_start':
            st.session_state.crawler_current_work = "æ­£åœ¨è£œé½Šç€è¦½æ•¸æ•¸æ“š..."
            st.session_state.crawler_logs.append("ğŸ‘ï¸ é–‹å§‹è£œé½Šç€è¦½æ•¸...")
            
        elif stage == 'views_fetched':
            post_id = data.get('post_id', '')
            views_formatted = data.get('views_formatted', '0')
            st.session_state.crawler_current_work = f"æ­£åœ¨ç²å–è²¼æ–‡ {post_id[-8:]} çš„ç€è¦½æ•¸..."
            st.session_state.crawler_logs.append(f"ğŸ‘ï¸ è²¼æ–‡ {post_id[-8:]}: {views_formatted} æ¬¡ç€è¦½")
            
        elif stage == 'fill_views_completed':
            st.session_state.crawler_current_work = "ç€è¦½æ•¸è£œé½Šå®Œæˆï¼Œæº–å‚™æœ€çµ‚è™•ç†..."
            st.session_state.crawler_logs.append("âœ… ç€è¦½æ•¸è£œé½Šå®Œæˆ")
            
        elif stage == 'completed':
            st.session_state.crawler_current_work = "ğŸ‰ çˆ¬å–ä»»å‹™å·²å®Œæˆï¼"
            st.session_state.crawler_logs.append("ğŸ‰ çˆ¬å–ä»»å‹™å®Œæˆï¼")
            st.session_state.crawler_progress = 1.0
            st.session_state.crawler_status = 'completed'
            
        elif stage == 'error':
            error_msg = data.get('error', 'æœªçŸ¥éŒ¯èª¤')
            st.session_state.crawler_logs.append(f"âŒ çˆ¬å–éŒ¯èª¤: {error_msg}")
            st.session_state.crawler_status = 'error'
    
    def _handle_sse_event(self, data: dict):
        """è™•ç† SSE äº‹ä»¶ï¼ˆç·šç¨‹å®‰å…¨ç‰ˆæœ¬ï¼‰"""
        stage = data.get('stage', '')
        
        def safe_log(message: str):
            """å®‰å…¨åœ°è¨˜éŒ„æ—¥èªŒåˆ° session state"""
            try:
                if hasattr(st.session_state, 'crawler_logs') and st.session_state.crawler_logs is not None:
                    st.session_state.crawler_logs.append(message)
                else:
                    print(message)  # å‚™ç”¨æ—¥èªŒ
            except Exception:
                print(message)  # å‚™ç”¨æ—¥èªŒ
        
        def safe_set_progress(progress: float):
            """å®‰å…¨åœ°è¨­ç½®é€²åº¦"""
            try:
                if hasattr(st.session_state, 'crawler_progress'):
                    st.session_state.crawler_progress = progress
            except Exception:
                print(f"ğŸ“Š é€²åº¦: {progress:.1%}")
        
        def safe_set_status(status: str):
            """å®‰å…¨åœ°è¨­ç½®ç‹€æ…‹"""
            try:
                if hasattr(st.session_state, 'crawler_status'):
                    st.session_state.crawler_status = status
            except Exception:
                print(f"ç‹€æ…‹: {status}")
        
        if stage == 'connected':
            safe_log("ğŸ“¡ SSE é€£æ¥å·²å»ºç«‹")
        elif stage == 'fetch_start':
            safe_log(f"ğŸ” é–‹å§‹çˆ¬å– @{data.get('username')} çš„è²¼æ–‡...")
        elif stage == 'fetch_progress':
            current = data.get('current', 0)
            total = data.get('total', 1)
            progress = data.get('progress', 0)
            safe_set_progress(progress)
            safe_log(f"ğŸ“Š é€²åº¦: {current}/{total} ç¯‡è²¼æ–‡ ({progress:.1%})")
        elif stage == 'post_parsed':
            # ğŸ”¥ æ–°å¢ï¼šæ¯è§£æä¸€å€‹è²¼æ–‡çš„è©³ç´°é€²åº¦
            current = data.get('current', 0)
            total = data.get('total', 1)
            progress = data.get('progress', 0)
            post_id = data.get('post_id', '')
            content_preview = data.get('content_preview', '')
            likes = data.get('likes', 0)
            safe_set_progress(progress)
            safe_log(f"âœ… è§£æè²¼æ–‡ {post_id[-8:]}: {likes}è®š - {content_preview}")
        elif stage == 'batch_parsed':
            # ğŸ”¥ æ–°å¢ï¼šæ¯æ‰¹è§£æå®Œæˆçš„é€²åº¦
            batch_size = data.get('batch_size', 0)
            current = data.get('current', 0)
            total = data.get('total', 1)
            query_name = data.get('query_name', '')
            safe_log(f"ğŸ“¦ å¾ {query_name} è§£æäº† {batch_size} å‰‡è²¼æ–‡ï¼Œç¸½è¨ˆ: {current}/{total}")
        elif stage == 'fill_views_start':
            safe_log("ğŸ‘ï¸ é–‹å§‹è£œé½Šç€è¦½æ•¸...")
        elif stage == 'views_fetched':
            # ğŸ”¥ æ–°å¢ï¼šæ¯ç²å–ä¸€å€‹ç€è¦½æ•¸çš„è©³ç´°é€²åº¦
            post_id = data.get('post_id', '')
            views_formatted = data.get('views_formatted', '0')
            safe_log(f"ğŸ‘ï¸ è²¼æ–‡ {post_id[-8:]}: {views_formatted} æ¬¡ç€è¦½")
        elif stage == 'fill_views_completed':
            safe_log("âœ… ç€è¦½æ•¸è£œé½Šå®Œæˆ")
        elif stage == 'completed':
            safe_log("ğŸ‰ çˆ¬å–ä»»å‹™å®Œæˆï¼")
            safe_set_progress(1.0)
        elif stage == 'error':
            error_msg = data.get('error', 'æœªçŸ¥éŒ¯èª¤')
            safe_log(f"âŒ çˆ¬å–éŒ¯èª¤: {error_msg}")
            safe_set_status('error')
        elif stage == 'heartbeat':
            # å¿ƒè·³äº‹ä»¶ï¼Œä¸é¡¯ç¤º
            pass

    def _render_crawler_progress(self):
        """æ¸²æŸ“çˆ¬èŸ²é€²åº¦ï¼ˆé©åˆå´é‚Šæ¬„ï¼‰"""
        target = st.session_state.get('crawler_target', {})
        username = target.get("username", "N/A")
        max_posts = target.get("max_posts", 0)
        
        # æª¢æŸ¥ä¸¦æ›´æ–°é€²åº¦
        progress_updated = self._check_and_update_progress()
        progress = st.session_state.get('crawler_progress', 0)
        status = st.session_state.get('crawler_status', 'idle')
        current_work = st.session_state.get('crawler_current_work', '')
        
        # ç·Šæ¹Šé¡¯ç¤º
        st.write(f"ğŸ‘¤ @{username}")
        
        # é€²åº¦æ¢
        if progress > 0:
            st.progress(progress)
            if max_posts > 0:
                estimated = int(progress * max_posts)
                st.write(f"ğŸ“Š {estimated}/{max_posts} ({progress:.1%})")
            else:
                st.write(f"ğŸ“Š {progress:.1%}")
        else:
            st.write("ğŸ“Š æº–å‚™ä¸­...")
        
        # ç‹€æ…‹
        status_emoji = {"idle": "âšª", "running": "ğŸŸ¡", "completed": "ğŸŸ¢", "error": "ğŸ”´"}
        st.write(f"{status_emoji.get(status, 'âšª')} {status}")
        
        # ç•¶å‰å·¥ä½œ
        if current_work:
            st.write(f"ğŸ”„ {current_work}")
        else:
            # å¦‚æœæ²’æœ‰ç•¶å‰å·¥ä½œä½†ç‹€æ…‹æ˜¯runningï¼Œé¡¯ç¤ºé»˜èªä¿¡æ¯
            if status == 'running':
                st.write("ğŸ”„ æ­£åœ¨è™•ç†ä¸­...")
        
        # æœ€è¿‘æ—¥èªŒï¼ˆç·Šæ¹Šé¡¯ç¤ºï¼‰
        logs = st.session_state.get('crawler_logs', [])
        if logs:
            with st.expander("ğŸ“ é€²åº¦æ—¥èªŒ", expanded=False):
                for log in logs[-5:]:  # æœ€è¿‘5æ¢
                    st.write(f"â€¢ {log}")
        
        # ğŸ”¥ å¯¦æ™‚æ›´æ–°æç¤º
        if status == 'running':
            st.info("â±ï¸ æ¯2ç§’è‡ªå‹•æ›´æ–°é€²åº¦")
        
        # èª¿è©¦ä¿¡æ¯ï¼ˆå¯é¸ï¼‰
        if st.session_state.get('show_debug_in_sidebar', False):
            with st.expander("ğŸ”§ èª¿è©¦ä¿¡æ¯"):
                st.write(f"ğŸ†” ä»»å‹™: {st.session_state.get('crawler_task_id', 'N/A')[-8:]}")
                st.write(f"ğŸ”„ æ›´æ–°: {progress_updated}")
                
                # é€²åº¦æ–‡ä»¶ç‹€æ…‹
                progress_file = st.session_state.get('crawler_progress_file')
                if progress_file and os.path.exists(progress_file):
                    st.write("âœ… é€²åº¦æ–‡ä»¶å­˜åœ¨")
                else:
                    st.write("âŒ é€²åº¦æ–‡ä»¶ä¸å­˜åœ¨")
            st.write(f"ğŸ“Š é€²åº¦å€¼: {progress:.1%}")
            st.write(f"ğŸ”„ å·²æ›´æ–°: {progress_updated}")
            st.write(f"ğŸ†” Task ID: {st.session_state.get('crawler_task_id', 'N/A')}")
            st.write(f"ğŸ“ é€²åº¦æ–‡ä»¶: {st.session_state.get('crawler_progress_file', 'N/A')}")
            st.write(f"â° æœ€å¾Œä¿®æ”¹: {st.session_state.get('crawler_progress_mtime', 'N/A')}")
            st.write(f"ğŸ“‹ ç‹€æ…‹: {st.session_state.get('crawler_status', 'N/A')}")
            st.write(f"ğŸ“ ç•¶å‰å·¥ä½œ: {st.session_state.get('crawler_current_work', 'N/A')}")
            
            # é¡¯ç¤ºèª¿è©¦æ¶ˆæ¯
            debug_messages = st.session_state.get('debug_messages', [])
            if debug_messages:
                st.write("ğŸ” æœ€æ–°èª¿è©¦æ¶ˆæ¯:")
                for msg in debug_messages[-10:]:  # é¡¯ç¤ºæœ€è¿‘10æ¢
                    st.write(f"  {msg}")
            
            # é¡¯ç¤ºé€²åº¦æ–‡ä»¶å…§å®¹
            progress_file = st.session_state.get('crawler_progress_file')
            if progress_file and os.path.exists(progress_file):
                try:
                    with open(progress_file, 'r') as f:
                        file_content = json.load(f)
                    st.write("ğŸ“„ é€²åº¦æ–‡ä»¶å…§å®¹:")
                    st.json(file_content)
                except Exception as e:
                    st.write(f"âŒ ç„¡æ³•è®€å–é€²åº¦æ–‡ä»¶: {e}")
            else:
                st.write("âŒ é€²åº¦æ–‡ä»¶ä¸å­˜åœ¨")
        
        # ğŸ”¥ å¼·åˆ¶é¡¯ç¤ºè²¼æ–‡é è¦½ï¼ˆä¸ç®¡ä»€éº¼ç‹€æ…‹éƒ½é¡¯ç¤ºï¼‰
        posts = st.session_state.get('crawler_posts', [])
        if posts:
            st.markdown("---")
            st.subheader("ğŸ“ å³æ™‚è²¼æ–‡é è¦½")
            
            # é¡¯ç¤ºæœ€æ–°çš„3å€‹è²¼æ–‡
            recent_posts = posts[-3:]
            
            for post in recent_posts:
                # ä½¿ç”¨å¡ç‰‡æ¨£å¼é¡¯ç¤ºè²¼æ–‡
                with st.container():
                    st.markdown(f"**ğŸ†” {post.get('summary', 'N/A')}** `{post.get('timestamp', 'N/A')}`")
                    
                    # é¡¯ç¤ºè²¼æ–‡è©³æƒ…
                    col1, col2 = st.columns([2, 1])
                    with col1:
                        # å…§å®¹é è¦½
                        content_preview = post.get('content_preview', post.get('content', ''))
                        if content_preview:
                            st.write(f"ğŸ’¬ {content_preview}")
                        else:
                            st.write("ğŸ’¬ ç„¡å…§å®¹")
                        
                        # åª’é«”ä¿¡æ¯
                        images_count = post.get('images_count', 0)
                        videos_count = post.get('videos_count', 0)
                        if images_count > 0 or videos_count > 0:
                            st.write(f"ğŸ“¸ åœ–ç‰‡: {images_count} | ğŸ¥ å½±ç‰‡: {videos_count}")
                    
                    with col2:
                        # ğŸ”¥ å®Œæ•´çš„çµ±è¨ˆæ•¸æ“šï¼ˆç¸½æ˜¯é¡¯ç¤ºï¼‰
                        likes_count = post.get('likes_count', 0)
                        comments_count = post.get('comments_count', 0)
                        reposts_count = post.get('reposts_count', 0)
                        shares_count = post.get('shares_count', 0)
                        views_count = post.get('views_count', 0)
                        calculated_score = post.get('calculated_score', 0)
                        
                        st.write(f"â¤ï¸ è®š: {likes_count:,}")
                        st.write(f"ğŸ’¬ ç•™è¨€: {comments_count:,}")
                        st.write(f"ğŸ”„ è½‰ç™¼: {reposts_count:,}")
                        st.write(f"ğŸ“¤ åˆ†äº«: {shares_count:,}")
                        st.write(f"ğŸ‘ï¸ ç€è¦½: {views_count:,}")
                        st.write(f"â­ åˆ†æ•¸: {calculated_score:.1f}")
                    
                    st.markdown("---")
        else:
            st.info("ğŸ“ æš«ç„¡è²¼æ–‡é è¦½ï¼Œç­‰å¾…çˆ¬å–æ•¸æ“š...")
        
        # ğŸ”¥ ç‹€æ…‹ç›¸é—œé¡¯ç¤º
        if status == 'running':
            st.markdown("---")
            st.subheader("ğŸ“Š çˆ¬å–ç‹€æ…‹")
            
            # é¡¯ç¤ºç•¶å‰é€²åº¦è©³æƒ…
            
            if progress > 0:
                estimated_posts = int(progress * max_posts)
                st.success(f"ğŸ“Š é€²åº¦: {estimated_posts}/{max_posts} ç¯‡è²¼æ–‡ ({progress:.1%})")
            else:
                st.info(f"ğŸ”„ åˆå§‹åŒ–ä¸­... ç•¶å‰é€²åº¦: {progress:.1%}")
            
            # ğŸ“Š å³æ™‚å·¥ä½œç‹€æ…‹å ±å‘Š
            task_id = st.session_state.get('crawler_task_id', 'N/A')
            current_work = st.session_state.get('crawler_current_work', 'æ­£åœ¨åˆå§‹åŒ–...')
            
            col1, col2 = st.columns([2, 1])
            with col1:
                st.write(f"ğŸ” **æ­£åœ¨çˆ¬å– @{username}**")
                st.info(f"âš¡ **ç•¶å‰å·¥ä½œ**: {current_work}")
            with col2:
                st.write(f"ğŸ†” Task: `{task_id[:8]}...`")
            
            # å·²ç§»åˆ°ä¸Šé¢ç¸½æ˜¯é¡¯ç¤ºçš„å€åŸŸ
            
            # è‡ªå‹•åˆ·æ–°
            if progress_updated:
                st.success(f"ğŸ”„ æœ‰é€²åº¦æ›´æ–°ï¼Œç«‹å³åˆ·æ–°")
                st.rerun()
            else:
                st.info(f"â³ ç„¡é€²åº¦æ›´æ–°ï¼Œç­‰å¾…2ç§’å¾Œåˆ·æ–°")
                time.sleep(2)  # æ¸›å°‘åˆ°2ç§’æ›´é »ç¹åˆ·æ–°
                st.rerun()
            
        elif status == 'completed':
            st.progress(1.0)
            st.success("âœ… çˆ¬å–å®Œæˆï¼")
            final_data = st.session_state.get('final_data')
            if final_data:
                posts_count = len(final_data.get("posts", []))
                st.text(f"æˆåŠŸç²å– {posts_count} ç¯‡è²¼æ–‡")
                
        elif status == 'error':
            st.progress(0.0)
            st.error("âŒ çˆ¬å–éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤")
        
        # å®Œæ•´æ—¥èªŒï¼ˆå¯é¸å±•é–‹æŸ¥çœ‹ï¼‰
        if st.session_state.get('crawler_logs'):
            with st.expander("ğŸ“‹ æŸ¥çœ‹å®Œæ•´æ—¥èªŒ", expanded=False):
                for log in st.session_state.crawler_logs[-10:]:  # æœ€å¤šé¡¯ç¤º10æ¢
                    st.text(log)
    
    def _render_crawler_results(self):
        """æ¸²æŸ“çˆ¬èŸ²çµæœ"""
        st.subheader("ğŸ“‹ çˆ¬å–çµæœ")
        
        final_data = st.session_state.get('final_data')
        if not final_data:
            st.warning("æ²’æœ‰çˆ¬å–åˆ°æ•¸æ“š")
            return
        
        # çµæœæ‘˜è¦
        posts = final_data.get("posts", [])
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("æ‰¹æ¬¡ ID", final_data.get('batch_id', 'N/A'))
        with col2:
            st.metric("ç”¨æˆ¶", final_data.get('username', 'N/A'))
        with col3:
            st.metric("ç¸½æ•¸é‡", final_data.get('total_count', 0))
        with col4:
            st.metric("æˆåŠŸçˆ¬å–", len(posts))
        
        # ä¸‹è¼‰æŒ‰éˆ•
        self._render_download_button(final_data)
        
        # è²¼æ–‡é è¦½
        if posts:
            st.subheader("ğŸ“ è²¼æ–‡é è¦½")
            
            # ğŸ”¥ ä¿®å¾©æ’åºå•é¡Œï¼šç¢ºä¿é è¦½ä¹ŸæŒ‰æ™‚é–“é™åºæ’åˆ—
            sorted_posts = sorted(posts, 
                                 key=lambda p: p.get('created_at', '') or '', 
                                 reverse=True)
            
            for i, post in enumerate(sorted_posts[:10]):  # é¡¯ç¤ºå‰10ç¯‡
                with st.expander(f"è²¼æ–‡ {i+1} - {post.get('post_id', 'N/A')}", expanded=i < 2):
                    col1, col2 = st.columns([3, 1])
                    
                    with col1:
                        # å…§å®¹
                        content = post.get('content', '')
                        if content:
                            st.write("**å…§å®¹:**")
                            st.write(content[:200] + "..." if len(content) > 200 else content)
                        
                        # åª’é«” URL
                        if post.get('media_urls'):
                            st.write("**åª’é«”:**")
                            for media_url in post['media_urls'][:3]:  # æœ€å¤šé¡¯ç¤º3å€‹
                                st.write(f"ğŸ”— {media_url}")
                    
                    with col2:
                        st.write("**çµ±è¨ˆ:**")
                        st.write(f"â¤ï¸ è®š: {post.get('likes_count', 0):,}")
                        st.write(f"ğŸ’¬ ç•™è¨€: {post.get('comments_count', 0):,}")
                        st.write(f"ğŸ”„ è½‰ç™¼: {post.get('reposts_count', 0):,}")
                        st.write(f"ğŸ“¤ åˆ†äº«: {post.get('shares_count', 0):,}")
                        st.write(f"ğŸ‘ï¸ ç€è¦½: {post.get('views_count', 0):,}")
                        st.write(f"â­ åˆ†æ•¸: {post.get('calculated_score', 0):.1f}")
                        
                        st.write("**è©³æƒ…:**")
                        st.write(f"ğŸ”— [åŸæ–‡]({post.get('url', '#')})")
                        st.write(f"ğŸ“… {post.get('created_at', 'N/A')}")
    
    def _render_download_button(self, final_data: Dict[str, Any]):
        """æ¸²æŸ“ä¸‹è¼‰æŒ‰éˆ•"""
        st.subheader("ğŸ’¾ ä¸‹è¼‰çµæœ")
        
        # æº–å‚™ä¸‹è¼‰æ•¸æ“š
        json_str = json.dumps(final_data, indent=2, ensure_ascii=False)
        filename = f"threads_crawl_{final_data.get('username', 'unknown')}_{int(time.time())}.json"
        
        col1, col2, col3 = st.columns([1, 1, 2])
        
        with col1:
            st.download_button(
                label="ğŸ“¥ ä¸‹è¼‰ JSON",
                data=json_str,
                file_name=filename,
                mime="application/json",
                use_container_width=True
            )
        
        with col2:
            # é¡¯ç¤ºæ–‡ä»¶å¤§å°
            file_size = len(json_str.encode('utf-8'))
            st.metric("æ–‡ä»¶å¤§å°", f"{file_size / 1024:.1f} KB")
        
        with col3:
            st.info(f"ğŸ“ æ–‡ä»¶å: {filename}")
    
    def _render_crawler_logs(self):
        """æ¸²æŸ“çˆ¬èŸ²æ—¥èªŒ"""
        if st.session_state.get('crawler_logs'):
            with st.expander("ğŸ“‹ çˆ¬å–æ—¥èªŒ", expanded=False):
                for log in st.session_state.crawler_logs[-10:]:  # æœ€å¤šé¡¯ç¤º10æ¢
                    st.text(log)
    

    
    def _reset_crawler(self):
        """é‡ç½®çˆ¬èŸ²ç‹€æ…‹"""
        keys_to_reset = [
            'crawler_status', 'crawler_target', 'crawler_logs', 'crawler_posts',
            'crawler_events', 'final_data', 'crawler_step', 'crawler_progress',
            'crawler_task_id', 'crawler_progress_file', 'crawler_progress_mtime',
            'crawler_current_work', 'debug_messages'
        ]
        for key in keys_to_reset:
            if key in st.session_state:
                del st.session_state[key]
        
        # ğŸ”¥ å¼·åˆ¶é‡ç½®è¼¸å…¥æ¡†ï¼ˆè§£æ±ºå¡ä½å•é¡Œï¼‰
        st.session_state.crawler_username = ""
        st.session_state.crawler_max_posts = 10