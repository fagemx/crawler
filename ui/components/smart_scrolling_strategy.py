"""
æ™ºèƒ½æ»‘å‹•ç­–ç•¥æ¨¡çµ„
å°‡ realtime_crawler çš„æ™ºèƒ½æ©Ÿåˆ¶ç§»æ¤çµ¦ playwright_crawler_component_v2.py ä½¿ç”¨
"""

import asyncio
import random
from typing import Dict, List, Optional

class SmartScrollingStrategy:
    """æ™ºèƒ½æ»¾å‹•ç­–ç•¥é¡"""
    
    def __init__(self):
        self.scroll_rounds = 0
        self.max_scroll_rounds = 80
        self.no_new_content_rounds = 0
        self.max_no_new_rounds = 15
        self.consecutive_existing_rounds = 0
        self.max_consecutive_existing = 15
        
    async def smart_scroll_and_collect(
        self, 
        page, 
        target_username: str, 
        max_posts: int, 
        existing_post_ids: set = None,
        is_incremental: bool = True
    ) -> List[str]:
        """
        æ™ºèƒ½æ»‘å‹•ä¸¦æ”¶é›†URLs
        æ¡ç”¨ realtime_crawler çš„ç­–ç•¥
        """
        urls = []
        existing_post_ids = existing_post_ids or set()
        
        print("ğŸ”„ é–‹å§‹æ™ºèƒ½æ»¾å‹•æ”¶é›†URLs...")
        print(f"ğŸ“‹ æ¨¡å¼: {'å¢é‡æ”¶é›†' if is_incremental else 'å…¨é‡æ”¶é›†'}")
        
        while len(urls) < max_posts and self.scroll_rounds < self.max_scroll_rounds:
            # æå–ç•¶å‰é é¢çš„URLs
            current_urls = await self._extract_current_urls(page, target_username)
            
            before_count = len(urls)
            new_urls_this_round = 0
            found_existing_this_round = False
            
            # è™•ç†æ–°ç™¼ç¾çš„URLs
            for url in current_urls:
                if len(urls) >= max_posts:
                    break
                    
                post_id = url.split('/')[-1] if url else None
                
                # è·³éå·²æ”¶é›†çš„URL
                if url in urls:
                    continue
                    
                # å¢é‡æ¨¡å¼ï¼šæª¢æŸ¥æ˜¯å¦å·²å­˜åœ¨æ–¼è³‡æ–™åº«
                if is_incremental and post_id in existing_post_ids:
                    print(f"   ğŸ” [{len(urls)+1}] ç™¼ç¾å·²çˆ¬å–è²¼æ–‡: {post_id} - è·³é (å¢é‡æ¨¡å¼)")
                    found_existing_this_round = True
                    continue
                
                urls.append(url)
                new_urls_this_round += 1
                
                status_icon = "ğŸ†•" if is_incremental else "ğŸ“"
                print(f"   {status_icon} [{len(urls)}] ç™¼ç¾: {post_id}")
            
            # å¢é‡æ¨¡å¼ï¼šæ™ºèƒ½åœæ­¢æ¢ä»¶
            if is_incremental and found_existing_this_round:
                self.consecutive_existing_rounds += 1
                if len(urls) >= max_posts:
                    print(f"   âœ… å¢é‡æª¢æ¸¬: å·²æ”¶é›†è¶³å¤ æ–°è²¼æ–‡ ({len(urls)} å€‹)")
                    break
                elif self.consecutive_existing_rounds >= self.max_consecutive_existing:
                    print(f"   â¹ï¸ å¢é‡æª¢æ¸¬: é€£çºŒ {self.consecutive_existing_rounds} è¼ªç™¼ç¾å·²å­˜åœ¨è²¼æ–‡ï¼Œåœæ­¢æ”¶é›†")
                    print(f"   ğŸ“Š æœ€çµ‚æ”¶é›†: {len(urls)} å€‹æ–°è²¼æ–‡ (ç›®æ¨™: {max_posts})")
                    break
                else:
                    print(f"   ğŸ” å¢é‡æª¢æ¸¬: ç™¼ç¾å·²å­˜åœ¨è²¼æ–‡ä½†æ•¸é‡ä¸è¶³ ({len(urls)}/{max_posts})ï¼Œç¹¼çºŒæ»¾å‹•... (é€£çºŒç™¼ç¾: {self.consecutive_existing_rounds}/{self.max_consecutive_existing})")
            else:
                self.consecutive_existing_rounds = 0
            
            # æª¢æŸ¥æ˜¯å¦æœ‰æ–°å…§å®¹
            new_urls_found = len(urls) - before_count
            
            if new_urls_found == 0:
                self.no_new_content_rounds += 1
                print(f"   â³ ç¬¬{self.scroll_rounds+1}è¼ªæœªç™¼ç¾æ–°URL ({self.no_new_content_rounds}/{self.max_no_new_rounds})")
                
                if self.no_new_content_rounds >= self.max_no_new_rounds:
                    # åŸ·è¡Œæœ€å¾Œå˜—è©¦
                    final_new_count = await self._final_attempt_scroll(page, target_username, urls, max_posts, existing_post_ids, is_incremental)
                    if final_new_count == 0:
                        print("   âœ… ç¢ºèªå·²åˆ°é”é é¢åº•éƒ¨")
                        break
                    else:
                        print(f"   ğŸ¯ æœ€å¾Œå˜—è©¦ç™¼ç¾{final_new_count}å€‹æ–°URLï¼Œç¹¼çºŒ...")
                        self.no_new_content_rounds = 0
                else:
                    # éå¢ç­‰å¾…æ™‚é–“
                    await self._progressive_wait()
            else:
                self.no_new_content_rounds = 0
                print(f"   âœ… ç¬¬{self.scroll_rounds+1}è¼ªç™¼ç¾{new_urls_found}å€‹æ–°URL")
            
            if len(urls) >= max_posts:
                print(f"   ğŸ¯ å·²é”åˆ°ç›®æ¨™æ•¸é‡ {max_posts}")
                break
            
            # åŸ·è¡Œæ™ºèƒ½æ»¾å‹•
            await self._execute_smart_scroll(page)
            
            # çµ±ä¸€çš„è¼‰å…¥æª¢æ¸¬
            await self._wait_for_loading(page)
            
            self.scroll_rounds += 1
            
            # æ¯5è¼ªé¡¯ç¤ºé€²åº¦
            if self.scroll_rounds % 5 == 0:
                print(f"   ğŸ“Š æ»¾å‹•é€²åº¦: ç¬¬{self.scroll_rounds}è¼ªï¼Œå·²æ”¶é›†{len(urls)}å€‹URL")
        
        if self.scroll_rounds >= self.max_scroll_rounds:
            print(f"   âš ï¸ é”åˆ°æœ€å¤§æ»¾å‹•è¼ªæ¬¡ ({self.max_scroll_rounds})ï¼Œåœæ­¢æ»¾å‹•")
        
        print(f"âœ… URLæ”¶é›†å®Œæˆï¼š{len(urls)} å€‹URLï¼Œæ»¾å‹• {self.scroll_rounds} è¼ª")
        return urls[:max_posts]
    
    async def _extract_current_urls(self, page, target_username: str) -> List[str]:
        """æå–ç•¶å‰é é¢çš„URLs"""
        current_urls = await page.evaluate(f"""
            () => {{
                const targetUsername = '{target_username}';
                const links = Array.from(document.querySelectorAll('a[href*="/post/"]'));
                return [...new Set(links.map(link => link.href)
                    .filter(url => url.includes('/post/'))
                    .filter(url => {{
                        // æª¢æŸ¥URLæ˜¯å¦å±¬æ–¼ç›®æ¨™ç”¨æˆ¶
                        const usernamePart = url.split('/@')[1];
                        if (!usernamePart) return false;
                        const extractedUsername = usernamePart.split('/')[0];
                        
                        const postId = url.split('/post/')[1];
                        // éæ¿¾æ‰ mediaã€ç„¡æ•ˆIDç­‰ï¼Œä¸¦ç¢ºä¿æ˜¯ç›®æ¨™ç”¨æˆ¶çš„è²¼æ–‡
                        return postId && 
                               postId !== 'media' && 
                               postId.length > 5 && 
                               /^[A-Za-z0-9_-]+$/.test(postId) &&
                               extractedUsername === targetUsername;
                    }}))];
            }}
        """)
        return current_urls
    
    async def _execute_smart_scroll(self, page):
        """åŸ·è¡Œæ™ºèƒ½æ»¾å‹•ç­–ç•¥"""
        if self.scroll_rounds % 6 == 5:  # æ¯6è¼ªé€²è¡Œä¸€æ¬¡æ¿€é€²æ»¾å‹•
            print("   ğŸš€ åŸ·è¡Œæ¿€é€²æ»¾å‹•æ¿€ç™¼è¼‰å…¥...")
            # æ¨¡æ“¬ç”¨æˆ¶å¿«é€Ÿæ»¾å‹•è¡Œç‚º
            await page.evaluate("window.scrollBy(0, 1600)")
            await asyncio.sleep(1.2)
            # ç¨å¾®å›æ»¾ï¼ˆåƒç”¨æˆ¶æ»¾éé ­äº†ï¼‰
            await page.evaluate("window.scrollBy(0, -250)")
            await asyncio.sleep(0.8)
            # å†ç¹¼çºŒå‘ä¸‹
            await page.evaluate("window.scrollBy(0, 1400)")
            await asyncio.sleep(3.5)
            
        elif self.scroll_rounds % 3 == 2:  # æ¯3è¼ªé€²è¡Œä¸€æ¬¡ä¸­åº¦æ»¾å‹•
            print("   ğŸ”„ åŸ·è¡Œä¸­åº¦æ»¾å‹•...")
            # åˆ†æ®µæ»¾å‹•ï¼Œæ›´åƒäººé¡è¡Œç‚º
            await page.evaluate("window.scrollBy(0, 800)")
            await asyncio.sleep(1)
            await page.evaluate("window.scrollBy(0, 600)")
            await asyncio.sleep(2.8)
            
        else:
            # æ­£å¸¸æ»¾å‹•ï¼ŒåŠ å…¥éš¨æ©Ÿæ€§å’Œäººæ€§åŒ–
            scroll_distance = 900 + (self.scroll_rounds % 3) * 100  # 900-1100pxéš¨æ©Ÿ
            await page.evaluate(f"window.scrollBy(0, {scroll_distance})")
            
            # çŸ­æš«æš«åœï¼ˆæ¨¡æ“¬ç”¨æˆ¶é–±è®€ï¼‰
            await asyncio.sleep(1.8 + (self.scroll_rounds % 2) * 0.4)  # 1.8-2.2ç§’éš¨æ©Ÿ
    
    async def _wait_for_loading(self, page):
        """ç­‰å¾…è¼‰å…¥å®Œæˆ"""
        has_loading = await page.evaluate("""
            () => {
                const indicators = document.querySelectorAll(
                    '[role="progressbar"], .loading, [aria-label*="loading"], [aria-label*="Loading"]'
                );
                return indicators.length > 0;
            }
        """)
        
        if has_loading:
            print("   â³ æª¢æ¸¬åˆ°è¼‰å…¥æŒ‡ç¤ºå™¨ï¼Œé¡å¤–ç­‰å¾…...")
            # éš¨æ©Ÿç­‰å¾…2-3.5ç§’
            loading_wait = random.uniform(2.0, 3.5)
            await asyncio.sleep(loading_wait)
    
    async def _progressive_wait(self):
        """éå¢ç­‰å¾…æ™‚é–“"""
        base_wait = min(1.2 + (self.no_new_content_rounds - 1) * 0.3, 3.5)  # 1.2s -> 3.5s
        random_factor = random.uniform(0.8, 1.2)  # Â±20%éš¨æ©Ÿè®ŠåŒ–
        progressive_wait = base_wait * random_factor
        print(f"   â²ï¸ éå¢ç­‰å¾… {progressive_wait:.1f}s...")
        await asyncio.sleep(progressive_wait)
    
    async def _final_attempt_scroll(self, page, target_username: str, urls: List[str], max_posts: int, existing_post_ids: set, is_incremental: bool) -> int:
        """æœ€å¾Œå˜—è©¦ï¼šå¤šé‡æ¿€é€²æ»¾å‹•æ¿€ç™¼è¼‰å…¥"""
        print("   ğŸš€ æœ€å¾Œå˜—è©¦ï¼šå¤šé‡æ¿€é€²æ»¾å‹•æ¿€ç™¼æ–°å…§å®¹...")
        
        # ç¬¬ä¸€æ¬¡ï¼šå¤§å¹…å‘ä¸‹
        await page.evaluate("window.scrollBy(0, 2500)")
        await asyncio.sleep(2)
        
        # ç¬¬äºŒæ¬¡ï¼šå‘ä¸Šå†å‘ä¸‹ï¼ˆæ¿€ç™¼è¼‰å…¥ï¼‰
        await page.evaluate("window.scrollBy(0, -500)")
        await asyncio.sleep(1)
        await page.evaluate("window.scrollBy(0, 3000)")
        await asyncio.sleep(3)
        
        # ç¬¬ä¸‰æ¬¡ï¼šæ»¾å‹•åˆ°æ›´åº•éƒ¨
        await page.evaluate("window.scrollBy(0, 2000)")
        await asyncio.sleep(2)
        
        print("   â³ ç­‰å¾…æ‰€æœ‰å…§å®¹è¼‰å…¥å®Œæˆ...")
        await asyncio.sleep(3)
        
        # å†æ¬¡æª¢æŸ¥æ–°URLs
        final_urls = await self._extract_current_urls(page, target_username)
        
        final_new_count = 0
        for url in final_urls:
            post_id = url.split('/')[-1] if url else None
            
            if url not in urls and len(urls) < max_posts:
                # å¢é‡æ¨¡å¼æª¢æŸ¥
                if is_incremental and post_id in existing_post_ids:
                    continue
                    
                urls.append(url)
                final_new_count += 1
                print(f"   ğŸ“ [{len(urls)}] æœ€å¾Œç™¼ç¾: {post_id}")
        
        return final_new_count
