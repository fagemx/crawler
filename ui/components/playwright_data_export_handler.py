"""
Playwright æ•¸æ“šå°å‡ºå’ŒCSVè™•ç†è™•ç†å™¨
æ‹†åˆ†è‡ª playwright_crawler_component_v2.py çš„å°å‡ºç›¸é—œåŠŸèƒ½
"""

import streamlit as st
import json
import os
from pathlib import Path
from datetime import datetime, date
from decimal import Decimal

from .playwright_utils import PlaywrightUtils


class PlaywrightDataExportHandler:
    """Playwright æ•¸æ“šå°å‡ºå’ŒCSVè™•ç†è™•ç†å™¨"""
    
    def __init__(self, db_handler):
        self.db_handler = db_handler
    
    def export_history_data(self, username: str, export_type: str, **kwargs):
        """å°å‡ºæ­·å²æ•¸æ“š"""
        try:
            import asyncio
            
            # ç²å–æ’åºåƒæ•¸
            sort_by = kwargs.get('sort_by', 'fetched_at')
            sort_order = kwargs.get('sort_order', 'DESC')
            
            with st.spinner(f"ğŸ”„ æ­£åœ¨å¾è³‡æ–™åº«ç²å– @{username} çš„{export_type}æ•¸æ“š..."):
                # ç•°æ­¥ç²å–è³‡æ–™åº«æ•¸æ“š
                posts_data = asyncio.run(self._fetch_history_from_db(username, export_type, **kwargs))
            
            if not posts_data:
                st.warning(f"âš ï¸ æ²’æœ‰æ‰¾åˆ°ç”¨æˆ¶ @{username} çš„æ­·å²æ•¸æ“š")
                return
            
            # æ’åºæ•¸æ“š
            def get_sort_key(post):
                value = post.get(sort_by, 0)
                if value is None:
                    return 0
                if isinstance(value, str):
                    try:
                        return float(value)
                    except:
                        return 0
                return value
            
            posts_data.sort(key=get_sort_key, reverse=(sort_order == 'DESC'))
            
            # æº–å‚™æ•¸æ“šçµæ§‹
            data = {
                "username": username,
                "export_type": export_type,
                "exported_at": PlaywrightUtils.get_current_taipei_time().isoformat(),
                "sort_by": sort_by,
                "sort_order": sort_order,
                "total_records": len(posts_data),
                "data": posts_data
            }
            
            # æ·»åŠ çµ±è¨ˆä¿¡æ¯
            if export_type == "analysis":
                data["summary"] = self._calculate_stats(posts_data)
            
            # åŒæ™‚æä¾› JSON å’Œ CSV ä¸‹è¼‰
            col1, col2 = st.columns(2)
            
            with col1:
                # JSON ä¸‹è¼‰
                import json
                from decimal import Decimal
                from datetime import datetime, date
                
                # è‡ªå®šç¾©JSONç·¨ç¢¼å™¨è™•ç†Decimalå’Œdatetimeé¡å‹
                def json_serializer(obj):
                    if isinstance(obj, Decimal):
                        return float(obj)
                    elif isinstance(obj, (datetime, date)):
                        return obj.isoformat()
                    raise TypeError(f"Object of type {type(obj)} is not JSON serializable")
                
                json_content = json.dumps(data, ensure_ascii=False, indent=2, default=json_serializer)
                timestamp = PlaywrightUtils.get_current_taipei_time().strftime('%Y%m%d_%H%M%S')
                json_filename = f"playwright_history_{username}_{export_type}_{timestamp}.json"
                
                st.download_button(
                    label=f"ğŸ“¥ ä¸‹è¼‰JSON ({len(posts_data)}ç­†)",
                    data=json_content,
                    file_name=json_filename,
                    mime="application/json",
                    help="ä¸‹è¼‰æ­·å²æ•¸æ“šJSONæ–‡ä»¶"
                )
            
            with col2:
                # CSV ä¸‹è¼‰
                csv_content = self.convert_to_csv(posts_data)
                # _convert_to_csv å·²ç¶“è¿”å› bytesï¼Œç„¡éœ€å†æ¬¡ç·¨ç¢¼
                csv_filename = f"playwright_history_{username}_{export_type}_{timestamp}.csv"
                
                st.download_button(
                    label=f"ğŸ“Š ä¸‹è¼‰CSV ({len(posts_data)}ç­†)",
                    data=csv_content,
                    file_name=csv_filename,
                    mime="text/csv",
                    help="ä¸‹è¼‰æ­·å²æ•¸æ“šCSVæ–‡ä»¶"
                )
            
            # é¡¯ç¤ºæ•¸æ“šé è¦½
            st.subheader("ğŸ“Š æ•¸æ“šé è¦½")
            if export_type == "analysis" and "summary" in data:
                col_s1, col_s2, col_s3, col_s4 = st.columns(4)
                summary = data["summary"]
                with col_s1:
                    st.metric("ç¸½è²¼æ–‡æ•¸", summary.get("total_posts", 0))
                with col_s2:
                    st.metric("å¹³å‡è§€çœ‹æ•¸", f"{summary.get('avg_views', 0):,.0f}")
                with col_s3:
                    st.metric("å¹³å‡æŒ‰è®šæ•¸", f"{summary.get('avg_likes', 0):,.0f}")
                with col_s4:
                    st.metric("æœ€é«˜åˆ†æ•¸", f"{summary.get('max_score', 0):,.0f}")
            
            # é¡¯ç¤ºå‰10ç­†æ•¸æ“š
            if posts_data:
                col_preview1, col_preview2 = st.columns([1, 1])
                with col_preview1:
                    st.write("**å‰10ç­†æ•¸æ“šï¼š**")
                with col_preview2:
                    show_full_history_content = st.checkbox("ğŸ“– é¡¯ç¤ºå®Œæ•´å…§å®¹", key="show_full_history_content_v2", help="å‹¾é¸å¾Œå°‡é¡¯ç¤ºå®Œæ•´è²¼æ–‡å…§å®¹")
                
                preview_data = []
                for i, post in enumerate(posts_data[:10], 1):
                    content = post.get('content', '')
                    content_display = content if show_full_history_content else ((content[:40] + "...") if content and len(content) > 40 else content or 'N/A')
                    
                    # è™•ç†ç™¼å¸ƒæ™‚é–“é¡¯ç¤ºï¼ˆå¼·åŒ–éŒ¯èª¤è™•ç†ï¼‰
                    published_at = post.get('post_published_at', '')
                    if published_at:
                        try:
                            # è½‰æ›ç‚ºå°åŒ—æ™‚é–“ä¸¦æ ¼å¼åŒ–é¡¯ç¤º
                            taipei_published = PlaywrightUtils.convert_to_taipei_time(published_at)
                            if taipei_published:
                                published_display = taipei_published.strftime('%Y-%m-%d %H:%M')
                            else:
                                # å¦‚æœè½‰æ›å¤±æ•—ï¼Œå˜—è©¦ç›´æ¥æ ¼å¼åŒ–å­—ç¬¦ä¸²
                                published_display = str(published_at)[:16] if len(str(published_at)) >= 16 else str(published_at)
                        except Exception as e:
                            print(f"ğŸ› ç™¼å¸ƒæ™‚é–“æ ¼å¼åŒ–éŒ¯èª¤: {published_at} -> {e}")
                            published_display = str(published_at)[:16] if published_at else 'N/A'
                    else:
                        published_display = 'N/A'
                    
                    # è™•ç†çˆ¬å–æ™‚é–“é¡¯ç¤ºï¼ˆå¼·åŒ–éŒ¯èª¤è™•ç†ï¼‰
                    fetched_at = post.get('fetched_at', '')
                    if fetched_at:
                        try:
                            taipei_fetched = PlaywrightUtils.convert_to_taipei_time(fetched_at)
                            if taipei_fetched:
                                fetched_display = taipei_fetched.strftime('%Y-%m-%d %H:%M')
                            else:
                                # å¦‚æœè½‰æ›å¤±æ•—ï¼Œå˜—è©¦ç›´æ¥æ ¼å¼åŒ–å­—ç¬¦ä¸²
                                fetched_display = str(fetched_at)[:16] if len(str(fetched_at)) >= 16 else str(fetched_at)
                        except Exception as e:
                            print(f"ğŸ› çˆ¬å–æ™‚é–“æ ¼å¼åŒ–éŒ¯èª¤: {fetched_at} -> {e}")
                            fetched_display = str(fetched_at)[:16] if fetched_at else 'N/A'
                    else:
                        fetched_display = 'N/A'
                    
                    preview_data.append({
                        "#": i,
                        "è²¼æ–‡ID": post.get('post_id', 'N/A')[:20] + "..." if len(post.get('post_id', '')) > 20 else post.get('post_id', 'N/A'),
                        "å…§å®¹" if show_full_history_content else "å…§å®¹é è¦½": content_display,
                        "è§€çœ‹æ•¸": f"{post.get('views_count', 0):,}",
                        "æŒ‰è®šæ•¸": f"{post.get('likes_count', 0):,}",
                        "åˆ†æ•¸": f"{post.get('calculated_score', 0):,.1f}" if post.get('calculated_score') else 'N/A',
                        "ç™¼å¸ƒæ™‚é–“": published_display,
                        "çˆ¬å–æ™‚é–“": fetched_display
                    })
                st.dataframe(preview_data, use_container_width=True)
            
            st.success(f"âœ… {export_type}æ•¸æ“šå°å‡ºå®Œæˆï¼å…± {len(posts_data)} ç­†è¨˜éŒ„")
            
        except Exception as e:
            st.error(f"âŒ æ­·å²æ•¸æ“šå°å‡ºå¤±æ•—: {str(e)}")
    
    async def _fetch_history_from_db(self, username: str, export_type: str, **kwargs):
        """å¾è³‡æ–™åº«ç²å–æ­·å²æ•¸æ“š"""
        try:
            posts = await self.db_handler.get_user_posts_async(username)
            
            # è½‰æ›æ‰€æœ‰æ™‚é–“å­—æ®µç‚ºå°åŒ—æ™‚é–“
            for post in posts:
                for time_field in ['created_at', 'fetched_at', 'post_published_at']:
                    if post.get(time_field):
                        taipei_time = PlaywrightUtils.convert_to_taipei_time(post[time_field])
                        if taipei_time:
                            post[time_field] = taipei_time.isoformat()
            
            if export_type == "recent":
                days_back = kwargs.get('days_back', 7)
                limit = kwargs.get('limit', 1000)
                
                # éæ¿¾æœ€è¿‘çš„æ•¸æ“š
                from datetime import datetime, timedelta
                cutoff_date = PlaywrightUtils.get_current_taipei_time() - timedelta(days=days_back)
                
                filtered_posts = []
                for post in posts:
                    try:
                        if post.get('fetched_at'):
                            fetch_time = datetime.fromisoformat(str(post['fetched_at']).replace('Z', '+00:00'))
                            if fetch_time >= cutoff_date:
                                filtered_posts.append(post)
                    except:
                        continue
                
                return filtered_posts[:limit]
                
            elif export_type == "all":
                limit = kwargs.get('limit', 5000)
                return posts[:limit]
                
            elif export_type == "analysis":
                return posts
                
        except Exception as e:
            st.error(f"âŒ è³‡æ–™åº«æŸ¥è©¢å¤±æ•—: {e}")
            return []
    
    def _calculate_stats(self, posts_data):
        """è¨ˆç®—çµ±è¨ˆæ•¸æ“š"""
        if not posts_data:
            return {
                "total_posts": 0,
                "avg_views": 0,
                "avg_likes": 0,
                "avg_comments": 0,
                "max_score": 0,
                "min_score": 0
            }
        
        total_posts = len(posts_data)
        views = [post.get('views_count', 0) for post in posts_data if post.get('views_count')]
        likes = [post.get('likes_count', 0) for post in posts_data if post.get('likes_count')]
        comments = [post.get('comments_count', 0) for post in posts_data if post.get('comments_count')]
        scores = [post.get('calculated_score', 0) for post in posts_data if post.get('calculated_score')]
        
        return {
            "total_posts": total_posts,
            "avg_views": sum(views) / len(views) if views else 0,
            "avg_likes": sum(likes) / len(likes) if likes else 0,
            "avg_comments": sum(comments) / len(comments) if comments else 0,
            "max_score": max(scores) if scores else 0,
            "min_score": min(scores) if scores else 0
        }
    
    def convert_to_csv(self, posts_data):
        """å°‡æ•¸æ“šè½‰æ›ç‚ºCSVæ ¼å¼"""
        import pandas as pd
        import io
        
        # æº–å‚™CSVæ•¸æ“šï¼Œèˆ‡ä¸»è¦å°å‡ºæ ¼å¼ä¸€è‡´
        csv_data = []
        for post in posts_data:
            # è™•ç†é™£åˆ—å­—æ®µ
            tags = post.get('tags', [])
            if isinstance(tags, str):
                try:
                    import json
                    tags = json.loads(tags)
                except:
                    tags = []
            tags_str = "|".join(tags) if tags else ""
            
            images = post.get('images', [])
            if isinstance(images, str):
                try:
                    import json
                    images = json.loads(images)
                except:
                    images = []
            images_str = "|".join(images) if images else ""
            
            videos = post.get('videos', [])
            if isinstance(videos, str):
                try:
                    import json
                    videos = json.loads(videos)
                except:
                    videos = []
            videos_str = "|".join(videos) if videos else ""
            
            # è™•ç†æ™‚é–“å­—æ®µ - è½‰æ›ç‚ºå°åŒ—æ™‚é–“
            created_at = post.get('created_at', '')
            if created_at:
                taipei_created = PlaywrightUtils.convert_to_taipei_time(created_at)
                created_at = taipei_created.isoformat() if taipei_created else created_at
            
            post_published_at = post.get('post_published_at', '')
            if post_published_at:
                taipei_published = PlaywrightUtils.convert_to_taipei_time(post_published_at)
                post_published_at = taipei_published.isoformat() if taipei_published else post_published_at
            
            fetched_at = post.get('fetched_at', '')
            if fetched_at:
                taipei_fetched = PlaywrightUtils.convert_to_taipei_time(fetched_at)
                fetched_at = taipei_fetched.isoformat() if taipei_fetched else fetched_at
            
            csv_data.append({
                "url": post.get('url', ''),
                "post_id": post.get('post_id', ''),
                "username": post.get('username', ''),
                "content": post.get('content', ''),
                "likes_count": post.get('likes_count', 0),
                "comments_count": post.get('comments_count', 0),
                "reposts_count": post.get('reposts_count', 0),
                "shares_count": post.get('shares_count', 0),
                "views_count": post.get('views_count', 0),
                "calculated_score": post.get('calculated_score', ''),
                "created_at": created_at,
                "post_published_at": post_published_at,
                "tags": tags_str,
                "images": images_str,
                "videos": videos_str,
                "source": post.get('source', 'playwright_agent'),
                "crawler_type": post.get('crawler_type', 'playwright'),
                "crawl_id": post.get('crawl_id', ''),
                "fetched_at": fetched_at
            })
        
        # è½‰æ›ç‚ºCSV
        df = pd.DataFrame(csv_data)
        output = io.BytesIO()
        df.to_csv(output, index=False, encoding='utf-8-sig')
        return output.getvalue()
    
    def show_advanced_export_options(self):
        """é¡¯ç¤ºé€²éšå°å‡ºé¸é …"""
        with st.expander("ğŸ” é€²éšå°å‡ºåŠŸèƒ½", expanded=True):
            # æ·»åŠ é—œé–‰æŒ‰éˆ•
            col_title, col_close = st.columns([4, 1])
            with col_title:
                st.markdown("**æ›´å¤šå°å‡ºé¸é …å’Œæ‰¹é‡æ“ä½œ**")
            with col_close:
                if st.button("âŒ é—œé–‰", key="close_playwright_advanced_exports"):
                    st.session_state.show_playwright_advanced_exports = False
                    st.rerun()
            
            tab1, tab2, tab3 = st.tabs(["ğŸ“Š å°æ¯”å ±å‘Š", "ğŸ”„ æ‰¹é‡å°å‡º", "âš¡ å¿«é€Ÿå·¥å…·"])
            
            with tab1:
                st.subheader("ğŸ“Š å¤šæ¬¡çˆ¬å–å°æ¯”å ±å‘Š")
                st.info("æ¯”è¼ƒå¤šæ¬¡çˆ¬å–çµæœçš„æ•ˆèƒ½å’ŒæˆåŠŸç‡")
                
                # æŸ¥æ‰¾æ‰€æœ‰Playwright JSONæ–‡ä»¶
                import glob
                from pathlib import Path
                
                # æª¢æŸ¥æ–°çš„è³‡æ–™å¤¾ä½ç½®
                extraction_dir = Path("crawl_data")
                if extraction_dir.exists():
                    json_files = list(extraction_dir.glob("crawl_data_*.json"))
                else:
                    json_files = [Path(f) for f in glob.glob("crawl_data_*.json")]
                
                if len(json_files) >= 2:
                    st.write(f"ğŸ” æ‰¾åˆ° {len(json_files)} å€‹Playwrightçˆ¬å–çµæœæ–‡ä»¶ï¼š")
                    
                    # é¡¯ç¤ºæ–‡ä»¶åˆ—è¡¨
                    file_options = {}
                    for file in sorted(json_files, reverse=True)[:10]:  # æœ€æ–°çš„10å€‹
                        file_time = self._extract_time_from_filename(str(file))
                        display_name = f"{file.name} ({file_time})"
                        file_options[display_name] = str(file)
                    
                    selected_displays = st.multiselect(
                        "é¸æ“‡è¦æ¯”å°çš„æ–‡ä»¶ï¼ˆè‡³å°‘2å€‹ï¼‰ï¼š",
                        options=list(file_options.keys()),
                        default=[],
                        help="é¸æ“‡å¤šå€‹æ–‡ä»¶é€²è¡Œæ¯”å°åˆ†æ",
                        key="playwright_comparison_file_selector"
                    )
                    
                    selected_files = [file_options[display] for display in selected_displays]
                    
                    if len(selected_files) >= 2:
                        if st.button("ğŸ“Š ç”Ÿæˆå°æ¯”å ±å‘Š", key="playwright_generate_comparison", type="primary"):
                            self._generate_comparison_report(selected_files)
                    else:
                        st.info("ğŸ’¡ è«‹é¸æ“‡è‡³å°‘2å€‹æ–‡ä»¶é€²è¡Œæ¯”å°åˆ†æ")
                else:
                    st.warning("âš ï¸ éœ€è¦è‡³å°‘2å€‹Playwrightçˆ¬å–çµæœæ–‡ä»¶æ‰èƒ½é€²è¡Œå°æ¯”")
            
            with tab2:
                st.subheader("ğŸ”„ æ‰¹é‡å°å‡ºåŠŸèƒ½")
                
                col1, col2 = st.columns(2)
                
                with col1:
                    if st.button("ğŸ“¥ å°å‡ºæ‰€æœ‰æœ€æ–°çµæœ", key="playwright_export_all_latest"):
                        self._export_all_latest_results()
                
                with col2:
                    if st.button("ğŸ“ˆ å°å‡ºæ‰€æœ‰å¸³è™Ÿçµ±è¨ˆ", key="playwright_export_all_stats"):
                        self._export_all_account_stats()
            
            with tab3:
                st.subheader("âš¡ å¿«é€Ÿå·¥å…·")
                
                col1, col2, col3 = st.columns(3)
                
                with col1:
                    if st.button("ğŸ§¹ æ¸…ç†æš«å­˜æª”æ¡ˆ", key="playwright_cleanup_temp"):
                        self._cleanup_temp_files()
                
                with col2:
                    if st.button("ğŸ“‹ è¤‡è£½çµæœæ‘˜è¦", key="playwright_copy_summary"):
                        if 'playwright_results' in st.session_state:
                            self._copy_results_summary()
                        else:
                            st.error("âŒ æ²’æœ‰å¯è¤‡è£½çš„çµæœ")
                
                with col3:
                    if st.button("ğŸ”— ç”Ÿæˆåˆ†äº«é€£çµ", key="playwright_share_link"):
                        self._generate_share_link()
    
    def _extract_time_from_filename(self, filename: str) -> str:
        """å¾æª”æ¡ˆåæå–æ™‚é–“"""
        import re
        match = re.search(r'(\d{8}_\d{6})', filename)
        if match:
            time_str = match.group(1)
            return f"{time_str[:4]}-{time_str[4:6]}-{time_str[6:8]} {time_str[9:11]}:{time_str[11:13]}"
        return "æœªçŸ¥æ™‚é–“"
    
    def _generate_comparison_report(self, selected_files: list):
        """ç”Ÿæˆå°æ¯”å ±å‘Š"""
        try:
            import pandas as pd
            
            comparison_data = []
            
            for file_path in selected_files:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                comparison_data.append({
                    "æª”æ¡ˆå": Path(file_path).name,
                    "æ™‚é–“æˆ³": data.get('timestamp', 'N/A'),
                    "ç”¨æˆ¶å": data.get('target_username', 'N/A'),
                    "çˆ¬èŸ²é¡å‹": data.get('crawler_type', 'playwright'),
                    "ç¸½è²¼æ–‡æ•¸": len(data.get('results', [])),
                    "æˆåŠŸæ•¸": data.get('api_success_count', 0),
                    "å¤±æ•—æ•¸": data.get('api_failure_count', 0),
                    "æˆåŠŸç‡": data.get('overall_success_rate', 0),
                })
            
            df = pd.DataFrame(comparison_data)
            
            st.subheader("ğŸ“Š å°æ¯”å ±å‘Š")
            st.dataframe(df, use_container_width=True)
            
            # æä¾›ä¸‹è¼‰
            csv_content = df.to_csv(index=False, encoding='utf-8-sig').encode('utf-8-sig')
            timestamp = PlaywrightUtils.get_current_taipei_time().strftime('%Y%m%d_%H%M%S')
            filename = f"playwright_comparison_report_{timestamp}.csv"
            
            st.download_button(
                label="ğŸ“¥ ä¸‹è¼‰å°æ¯”å ±å‘Š",
                data=csv_content,
                file_name=filename,
                mime="text/csv"
            )
            
        except Exception as e:
            st.error(f"âŒ ç”Ÿæˆå°æ¯”å ±å‘Šå¤±æ•—: {e}")
    
    def _export_all_latest_results(self):
        """å°å‡ºæ‰€æœ‰æœ€æ–°çµæœ"""
        st.info("ğŸ“¦ æ‰¹é‡å°å‡ºåŠŸèƒ½é–‹ç™¼ä¸­...")
    
    def _export_all_account_stats(self):
        """å°å‡ºæ‰€æœ‰å¸³è™Ÿçµ±è¨ˆ"""
        st.info("ğŸ“ˆ å¸³è™Ÿçµ±è¨ˆå°å‡ºåŠŸèƒ½é–‹ç™¼ä¸­...")
    
    def _cleanup_temp_files(self):
        """æ¸…ç†æš«å­˜æª”æ¡ˆ - ä½¿ç”¨ FolderManager"""
        try:
            from pathlib import Path
            from common.folder_manager import FolderManager
            
            # æ¸…ç†èˆŠæ ¼å¼çš„é€²åº¦æª”æ¡ˆï¼ˆæ ¹ç›®éŒ„ä¸‹çš„ï¼‰
            import glob
            old_temp_files = glob.glob("temp_playwright_progress_*.json")
            old_cleaned = 0
            for file in old_temp_files:
                try:
                    os.remove(file)
                    old_cleaned += 1
                except:
                    pass
            
            # æ¸…ç†æ–°æ ¼å¼çš„é€²åº¦æª”æ¡ˆè³‡æ–™å¤¾
            temp_progress_dir = Path("temp_progress")
            if temp_progress_dir.exists():
                deleted_count = FolderManager.cleanup_old_files(
                    temp_progress_dir, 
                    max_files=50,  # ä¿ç•™æœ€æ–°çš„ 50 å€‹é€²åº¦æª”æ¡ˆ
                    pattern="*.json"
                )
                total_cleaned = old_cleaned + deleted_count
                if total_cleaned > 0:
                    st.success(f"ğŸ§¹ å·²æ¸…ç† {total_cleaned} å€‹æš«å­˜é€²åº¦æª”æ¡ˆ (èˆŠæ ¼å¼: {old_cleaned}, æ–°æ ¼å¼: {deleted_count})")
                else:
                    st.info("âœ… æš«å­˜æª”æ¡ˆå·²ç¶“æ˜¯æœ€æ–°ç‹€æ…‹")
            
            # åŒæ™‚æ¸…ç†å…¶ä»–å°ˆæ¡ˆè³‡æ–™å¤¾
            FolderManager.setup_project_folders()
            
        except Exception as e:
            st.warning(f"âš ï¸ æ¸…ç†æš«å­˜æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
    
    def _copy_results_summary(self):
        """è¤‡è£½çµæœæ‘˜è¦"""
        results = st.session_state.get('playwright_results', {})
        posts = results.get('results', [])
        
        summary = f"""Playwright çˆ¬èŸ²çµæœæ‘˜è¦
ç”¨æˆ¶: @{results.get('target_username', 'unknown')}
æ™‚é–“: {results.get('timestamp', 'N/A')}
ç¸½è²¼æ–‡: {len(posts)}
æˆåŠŸç‡: {results.get('overall_success_rate', 0):.1f}%
"""
        
        st.text_area("ğŸ“‹ çµæœæ‘˜è¦ï¼ˆè«‹è¤‡è£½ï¼‰", value=summary, key="playwright_summary_copy")
    
    def _generate_share_link(self):
        """ç”Ÿæˆåˆ†äº«é€£çµ"""
        st.info("ğŸ”— åˆ†äº«é€£çµåŠŸèƒ½é–‹ç™¼ä¸­...")
    
    def clear_results(self):
        """æ¸…é™¤çµæœ"""
        if 'playwright_results' in st.session_state:
            del st.session_state.playwright_results
        if 'playwright_results_file' in st.session_state:
            del st.session_state.playwright_results_file
        # é‡ç½®ä¿å­˜æ¨™è¨˜
        st.session_state.playwright_results_saved = False
        st.success("ğŸ—‘ï¸ çµæœå·²æ¸…é™¤")
        st.rerun()
    
    def load_csv_file(self, uploaded_file):
        """è¼‰å…¥CSVæ–‡ä»¶"""
        try:
            import pandas as pd
            import io
            
            # æ¸…ç†å¯èƒ½çš„èˆŠæ–‡ä»¶å¼•ç”¨ï¼Œé¿å… MediaFileStorageError
            if hasattr(st.session_state, 'get'):
                file_related_keys = [k for k in st.session_state.keys() if 'file' in k.lower() or 'upload' in k.lower()]
                for key in file_related_keys:
                    if key != "playwright_csv_uploader_v2":  # ä¿ç•™ç•¶å‰ä¸Šå‚³å™¨
                        try:
                            del st.session_state[key]
                        except:
                            pass
            
            # è®€å–CSVæ–‡ä»¶
            content = uploaded_file.getvalue()
            df = pd.read_csv(io.StringIO(content.decode('utf-8-sig')))
            
            # æª¢æŸ¥CSVæ ¼å¼æ˜¯å¦æ­£ç¢ºï¼ˆæ›´éˆæ´»çš„é©—è­‰ï¼‰
            # ğŸ”§ ä¿®å¾©ï¼šæ”¯æ´æ–°æ ¼å¼å’ŒèˆŠæ ¼å¼çš„å…¼å®¹æ€§
            # æ ¸å¿ƒå¿…è¦æ¬„ä½ - è‡³å°‘éœ€è¦ç”¨æˆ¶è­˜åˆ¥å’Œè²¼æ–‡è­˜åˆ¥
            has_username = 'username' in df.columns
            has_user_id = 'user_id' in df.columns
            has_post_id = 'post_id' in df.columns
            has_real_post_id = 'real_post_id' in df.columns
            
            # å¿…é ˆæœ‰ç”¨æˆ¶è­˜åˆ¥æ¬„ä½
            if not (has_username or has_user_id):
                st.error("âŒ CSVæ ¼å¼ä¸æ­£ç¢ºï¼Œå¿…é ˆåŒ…å« 'username' æˆ– 'user_id' æ¬„ä½")
                return
            
            # å¿…é ˆæœ‰è²¼æ–‡è­˜åˆ¥æ¬„ä½
            if not (has_post_id or has_real_post_id):
                st.error("âŒ CSVæ ¼å¼ä¸æ­£ç¢ºï¼Œå¿…é ˆåŒ…å« 'post_id' æˆ– 'real_post_id' æ¬„ä½")
                return
            
            # å¿…é ˆæœ‰å…§å®¹æ¬„ä½
            if 'content' not in df.columns:
                st.error("âŒ CSVæ ¼å¼ä¸æ­£ç¢ºï¼Œç¼ºå°‘ 'content' æ¬„ä½")
                return
            
            # æª¢æŸ¥å¯é¸æ¬„ä½ï¼Œå¦‚æœæ²’æœ‰å‰‡æä¾›é è¨­å€¼
            optional_columns = ['url', 'views', 'likes_count', 'comments_count', 'reposts_count', 'shares_count']
            for col in optional_columns:
                if col not in df.columns:
                    if col == 'views':
                        df[col] = df.get('views_count', 0)  # å˜—è©¦ä½¿ç”¨ views_count ä½œç‚º views
                    elif col == 'url':
                        df[col] = ''  # URLå¯ä»¥ç‚ºç©º
                    else:
                        df[col] = 0  # é è¨­å€¼ç‚º 0
            
            st.info(f"âœ… æˆåŠŸè¼‰å…¥CSVï¼ŒåŒ…å« {len(df)} ç­†è¨˜éŒ„")
            
            # è½‰æ›ç‚ºçµæœæ ¼å¼
            results = []
            for _, row in df.iterrows():
                # è™•ç†é™£åˆ—å­—æ®µ (tags, images, videos)
                tags_str = str(row.get('tags', '')).strip()
                tags = tags_str.split('|') if tags_str else []
                
                images_str = str(row.get('images', '')).strip()
                images = images_str.split('|') if images_str else []
                
                videos_str = str(row.get('videos', '')).strip()
                videos = videos_str.split('|') if videos_str else []
                
                # ğŸ”§ ä¿®å¾©ï¼šæ™ºèƒ½è™•ç†æ–°èˆŠæ ¼å¼çš„ç”¨æˆ¶åå’Œè²¼æ–‡ID
                # å„ªå…ˆä½¿ç”¨æ–°æ ¼å¼æ¬„ä½ï¼Œå›é€€åˆ°èˆŠæ ¼å¼
                user_id = str(row.get('user_id', '')).strip() or str(row.get('username', '')).strip()
                real_post_id = str(row.get('real_post_id', '')).strip()
                original_post_id = str(row.get('post_id', '')).strip()
                
                # å¦‚æœæ²’æœ‰ real_post_idï¼Œå˜—è©¦å¾ post_id åˆ†é›¢
                if not real_post_id and original_post_id and '_' in original_post_id:
                    parts = original_post_id.split('_', 1)
                    if len(parts) > 1:
                        if not user_id:  # å¦‚æœé‚„æ²’æœ‰ç”¨æˆ¶IDï¼Œå¾post_idæå–
                            user_id = parts[0]
                        real_post_id = parts[1]
                else:
                    # å¦‚æœæ²’æœ‰åˆ†é›¢æ ¼å¼ï¼Œä½¿ç”¨åŸå§‹post_idä½œç‚ºreal_post_id
                    real_post_id = real_post_id or original_post_id
                
                # é‡å»ºå…¼å®¹çš„post_idæ ¼å¼ï¼ˆèˆŠç³»çµ±å…¼å®¹æ€§ï¼‰
                combined_post_id = f"{user_id}_{real_post_id}" if user_id and real_post_id else original_post_id
                
                result = {
                    "url": str(row.get('url', '')).strip(),
                    "post_id": combined_post_id,  # ä¿æŒèˆŠæ ¼å¼å…¼å®¹æ€§
                    "username": user_id,  # ä½¿ç”¨åˆ†é›¢çš„ç”¨æˆ¶ID
                    "content": str(row.get('content', '')).strip(),
                    "likes_count": row.get('likes_count', 0) if pd.notna(row.get('likes_count')) else 0,
                    "comments_count": row.get('comments_count', 0) if pd.notna(row.get('comments_count')) else 0,
                    "reposts_count": row.get('reposts_count', 0) if pd.notna(row.get('reposts_count')) else 0,
                    "shares_count": row.get('shares_count', 0) if pd.notna(row.get('shares_count')) else 0,
                    "views_count": row.get('views_count', 0) if pd.notna(row.get('views_count')) else 0,
                    "calculated_score": row.get('calculated_score', 0) if pd.notna(row.get('calculated_score')) else 0,
                    "created_at": str(row.get('created_at', '')).strip(),
                    "post_published_at": str(row.get('post_published_at', '')).strip(),
                    "tags": tags,
                    "images": images,
                    "videos": videos,
                    "source": str(row.get('source', 'playwright_agent')).strip(),
                    "crawler_type": str(row.get('crawler_type', 'playwright')).strip(),
                    "crawl_id": str(row.get('crawl_id', '')).strip(),
                    "extracted_at": str(row.get('extracted_at', '')).strip(),
                    "success": row.get('success', True) if pd.notna(row.get('success')) else True
                }
                results.append(result)
            
            # ğŸ”§ ä¿®å¾©ï¼šå¾çµæœä¸­æ™ºèƒ½æå–ç›®æ¨™ç”¨æˆ¶å
            target_username = ""
            if results:
                # å˜—è©¦å¾ç¬¬ä¸€ç­†è¨˜éŒ„ç²å–ç”¨æˆ¶å
                first_result = results[0]
                target_username = first_result.get('username', '')
                
                # å¦‚æœæ‰€æœ‰è¨˜éŒ„çš„ç”¨æˆ¶åéƒ½ç›¸åŒï¼Œä½¿ç”¨è©²ç”¨æˆ¶å
                all_usernames = set(r.get('username', '') for r in results if r.get('username'))
                if len(all_usernames) == 1:
                    target_username = list(all_usernames)[0]
                elif len(all_usernames) > 1:
                    st.info(f"ğŸ“Š æª¢æ¸¬åˆ°å¤šå€‹ç”¨æˆ¶çš„è³‡æ–™ï¼š{', '.join(sorted(all_usernames))}")
            
            # åŒ…è£ç‚ºå®Œæ•´çµæœæ ¼å¼
            final_results = {
                "crawl_id": f"imported_{PlaywrightUtils.get_current_taipei_time().strftime('%Y%m%d_%H%M%S')}",
                "timestamp": PlaywrightUtils.get_current_taipei_time().isoformat(),
                "target_username": target_username,  # ğŸ”§ ä¿®å¾©ï¼šä½¿ç”¨æ™ºèƒ½æå–çš„ç”¨æˆ¶å
                "source": "csv_import",
                "crawler_type": "playwright",
                "total_processed": len(results),
                "results": results
            }
            
            st.session_state.playwright_results = final_results
            st.session_state.playwright_crawl_status = "completed"  # è¨­ç½®ç‹€æ…‹ç‚ºå®Œæˆ
            st.success(f"âœ… æˆåŠŸè¼‰å…¥ {len(results)} ç­†è¨˜éŒ„")
            st.rerun()
            
        except Exception as e:
            st.error(f"âŒ è¼‰å…¥CSVå¤±æ•—: {e}")