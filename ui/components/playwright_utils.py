"""
Playwright çˆ¬èŸ²å·¥å…·å‡½å¼
åŒ…å«æ•¸æ“šè½‰æ›ã€JSONä¿å­˜ã€æ—¥èªŒè™•ç†ç­‰è¼”åŠ©åŠŸèƒ½
"""

import json
import time
import tempfile
import shutil
import uuid
import os
from datetime import datetime, timezone, timedelta
from pathlib import Path
from typing import Dict, Any


class PlaywrightUtils:
    """Playwright çˆ¬èŸ²å·¥å…·é¡"""
    
    @staticmethod
    def convert_to_taipei_time(datetime_str: str) -> datetime:
        """å°‡å„ç¨®æ ¼å¼çš„æ—¥æœŸæ™‚é–“å­—ç¬¦ä¸²è½‰æ›ç‚ºå°åŒ—æ™‚å€çš„ datetime ç‰©ä»¶ï¼ˆç„¡æ™‚å€ä¿¡æ¯ï¼‰"""
        try:
            if not datetime_str:
                return None
            
            # æ¸…ç†è¼¸å…¥å­—ç¬¦ä¸²
            datetime_str = str(datetime_str).strip()
            
            # å˜—è©¦å¤šç¨®æ™‚é–“æ ¼å¼è§£æ
            dt = None
            
            # æ–¹æ³•1ï¼šå˜—è©¦ ISO æ ¼å¼
            try:
                dt = datetime.fromisoformat(datetime_str.replace('Z', '+00:00'))
            except ValueError:
                pass
            
            # æ–¹æ³•2ï¼šå˜—è©¦æ¨™æº–æ ¼å¼ï¼ˆç©ºæ ¼åˆ†éš”ï¼‰
            if dt is None:
                try:
                    # è™•ç† "YYYY-MM-DD HH:MM:SS" æ ¼å¼
                    if len(datetime_str) == 19 and ' ' in datetime_str:
                        dt = datetime.strptime(datetime_str, '%Y-%m-%d %H:%M:%S')
                    # è™•ç† "YYYY-MM-DD HH:MM:SS.ffffff" æ ¼å¼
                    elif '.' in datetime_str and ' ' in datetime_str:
                        # æˆªå–åˆ°å¾®ç§’æœ€å¤š6ä½
                        if '.' in datetime_str:
                            base_part, micro_part = datetime_str.split('.')
                            micro_part = micro_part[:6].ljust(6, '0')  # ç¢ºä¿6ä½å¾®ç§’
                            datetime_str = f"{base_part}.{micro_part}"
                        dt = datetime.strptime(datetime_str, '%Y-%m-%d %H:%M:%S.%f')
                except ValueError:
                    pass
            
            # æ–¹æ³•3ï¼šå˜—è©¦æ›¿æ›Tç‚ºç©ºæ ¼å¾Œè§£æ
            if dt is None:
                try:
                    modified_str = datetime_str.replace('T', ' ')
                    if '.' in modified_str:
                        dt = datetime.strptime(modified_str, '%Y-%m-%d %H:%M:%S.%f')
                    else:
                        dt = datetime.strptime(modified_str, '%Y-%m-%d %H:%M:%S')
                except ValueError:
                    pass
            
            # å¦‚æœæ‰€æœ‰æ–¹æ³•éƒ½å¤±æ•—
            if dt is None:
                print(f"âš ï¸ ç„¡æ³•è§£ææ™‚é–“æ ¼å¼: {datetime_str}")
                return None
            
            # å¦‚æœè§£æçš„æ™‚é–“æ²’æœ‰æ™‚å€ä¿¡æ¯ï¼Œå‡è¨­å®ƒæ˜¯UTCæ™‚é–“
            if dt.tzinfo is None:
                dt = dt.replace(tzinfo=timezone.utc)
            
            # è½‰æ›ç‚ºå°åŒ—æ™‚å€ (UTC+8)
            taipei_tz = timezone(timedelta(hours=8))
            taipei_dt = dt.astimezone(taipei_tz)
            
            # è¿”å›ç„¡æ™‚å€ä¿¡æ¯çš„å°åŒ—æ™‚é–“ï¼Œç”¨æ–¼è³‡æ–™åº«å­˜å„²
            return taipei_dt.replace(tzinfo=None)
            
        except Exception as e:
            print(f"âš ï¸ æ™‚é–“è½‰æ›å¤±æ•— {datetime_str}: {e}")
            return None
    
    @staticmethod
    def get_current_taipei_time() -> datetime:
        """ç²å–ç•¶å‰å°åŒ—æ™‚é–“ï¼ˆç„¡æ™‚å€ä¿¡æ¯ï¼‰"""
        taipei_tz = timezone(timedelta(hours=8))
        return datetime.now(taipei_tz).replace(tzinfo=None)
    
    @staticmethod
    def write_progress(path: str, data: Dict[str, Any]):
        """ç·šç¨‹å®‰å…¨å¯«å…¥é€²åº¦æ–‡ä»¶"""
        old: Dict[str, Any] = {}
        if os.path.exists(path):
            try:
                with open(path, "r", encoding="utf-8") as f:
                    old = json.load(f)
            except Exception:
                pass

        # åˆä½µé‚è¼¯
        stage_priority = {
            "initialization": 0, "fetch_start": 1, "post_parsed": 2,
            "batch_parsed": 3, "fill_views_start": 4, "fill_views_completed": 5,
            "api_completed": 6, "completed": 7, "error": 8
        }
        old_stage = old.get("stage", "")
        new_stage = data.get("stage", old_stage)
        if stage_priority.get(new_stage, 0) < stage_priority.get(old_stage, 0):
            data.pop("stage", None)

        if "progress" not in data and "progress" in old:
            data["progress"] = old["progress"]
        if "current_work" not in data and "current_work" in old:
            data["current_work"] = old["current_work"]

        merged = {**old, **data, "timestamp": time.time()}

        # å…ˆå¯«åˆ° tmpï¼Œå† atomic rename
        dir_ = os.path.dirname(path)
        os.makedirs(dir_, exist_ok=True)
        
        try:
            with tempfile.NamedTemporaryFile("w", delete=False, dir=dir_, suffix=".tmp", encoding='utf-8') as tmp:
                json.dump(merged, tmp, ensure_ascii=False)
                tmp.flush()
                os.fsync(tmp.fileno())
                tmp_path = tmp.name
            
            shutil.move(tmp_path, path)
        except Exception as e:
            print(f"âŒ å¯«å…¥é€²åº¦æ–‡ä»¶å¤±æ•—: {e}")

    @staticmethod
    def read_progress(path: str) -> Dict[str, Any]:
        """è®€å–é€²åº¦æ–‡ä»¶"""
        if not os.path.exists(path):
            return {}
        try:
            with open(path, "r", encoding="utf-8") as f:
                return json.load(f)
        except:
            return {}
    
    @staticmethod
    def convert_playwright_results(playwright_data: Dict[str, Any]) -> Dict[str, Any]:
        """è½‰æ› Playwright API çµæœç‚ºå°ˆç”¨æ ¼å¼"""
        # ğŸ”¥ ä¿®å¾©ï¼šæ”¯æ´å…©ç¨®æ ¼å¼ - API éŸ¿æ‡‰ç”¨ "posts"ï¼ŒRedis final_data ç”¨ "results"
        posts = playwright_data.get("posts", []) or playwright_data.get("results", [])
        
        # ğŸ”§ ä¿®å¾©ï¼šå¾å¤šå€‹ä¾†æºç²å–æ­£ç¢ºçš„ç”¨æˆ¶åç¨±
        username = (playwright_data.get("username", "") or 
                   playwright_data.get("target_username", "") or
                   (posts[0].get("username", "") if posts else ""))
        
        # è½‰æ›ç‚º Playwright å°ˆç”¨æ ¼å¼
        converted_results = []
        for post in posts:
            # æª¢æŸ¥æ•¸æ“šæ ¼å¼ä¸¦è½‰æ› - ä¿æŒæ‰€æœ‰åŸå§‹æ•¸æ“š
            result = {
                "post_id": post.get("post_id", ""),
                "url": post.get("url", ""),
                "content": post.get("content", ""),
                # æ•¸é‡æ¬„ä½ï¼ˆä¿æŒåŸå§‹æ•¸å€¼æ ¼å¼ï¼‰
                "views_count": post.get("views_count", 0),
                "likes_count": post.get("likes_count", 0),
                "comments_count": post.get("comments_count", 0),
                "reposts_count": post.get("reposts_count", 0),
                "shares_count": post.get("shares_count", 0),
                # å‘å¾Œå…¼å®¹çš„å­—ç¬¦ä¸²æ ¼å¼
                "views": str(post.get("views_count", "") or ""),
                "likes": str(post.get("likes_count", "") or ""),
                "comments": str(post.get("comments_count", "") or ""),
                "reposts": str(post.get("reposts_count", "") or ""),
                "shares": str(post.get("shares_count", "") or ""),
                # è¨ˆç®—åˆ†æ•¸
                "calculated_score": post.get("calculated_score", 0),
                # æ™‚é–“æ¬„ä½
                "created_at": post.get("created_at", ""),
                "post_published_at": post.get("post_published_at", ""),
                # é™£åˆ—æ¬„ä½
                "tags": post.get("tags", []),
                "images": post.get("images", []),
                "videos": post.get("videos", []),
                # å…ƒæ•¸æ“š
                "source": "playwright_agent",
                "crawler_type": "playwright",
                "success": True,
                "has_views": bool(post.get("views_count")),
                "has_content": bool(post.get("content")),
                "has_likes": bool(post.get("likes_count")),
                "has_comments": bool(post.get("comments_count")),
                "has_reposts": bool(post.get("reposts_count")),
                "has_shares": bool(post.get("shares_count")),
                "content_length": len(post.get("content", "")),
                "extracted_at": PlaywrightUtils.get_current_taipei_time().isoformat(),
                "username": post.get("username", "") or username  # ğŸ”§ ä¿®å¾©ï¼šå„ªå…ˆä½¿ç”¨è²¼æ–‡ä¸­çš„usernameï¼Œå›é€€åˆ°æ•´é«”username
            }
            converted_results.append(result)
        
        # ç”Ÿæˆå”¯ä¸€IDï¼ˆæ™‚é–“æˆ³ + éš¨æ©Ÿå­—ç¬¦ï¼‰
        timestamp = PlaywrightUtils.get_current_taipei_time().strftime('%Y%m%d_%H%M%S')
        unique_id = str(uuid.uuid4())[:8]
        
        # åŒ…è£ç‚º Playwright å°ˆç”¨çµæ§‹
        return {
            "crawl_id": f"{timestamp}_{unique_id}",
            "timestamp": PlaywrightUtils.get_current_taipei_time().isoformat(),
            "target_username": username,
            "crawler_type": "playwright",
            "max_posts": len(posts),
            "total_processed": len(posts),
            "api_success_count": len(posts),
            "api_failure_count": 0,
            "overall_success_rate": 100.0 if posts else 0.0,
            "timing": {
                "total_time": 0,  # Playwright API ä¸æä¾›è©³ç´°è¨ˆæ™‚
                "url_collection_time": 0,
                "content_extraction_time": 0
            },
            "results": converted_results,
            "source": "playwright_agent",
            "database_saved": False,  # å°‡åœ¨ä¿å­˜å¾Œæ›´æ–°
            "database_saved_count": 0
        }
    
    @staticmethod
    def save_json_results(results_data: Dict[str, Any]) -> Path:
        """ä¿å­˜çµæœç‚ºJSONæ–‡ä»¶ï¼Œä½¿ç”¨æŒ‡å®šæ ¼å¼"""
        try:
            # å‰µå»º playwright_results ç›®éŒ„
            results_dir = Path("playwright_results")
            results_dir.mkdir(exist_ok=True)
            
            # ç”Ÿæˆæ–‡ä»¶åï¼šcrawl_data_20250803_121452_934d52b1.json
            crawl_id = results_data.get("crawl_id", "unknown")
            filename = f"crawl_data_{crawl_id}.json"
            json_file_path = results_dir / filename
            
            # ä¿å­˜JSONæ–‡ä»¶
            with open(json_file_path, 'w', encoding='utf-8') as f:
                json.dump(results_data, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ’¾ çµæœå·²ä¿å­˜: {json_file_path}")
            return json_file_path
            
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜JSONæ–‡ä»¶å¤±æ•—: {e}")
            return None
    
    @staticmethod
    def parse_number_safe(value):
        """å®‰å…¨è§£ææ•¸å­—å­—ç¬¦ä¸²"""
        try:
            if not value or value == 'N/A':
                return None
            # ç§»é™¤éæ•¸å­—å­—ç¬¦ï¼ˆé™¤äº†å°æ•¸é»ï¼‰
            clean_value = str(value).replace(',', '').replace(' ', '')
            if 'K' in clean_value:
                return int(float(clean_value.replace('K', '')) * 1000)
            elif 'M' in clean_value:
                return int(float(clean_value.replace('M', '')) * 1000000)
            elif 'B' in clean_value:
                return int(float(clean_value.replace('B', '')) * 1000000000)
            else:
                return int(float(clean_value))
        except:
            return None

    # -------------------- å»é‡å·¥å…·ï¼šé¡¯ç¤º/å­˜å‰é˜²ç¦¦æ€§å®ˆé–€ --------------------
    @staticmethod
    def _normalize_content(text: str) -> str:
        """è¼•åº¦æ­£è¦åŒ– contentï¼šå»é ­å°¾ç©ºç™½ä¸¦å£“ç¸®ä¸­é–“ç©ºç™½ç‚ºå–®ä¸€ç©ºæ ¼"""
        if not isinstance(text, str):
            return ""
        # å»é ­å°¾ç©ºç™½ï¼Œä¸¦å°‡é€£çºŒç©ºç™½å£“ç¸®ç‚ºå–®ä¸€ç©ºæ ¼
        return " ".join(text.strip().split())

    @staticmethod
    def deduplicate_results_by_content_keep_max_views(results_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        é˜²ç¦¦æ€§å®ˆé–€ï¼šä¾æ“šã€Œç›¸åŒ contentã€ä¸åŒ post_id â†’ ä¿ç•™ views è¼ƒé«˜è€…ã€éæ¿¾ã€‚
        - åªåœ¨ content éç©ºæ™‚æ‰åƒèˆ‡åŒçµ„æ¯”è¼ƒ
        - æ¯”è¼ƒä¸»éµï¼šåŒ (username, normalized_content)
        - views æ¯”è¼ƒï¼šå„ªå…ˆä½¿ç”¨ views_countï¼Œå›é€€è§£æ views å­—ä¸²
        - å¹³æ‰‹æ™‚ä»¥ likes_count ä½œç‚ºæ¬¡åºï¼Œä»å¹³æ‰‹å‰‡ä¿ç•™å…ˆåˆ°è€…

        åƒæ•¸:
            results_data: å…·æœ‰ key "results" çš„å­—å…¸çµæ§‹
        å›å‚³:
            æ–°çš„ results_dataï¼ˆæ·ºè¤‡è£½ï¼‰ï¼Œå…¶ä¸­ "results" å·²éæ¿¾
        """
        try:
            results_list = list(results_data.get("results", []) or [])
            if not results_list:
                return results_data

            target_username = results_data.get("target_username", "")

            # ä»¥ (username, normalized_content) åˆ†çµ„
            groups = {}
            singles = []

            for item in results_list:
                content = item.get("content") or ""
                normalized = PlaywrightUtils._normalize_content(content)
                # åƒ…å°éç©ºå…§å®¹é€²è¡Œåˆ†çµ„å»é‡
                if not normalized:
                    singles.append(item)
                    continue

                username = item.get("username") or target_username or ""
                key = (username, normalized)
                groups.setdefault(key, []).append(item)

            deduped = []

            # ä¿ç•™æ¯çµ„ä¸­ views æœ€å¤§è€…
            def views_of(x: Dict[str, Any]) -> int:
                v = x.get("views_count")
                if v is None or v == "":
                    v = x.get("views")
                parsed = PlaywrightUtils.parse_number_safe(v)
                return int(parsed or 0)

            def likes_of(x: Dict[str, Any]) -> int:
                l = x.get("likes_count")
                if l is None or l == "":
                    l = x.get("likes")
                parsed = PlaywrightUtils.parse_number_safe(l)
                return int(parsed or 0)

            dropped = []
            for _, items in groups.items():
                if len(items) == 1:
                    deduped.append(items[0])
                else:
                    # å…ˆä»¥ views ç”±å¤§åˆ°å°æ’åºï¼Œå¹³æ‰‹å†ä»¥ likes ç”±å¤§åˆ°å°
                    items_sorted = sorted(
                        items,
                        key=lambda x: (views_of(x), likes_of(x)),
                        reverse=True,
                    )
                    deduped.append(items_sorted[0])
                    dropped.extend(items_sorted[1:])

            # åˆä½µå–®ç­†ï¼ˆç„¡å…§å®¹æˆ–æœªåˆ†çµ„ï¼‰+ å»é‡å¾Œçµæœ
            final_results = singles + deduped

            # å›å¯«åˆ°æ–°çš„è³‡æ–™çµæ§‹ï¼Œé¿å…å¤–éƒ¨å¼•ç”¨è¢«å°±åœ°ä¿®æ”¹
            new_data = dict(results_data)
            new_data["results"] = final_results
            # è¨˜éŒ„è¢«ä¸Ÿæ£„çš„æ¸…å–®ï¼ˆä½œç‚ºå¢é‡è·³éçš„æ¨™è¨˜ç”¨é€”ï¼‰
            if dropped:
                new_data["dedup_filtered"] = [
                    {
                        "post_id": x.get("post_id", ""),
                        "url": x.get("url", ""),
                        "username": x.get("username", "") or target_username,
                        "views_count": views_of(x),
                        "likes_count": likes_of(x),
                        "content": "",  # ä¸ä¿å­˜å…§å®¹
                        "source": "playwright_dedup_filtered",
                    }
                    for x in dropped
                    if x and (x.get("post_id") or x.get("url"))
                ]
            # è‹¥æœ‰çµ±è¨ˆæ¬„ä½ï¼Œé †ä¾¿æ›´æ–°
            new_data["total_processed"] = len(final_results)
            new_data["api_success_count"] = len(final_results)
            return new_data
        except Exception:
            # å‡ºéŒ¯å‰‡è¿”å›åŸè³‡æ–™ï¼Œé¿å…ä¸­æ–·ä¸»æµç¨‹
            return results_data