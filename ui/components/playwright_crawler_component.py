"""
Playwright çˆ¬èŸ²çµ„ä»¶ - åŸºæ–¼ Playwright Agent API
åŒ…å«å®Œæ•´UIåŠŸèƒ½ï¼Œèˆ‡realtime_crawler_component.pyç›¸åŒï¼Œä½†ä½¿ç”¨Playwright Agentä½œç‚ºå¾Œç«¯
"""

import streamlit as st
import asyncio
import json
import time
import threading
import httpx
import requests
import tempfile
import shutil
import uuid
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, Optional, List
import sys
import os

# æ·»åŠ å°ˆæ¡ˆæ ¹ç›®éŒ„åˆ° Python è·¯å¾‘
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

class PlaywrightCrawlerComponent:
    def __init__(self):
        self.is_running = False
        self.current_task = None
        self.agent_url = "http://localhost:8006/v1/playwright/crawl"
        self.sse_url = "http://localhost:8000/stream"  # SSEæœå‹™å™¨URL
        # ä½¿ç”¨çµ±ä¸€çš„é…ç½®ç®¡ç†
        from common.config import get_auth_file_path
        self.auth_file_path = get_auth_file_path(from_project_root=True)
        
    def render(self):
        """æ¸²æŸ“Playwrightçˆ¬èŸ²çµ„ä»¶"""
        st.header("ğŸ­ Playwright æ™ºèƒ½çˆ¬èŸ²")
        st.markdown("**åŸºæ–¼ Playwright Agent API + å®Œæ•´äº’å‹•æ•¸æ“š + è³‡æ–™åº«æ•´åˆ**")
        
        # æª¢æŸ¥èªè­‰æ–‡ä»¶
        if not self._check_auth_file():
            st.error("âŒ æ‰¾ä¸åˆ°èªè­‰æª”æ¡ˆ")
            st.info("è«‹å…ˆåŸ·è¡Œ: `python tests/threads_fetch/save_auth.py` ä¾†ç”¢ç”Ÿèªè­‰æª”æ¡ˆ")
            return
        
        st.success("âœ… èªè­‰æª”æ¡ˆå·²å°±ç·’")
        
        # åƒæ•¸è¨­å®šå€åŸŸ
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("âš™ï¸ çˆ¬å–è¨­å®š")
            username = st.text_input(
                "ç›®æ¨™å¸³è™Ÿ", 
                value="gvmonthly",
                help="è¦çˆ¬å–çš„Threadså¸³è™Ÿç”¨æˆ¶å",
                key="playwright_username"
            )
            
            max_posts = st.number_input(
                "çˆ¬å–æ•¸é‡", 
                min_value=1, 
                max_value=500, 
                value=50,
                help="è¦çˆ¬å–çš„è²¼æ–‡æ•¸é‡",
                key="playwright_max_posts"
            )
            
            # é¡¯ç¤ºçˆ¬å–éç¨‹æ—¥èªŒï¼ˆç§»åˆ°é€™è£¡ï¼Œé¿å…é‡æ–°æ¸²æŸ“å½±éŸ¿ï¼‰
            if 'playwright_crawl_logs' in st.session_state and st.session_state.playwright_crawl_logs:
                with st.expander("ğŸ“‹ çˆ¬å–éç¨‹æ—¥èªŒ", expanded=False):
                    # é¡¯ç¤ºæœ€å¾Œ50è¡Œæ—¥èªŒ
                    log_lines = st.session_state.playwright_crawl_logs[-50:] if len(st.session_state.playwright_crawl_logs) > 50 else st.session_state.playwright_crawl_logs
                    st.code('\n'.join(log_lines), language='text')
            
        with col2:
            col_title, col_refresh = st.columns([3, 1])
            with col_title:
                st.subheader("ğŸ“Š è³‡æ–™åº«çµ±è¨ˆ")
            with col_refresh:
                if st.button("ğŸ”„ åˆ·æ–°", key="refresh_playwright_db_stats", help="åˆ·æ–°è³‡æ–™åº«çµ±è¨ˆä¿¡æ¯", type="secondary"):
                    # æ¸…ç†å¯èƒ½çš„ç·©å­˜ç‹€æ…‹
                    if 'playwright_db_stats_cache' in st.session_state:
                        del st.session_state.playwright_db_stats_cache
                    st.success("ğŸ”„ æ­£åœ¨åˆ·æ–°çµ±è¨ˆ...")
                    st.rerun()  # é‡æ–°é‹è¡Œé é¢ä¾†åˆ·æ–°çµ±è¨ˆ
            
            self._display_database_stats()
        
        # æ§åˆ¶æŒ‰éˆ•
        col1, col2, col3 = st.columns([1, 1, 2])
        
        with col1:
            if st.button("ğŸš€ é–‹å§‹çˆ¬å–", key="start_playwright"):
                with st.spinner("æ­£åœ¨åŸ·è¡Œçˆ¬å–..."):
                    self._execute_playwright_crawling(username, max_posts)
                
        with col2:
            # è¼‰å…¥CSVæ–‡ä»¶åŠŸèƒ½
            uploaded_file = st.file_uploader(
                "ğŸ“ è¼‰å…¥CSVæ–‡ä»¶", 
                type=['csv'], 
                key="playwright_csv_uploader",
                help="ä¸Šå‚³ä¹‹å‰å°å‡ºçš„CSVæ–‡ä»¶ä¾†æŸ¥çœ‹çµæœ"
            )
            if uploaded_file is not None:
                self._load_csv_file(uploaded_file)
        
        with col3:
            # æ¸…é™¤çµæœæŒ‰éˆ• (åªåœ¨æœ‰çµæœæ™‚é¡¯ç¤º)
            if 'playwright_results' in st.session_state:
                if st.button("ğŸ—‘ï¸ æ¸…é™¤çµæœ", key="clear_playwright_results", help="æ¸…é™¤ç•¶å‰é¡¯ç¤ºçš„çµæœ"):
                    self._clear_results()
        
        # çµæœé¡¯ç¤º
        self._render_results_area()
    
    def _check_auth_file(self):
        """æª¢æŸ¥èªè­‰æª”æ¡ˆæ˜¯å¦å­˜åœ¨"""
        return self.auth_file_path.exists()
    
    def _write_progress(self, path: str, data: Dict[str, Any]):
        """ç·šç¨‹å®‰å…¨å¯«å…¥é€²åº¦æ–‡ä»¶"""
        old: Dict[str, Any] = {}
        if os.path.exists(path):
            try:
                with open(path, "r", encoding="utf-8") as f:
                    old = json.load(f)
            except Exception:
                pass

        # åˆä½µé‚è¼¯
        stage_priority = {
            "initialization": 0, "fetch_start": 1, "post_parsed": 2,
            "batch_parsed": 3, "fill_views_start": 4, "fill_views_completed": 5,
            "api_completed": 6, "completed": 7, "error": 8
        }
        old_stage = old.get("stage", "")
        new_stage = data.get("stage", old_stage)
        if stage_priority.get(new_stage, 0) < stage_priority.get(old_stage, 0):
            data.pop("stage", None)

        if "progress" not in data and "progress" in old:
            data["progress"] = old["progress"]
        if "current_work" not in data and "current_work" in old:
            data["current_work"] = old["current_work"]

        merged = {**old, **data, "timestamp": time.time()}

        # å…ˆå¯«åˆ° tmpï¼Œå† atomic rename
        dir_ = os.path.dirname(path)
        os.makedirs(dir_, exist_ok=True)
        
        try:
            with tempfile.NamedTemporaryFile("w", delete=False, dir=dir_, suffix=".tmp", encoding='utf-8') as tmp:
                json.dump(merged, tmp, ensure_ascii=False)
                tmp.flush()
                os.fsync(tmp.fileno())
                tmp_path = tmp.name
            
            shutil.move(tmp_path, path)
        except Exception as e:
            print(f"âŒ å¯«å…¥é€²åº¦æ–‡ä»¶å¤±æ•—: {e}")

    def _read_progress(self, path: str) -> Dict[str, Any]:
        """è®€å–é€²åº¦æ–‡ä»¶"""
        if not os.path.exists(path):
            return {}
        try:
            with open(path, "r", encoding="utf-8") as f:
                return json.load(f)
        except:
            return {}
    
    def _sse_listener(self, task_id: str, progfile: str):
        """SSE äº‹ä»¶ç›£è½ç·šç¨‹"""
        url = f"{self.sse_url}/{task_id}"
        self._add_log(f"ğŸ”¥ SSEç›£è½å•Ÿå‹•: {url}")
        
        try:
            with requests.get(url, stream=True, timeout=600) as response:  # èˆ‡åŸç‰ˆç›¸åŒçš„10åˆ†é˜è¶…æ™‚
                print(f"ğŸ”¥ SSEé€£æ¥æˆåŠŸï¼Œç‹€æ…‹ç¢¼: {response.status_code}")
                
                current_cnt = 0
                total_cnt = None      # ç¬¬ä¸€æ¬¡æ‹¿åˆ°å†æ”¾é€²ä¾†
                for line in response.iter_lines():
                    if line and line.startswith(b"data:"):
                        try:
                            data = json.loads(line[5:].decode().strip())
                            stage = data.get('stage', 'unknown')
                            print(f"ğŸ”¥ æ”¶åˆ°SSEäº‹ä»¶: {stage}")
                            
                            # --- é€šç”¨äº‹ä»¶è™•ç† ---
                            # å°æ–¼æ‰€æœ‰äº‹ä»¶ï¼Œéƒ½æº–å‚™ä¸€å€‹åŸºç¤çš„ payload
                            payload = {'stage': stage}

                            # æå–å·¥ä½œæè¿°
                            work_description = None
                            if "current_work" in data:
                                work_description = data["current_work"]
                            elif "message" in data:
                                work_description = data["message"]
                            
                            if work_description:
                                payload['current_work'] = work_description

                            # --- é‡å°æ€§è¨ˆç®—é€²åº¦ (V2 - åˆ†æ®µæ¬Šé‡) ---
                            PARSE_WEIGHT = 0.60   # è§£æéšæ®µä½” 60%
                            POST_PROCESS_W = 0.40   # å¾Œè™•ç†ä½” 40%

                            if stage == "post_parsed":
                                current_cnt += 1
                                total_cnt = total_cnt or data.get("total") # åªè¦æ‹¿ä¸€æ¬¡å°±å¥½
                                
                                if total_cnt:
                                    unit_progress = min(1.0, current_cnt / total_cnt)
                                    payload['progress'] = unit_progress * PARSE_WEIGHT # æ˜ å°„åˆ° 0% -> 60%
                                else:
                                    # æ²’ total æ™‚ï¼Œçµ¦ä¸€å€‹éå¢ä½†æ¥è¿‘60%çš„å‡é€²åº¦
                                    progress = min(PARSE_WEIGHT * 0.99, current_cnt * (PARSE_WEIGHT * 0.02))
                                    payload['progress'] = progress
                                payload['current_work'] = f"å·²è§£æ {current_cnt}/{total_cnt or '?'} ç¯‡"
                            
                            elif stage == "fill_views_start":
                                payload["progress"] = PARSE_WEIGHT # åˆ°é” 60%
                                payload["current_work"] = "æ­£åœ¨è£œé½Šç€è¦½æ•¸..."

                            elif stage == "fill_views_completed":
                                payload["progress"] = PARSE_WEIGHT + POST_PROCESS_W * 0.75 # 60% + 30% = 90%
                                payload["current_work"] = "ç€è¦½æ•¸å·²è£œé½Šï¼Œæº–å‚™æ”¶å°¾..."

                            elif stage in ("completed", "api_completed"):
                                payload["progress"] = 1.0
                                if not payload.get('current_work'):
                                    payload['current_work'] = "å…¨éƒ¨å®Œæˆï¼"

                            elif stage == "fetch_progress" and "progress" in data:
                                payload['progress'] = max(0.0, min(1.0, float(data["progress"])))
                            
                            # ç„¡è«–å“ªç¨®äº‹ä»¶ï¼Œéƒ½ç”¨ä¸€å€‹ write å®Œæˆ
                            self._write_progress(progfile, payload)
                            
                            # æª¢æŸ¥æ˜¯å¦å®Œæˆ
                            if stage in ("completed", "error"):
                                print(f"ğŸ”¥ SSEç›£è½çµæŸ: {stage}")
                                break
                        except json.JSONDecodeError as e:
                            print(f"âš ï¸ JSONè§£æå¤±æ•—: {e}")
                            continue
                            
        except Exception as e:
            self._add_log(f"âŒ SSEé€£æ¥å¤±æ•—: {e}")
            self._write_progress(progfile, {
                "stage": "error",
                "error": f"SSEé€£æ¥å¤±æ•—: {str(e)}",
                "status": "error"
            })
    
    def _execute_playwright_crawling(self, username: str, max_posts: int):
        """åŸ·è¡Œ Playwright çˆ¬èŸ²ï¼ˆä½¿ç”¨SSEï¼‰"""
        if not username.strip():
            st.error("è«‹è¼¸å…¥ç›®æ¨™å¸³è™Ÿï¼")
            return
            
        try:
            st.info(f"ğŸ”„ æ­£åœ¨é€é Playwright Agent çˆ¬å– @{username}ï¼Œé è¨ˆéœ€è¦è¼ƒé•·æ™‚é–“...")
            
            # åˆå§‹åŒ–æ—¥èªŒ
            st.session_state.playwright_crawl_logs = []
            
            # ç”Ÿæˆå”¯ä¸€çš„ä»»å‹™IDå’Œé€²åº¦æ–‡ä»¶
            task_id = str(uuid.uuid4())
            progress_dir = Path("temp_progress")
            progress_dir.mkdir(exist_ok=True)
            progress_file = progress_dir / f"playwright_crawl_{task_id}.json"
            
            # è®€å–èªè­‰æ–‡ä»¶
            try:
                with open(self.auth_file_path, "r", encoding="utf-8") as f:
                    auth_content = json.load(f)
            except Exception as e:
                st.error(f"âŒ è®€å–èªè­‰æª”æ¡ˆå¤±æ•—: {e}")
                return
            
            # æº–å‚™ API è«‹æ±‚çš„ payload
            payload = {
                "username": username,
                "max_posts": max_posts,
                "auth_json_content": auth_content,
                "task_id": task_id  # æ·»åŠ ä»»å‹™IDä»¥æ”¯æŒSSE
            }
            
            # æ·»åŠ æ—¥èªŒ
            self._add_log(f"ğŸ”§ æº–å‚™çˆ¬å– @{username}...")
            self._add_log(f"ğŸ“Š ç›®æ¨™è²¼æ–‡æ•¸: {max_posts}")
            self._add_log(f"ğŸ†” ä»»å‹™ID: {task_id}")
            self._add_log(f"ğŸ”— API ç«¯é»: {self.agent_url}")
            self._add_log(f"ğŸ“¡ SSE ç«¯é»: {self.sse_url}")
            
            # å•Ÿå‹•SSEç›£è½ç·šç¨‹
            sse_thread = threading.Thread(
                target=self._sse_listener,
                args=(task_id, str(progress_file)),
                daemon=True
            )
            sse_thread.start()
            
            # é¡¯ç¤ºé€²åº¦å€åŸŸ
            progress_container = st.empty()
            log_container = st.empty()
            
            with st.expander("ğŸ“‹ çˆ¬å–éç¨‹æ—¥å¿—", expanded=True):
                log_placeholder = st.empty()
                self._update_log_display(log_placeholder)
                
                # å•Ÿå‹•çˆ¬èŸ²ä»»å‹™
                self._add_log("ğŸš€ ç™¼é€APIè«‹æ±‚...")
                self._update_log_display(log_placeholder)
                
                # ä½¿ç”¨ç•°æ­¥æ–¹å¼ç™¼é€è«‹æ±‚ä½†ä¸ç­‰å¾…å®Œæˆ
                asyncio.run(self._execute_async_api_request(payload))
                
                # ç›£æ§é€²åº¦ç›´åˆ°å®Œæˆ
                self._monitor_progress_with_display(
                    str(progress_file), 
                    progress_container, 
                    log_placeholder,
                    task_id
                )
                
        except Exception as e:
            st.error(f"âŒ åŸ·è¡ŒéŒ¯èª¤ï¼š{str(e)}")
            st.session_state.playwright_error = str(e)
    
    async def _execute_async_api_request(self, payload):
        """ç•°æ­¥ç™¼é€APIè«‹æ±‚ï¼ˆä¸ç­‰å¾…å®Œæˆï¼‰"""
        try:
            timeout = httpx.Timeout(600.0)  # 10åˆ†é˜è¶…æ™‚ï¼Œèˆ‡åŸç‰ˆä¸€è‡´
            
            async with httpx.AsyncClient(timeout=timeout) as client:
                self._add_log("ğŸš€ å·²ç™¼é€ç•°æ­¥APIè«‹æ±‚ï¼Œç­‰å¾…SSEå›æ‡‰...")
                
                # ç™¼é€è«‹æ±‚ä½†åœ¨èƒŒæ™¯åŸ·è¡Œ
                response = await client.post(self.agent_url, json=payload)
                
                if response.status_code != 200:
                    self._add_log(f"âŒ API è«‹æ±‚å¤±æ•—ï¼Œç‹€æ…‹ç¢¼: {response.status_code}")
                    self._add_log(f"éŒ¯èª¤å…§å®¹: {response.text}")
                    return None
                
                # è§£æéŸ¿æ‡‰
                try:
                    final_data = response.json()
                    self._add_log("âœ… APIè«‹æ±‚æˆåŠŸå®Œæˆï¼")
                    return final_data
                except json.JSONDecodeError as e:
                    self._add_log(f"âŒ ç„¡æ³•è§£æéŸ¿æ‡‰ JSON: {e}")
                    return None
                    
        except httpx.TimeoutException:
            self._add_log("âŒ APIè«‹æ±‚è¶…æ™‚ï¼ˆ30åˆ†é˜ï¼‰")
            return None
        except Exception as e:
            self._add_log(f"âŒ APIè«‹æ±‚éŒ¯èª¤: {e}")
            return None
    
    def _monitor_progress_with_display(self, progress_file: str, progress_container, log_placeholder, task_id: str):
        """ç›£æ§é€²åº¦ä¸¦æ›´æ–°UIé¡¯ç¤º"""
        max_wait_time = 600  # 10åˆ†é˜æœ€å¤§ç­‰å¾…æ™‚é–“ï¼Œèˆ‡SSEè¶…æ™‚ä¸€è‡´
        start_time = time.time()
        last_activity = start_time
        
        while True:
            current_time = time.time()
            
            # æª¢æŸ¥ç¸½è¶…æ™‚
            if current_time - start_time > max_wait_time:
                self._add_log("âŒ çˆ¬èŸ²ç¸½è¶…æ™‚ï¼ˆ10åˆ†é˜ï¼‰ï¼Œåœæ­¢ç›£æ§")
                break
            
            # è®€å–é€²åº¦
            progress_data = self._read_progress(progress_file)
            
            if progress_data:
                last_activity = current_time
                stage = progress_data.get("stage", "unknown")
                progress = progress_data.get("progress", 0.0)
                current_work = progress_data.get("current_work", "è™•ç†ä¸­...")
                
                # æ›´æ–°é€²åº¦é¡¯ç¤º
                with progress_container.container():
                    st.subheader("ğŸ“Š çˆ¬èŸ²é€²åº¦")
                    
                    # é€²åº¦æ¢
                    progress_percent = int(progress * 100)
                    st.progress(progress, text=f"{progress_percent}% - {current_work}")
                    
                    # éšæ®µä¿¡æ¯
                    stage_names = {
                        "initialization": "ğŸ”§ åˆå§‹åŒ–",
                        "fetch_start": "ğŸ” é–‹å§‹çˆ¬å–",
                        "post_parsed": "ğŸ“ è§£æè²¼æ–‡",
                        "batch_parsed": "ğŸ“¦ æ‰¹æ¬¡è™•ç†",
                        "fill_views_start": "ğŸ‘ï¸ è£œå……è§€çœ‹æ•¸",
                        "fill_views_completed": "âœ… è§€çœ‹æ•¸å®Œæˆ",
                        "api_completed": "ğŸ¯ APIå®Œæˆ",
                        "completed": "ğŸ‰ å…¨éƒ¨å®Œæˆ",
                        "error": "âŒ ç™¼ç”ŸéŒ¯èª¤"
                    }
                    
                    stage_display = stage_names.get(stage, f"ğŸ”„ {stage}")
                    st.info(f"**ç•¶å‰éšæ®µ**: {stage_display}")
                    
                    if "error" in progress_data:
                        st.error(f"éŒ¯èª¤: {progress_data['error']}")
                
                # æ›´æ–°æ—¥èªŒé¡¯ç¤º
                self._update_log_display(log_placeholder)
                
                # æª¢æŸ¥æ˜¯å¦å®Œæˆ
                if stage in ("completed", "api_completed"):
                    self._add_log("ğŸ‰ çˆ¬èŸ²ä»»å‹™å®Œæˆï¼")
                    self._update_log_display(log_placeholder)
                    
                    # è™•ç†å®Œæˆå¾Œçš„çµæœ
                    self._handle_crawl_completion(task_id, progress_data)
                    break
                elif stage == "error":
                    self._add_log("âŒ çˆ¬èŸ²ä»»å‹™å¤±æ•—")
                    self._update_log_display(log_placeholder)
                    break
            else:
                # æª¢æŸ¥ç„¡æ´»å‹•è¶…æ™‚ï¼ˆ10åˆ†é˜æ²’æœ‰é€²åº¦æ›´æ–°ï¼‰
                if current_time - last_activity > 600:
                    self._add_log("âš ï¸ é•·æ™‚é–“ç„¡é€²åº¦æ›´æ–°ï¼Œä½†ç¹¼çºŒç­‰å¾…...")
                    last_activity = current_time  # é‡è¨­è¨ˆæ™‚å™¨
                    self._update_log_display(log_placeholder)
            
            # çŸ­æš«ä¼‘çœ å¾Œå†æª¢æŸ¥
            time.sleep(2)
    
    def _handle_crawl_completion(self, task_id: str, progress_data: Dict):
        """è™•ç†çˆ¬èŸ²å®Œæˆå¾Œçš„çµæœ"""
        try:
            self._add_log("ğŸ”„ æ­£åœ¨ç²å–æœ€çµ‚çµæœ...")
            
            # é€™è£¡éœ€è¦å¾æŸå€‹åœ°æ–¹ç²å–å¯¦éš›çš„çˆ¬èŸ²çµæœ
            # å¯èƒ½éœ€è¦èª¿ç”¨ä¸€å€‹ç²å–çµæœçš„API
            # æš«æ™‚å…ˆå‰µå»ºä¸€å€‹ç¤ºä¾‹çµæœ
            
            # å‰µå»ºçµæœæ•¸æ“š
            converted_results = {
                "crawl_id": f"{datetime.now().strftime('%Y%m%d_%H%M%S')}_{str(uuid.uuid4())[:8]}",
                "timestamp": datetime.now().isoformat(),
                "target_username": "unknown",  # éœ€è¦å¾æŸè™•ç²å–
                "crawler_type": "playwright",
                "total_processed": 0,
                "results": [],
                "source": "playwright_agent",
                "database_saved": False,
                "database_saved_count": 0
            }
            
            # ä¿å­˜JSONæ–‡ä»¶
            json_file_path = self._save_json_results(converted_results)
            
            # ä¿å­˜åˆ°session_state
            st.session_state.playwright_results = converted_results
            st.session_state.playwright_results_file = json_file_path
            
            # è‡ªå‹•ä¿å­˜åˆ°è³‡æ–™åº«
            asyncio.run(self._save_to_database_async(converted_results))
            
            st.success(f"âœ… çˆ¬å–å®Œæˆï¼")
            
            # æ¸…ç†è³‡æ–™åº«çµ±è¨ˆç·©å­˜
            if 'playwright_db_stats_cache' in st.session_state:
                del st.session_state.playwright_db_stats_cache
            
            st.info("ğŸ“Š çˆ¬å–çµæœå·²è‡ªå‹•ä¿å­˜ï¼Œæ‚¨å¯ä»¥é»æ“Šå³å´ã€ŒğŸ”„ åˆ·æ–°ã€æŸ¥çœ‹æ›´æ–°çš„çµ±è¨ˆ")
            st.balloons()
            
        except Exception as e:
            self._add_log(f"âŒ è™•ç†å®Œæˆçµæœæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
    
    def _convert_playwright_results(self, playwright_data):
        """è½‰æ› Playwright API çµæœç‚ºå°ˆç”¨æ ¼å¼"""
        posts = playwright_data.get("posts", [])
        username = playwright_data.get("username", "")
        
        # è½‰æ›ç‚º Playwright å°ˆç”¨æ ¼å¼
        converted_results = []
        for post in posts:
            # æª¢æŸ¥æ•¸æ“šæ ¼å¼ä¸¦è½‰æ›
            result = {
                "post_id": post.get("post_id", ""),
                "url": post.get("url", ""),
                "content": post.get("content", ""),
                "views": str(post.get("views_count", "") or ""),
                "likes": str(post.get("likes_count", "") or ""),
                "comments": str(post.get("comments_count", "") or ""),
                "reposts": str(post.get("reposts_count", "") or ""),
                "shares": str(post.get("shares_count", "") or ""),
                "source": "playwright_agent",
                "crawler_type": "playwright",  # æ¨™è¨˜çˆ¬èŸ²é¡å‹
                "success": True,
                "has_views": bool(post.get("views_count")),
                "has_content": bool(post.get("content")),
                "has_likes": bool(post.get("likes_count")),
                "has_comments": bool(post.get("comments_count")),
                "has_reposts": bool(post.get("reposts_count")),
                "has_shares": bool(post.get("shares_count")),
                "content_length": len(post.get("content", "")),
                "extracted_at": datetime.now().isoformat(),
                "created_at": post.get("created_at", ""),
                "username": username
            }
            converted_results.append(result)
        
        # ç”Ÿæˆå”¯ä¸€IDï¼ˆæ™‚é–“æˆ³ + éš¨æ©Ÿå­—ç¬¦ï¼‰
        import uuid
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        unique_id = str(uuid.uuid4())[:8]
        
        # åŒ…è£ç‚º Playwright å°ˆç”¨çµæ§‹
        return {
            "crawl_id": f"{timestamp}_{unique_id}",
            "timestamp": datetime.now().isoformat(),
            "target_username": username,
            "crawler_type": "playwright",
            "max_posts": len(posts),
            "total_processed": len(posts),
            "api_success_count": len(posts),
            "api_failure_count": 0,
            "overall_success_rate": 100.0 if posts else 0.0,
            "timing": {
                "total_time": 0,  # Playwright API ä¸æä¾›è©³ç´°è¨ˆæ™‚
                "url_collection_time": 0,
                "content_extraction_time": 0
            },
            "results": converted_results,
            "source": "playwright_agent",
            "database_saved": False,  # å°‡åœ¨ä¿å­˜å¾Œæ›´æ–°
            "database_saved_count": 0
        }
    
    async def _save_to_database_async(self, results_data):
        """ç•°æ­¥ä¿å­˜çµæœåˆ° Playwright å°ˆç”¨è³‡æ–™è¡¨"""
        try:
            from common.db_client import DatabaseClient
            
            db = DatabaseClient()
            await db.init_pool()
            
            try:
                results = results_data.get("results", [])
                target_username = results_data.get("target_username", "")
                crawl_id = results_data.get("crawl_id", "")
                
                if results and target_username:
                    saved_count = 0
                    
                    async with db.get_connection() as conn:
                        # å‰µå»º Playwright å°ˆç”¨è³‡æ–™è¡¨ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
                        await conn.execute("""
                            CREATE TABLE IF NOT EXISTS playwright_post_metrics (
                                id SERIAL PRIMARY KEY,
                                username VARCHAR(255) NOT NULL,
                                post_id VARCHAR(255) NOT NULL,
                                url TEXT,
                                content TEXT,
                                views_count INTEGER,
                                likes_count INTEGER,
                                comments_count INTEGER,
                                reposts_count INTEGER,
                                shares_count INTEGER,
                                source VARCHAR(100) DEFAULT 'playwright_agent',
                                crawler_type VARCHAR(50) DEFAULT 'playwright',
                                crawl_id VARCHAR(255),
                                created_at TIMESTAMP,
                                fetched_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                                UNIQUE(username, post_id, crawler_type)
                            )
                        """)
                        
                        # å‰µå»ºç´¢å¼•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
                        await conn.execute("""
                            CREATE INDEX IF NOT EXISTS idx_playwright_username_created 
                            ON playwright_post_metrics(username, created_at DESC)
                        """)
                        
                        await conn.execute("""
                            CREATE INDEX IF NOT EXISTS idx_playwright_crawl_id 
                            ON playwright_post_metrics(crawl_id)
                        """)
                        
                        # æ’å…¥æ•¸æ“š
                        for result in results:
                            try:
                                # è§£ææ•¸å­—å­—æ®µ
                                views_count = self._parse_number_safe(result.get('views', ''))
                                likes_count = self._parse_number_safe(result.get('likes', ''))
                                comments_count = self._parse_number_safe(result.get('comments', ''))
                                reposts_count = self._parse_number_safe(result.get('reposts', ''))
                                shares_count = self._parse_number_safe(result.get('shares', ''))
                                
                                # ä½¿ç”¨ UPSERT é¿å…é‡è¤‡
                                await conn.execute("""
                                    INSERT INTO playwright_post_metrics (
                                        username, post_id, url, content, 
                                        views_count, likes_count, comments_count, reposts_count, shares_count,
                                        source, crawler_type, crawl_id, created_at
                                    ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, NOW())
                                    ON CONFLICT (username, post_id, crawler_type) 
                                    DO UPDATE SET
                                        url = EXCLUDED.url,
                                        content = EXCLUDED.content,
                                        views_count = EXCLUDED.views_count,
                                        likes_count = EXCLUDED.likes_count,
                                        comments_count = EXCLUDED.comments_count,
                                        reposts_count = EXCLUDED.reposts_count,
                                        shares_count = EXCLUDED.shares_count,
                                        crawl_id = EXCLUDED.crawl_id,
                                        fetched_at = CURRENT_TIMESTAMP
                                """, 
                                    target_username,
                                    result.get('post_id', ''),
                                    result.get('url', ''),
                                    result.get('content', ''),
                                    views_count,
                                    likes_count,
                                    comments_count,
                                    reposts_count,
                                    shares_count,
                                    'playwright_agent',
                                    'playwright',
                                    crawl_id
                                )
                                saved_count += 1
                                
                            except Exception as e:
                                self._add_log(f"âš ï¸ ä¿å­˜å–®å€‹è²¼æ–‡å¤±æ•— {result.get('post_id', 'N/A')}: {e}")
                                continue
                        
                        # æ›´æ–° Playwright çˆ¬å–æª¢æŸ¥é»è¡¨
                        await conn.execute("""
                            CREATE TABLE IF NOT EXISTS playwright_crawl_state (
                                id SERIAL PRIMARY KEY,
                                username VARCHAR(255) UNIQUE NOT NULL,
                                latest_post_id VARCHAR(255),
                                total_crawled INTEGER DEFAULT 0,
                                last_crawl_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                                crawl_id VARCHAR(255)
                            )
                        """)
                        
                        if results and saved_count > 0:
                            latest_post_id = results[0].get('post_id')
                            await conn.execute("""
                                INSERT INTO playwright_crawl_state (username, latest_post_id, total_crawled, crawl_id)
                                VALUES ($1, $2, $3, $4)
                                ON CONFLICT (username)
                                DO UPDATE SET
                                    latest_post_id = EXCLUDED.latest_post_id,
                                    total_crawled = playwright_crawl_state.total_crawled + EXCLUDED.total_crawled,
                                    last_crawl_at = CURRENT_TIMESTAMP,
                                    crawl_id = EXCLUDED.crawl_id
                            """, target_username, latest_post_id, saved_count, crawl_id)
                    
                    # æ›´æ–°çµæœç‹€æ…‹
                    results_data["database_saved"] = True
                    results_data["database_saved_count"] = saved_count
                    
                    self._add_log(f"ğŸ’¾ å·²ä¿å­˜ {saved_count} å€‹è²¼æ–‡åˆ° Playwright å°ˆç”¨è³‡æ–™è¡¨")
                    
            finally:
                await db.close_pool()
                
        except Exception as e:
            self._add_log(f"âš ï¸ è³‡æ–™åº«ä¿å­˜è­¦å‘Š: {e}")
            # ä¸é˜»æ­¢ä¸»è¦æµç¨‹ï¼Œä½†è¨˜éŒ„è­¦å‘Š
    
    def _parse_number_safe(self, value):
        """å®‰å…¨è§£ææ•¸å­—å­—ç¬¦ä¸²"""
        try:
            if not value or value == 'N/A':
                return None
            # ç§»é™¤éæ•¸å­—å­—ç¬¦ï¼ˆé™¤äº†å°æ•¸é»ï¼‰
            clean_value = str(value).replace(',', '').replace(' ', '')
            if 'K' in clean_value:
                return int(float(clean_value.replace('K', '')) * 1000)
            elif 'M' in clean_value:
                return int(float(clean_value.replace('M', '')) * 1000000)
            elif 'B' in clean_value:
                return int(float(clean_value.replace('B', '')) * 1000000000)
            else:
                return int(float(clean_value))
        except:
            return None
    
    def _save_json_results(self, results_data):
        """ä¿å­˜çµæœç‚ºJSONæ–‡ä»¶ï¼Œä½¿ç”¨æŒ‡å®šæ ¼å¼"""
        try:
            # å‰µå»º playwright_results ç›®éŒ„
            results_dir = Path("playwright_results")
            results_dir.mkdir(exist_ok=True)
            
            # ç”Ÿæˆæ–‡ä»¶åï¼šcrawl_data_20250803_121452_934d52b1.json
            crawl_id = results_data.get("crawl_id", "unknown")
            filename = f"crawl_data_{crawl_id}.json"
            json_file_path = results_dir / filename
            
            # ä¿å­˜JSONæ–‡ä»¶
            with open(json_file_path, 'w', encoding='utf-8') as f:
                json.dump(results_data, f, ensure_ascii=False, indent=2)
            
            self._add_log(f"ğŸ’¾ çµæœå·²ä¿å­˜: {json_file_path}")
            return json_file_path
            
        except Exception as e:
            self._add_log(f"âš ï¸ ä¿å­˜JSONæ–‡ä»¶å¤±æ•—: {e}")
            return None
    
    def _add_log(self, message):
        """æ·»åŠ æ—¥èªŒæ¶ˆæ¯"""
        if 'playwright_crawl_logs' not in st.session_state:
            st.session_state.playwright_crawl_logs = []
        
        timestamp = datetime.now().strftime("%H:%M:%S")
        log_message = f"[{timestamp}] {message}"
        st.session_state.playwright_crawl_logs.append(log_message)
    
    def _update_log_display(self, log_placeholder):
        """æ›´æ–°æ—¥èªŒé¡¯ç¤º"""
        if 'playwright_crawl_logs' in st.session_state:
            log_lines = st.session_state.playwright_crawl_logs[-30:]  # é¡¯ç¤ºæœ€å¾Œ30è¡Œ
            log_placeholder.code('\n'.join(log_lines), language='text')
    
    def _load_csv_file(self, uploaded_file):
        """è¼‰å…¥CSVæ–‡ä»¶ä¸¦è½‰æ›ç‚ºçµæœæ ¼å¼"""
        try:
            import pandas as pd
            import io
            
            # è®€å–CSVæ–‡ä»¶
            content = uploaded_file.getvalue()
            df = pd.read_csv(io.StringIO(content.decode('utf-8-sig')))
            
            # æª¢æŸ¥CSVæ ¼å¼æ˜¯å¦æ­£ç¢ºï¼ˆæ›´éˆæ´»çš„é©—è­‰ï¼‰
            # æ ¸å¿ƒå¿…è¦æ¬„ä½
            core_required = ['username', 'post_id', 'content']
            missing_core = [col for col in core_required if col not in df.columns]
            
            if missing_core:
                st.error(f"âŒ CSVæ ¼å¼ä¸æ­£ç¢ºï¼Œç¼ºå°‘æ ¸å¿ƒæ¬„ä½: {', '.join(missing_core)}")
                return
            
            # æª¢æŸ¥å¯é¸æ¬„ä½ï¼Œå¦‚æœæ²’æœ‰å‰‡æä¾›é è¨­å€¼
            optional_columns = ['views', 'likes_count', 'comments_count', 'reposts_count', 'shares_count']
            for col in optional_columns:
                if col not in df.columns:
                    if col == 'views':
                        df[col] = df.get('views_count', 0)  # å˜—è©¦ä½¿ç”¨ views_count ä½œç‚º views
                    else:
                        df[col] = 0  # é è¨­å€¼ç‚º 0
            
            st.info(f"âœ… æˆåŠŸè¼‰å…¥CSVï¼ŒåŒ…å« {len(df)} ç­†è¨˜éŒ„")
            
            # è½‰æ›ç‚ºçµæœæ ¼å¼
            results = []
            for _, row in df.iterrows():
                # è½‰æ›æ•¸æ“šä¸¦è™•ç†ç©ºå€¼
                views = str(row.get('views', '')).strip()
                likes = str(row.get('likes', '')).strip()
                comments = str(row.get('comments', '')).strip()
                reposts = str(row.get('reposts', '')).strip()
                shares = str(row.get('shares', '')).strip()
                content = str(row.get('content', '')).strip()
                
                result = {
                    'username': str(row.get('username', '')),
                    'post_id': str(row.get('post_id', '')),
                    'content': content,
                    'views': views,
                    'likes': likes,
                    'comments': comments,
                    'reposts': reposts,
                    'shares': shares,
                    'url': str(row.get('url', '')),
                    'source': str(row.get('source', 'csv_import')),
                    'created_at': str(row.get('created_at', '')),
                    'fetched_at': str(row.get('fetched_at', '')),
                    'success': True,
                    # æ·»åŠ has_*æ¬„ä½ä»¥å…¼å®¹é¡¯ç¤ºé‚è¼¯
                    'has_views': bool(views and views != 'nan' and views != ''),
                    'has_content': bool(content and content != 'nan' and content != ''),
                    'has_likes': bool(likes and likes != 'nan' and likes != ''),
                    'has_comments': bool(comments and comments != 'nan' and comments != ''),
                    'has_reposts': bool(reposts and reposts != 'nan' and reposts != ''),
                    'has_shares': bool(shares and shares != 'nan' and shares != ''),
                    'content_length': len(content) if content else 0,
                    'extracted_at': datetime.now().isoformat()
                }
                results.append(result)
            
            # ä¿å­˜åˆ°æœƒè©±ç‹€æ…‹
            st.session_state.playwright_results = {
                'results': results,
                'total_count': len(results),
                'username': results[0]['username'] if results else '',
                'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'source': f"CSVæ–‡ä»¶: {uploaded_file.name}"
            }
            
            st.success(f"âœ… æˆåŠŸè¼‰å…¥ {len(results)} ç­†è¨˜éŒ„")
            st.info(f"ğŸ“Š ä¾†æº: {uploaded_file.name}")
            
        except Exception as e:
            st.error(f"âŒ è¼‰å…¥CSVæ–‡ä»¶å¤±æ•—: {str(e)}")
    
    def _clear_results(self):
        """æ¸…é™¤ç•¶å‰çµæœ"""
        if 'playwright_results' in st.session_state:
            del st.session_state.playwright_results
        if 'playwright_results_file' in st.session_state:
            del st.session_state.playwright_results_file
        if 'playwright_error' in st.session_state:
            del st.session_state.playwright_error
        if 'latest_playwright_csv_file' in st.session_state:
            del st.session_state.latest_playwright_csv_file
        st.success("ğŸ—‘ï¸ çµæœå·²æ¸…é™¤")
        st.rerun()  # é‡æ–°é‹è¡Œé é¢ä¾†åˆ·æ–°UI
    
    def _render_results_area(self):
        """æ¸²æŸ“çµæœå€åŸŸ"""
        if 'playwright_results' in st.session_state:
            self._show_results()
        elif 'playwright_error' in st.session_state:
            st.error(f"âŒ çˆ¬å–éŒ¯èª¤ï¼š{st.session_state.playwright_error}")
        else:
            st.info("ğŸ‘† é»æ“Šã€Œé–‹å§‹çˆ¬å–ã€ä¾†é–‹å§‹ï¼Œæˆ–ä¸Šå‚³CSVæ–‡ä»¶æŸ¥çœ‹ä¹‹å‰çš„çµæœ")
    
    def _show_results(self):
        """é¡¯ç¤ºçˆ¬å–çµæœ"""
        # å¾session stateç²å–çµæœï¼ˆå¯èƒ½æ˜¯å­—å…¸æ ¼å¼ï¼‰
        playwright_results = st.session_state.playwright_results
        
        # æª¢æŸ¥resultsçš„æ ¼å¼ï¼Œå¦‚æœæ˜¯å­—å…¸å‰‡æå–resultsåˆ—è¡¨
        if isinstance(playwright_results, dict):
            results = playwright_results.get('results', [])
        else:
            results = playwright_results if playwright_results else []
        
        st.subheader("ğŸ“Š çˆ¬å–çµæœ")
        
        # ç¢ºä¿resultsæ˜¯åˆ—è¡¨
        if not isinstance(results, list):
            st.error("âŒ çµæœæ ¼å¼éŒ¯èª¤ï¼Œè«‹é‡æ–°è¼‰å…¥")
            return
        
        # åŸºæœ¬çµ±è¨ˆ
        total_posts = len(results)
        successful_views = len([r for r in results if isinstance(r, dict) and r.get('has_views')])
        successful_content = len([r for r in results if isinstance(r, dict) and r.get('has_content')])
        successful_likes = len([r for r in results if isinstance(r, dict) and r.get('has_likes')])
        successful_comments = len([r for r in results if isinstance(r, dict) and r.get('has_comments')])
        
        # çµ±è¨ˆæŒ‡æ¨™
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("ç¸½è²¼æ–‡æ•¸", total_posts)
        with col2:
            st.metric("è§€çœ‹æ•¸æˆåŠŸ", f"{successful_views}/{total_posts}")
        with col3:
            st.metric("å…§å®¹æˆåŠŸ", f"{successful_content}/{total_posts}")
        with col4:
            st.metric("äº’å‹•æ•¸æ“š", f"{successful_likes}/{total_posts}")
        
        # æˆåŠŸç‡æŒ‡æ¨™
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            view_rate = (successful_views / total_posts * 100) if total_posts > 0 else 0
            st.metric("è§€çœ‹æ•¸æˆåŠŸç‡", f"{view_rate:.1f}%")
        with col2:
            content_rate = (successful_content / total_posts * 100) if total_posts > 0 else 0
            st.metric("å…§å®¹æˆåŠŸç‡", f"{content_rate:.1f}%")
        with col3:
            like_rate = (successful_likes / total_posts * 100) if total_posts > 0 else 0
            st.metric("æŒ‰è®šæ•¸æˆåŠŸç‡", f"{like_rate:.1f}%")
        with col4:
            comment_rate = (successful_comments / total_posts * 100) if total_posts > 0 else 0
            st.metric("ç•™è¨€æ•¸æˆåŠŸç‡", f"{comment_rate:.1f}%")
        
        # è©³ç´°çµæœè¡¨æ ¼
        if st.checkbox("ğŸ“‹ é¡¯ç¤ºè©³ç´°çµæœ", key="show_playwright_detailed_results"):
            self._show_detailed_table(results)
        
        # è³‡æ–™åº«ç‹€æ…‹å’Œå‚™ç”¨ä¿å­˜
        if isinstance(playwright_results, dict):
            db_saved = playwright_results.get('database_saved', False)
            saved_count = playwright_results.get('database_saved_count', 0)
            if db_saved:
                st.success(f"âœ… å·²ä¿å­˜åˆ°è³‡æ–™åº« ({saved_count} å€‹è²¼æ–‡)")
            else:
                # é¡¯ç¤ºå‚™ç”¨ä¿å­˜é¸é …
                col_info, col_save = st.columns([3, 1])
                with col_info:
                    st.info("â„¹ï¸ çˆ¬èŸ²é€šå¸¸æœƒè‡ªå‹•ä¿å­˜åˆ°è³‡æ–™åº«ã€‚å¦‚æœçµ±è¨ˆä¸­æ²’æœ‰çœ‹åˆ°æ–°æ•¸æ“šï¼Œæ‚¨å¯ä»¥ä½¿ç”¨å‚™ç”¨ä¿å­˜åŠŸèƒ½")
                with col_save:
                    if st.button("ğŸ’¾ å‚™ç”¨ä¿å­˜", key="save_playwright_to_database", help="æ‰‹å‹•ä¿å­˜åˆ°è³‡æ–™åº«ï¼ˆå‚™ç”¨åŠŸèƒ½ï¼‰"):
                        self._save_results_to_database()
        else:
            st.info("ğŸ’¡ Playwright çˆ¬å–æ¨¡å¼æœƒè‡ªå‹•ä¿å­˜åˆ°è³‡æ–™åº«ä¸¦æ›´æ–°çµ±è¨ˆ")

        st.divider()
        
        # ä¸‹è¼‰å’Œå°å‡ºæŒ‰éˆ•
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            if st.button("ğŸ’¾ ä¸‹è¼‰JSON", key="download_playwright_json"):
                self._show_json_download_button()
        
        with col2:
            if st.button("ğŸ“Š å°å‡ºCSV", key="export_playwright_csv"):
                self._show_csv_export_options()
        
        with col3:
            if st.button("ğŸ“ˆ æ­·å²åˆ†æ", key="export_playwright_history"):
                self._show_export_history_options()
        
        with col4:
            if st.button("ğŸ” æ›´å¤šå°å‡º", key="more_playwright_exports"):
                self._show_advanced_export_options()
    
    def _show_json_download_button(self):
        """é¡¯ç¤ºJSONä¸‹è¼‰æŒ‰éˆ•"""
        if 'playwright_results' in st.session_state:
            try:
                # å°‡çµæœè½‰æ›ç‚ºJSONæ ¼å¼
                json_content = json.dumps(
                    st.session_state.playwright_results, 
                    ensure_ascii=False, 
                    indent=2
                )
                
                # ç”Ÿæˆä¸‹è¼‰æ–‡ä»¶åï¼ˆåŒ…å«æ™‚é–“æˆ³ï¼‰
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                download_filename = f"playwright_crawl_results_{timestamp}.json"
                
                # ä½¿ç”¨ st.download_button æä¾›ä¸‹è¼‰
                st.download_button(
                    label="ğŸ’¾ ä¸‹è¼‰JSON",
                    data=json_content,
                    file_name=download_filename,
                    mime="application/json",
                    help="ä¸‹è¼‰çˆ¬å–çµæœJSONæ–‡ä»¶åˆ°æ‚¨çš„ä¸‹è¼‰è³‡æ–™å¤¾",
                    key="download_playwright_json_btn"
                )
                
            except Exception as e:
                st.error(f"âŒ æº–å‚™ä¸‹è¼‰æ–‡ä»¶å¤±æ•—: {e}")
        else:
            st.button("ğŸ’¾ ä¸‹è¼‰JSON", disabled=True, help="æš«ç„¡å¯ä¸‹è¼‰çš„çµæœæ–‡ä»¶")
    
    def _show_csv_export_options(self):
        """é¡¯ç¤ºCSVå°å‡ºé¸é …"""
        # å¾©ç”¨åŸæœ‰çš„CSVå°å‡ºé‚è¼¯ï¼Œä½†ä½¿ç”¨playwrightçµæœ
        # é€™è£¡ç°¡åŒ–å¯¦ç¾
        if 'playwright_results' not in st.session_state:
            st.error("âŒ æ²’æœ‰å¯å°å‡ºçš„çµæœ")
            return
        
        with st.expander("ğŸ“Š CSVå°å‡ºé¸é …", expanded=True):
            st.write("**é¸æ“‡æ’åºæ–¹å¼ï¼ˆå»ºè­°æŒ‰è§€çœ‹æ•¸æ’åºï¼‰**")
            
            sort_options = {
                "è§€çœ‹æ•¸ (é«˜â†’ä½)": "views",
                "æŒ‰è®šæ•¸ (é«˜â†’ä½)": "likes", 
                "ç•™è¨€æ•¸ (é«˜â†’ä½)": "comments",
                "è½‰ç™¼æ•¸ (é«˜â†’ä½)": "reposts",
                "åˆ†äº«æ•¸ (é«˜â†’ä½)": "shares",
                "è²¼æ–‡ID (Aâ†’Z)": "post_id",
                "åŸå§‹é †åº (ä¸æ’åº)": "none"
            }
            
            selected_sort = st.selectbox(
                "æ’åºæ–¹å¼",
                options=list(sort_options.keys()),
                index=0,  # é è¨­é¸æ“‡è§€çœ‹æ•¸æ’åº
                help="é¸æ“‡CSVæ–‡ä»¶ä¸­æ•¸æ“šçš„æ’åºæ–¹å¼ï¼Œå»ºè­°é¸æ“‡è§€çœ‹æ•¸ä»¥ä¾¿åˆ†ææœ€å—æ­¡è¿çš„è²¼æ–‡",
                key="playwright_sort_selector"
            )
            
            if st.button("ğŸ“¥ ç”ŸæˆCSV", key="export_playwright_csv_generate"):
                sort_by = sort_options[selected_sort]
                self._export_current_to_csv(sort_by)
            
            # æª¢æŸ¥æ˜¯å¦æœ‰ç”Ÿæˆå¥½çš„CSVå¯ä»¥ä¸‹è¼‰
            self._show_csv_download_if_available()
    
    def _export_current_to_csv(self, sort_by: str = 'views'):
        """å°å‡ºç•¶æ¬¡çµæœåˆ°CSV"""
        try:
            import pandas as pd
            
            # ç²å–çµæœæ•¸æ“š
            playwright_results = st.session_state.playwright_results
            if isinstance(playwright_results, dict):
                results = playwright_results.get('results', [])
            else:
                results = playwright_results if playwright_results else []
            
            if not results:
                st.error("âŒ æ²’æœ‰å¯å°å‡ºçš„çµæœ")
                return
            
            # è½‰æ›ç‚ºDataFrame
            df_data = []
            for r in results:
                df_data.append({
                    'username': r.get('username', ''),
                    'post_id': r.get('post_id', ''),
                    'url': r.get('url', ''),
                    'content': r.get('content', ''),
                    'views': r.get('views', ''),
                    'likes': r.get('likes', ''),
                    'comments': r.get('comments', ''),
                    'reposts': r.get('reposts', ''),
                    'shares': r.get('shares', ''),
                    'source': r.get('source', ''),
                    'created_at': r.get('created_at', ''),
                    'fetched_at': r.get('extracted_at', '')
                })
            
            df = pd.DataFrame(df_data)
            
            # æ’åºï¼ˆç°¡åŒ–ç‰ˆæœ¬ï¼‰
            if sort_by != 'none' and sort_by in df.columns:
                # å°æ–¼æ•¸å­—æ¬„ä½ï¼Œéœ€è¦å…ˆè½‰æ›
                if sort_by in ['views', 'likes', 'comments', 'reposts', 'shares']:
                    # ç°¡å–®çš„æ•¸å­—è½‰æ›ï¼ˆå¯¦éš›æ‡‰ç”¨ä¸­å¯èƒ½éœ€è¦æ›´è¤‡é›œçš„è™•ç†ï¼‰
                    df[f'{sort_by}_numeric'] = pd.to_numeric(df[sort_by].str.replace(r'[^\d.]', '', regex=True), errors='coerce')
                    df = df.sort_values(f'{sort_by}_numeric', ascending=False)
                    df = df.drop(columns=[f'{sort_by}_numeric'])
                else:
                    df = df.sort_values(sort_by)
            
            # ç”ŸæˆCSVæ–‡ä»¶
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            csv_filename = f"playwright_crawl_results_{timestamp}.csv"
            
            # ä¿å­˜æ–‡ä»¶
            csv_content = df.to_csv(index=False, encoding='utf-8-sig')
            
            # ä¿å­˜CSVæ–‡ä»¶è·¯å¾‘åˆ°æœƒè©±ç‹€æ…‹
            st.session_state.latest_playwright_csv_file = csv_content
            st.session_state.latest_playwright_csv_filename = csv_filename
            
            st.success(f"âœ… CSVç”ŸæˆæˆåŠŸï¼")
            st.info(f"ğŸ“Š åŒ…å« {len(df)} ç­†è¨˜éŒ„")
            
        except Exception as e:
            st.error(f"âŒ CSVç”Ÿæˆå¤±æ•—: {str(e)}")
    
    def _show_csv_download_if_available(self):
        """é¡¯ç¤ºCSVä¸‹è¼‰æŒ‰éˆ•ï¼ˆå¦‚æœæœ‰å¯ç”¨çš„CSVæ–‡ä»¶ï¼‰"""
        if 'latest_playwright_csv_file' in st.session_state:
            csv_content = st.session_state.latest_playwright_csv_file
            filename = st.session_state.get('latest_playwright_csv_filename', 'playwright_results.csv')
            
            st.download_button(
                label="ğŸ“¥ ä¸‹è¼‰CSVæ–‡ä»¶",
                data=csv_content,
                file_name=filename,
                mime="text/csv",
                help="ä¸‹è¼‰CSVæ–‡ä»¶åˆ°æ‚¨çš„ä¸‹è¼‰è³‡æ–™å¤¾",
                key="download_playwright_csv_file_btn"
            )
    
    def _show_export_history_options(self):
        """é¡¯ç¤ºæ­·å²å°å‡ºé¸é …ï¼ˆç°¡åŒ–ç‰ˆæœ¬ï¼‰"""
        st.info("ğŸ“ˆ æ­·å²åˆ†æåŠŸèƒ½èˆ‡ realtime_crawler_component.py å…±ç”¨ï¼Œè«‹åœ¨è³‡æ–™åº«çµ±è¨ˆä¸­æŸ¥çœ‹æ­·å²æ•¸æ“š")
    
    def _show_advanced_export_options(self):
        """é¡¯ç¤ºé€²éšå°å‡ºé¸é …ï¼ˆç°¡åŒ–ç‰ˆæœ¬ï¼‰"""
        st.info("ğŸ” é€²éšå°å‡ºåŠŸèƒ½èˆ‡ realtime_crawler_component.py å…±ç”¨")
    
    def _show_detailed_table(self, results: List[Dict]):
        """é¡¯ç¤ºè©³ç´°çµæœè¡¨æ ¼"""
        st.subheader("ğŸ“‹ è©³ç´°çµæœ")
        
        # æº–å‚™è¡¨æ ¼æ•¸æ“š
        table_data = []
        for r in results:
            table_data.append({
                "è²¼æ–‡ID": r.get('post_id', 'N/A'),
                "è§€çœ‹æ•¸": r.get('views', 'N/A'),
                "æŒ‰è®šæ•¸": r.get('likes', 'N/A'),
                "ç•™è¨€æ•¸": r.get('comments', 'N/A'),
                "è½‰ç™¼æ•¸": r.get('reposts', 'N/A'),
                "åˆ†äº«æ•¸": r.get('shares', 'N/A'),
                "å…§å®¹é è¦½": (r.get('content', '')[:50] + "...") if r.get('content') else 'N/A',
                "ä¾†æº": r.get('source', 'N/A')
            })
        
        # é¡¯ç¤ºè¡¨æ ¼
        st.dataframe(
            table_data,
            use_container_width=True,
            height=400
        )
    
    def _display_database_stats(self):
        """é¡¯ç¤ºè³‡æ–™åº«çµ±è¨ˆä¿¡æ¯ï¼ˆå¾©ç”¨ realtime_crawler_component.py çš„é‚è¼¯ï¼‰"""
        # æª¢æŸ¥æ˜¯å¦æœ‰ç·©å­˜çš„çµ±è¨ˆä¿¡æ¯
        if 'playwright_db_stats_cache' in st.session_state:
            self._render_cached_stats(st.session_state.playwright_db_stats_cache)
            return
        
        try:
            # ä½¿ç”¨ asyncio å’Œ subprocess ä¾†ç²å–è³‡æ–™åº«çµ±è¨ˆ
            import subprocess
            import json
            import sys
            import os
            
            # å‰µå»ºä¸€å€‹è‡¨æ™‚è…³æœ¬ä¾†ç²å– Playwright è³‡æ–™åº«çµ±è¨ˆ
            script_content = '''
import asyncio
import sys
import os
import json
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from common.db_client import DatabaseClient

async def get_playwright_database_stats():
    db = DatabaseClient()
    try:
        await db.init_pool()
        
        # ç²å–æ‰€æœ‰ç”¨æˆ¶çš„çµ±è¨ˆä¿¡æ¯
        async with db.get_connection() as conn:
            # æª¢æŸ¥ Playwright è³‡æ–™è¡¨æ˜¯å¦å­˜åœ¨
            table_exists = await conn.fetchval("""
                SELECT EXISTS (
                    SELECT FROM information_schema.tables 
                    WHERE table_name = 'playwright_post_metrics'
                )
            """)
            
            if not table_exists:
                print(json.dumps({"total_stats": {}, "user_stats": []}))
                return
            
            # çµ±è¨ˆæ¯å€‹ç”¨æˆ¶çš„è²¼æ–‡æ•¸é‡
            user_stats = await conn.fetch("""
                SELECT 
                    username,
                    COUNT(*) as post_count,
                    MAX(created_at) as latest_crawl,
                    MIN(created_at) as first_crawl,
                    MAX(crawl_id) as latest_crawl_id
                FROM playwright_post_metrics 
                GROUP BY username 
                ORDER BY post_count DESC, latest_crawl DESC
                LIMIT 20
            """)
            
            # ç¸½é«”çµ±è¨ˆ
            total_stats = await conn.fetchrow("""
                SELECT 
                    COUNT(*) as total_posts,
                    COUNT(DISTINCT username) as total_users,
                    MAX(created_at) as latest_activity,
                    COUNT(DISTINCT crawl_id) as total_crawls
                FROM playwright_post_metrics
            """)
            
            stats = {
                "total_stats": dict(total_stats) if total_stats else {},
                "user_stats": [dict(row) for row in user_stats] if user_stats else []
            }
            
            print(json.dumps(stats, default=str))
            
    except Exception as e:
        print(json.dumps({"error": str(e)}))
    finally:
        await db.close_pool()

if __name__ == "__main__":
    asyncio.run(get_playwright_database_stats())
'''
            
            # å°‡è…³æœ¬å¯«å…¥è‡¨æ™‚æ–‡ä»¶
            import tempfile
            with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False, encoding='utf-8') as f:
                f.write(script_content)
                temp_script = f.name
            
            try:
                # åŸ·è¡Œè…³æœ¬ç²å–çµ±è¨ˆä¿¡æ¯
                result = subprocess.run(
                    [sys.executable, temp_script],
                    capture_output=True,
                    text=True,
                    encoding='utf-8',
                    cwd=os.path.dirname(os.path.dirname(os.path.dirname(__file__))),
                    timeout=10
                )
                
                if result.returncode == 0 and result.stdout.strip():
                    stats = json.loads(result.stdout.strip())
                    
                    if "error" in stats:
                        st.error(f"âŒ è³‡æ–™åº«éŒ¯èª¤: {stats['error']}")
                        return
                    
                    # ä¿å­˜åˆ°ç·©å­˜
                    st.session_state.playwright_db_stats_cache = stats
                    
                    # æ¸²æŸ“çµ±è¨ˆä¿¡æ¯
                    self._render_cached_stats(stats)
                    
                else:
                    st.warning("âš ï¸ ç„¡æ³•ç²å–è³‡æ–™åº«çµ±è¨ˆä¿¡æ¯")
                    if result.stderr:
                        st.text(f"éŒ¯èª¤: {result.stderr}")
                        
            finally:
                # æ¸…ç†è‡¨æ™‚æ–‡ä»¶
                try:
                    os.unlink(temp_script)
                except:
                    pass
                    
        except Exception as e:
            st.error(f"âŒ ç²å–çµ±è¨ˆä¿¡æ¯å¤±æ•—: {str(e)}")
    
    def _render_cached_stats(self, stats):
        """æ¸²æŸ“ Playwright å°ˆç”¨ç·©å­˜çµ±è¨ˆä¿¡æ¯"""
        # é¡¯ç¤ºç¸½é«”çµ±è¨ˆ
        total_stats = stats.get("total_stats", {})
        if total_stats:
            st.info(f"""
            **ğŸ­ Playwright çˆ¬èŸ²çµ±è¨ˆ**
            - ğŸ“Š ç¸½è²¼æ–‡æ•¸: {total_stats.get('total_posts', 0):,}
            - ğŸ‘¥ å·²çˆ¬å–ç”¨æˆ¶: {total_stats.get('total_users', 0)} å€‹
            - ğŸ”„ ç¸½çˆ¬å–æ¬¡æ•¸: {total_stats.get('total_crawls', 0):,}
            - â° æœ€å¾Œæ´»å‹•: {str(total_stats.get('latest_activity', 'N/A'))[:16] if total_stats.get('latest_activity') else 'N/A'}
            """)
        
        # é¡¯ç¤ºç”¨æˆ¶çµ±è¨ˆ
        user_stats = stats.get("user_stats", [])
        if user_stats:
            st.write("**ğŸ‘¥ å„ç”¨æˆ¶çµ±è¨ˆ (Playwright):**")
            
            # ä½¿ç”¨è¡¨æ ¼é¡¯ç¤º
            import pandas as pd
            df_data = []
            for user in user_stats:
                latest = str(user.get('latest_crawl', 'N/A'))[:16] if user.get('latest_crawl') else 'N/A'
                crawl_id = user.get('latest_crawl_id', 'N/A')[:12] + '...' if user.get('latest_crawl_id') else 'N/A'
                df_data.append({
                    "ç”¨æˆ¶å": f"@{user.get('username', 'N/A')}",
                    "è²¼æ–‡æ•¸": f"{user.get('post_count', 0):,}",
                    "æœ€å¾Œçˆ¬å–": latest,
                    "æœ€æ–°çˆ¬å–ID": crawl_id
                })
            
            if df_data:
                df = pd.DataFrame(df_data)
                st.dataframe(
                    df, 
                    use_container_width=True,
                    hide_index=True,
                    height=min(300, len(df_data) * 35 + 38)  # å‹•æ…‹é«˜åº¦
                )
                
                # æ·»åŠ èªªæ˜
                st.caption("ğŸ’¡ é€™æ˜¯ Playwright çˆ¬èŸ²çš„å°ˆç”¨çµ±è¨ˆï¼Œèˆ‡ Realtime çˆ¬èŸ²åˆ†é›¢å„²å­˜")
        else:
            st.warning("ğŸ“ Playwright è³‡æ–™åº«ä¸­æš«ç„¡çˆ¬å–è¨˜éŒ„")
    
    def _save_results_to_database(self):
        """å°‡ç•¶å‰çˆ¬å–çµæœä¿å­˜åˆ°è³‡æ–™åº«ï¼ˆå‚™ç”¨åŠŸèƒ½ï¼‰"""
        if 'playwright_results' not in st.session_state:
            st.error("âŒ æ²’æœ‰å¯ä¿å­˜çš„çµæœ")
            return
        
        # ä½¿ç”¨èˆ‡ realtime_crawler_component.py ç›¸åŒçš„é‚è¼¯
        # ä½†èª¿æ•´è®Šæ•¸åç¨±
        playwright_results = st.session_state.playwright_results
        
        # æª¢æŸ¥resultsçš„æ ¼å¼ï¼Œå¦‚æœæ˜¯å­—å…¸å‰‡æå–resultsåˆ—è¡¨
        if isinstance(playwright_results, dict):
            results = playwright_results.get('results', [])
            target_username = playwright_results.get('target_username', '')
        else:
            results = playwright_results if playwright_results else []
            target_username = results[0].get('username', '') if results else ''
        
        if not results:
            st.error("âŒ æ²’æœ‰æ‰¾åˆ°å¯ä¿å­˜çš„çµæœ")
            return
        
        if not target_username:
            st.error("âŒ ç„¡æ³•è­˜åˆ¥ç›®æ¨™ç”¨æˆ¶å")
            return
        
        try:
            import subprocess
            import json
            import sys
            import os
            import tempfile
            
            # å‰µå»ºä¿å­˜è…³æœ¬ï¼ˆèˆ‡ realtime_crawler_component.py ç›¸åŒï¼‰
            save_script_content = f'''
import asyncio
import sys
import os
import json
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from common.incremental_crawl_manager import IncrementalCrawlManager

async def save_to_database():
    crawl_manager = IncrementalCrawlManager()
    try:
        await crawl_manager.db.init_pool()
        
        # æº–å‚™çµæœæ•¸æ“š
        results = {json.dumps(results, ensure_ascii=False)}
        target_username = "{target_username}"
        
        # ä¿å­˜çµæœåˆ°è³‡æ–™åº«
        saved_count = await crawl_manager.save_quick_crawl_results(results, target_username)
        
        # æ›´æ–°æª¢æŸ¥é»ï¼ˆä½¿ç”¨æœ€æ–°çš„è²¼æ–‡IDï¼‰
        if results and saved_count > 0:
            latest_post_id = results[0].get('post_id')  # ç¬¬ä¸€å€‹æ˜¯æœ€æ–°çš„
            if latest_post_id:
                await crawl_manager.update_crawl_checkpoint(
                    target_username, 
                    latest_post_id, 
                    saved_count
                )
        
        result = {{
            "success": True,
            "saved_count": saved_count,
            "target_username": target_username
        }}
        
        print(json.dumps(result))
        
    except Exception as e:
        print(json.dumps({{"success": False, "error": str(e)}}))
    finally:
        await crawl_manager.db.close_pool()

if __name__ == "__main__":
    asyncio.run(save_to_database())
'''
            
            # å¯«å…¥è‡¨æ™‚æ–‡ä»¶
            with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False, encoding='utf-8') as f:
                f.write(save_script_content)
                temp_script = f.name
            
            try:
                # åŸ·è¡Œä¿å­˜è…³æœ¬
                with st.spinner(f"ğŸ’¾ æ­£åœ¨ä¿å­˜ {len(results)} å€‹è²¼æ–‡åˆ°è³‡æ–™åº«..."):
                    result = subprocess.run(
                        [sys.executable, temp_script],
                        capture_output=True,
                        text=True,
                        encoding='utf-8',
                        cwd=os.path.dirname(os.path.dirname(os.path.dirname(__file__))),
                        timeout=60
                    )
                
                if result.returncode == 0 and result.stdout.strip():
                    save_result = json.loads(result.stdout.strip())
                    
                    if save_result.get("success"):
                        saved_count = save_result.get("saved_count", 0)
                        
                        st.success(f"""
                        âœ… **ä¿å­˜æˆåŠŸï¼**
                        
                        å·²æˆåŠŸå°‡ @{target_username} çš„è²¼æ–‡ä¿å­˜åˆ°è³‡æ–™åº«ï¼š
                        - ğŸ’¾ ä¿å­˜è²¼æ–‡æ•¸: {saved_count} å€‹
                        - ğŸ”„ æª¢æŸ¥é»å·²æ›´æ–°
                        """)
                        
                        # æ›´æ–°session stateï¼Œæ¨™è¨˜ç‚ºå·²ä¿å­˜
                        if isinstance(st.session_state.playwright_results, dict):
                            st.session_state.playwright_results['database_saved'] = True
                            st.session_state.playwright_results['database_saved_count'] = saved_count
                        
                        # æ¸…ç†è³‡æ–™åº«çµ±è¨ˆç·©å­˜ï¼Œä¸‹æ¬¡æŸ¥çœ‹æœƒæ›´æ–°
                        if 'playwright_db_stats_cache' in st.session_state:
                            del st.session_state.playwright_db_stats_cache
                        
                        st.info("ğŸ“Š è³‡æ–™åº«çµ±è¨ˆå·²æ›´æ–°ï¼Œæ‚¨å¯ä»¥é»æ“Šåˆ·æ–°æŒ‰éˆ•æŸ¥çœ‹æœ€æ–°æ•¸æ“š")
                        
                    else:
                        st.error(f"âŒ ä¿å­˜å¤±æ•—: {save_result.get('error', 'æœªçŸ¥éŒ¯èª¤')}")
                else:
                    st.error(f"âŒ ä¿å­˜è…³æœ¬åŸ·è¡Œå¤±æ•—")
                    if result.stderr:
                        st.text(f"éŒ¯èª¤è©³æƒ…: {result.stderr}")
                        
            finally:
                # æ¸…ç†è‡¨æ™‚æ–‡ä»¶
                try:
                    os.unlink(temp_script)
                except:
                    pass
                    
        except Exception as e:
            st.error(f"âŒ ä¿å­˜æ“ä½œå¤±æ•—: {str(e)}")