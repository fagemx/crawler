"""
å¯¦æ™‚çˆ¬èŸ²çµ„ä»¶ - æ™ºèƒ½URLæ”¶é›† + è¼ªè¿´ç­–ç•¥æå–
åŒ…å«å®Œæ•´äº’å‹•æ•¸æ“šæå–åŠŸèƒ½
"""

import streamlit as st
import asyncio
import json
import time
import threading
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, Optional, List
import sys
import os

# æ·»åŠ å°ˆæ¡ˆæ ¹ç›®éŒ„åˆ° Python è·¯å¾‘
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

class RealtimeDatabaseHandler:
    """è™•ç† Realtime Crawler è³‡æ–™åº«æ“ä½œçš„è¼”åŠ©é¡"""

    def __init__(self):
        # å»¶é²å°å…¥ï¼Œé¿å…å¾ªç’°ä¾è³´å’Œå•Ÿå‹•å•é¡Œ
        from common.incremental_crawl_manager import IncrementalCrawlManager
        self.crawl_manager = IncrementalCrawlManager()

    async def _get_connection(self):
        await self.crawl_manager.db.init_pool()
        return self.crawl_manager.db.get_connection()

    async def delete_user_data_async(self, username: str) -> dict:
        """ç•°æ­¥åˆªé™¤ç‰¹å®šç”¨æˆ¶çš„æ‰€æœ‰æ•¸æ“šä¸¦è¿”å›è©³ç´°çµæœ"""
        if not username:
            return {"success": False, "error": "ç”¨æˆ¶åä¸èƒ½ç‚ºç©º"}

        try:
            async with await self._get_connection() as conn:
                async with conn.transaction():
                    # 1. ç²å–è¦åˆªé™¤çš„è¨˜éŒ„æ•¸
                    posts_count = await conn.fetchval(
                        "SELECT COUNT(*) FROM post_metrics_sql WHERE username = $1", username
                    )
                    crawl_state_count = await conn.fetchval(
                        "SELECT COUNT(*) FROM crawl_state WHERE username = $1", username
                    )

                    # 2. åŸ·è¡Œåˆªé™¤
                    await conn.execute("DELETE FROM post_metrics_sql WHERE username = $1", username)
                    await conn.execute("DELETE FROM crawl_state WHERE username = $1", username)
                    
                    # 3. é©—è­‰åˆªé™¤
                    remaining_posts = await conn.fetchval(
                        "SELECT COUNT(*) FROM post_metrics_sql WHERE username = $1", username
                    )
                    
                    if remaining_posts == 0:
                        return {
                            "success": True, 
                            "deleted_posts": posts_count, 
                            "deleted_states": crawl_state_count
                        }
                    else:
                        return {
                            "success": False, 
                            "error": "åˆªé™¤å¾Œé©—è­‰å¤±æ•—ï¼Œä»æœ‰æ•¸æ“šæ®˜ç•™",
                            "remaining_posts": remaining_posts
                        }

        except Exception as e:
            return {"success": False, "error": str(e)}

        finally:
            await self.crawl_manager.db.close_pool()

class RealtimeCrawlerComponent:
    def __init__(self):
        self.is_running = False
        self.current_task = None
        self.db_handler = RealtimeDatabaseHandler() # åˆå§‹åŒ–æ–°çš„è³‡æ–™åº«è™•ç†å™¨
        
    def render(self):
        """æ¸²æŸ“å¯¦æ™‚çˆ¬èŸ²çµ„ä»¶"""
        st.header("ğŸš€ å¯¦æ™‚æ™ºèƒ½çˆ¬èŸ²")
        st.markdown("**æ™ºèƒ½æ»¾å‹•æ”¶é›†URLs + è¼ªè¿´ç­–ç•¥å¿«é€Ÿæå– + å®Œæ•´äº’å‹•æ•¸æ“š**")
        
        # åƒæ•¸è¨­å®šå€åŸŸ
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("âš™ï¸ çˆ¬å–è¨­å®š")
            username = st.text_input(
                "ç›®æ¨™å¸³è™Ÿ", 
                value="gvmonthly",
                help="è¦çˆ¬å–çš„Threadså¸³è™Ÿç”¨æˆ¶å",
                key="realtime_username"
            )
            
            max_posts = st.number_input(
                "çˆ¬å–æ•¸é‡", 
                min_value=1, 
                max_value=500, 
                value=50,
                help="è¦çˆ¬å–çš„è²¼æ–‡æ•¸é‡",
                key="realtime_max_posts"
            )
            
            # å¢é‡çˆ¬å–æ¨¡å¼é¸é …
            crawl_mode = st.radio(
                "çˆ¬å–æ¨¡å¼",
                options=["å¢é‡çˆ¬å–", "å…¨é‡çˆ¬å–"],
                index=0,
                help="å¢é‡çˆ¬å–ï¼šåªæŠ“å–æ–°è²¼æ–‡ï¼Œé¿å…é‡è¤‡ï¼›å…¨é‡çˆ¬å–ï¼šæŠ“å–æ‰€æœ‰æ‰¾åˆ°çš„è²¼æ–‡",
                key="crawl_mode"
            )
            
            # é¡¯ç¤ºçˆ¬å–éç¨‹æ—¥èªŒï¼ˆç§»åˆ°é€™è£¡ï¼Œé¿å…é‡æ–°æ¸²æŸ“å½±éŸ¿ï¼‰
            if 'realtime_crawl_logs' in st.session_state and st.session_state.realtime_crawl_logs:
                with st.expander("ğŸ“‹ çˆ¬å–éç¨‹æ—¥èªŒ", expanded=False):
                    # é¡¯ç¤ºæœ€å¾Œ50è¡Œæ—¥èªŒ
                    log_lines = st.session_state.realtime_crawl_logs[-50:] if len(st.session_state.realtime_crawl_logs) > 50 else st.session_state.realtime_crawl_logs
                    st.code('\n'.join(log_lines), language='text')
            
        with col2:
            col_title, col_refresh = st.columns([3, 1])
            with col_title:
                st.subheader("ğŸ“Š è³‡æ–™åº«çµ±è¨ˆ")
            with col_refresh:
                if st.button("ğŸ”„ åˆ·æ–°", key="refresh_db_stats", help="åˆ·æ–°è³‡æ–™åº«çµ±è¨ˆä¿¡æ¯", type="secondary"):
                    # æ¸…ç†å¯èƒ½çš„ç·©å­˜ç‹€æ…‹
                    if 'db_stats_cache' in st.session_state:
                        del st.session_state.db_stats_cache
                    st.success("ğŸ”„ æ­£åœ¨åˆ·æ–°çµ±è¨ˆ...")
                    st.rerun()  # é‡æ–°é‹è¡Œé é¢ä¾†åˆ·æ–°çµ±è¨ˆ
            
            self._display_database_stats()
        
        # æ§åˆ¶æŒ‰éˆ•
        col1, col2, col3 = st.columns([1, 1, 2])
        
        with col1:
            if st.button("ğŸš€ é–‹å§‹çˆ¬å–", key="start_realtime"):
                with st.spinner("æ­£åœ¨åŸ·è¡Œçˆ¬å–..."):
                    is_incremental = crawl_mode == "å¢é‡çˆ¬å–"
                    self._execute_crawling_simple(username, max_posts, is_incremental)
                
        with col2:
            # è¼‰å…¥CSVæ–‡ä»¶åŠŸèƒ½
            uploaded_file = st.file_uploader(
                "ğŸ“ è¼‰å…¥CSVæ–‡ä»¶", 
                type=['csv'], 
                key="csv_uploader",
                help="ä¸Šå‚³ä¹‹å‰å°å‡ºçš„CSVæ–‡ä»¶ä¾†æŸ¥çœ‹çµæœ"
            )
            if uploaded_file is not None:
                self._load_csv_file(uploaded_file)
        
        with col3:
            # æ¸…é™¤çµæœæŒ‰éˆ• (åªåœ¨æœ‰çµæœæ™‚é¡¯ç¤º)
            if 'realtime_results' in st.session_state:
                if st.button("ğŸ—‘ï¸ æ¸…é™¤çµæœ", key="clear_results", help="æ¸…é™¤ç•¶å‰é¡¯ç¤ºçš„çµæœ"):
                    self._clear_results()
        
        # çµæœé¡¯ç¤º
        self._render_results_area()
    
    def _load_csv_file(self, uploaded_file):
        """è¼‰å…¥CSVæ–‡ä»¶ä¸¦è½‰æ›ç‚ºçµæœæ ¼å¼"""
        try:
            import pandas as pd
            import io
            
            # è®€å–CSVæ–‡ä»¶
            content = uploaded_file.getvalue()
            df = pd.read_csv(io.StringIO(content.decode('utf-8-sig')))
            
            # æª¢æŸ¥CSVæ ¼å¼æ˜¯å¦æ­£ç¢ºï¼ˆæ›´éˆæ´»çš„é©—è­‰ï¼‰
            # æ ¸å¿ƒå¿…è¦æ¬„ä½
            core_required = ['username', 'post_id', 'content']
            missing_core = [col for col in core_required if col not in df.columns]
            
            if missing_core:
                st.error(f"âŒ CSVæ ¼å¼ä¸æ­£ç¢ºï¼Œç¼ºå°‘æ ¸å¿ƒæ¬„ä½: {', '.join(missing_core)}")
                return
            
            # æª¢æŸ¥å¯é¸æ¬„ä½ï¼Œå¦‚æœæ²’æœ‰å‰‡æä¾›é è¨­å€¼
            optional_columns = ['views', 'likes_count', 'comments_count', 'reposts_count', 'shares_count']
            for col in optional_columns:
                if col not in df.columns:
                    if col == 'views':
                        df[col] = df.get('views_count', 0)  # å˜—è©¦ä½¿ç”¨ views_count ä½œç‚º views
                    else:
                        df[col] = 0  # é è¨­å€¼ç‚º 0
            
            st.info(f"âœ… æˆåŠŸè¼‰å…¥CSVï¼ŒåŒ…å« {len(df)} ç­†è¨˜éŒ„")
            
            # è½‰æ›ç‚ºçµæœæ ¼å¼
            results = []
            for _, row in df.iterrows():
                # è½‰æ›æ•¸æ“šä¸¦è™•ç†ç©ºå€¼
                views = str(row.get('views', '')).strip()
                likes = str(row.get('likes', '')).strip()
                comments = str(row.get('comments', '')).strip()
                reposts = str(row.get('reposts', '')).strip()
                shares = str(row.get('shares', '')).strip()
                content = str(row.get('content', '')).strip()
                
                # ğŸ”§ ä¿®å¾©ï¼šè™•ç†ç”¨æˆ¶IDåˆ†é›¢
                original_post_id = str(row.get('post_id', ''))
                username_from_csv = str(row.get('username', ''))
                user_id_from_csv = str(row.get('user_id', '')).strip()
                
                # æå–ç”¨æˆ¶IDå’ŒçœŸå¯¦è²¼æ–‡ID
                if '_' in original_post_id and len(original_post_id.split('_')) >= 2:
                    parts = original_post_id.split('_', 1)
                    user_id = parts[0] if len(parts) > 1 else ''
                    real_post_id = parts[1] if len(parts) > 1 else original_post_id
                else:
                    # å„ªå…ˆä½¿ç”¨CSVä¸­çš„user_idï¼Œå…¶æ¬¡ä½¿ç”¨username
                    user_id = user_id_from_csv or username_from_csv
                    real_post_id = original_post_id
                
                # å¦‚æœä»ç„¶æ²’æœ‰user_idï¼Œå¾post_idæå–
                if not user_id and original_post_id:
                    if '_' in original_post_id:
                        user_id = original_post_id.split('_')[0]
                
                result = {
                    'username': user_id or username_from_csv,  # ğŸ”§ ä¿®å¾©ï¼šä½¿ç”¨åˆ†é›¢çš„user_id
                    'user_id': user_id,  # ğŸ”§ æ–°å¢ï¼šåˆ†é›¢çš„ç”¨æˆ¶ID
                    'post_id': original_post_id,
                    'real_post_id': real_post_id,  # ğŸ”§ æ–°å¢ï¼šçœŸå¯¦è²¼æ–‡ID
                    'content': content,
                    'views': views,
                    'likes': likes,
                    'comments': comments,
                    'reposts': reposts,
                    'shares': shares,
                    'url': str(row.get('url', '')),
                    'source': str(row.get('source', 'csv_import')),
                    'created_at': str(row.get('created_at', '')),
                    'fetched_at': str(row.get('fetched_at', '')),
                    'success': True,
                    # æ·»åŠ has_*æ¬„ä½ä»¥å…¼å®¹é¡¯ç¤ºé‚è¼¯
                    'has_views': bool(views and views != 'nan' and views != ''),
                    'has_content': bool(content and content != 'nan' and content != ''),
                    'has_likes': bool(likes and likes != 'nan' and likes != ''),
                    'has_comments': bool(comments and comments != 'nan' and comments != ''),
                    'has_reposts': bool(reposts and reposts != 'nan' and reposts != ''),
                    'has_shares': bool(shares and shares != 'nan' and shares != ''),
                    'content_length': len(content) if content else 0,
                    'extracted_at': datetime.now().isoformat()
                }
                results.append(result)
            
            # ä¿å­˜åˆ°æœƒè©±ç‹€æ…‹
            st.session_state.realtime_results = {
                'results': results,
                'total_count': len(results),
                'username': results[0]['username'] if results else '',
                'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'source': f"CSVæ–‡ä»¶: {uploaded_file.name}"
            }
            
            st.success(f"âœ… æˆåŠŸè¼‰å…¥ {len(results)} ç­†è¨˜éŒ„")
            st.info(f"ğŸ“Š ä¾†æº: {uploaded_file.name}")
            
        except Exception as e:
            st.error(f"âŒ è¼‰å…¥CSVæ–‡ä»¶å¤±æ•—: {str(e)}")
    
    def _execute_crawling_simple(self, username: str, max_posts: int, is_incremental: bool = True):
        """ç°¡åŒ–çš„çˆ¬å–åŸ·è¡Œæ–¹æ³• - ä½¿ç”¨åŒæ­¥ç‰ˆæœ¬é¿å…asyncioè¡çª"""
        if not username.strip():
            st.error("è«‹è¼¸å…¥ç›®æ¨™å¸³è™Ÿï¼")
            return
            
        try:
            # è¨˜éŒ„é–‹å§‹æ™‚é–“
            import time
            start_time = time.time()
            st.session_state.realtime_crawl_start_time = start_time
            
            mode_text = "å¢é‡çˆ¬å–" if is_incremental else "å…¨é‡çˆ¬å–"
            st.info(f"ğŸ”„ æ­£åœ¨åŸ·è¡Œ{mode_text}ï¼Œè«‹ç¨å€™...")
            
            # ä½¿ç”¨subprocessä¾†é¿å…asyncioè¡çª
            import subprocess
            import json
            import sys
            import os
            
            # æ§‹å»ºå‘½ä»¤
            script_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), '..', 'scripts', 'realtime_crawler_extractor.py')
            
            # ä¿®æ”¹è…³æœ¬ä»¥æ¥å—å‘½ä»¤è¡Œåƒæ•¸
            cmd = [
                sys.executable, 
                script_path,
                '--username', username,
                '--max_posts', str(max_posts)
            ]
            
            # æ·»åŠ çˆ¬å–æ¨¡å¼åƒæ•¸
            if is_incremental:
                cmd.append('--incremental')  # å¢é‡æ¨¡å¼
            else:
                cmd.append('--full')  # å…¨é‡æ¨¡å¼
            
            # åŸ·è¡Œè…³æœ¬ - è¨­ç½®UTF-8ç·¨ç¢¼
            import locale
            env = os.environ.copy()
            env['PYTHONIOENCODING'] = 'utf-8'
            env['PYTHONUTF8'] = '1'
            
            # å‰µå»ºä¸€å€‹æ—¥å¿—å®¹å™¨ä¾†å¯¦æ™‚é¡¯ç¤ºè¼¸å‡º
            log_container = st.empty()
            # å°‡æ—¥èªŒä¿å­˜åˆ°æœƒè©±ç‹€æ…‹ï¼Œé¿å…é é¢é‡æ–°æ¸²æŸ“æ™‚ä¸Ÿå¤±
            # æ¯æ¬¡æ–°çš„çˆ¬å–é–‹å§‹æ™‚æ¸…ç©ºä¹‹å‰çš„æ—¥èªŒ
            st.session_state.realtime_crawl_logs = []
            log_text = st.session_state.realtime_crawl_logs
            
            with st.expander("ğŸ“‹ çˆ¬å–éç¨‹æ—¥å¿—", expanded=True):
                log_placeholder = st.empty()
                
                # ä½¿ç”¨Popenä¾†å¯¦æ™‚æ•ç²è¼¸å‡º
                process = subprocess.Popen(
                    cmd,
                    stdout=subprocess.PIPE,
                    stderr=subprocess.STDOUT,  # åˆä½µstderråˆ°stdout
                    text=True,
                    encoding='utf-8',
                    errors='replace',
                    env=env,
                    cwd=os.path.dirname(os.path.dirname(os.path.dirname(__file__))),
                    bufsize=1,  # è¡Œç·©è¡
                    universal_newlines=True
                )
                
                # å¯¦æ™‚è®€å–è¼¸å‡º
                all_output = []
                while process.poll() is None:
                    output = process.stdout.readline()
                    if output:
                        line = output.strip()
                        all_output.append(line)
                        log_text.append(line)
                        
                        # åªé¡¯ç¤ºæœ€å¾Œ30è¡Œï¼Œé¿å…ç•Œé¢éé•·
                        display_lines = log_text[-30:] if len(log_text) > 30 else log_text
                        log_placeholder.code('\n'.join(display_lines), language='text')
                    else:
                        # çŸ­æš«ä¼‘çœ ï¼Œé¿å…ä¸»ç·šç¨‹å®Œå…¨é˜»å¡
                        time.sleep(0.1)

                # æ•ç²é€²ç¨‹çµæŸå¾Œå‰©é¤˜çš„è¼¸å‡º
                for output in process.stdout.readlines():
                    line = output.strip()
                    all_output.append(line)
                    log_text.append(line)
                
                # æœ€å¾Œå†æ›´æ–°ä¸€æ¬¡UI
                display_lines = log_text[-30:] if len(log_text) > 30 else log_text
                log_placeholder.code('\n'.join(display_lines), language='text')

                return_code = process.poll()
                
            if return_code == 0:
                # æˆåŠŸåŸ·è¡Œï¼Œå°‹æ‰¾æœ€æ–°çš„çµæœæ–‡ä»¶
                import glob
                from pathlib import Path
                
                # å…ˆæª¢æŸ¥æ–°çš„è³‡æ–™å¤¾ä½ç½®
                extraction_dir = Path("extraction_results")
                if extraction_dir.exists():
                    results_files = list(extraction_dir.glob("realtime_extraction_results_*.json"))
                else:
                    # å›é€€åˆ°æ ¹ç›®éŒ„æŸ¥æ‰¾ï¼ˆå‘å¾Œå…¼å®¹ï¼‰
                    results_files = [Path(f) for f in glob.glob("realtime_extraction_results_*.json")]
                
                if results_files:
                    # å–æœ€æ–°çš„æ–‡ä»¶
                    latest_file = max(results_files, key=lambda f: f.stat().st_mtime)
                    
                    # è®€å–çµæœ
                    with open(latest_file, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    
                    # ä¿å­˜åˆ°session_state
                    st.session_state.realtime_results = data.get('results', [])
                    st.session_state.realtime_results_file = latest_file
                    
                    total_processed = len(st.session_state.realtime_results)
                    
                    # è¨ˆç®—ç¸½è€—æ™‚
                    end_time = time.time()
                    start_time = st.session_state.get('realtime_crawl_start_time', end_time)
                    total_duration = end_time - start_time
                    
                    # å°‡è€—æ™‚ä¿¡æ¯ä¿å­˜åˆ°å–®ç¨çš„session state
                    st.session_state.realtime_crawl_duration = total_duration
                    
                    # æ ¼å¼åŒ–è€—æ™‚é¡¯ç¤º
                    if total_duration < 60:
                        duration_text = f"{total_duration:.1f} ç§’"
                    else:
                        duration_text = f"{total_duration/60:.1f} åˆ†é˜"
                    
                    st.success(f"âœ… çˆ¬å–å®Œæˆï¼è™•ç†äº† {total_processed} ç¯‡è²¼æ–‡ï¼Œè€—æ™‚: {duration_text}")
                    
                    # æ¸…ç†è³‡æ–™åº«çµ±è¨ˆç·©å­˜ï¼Œä¸‹æ¬¡æœƒè‡ªå‹•åˆ·æ–°
                    if 'db_stats_cache' in st.session_state:
                        del st.session_state.db_stats_cache
                    
                    st.info("ğŸ“Š å¢é‡çˆ¬å–å·²è‡ªå‹•ä¿å­˜åˆ°è³‡æ–™åº«ï¼Œæ‚¨å¯ä»¥é»æ“Šå³å´ã€ŒğŸ”„ åˆ·æ–°ã€æŸ¥çœ‹æ›´æ–°çš„çµ±è¨ˆ")
                    st.balloons()
                else:
                    st.error("âŒ æœªæ‰¾åˆ°çµæœæ–‡ä»¶")
            else:
                st.error(f"âŒ çˆ¬å–å¤±æ•— (è¿”å›ç¢¼: {return_code})")
                # é¡¯ç¤ºæœ€å¾Œçš„éŒ¯èª¤æ—¥å¿—
                if all_output:
                    error_lines = [line for line in all_output if 'âŒ' in line or 'Error' in line or 'Exception' in line]
                    if error_lines:
                        st.error("éŒ¯èª¤è©³æƒ…ï¼š")
                        for error_line in error_lines[-5:]:  # é¡¯ç¤ºæœ€å¾Œ5æ¢éŒ¯èª¤
                            st.text(error_line)
                
        except Exception as e:
            st.error(f"âŒ åŸ·è¡ŒéŒ¯èª¤ï¼š{str(e)}")
            st.session_state.realtime_error = str(e)
    
    def _display_database_stats(self):
        """é¡¯ç¤ºè³‡æ–™åº«çµ±è¨ˆä¿¡æ¯"""
        # æª¢æŸ¥æ˜¯å¦æœ‰ç·©å­˜çš„çµ±è¨ˆä¿¡æ¯
        if 'db_stats_cache' in st.session_state:
            self._render_cached_stats(st.session_state.db_stats_cache)
            return
        
        try:
            # ä½¿ç”¨ asyncio å’Œ subprocess ä¾†ç²å–è³‡æ–™åº«çµ±è¨ˆ
            import subprocess
            import json
            import sys
            import os
            
            # å‰µå»ºä¸€å€‹è‡¨æ™‚è…³æœ¬ä¾†ç²å–è³‡æ–™åº«çµ±è¨ˆ
            script_content = '''
import asyncio
import sys
import os
import json
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from common.incremental_crawl_manager import IncrementalCrawlManager

async def get_database_stats():
    crawl_manager = IncrementalCrawlManager()
    try:
        await crawl_manager.db.init_pool()
        
        # ç²å–æ‰€æœ‰ç”¨æˆ¶çš„çµ±è¨ˆä¿¡æ¯
        async with crawl_manager.db.get_connection() as conn:
            # çµ±è¨ˆæ¯å€‹ç”¨æˆ¶çš„è²¼æ–‡æ•¸é‡
            user_stats = await conn.fetch("""
                SELECT 
                    username,
                    COUNT(*) as post_count,
                    MAX(created_at) as latest_crawl,
                    MIN(created_at) as first_crawl
                FROM post_metrics_sql 
                GROUP BY username 
                ORDER BY post_count DESC, latest_crawl DESC
                LIMIT 20
            """)
            
            # ç¸½é«”çµ±è¨ˆ
            total_stats = await conn.fetchrow("""
                SELECT 
                    COUNT(*) as total_posts,
                    COUNT(DISTINCT username) as total_users,
                    MAX(created_at) as latest_activity
                FROM post_metrics_sql
            """)
            
            stats = {
                "total_stats": dict(total_stats) if total_stats else {},
                "user_stats": [dict(row) for row in user_stats] if user_stats else []
            }
            
            print(json.dumps(stats, default=str))
            
    except Exception as e:
        print(json.dumps({"error": str(e)}))
    finally:
        await crawl_manager.db.close_pool()

if __name__ == "__main__":
    asyncio.run(get_database_stats())
'''
            
            # å°‡è…³æœ¬å¯«å…¥è‡¨æ™‚æ–‡ä»¶
            import tempfile
            with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False, encoding='utf-8') as f:
                f.write(script_content)
                temp_script = f.name
            
            try:
                # åŸ·è¡Œè…³æœ¬ç²å–çµ±è¨ˆä¿¡æ¯
                result = subprocess.run(
                    [sys.executable, temp_script],
                    capture_output=True,
                    text=True,
                    encoding='utf-8',
                    cwd=os.path.dirname(os.path.dirname(os.path.dirname(__file__))),
                    timeout=10
                )
                
                if result.returncode == 0 and result.stdout.strip():
                    stats = json.loads(result.stdout.strip())
                    
                    if "error" in stats:
                        st.error(f"âŒ è³‡æ–™åº«éŒ¯èª¤: {stats['error']}")
                        return
                    
                    # ä¿å­˜åˆ°ç·©å­˜
                    st.session_state.db_stats_cache = stats
                    
                    # æ¸²æŸ“çµ±è¨ˆä¿¡æ¯
                    self._render_cached_stats(stats)
                    
                else:
                    st.warning("âš ï¸ ç„¡æ³•ç²å–è³‡æ–™åº«çµ±è¨ˆä¿¡æ¯")
                    if result.stderr:
                        st.text(f"éŒ¯èª¤: {result.stderr}")
                        
            finally:
                # æ¸…ç†è‡¨æ™‚æ–‡ä»¶
                try:
                    os.unlink(temp_script)
                except:
                    pass
                    
        except Exception as e:
            st.error(f"âŒ ç²å–çµ±è¨ˆä¿¡æ¯å¤±æ•—: {str(e)}")
    
    def _execute_user_deletion(self, username: str):
        """åŸ·è¡Œå¯¦éš›çš„ç”¨æˆ¶åˆªé™¤æ“ä½œï¼Œç›´æ¥èª¿ç”¨ Database Handler"""
        try:
            with st.spinner(f"ğŸ—‘ï¸ æ­£åœ¨åˆªé™¤ç”¨æˆ¶ @{username} çš„è³‡æ–™..."):
                result = asyncio.run(self.db_handler.delete_user_data_async(username))

            if result.get("success"):
                st.success(f"""
                âœ… **åˆªé™¤æˆåŠŸï¼**
                ç”¨æˆ¶ @{username} çš„è³‡æ–™å·²è¢«å®Œå…¨åˆªé™¤ï¼š
                - ğŸ—‘ï¸ åˆªé™¤è²¼æ–‡æ•¸: {result.get('deleted_posts', 0)} å€‹
                - ğŸ—‘ï¸ åˆªé™¤çˆ¬å–ç‹€æ…‹: {result.get('deleted_states', 0)} å€‹
                """)
                # æ¸…ç†ç›¸é—œ session state
                if 'realtime_confirm_delete_user' in st.session_state:
                    del st.session_state['realtime_confirm_delete_user']
                if 'db_stats_cache' in st.session_state:
                    del st.session_state['db_stats_cache']
                
                st.info("ğŸ“Š æ­£åœ¨åˆ·æ–°çµ±è¨ˆè³‡æ–™...")
                time.sleep(1)
                st.rerun()
            else:
                st.error(f"âŒ åˆªé™¤å¤±æ•—: {result.get('error', 'æœªçŸ¥éŒ¯èª¤')}")

        except Exception as e:
            st.error(f"âŒ åˆªé™¤éç¨‹ä¸­ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}")
            import traceback
            st.code(traceback.format_exc())

    def handle_delete_button(self, username: str):
        """ç®¡ç†åˆªé™¤æŒ‰éˆ•çš„é¡¯ç¤ºå’Œå…©æ­¥ç¢ºèªæµç¨‹"""
        delete_confirm_key = "realtime_confirm_delete_user"

        # è‡ªè¨‚ç´…è‰²æ¨£å¼
        st.markdown("""
        <style>
        div.stButton > button[key*="realtime_delete_"] {
            background-color: #ff4b4b !important; color: white !important; border-color: #ff4b4b !important;
        }
        div.stButton > button[key*="realtime_delete_"]:hover {
            background-color: #ff2b2b !important; border-color: #ff2b2b !important;
        }
        </style>
        """, unsafe_allow_html=True)

        if st.session_state.get(delete_confirm_key) == username:
            # ç¬¬äºŒæ­¥ï¼šæœ€çµ‚ç¢ºèª
            st.error(f"âš ï¸ **æœ€çµ‚ç¢ºèª: ç¢ºå®šåˆªé™¤ @{username} çš„æ‰€æœ‰ Realtime è³‡æ–™?**")
            
            if st.button(f"ğŸ—‘ï¸ æ˜¯ï¼Œæ°¸ä¹…åˆªé™¤ @{username}", key=f"realtime_delete_confirm_final_{username}", use_container_width=True):
                self._execute_user_deletion(username)
                # åŸ·è¡Œåˆªé™¤å¾Œæœƒè‡ªå‹• rerun
            
            if st.button("âŒ å–æ¶ˆ", key=f"realtime_delete_cancel_{username}", use_container_width=True):
                del st.session_state[delete_confirm_key]
                st.success("âœ… å·²å–æ¶ˆåˆªé™¤æ“ä½œã€‚")
                st.rerun()
        else:
            # ç¬¬ä¸€æ­¥ï¼šè§¸ç™¼ç¢ºèª
            if st.button("ğŸ—‘ï¸ åˆªé™¤ç”¨æˆ¶è³‡æ–™", key=f"realtime_delete_init_{username}", help=f"åˆªé™¤ @{username} çš„æ‰€æœ‰ Realtime çˆ¬èŸ²è³‡æ–™", use_container_width=True):
                st.session_state[delete_confirm_key] = username
                st.rerun()
    

    
    def _export_user_csv(self, username: str):
        """å°å‡ºæŒ‡å®šç”¨æˆ¶çš„æ‰€æœ‰è²¼æ–‡ç‚ºCSVæ ¼å¼"""
        if not username:
            st.error("âŒ è«‹é¸æ“‡ä¸€å€‹æœ‰æ•ˆçš„ç”¨æˆ¶")
            return
        
        try:
            import subprocess
            import json
            import sys
            import os
            import tempfile
            from datetime import datetime
            
            # å‰µå»ºå°å‡ºè…³æœ¬
            export_script_content = f'''
import asyncio
import sys
import os
import json
import csv
from datetime import datetime
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from common.incremental_crawl_manager import IncrementalCrawlManager

async def export_user_csv(username):
    crawl_manager = IncrementalCrawlManager()
    try:
        await crawl_manager.db.init_pool()
        
        async with crawl_manager.db.get_connection() as conn:
            # æŸ¥è©¢ç”¨æˆ¶çš„æ‰€æœ‰è²¼æ–‡æ•¸æ“š
            posts = await conn.fetch("""
                SELECT 
                    post_id,
                    url,
                    content,
                    views_count,
                    likes_count,
                    comments_count,
                    reposts_count,
                    shares_count,
                    source,
                    created_at,
                    fetched_at
                FROM post_metrics_sql 
                WHERE username = $1
                ORDER BY created_at DESC
            """, username)
            
            if not posts:
                print(json.dumps({{"success": False, "error": "ç”¨æˆ¶æ²’æœ‰è²¼æ–‡è³‡æ–™"}}))
                return
            
            # æº–å‚™CSVæ–‡ä»¶è·¯å¾‘
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            csv_filename = f"user_posts_{{username}}_{{timestamp}}.csv"
            csv_filepath = os.path.join("exports", csv_filename)
            
            # ç¢ºä¿exportsç›®éŒ„å­˜åœ¨
            os.makedirs("exports", exist_ok=True)
            
            # å¯«å…¥CSVæ–‡ä»¶
            with open(csv_filepath, 'w', newline='', encoding='utf-8-sig') as csvfile:
                fieldnames = [
                    'username', 'user_id', 'post_id', 'real_post_id', 'url', 'content', 'views', 
                    'likes', 'comments', 'reposts', 'shares', 'source', 'created_at', 'fetched_at'
                ]
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                
                # å¯«å…¥æ¨™é¡Œè¡Œ
                writer.writeheader()
                
                # å¯«å…¥æ•¸æ“š
                for post in posts:
                    # ğŸ”§ ä¿®å¾©ï¼šåˆ†é›¢post_idç‚ºuser_idå’Œreal_post_id
                    original_post_id = post['post_id']
                    if '_' in original_post_id and len(original_post_id.split('_')) >= 2:
                        parts = original_post_id.split('_', 1)
                        user_id = parts[0] if len(parts) > 1 else username
                        real_post_id = parts[1] if len(parts) > 1 else original_post_id
                    else:
                        user_id = username
                        real_post_id = original_post_id
                    
                    writer.writerow({
                        'username': username,
                        'user_id': user_id,  # ğŸ”§ æ–°å¢ï¼šåˆ†é›¢çš„ç”¨æˆ¶ID
                        'post_id': original_post_id,
                        'real_post_id': real_post_id,  # ğŸ”§ æ–°å¢ï¼šçœŸå¯¦è²¼æ–‡ID
                        'url': post['url'],
                        'content': post['content'] or '',
                        'views': post['views_count'] or '',
                        'likes': post['likes_count'] or '',
                        'comments': post['comments_count'] or '',
                        'reposts': post['reposts_count'] or '',
                        'shares': post['shares_count'] or '',
                        'source': post['source'] or '',
                        'created_at': str(post['created_at']) if post['created_at'] else '',
                        'fetched_at': str(post['fetched_at']) if post['fetched_at'] else ''
                    })
            
            result = {
                "success": True,
                "csv_file": csv_filepath,
                "post_count": len(posts),
                "username": username
            }
            
            print(json.dumps(result))
            
    except Exception as e:
        print(json.dumps({"success": False, "error": str(e)}))
    finally:
        await crawl_manager.db.close_pool()

if __name__ == "__main__":
    asyncio.run(export_user_csv("{username}"))
'''
            
            # å¯«å…¥è‡¨æ™‚æ–‡ä»¶
            with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False, encoding='utf-8') as f:
                f.write(export_script_content)
                temp_script = f.name
            
            try:
                # åŸ·è¡Œå°å‡ºè…³æœ¬
                with st.spinner(f"ğŸ“Š æ­£åœ¨å°å‡ºç”¨æˆ¶ @{username} çš„è²¼æ–‡è³‡æ–™..."):
                    result = subprocess.run(
                        [sys.executable, temp_script],
                        capture_output=True,
                        text=True,
                        encoding='utf-8',
                        cwd=os.path.dirname(os.path.dirname(os.path.dirname(__file__))),
                        timeout=60
                    )
                
                if result.returncode == 0 and result.stdout.strip():
                    export_result = json.loads(result.stdout.strip())
                    
                    if export_result.get("success"):
                        csv_file_path = export_result.get("csv_file")
                        post_count = export_result.get("post_count", 0)
                        
                        st.success(f"""
                        âœ… **å°å‡ºæˆåŠŸï¼**
                        
                        ç”¨æˆ¶ @{username} çš„è²¼æ–‡å·²å°å‡ºç‚ºCSVï¼š
                        - ğŸ“Š å°å‡ºè²¼æ–‡æ•¸: {post_count:,} å€‹
                        - ğŸ“ æ–‡ä»¶è·¯å¾‘: {csv_file_path}
                        """)
                        
                        # æä¾›ä¸‹è¼‰æŒ‰éˆ•
                        if os.path.exists(csv_file_path):
                            with open(csv_file_path, 'rb') as f:
                                csv_content = f.read()
                            
                            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                            download_filename = f"user_posts_{username}_{timestamp}.csv"
                            
                            st.download_button(
                                label="ğŸ“¥ ä¸‹è¼‰CSVæ–‡ä»¶",
                                data=csv_content,
                                file_name=download_filename,
                                mime="text/csv",
                                key=f"download_user_csv_{username}"
                            )
                        
                    else:
                        st.error(f"âŒ å°å‡ºå¤±æ•—: {export_result.get('error', 'æœªçŸ¥éŒ¯èª¤')}")
                else:
                    st.error(f"âŒ å°å‡ºè…³æœ¬åŸ·è¡Œå¤±æ•—")
                    if result.stderr:
                        st.text(f"éŒ¯èª¤è©³æƒ…: {result.stderr}")
                        
            finally:
                # æ¸…ç†è‡¨æ™‚æ–‡ä»¶
                try:
                    os.unlink(temp_script)
                except:
                    pass
                    
        except Exception as e:
            st.error(f"âŒ å°å‡ºæ“ä½œå¤±æ•—: {str(e)}")
    
    def _show_json_download_button(self, results_file):
        """é¡¯ç¤ºJSONä¸‹è¼‰æŒ‰éˆ•"""
        if results_file and Path(results_file).exists():
            try:
                # è®€å–JSONæ–‡ä»¶å…§å®¹
                with open(results_file, 'r', encoding='utf-8') as f:
                    json_content = f.read()
                
                # ç”Ÿæˆä¸‹è¼‰æ–‡ä»¶åï¼ˆåŒ…å«æ™‚é–“æˆ³ï¼‰
                file_path = Path(results_file)
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                download_filename = f"crawl_results_{timestamp}.json"
                
                # ä½¿ç”¨ st.download_button æä¾›ä¸‹è¼‰
                st.download_button(
                    label="ğŸ’¾ ä¸‹è¼‰JSON",
                    data=json_content,
                    file_name=download_filename,
                    mime="application/json",
                    help="ä¸‹è¼‰çˆ¬å–çµæœJSONæ–‡ä»¶åˆ°æ‚¨çš„ä¸‹è¼‰è³‡æ–™å¤¾",
                    key="download_json_btn"
                )
                
            except Exception as e:
                st.error(f"âŒ æº–å‚™ä¸‹è¼‰æ–‡ä»¶å¤±æ•—: {e}")
        else:
            st.button("ğŸ’¾ ä¸‹è¼‰JSON", disabled=True, help="æš«ç„¡å¯ä¸‹è¼‰çš„çµæœæ–‡ä»¶")
    
    def _clear_results(self):
        """æ¸…é™¤ç•¶å‰çµæœ"""
        if 'realtime_results' in st.session_state:
            del st.session_state.realtime_results
        if 'realtime_results_file' in st.session_state:
            del st.session_state.realtime_results_file
        if 'realtime_error' in st.session_state:
            del st.session_state.realtime_error
        if 'latest_csv_file' in st.session_state:
            del st.session_state.latest_csv_file
        st.success("ğŸ—‘ï¸ çµæœå·²æ¸…é™¤")
        st.rerun()  # é‡æ–°é‹è¡Œé é¢ä¾†åˆ·æ–°UI
    
    def _render_results_area(self):
        """æ¸²æŸ“çµæœå€åŸŸ"""
        if 'realtime_results' in st.session_state:
            self._show_results()
        elif 'realtime_error' in st.session_state:
            st.error(f"âŒ çˆ¬å–éŒ¯èª¤ï¼š{st.session_state.realtime_error}")
        else:
            st.info("ğŸ‘† é»æ“Šã€Œé–‹å§‹çˆ¬å–ã€ä¾†é–‹å§‹ï¼Œæˆ–ä¸Šå‚³CSVæ–‡ä»¶æŸ¥çœ‹ä¹‹å‰çš„çµæœ")
    
    def _show_results(self):
        """é¡¯ç¤ºçˆ¬å–çµæœ"""
        # å¾session stateç²å–çµæœï¼ˆå¯èƒ½æ˜¯å­—å…¸æ ¼å¼ï¼‰
        realtime_results = st.session_state.realtime_results
        
        # æª¢æŸ¥resultsçš„æ ¼å¼ï¼Œå¦‚æœæ˜¯å­—å…¸å‰‡æå–resultsåˆ—è¡¨
        if isinstance(realtime_results, dict):
            results = realtime_results.get('results', [])
        else:
            results = realtime_results if realtime_results else []
        
        results_file = st.session_state.get('realtime_results_file', 'unknown.json')
        
        st.subheader("ğŸ“Š çˆ¬å–çµæœ")
        
        # ç¢ºä¿resultsæ˜¯åˆ—è¡¨
        if not isinstance(results, list):
            st.error("âŒ çµæœæ ¼å¼éŒ¯èª¤ï¼Œè«‹é‡æ–°è¼‰å…¥")
            return
        
        # åŸºæœ¬çµ±è¨ˆ
        total_posts = len(results)
        successful_views = len([r for r in results if isinstance(r, dict) and r.get('has_views')])
        successful_content = len([r for r in results if isinstance(r, dict) and r.get('has_content')])
        successful_likes = len([r for r in results if isinstance(r, dict) and r.get('has_likes')])
        successful_comments = len([r for r in results if isinstance(r, dict) and r.get('has_comments')])
        
        # çµ±è¨ˆæŒ‡æ¨™
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("ç¸½è²¼æ–‡æ•¸", total_posts)
        with col2:
            st.metric("è§€çœ‹æ•¸æˆåŠŸ", f"{successful_views}/{total_posts}")
        with col3:
            st.metric("å…§å®¹æˆåŠŸ", f"{successful_content}/{total_posts}")
        with col4:
            st.metric("äº’å‹•æ•¸æ“š", f"{successful_likes}/{total_posts}")
        
        # é¡¯ç¤ºçˆ¬å–è€—æ™‚
        crawl_duration = st.session_state.get('realtime_crawl_duration')
        if crawl_duration is not None:
            st.markdown("---")
            if crawl_duration < 60:
                duration_display = f"{crawl_duration:.1f} ç§’"
            else:
                duration_display = f"{crawl_duration/60:.1f} åˆ†é˜"
            
            col_time = st.columns(1)[0]
            with col_time:
                st.metric(
                    label="â±ï¸ çˆ¬å–è€—æ™‚", 
                    value=duration_display,
                    help="å¾é–‹å§‹çˆ¬å–åˆ°å®Œæˆçš„ç¸½æ™‚é–“"
                )
        
        # æˆåŠŸç‡æŒ‡æ¨™
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            view_rate = (successful_views / total_posts * 100) if total_posts > 0 else 0
            st.metric("è§€çœ‹æ•¸æˆåŠŸç‡", f"{view_rate:.1f}%")
        with col2:
            content_rate = (successful_content / total_posts * 100) if total_posts > 0 else 0
            st.metric("å…§å®¹æˆåŠŸç‡", f"{content_rate:.1f}%")
        with col3:
            like_rate = (successful_likes / total_posts * 100) if total_posts > 0 else 0
            st.metric("æŒ‰è®šæ•¸æˆåŠŸç‡", f"{like_rate:.1f}%")
        with col4:
            comment_rate = (successful_comments / total_posts * 100) if total_posts > 0 else 0
            st.metric("ç•™è¨€æ•¸æˆåŠŸç‡", f"{comment_rate:.1f}%")
        
        # é‡è¤‡è™•ç†åŠŸèƒ½
        st.divider()
        col1, col2, col3 = st.columns([2, 1, 1])
        with col1:
            st.write("**ğŸ”„ é‡è¤‡è™•ç†**")
            st.caption("æª¢æ¸¬é‡è¤‡è²¼æ–‡ï¼Œè§€çœ‹æ•¸ä½çš„ç”¨APIé‡æ–°æå–")
        with col2:
            if st.button("ğŸ” æª¢æ¸¬é‡è¤‡", key="detect_duplicates"):
                self._detect_duplicates()
        with col3:
            if st.button("ğŸ”„ è™•ç†é‡è¤‡", key="process_duplicates"):
                self._process_duplicates()
        
        # è©³ç´°çµæœè¡¨æ ¼
        if st.checkbox("ğŸ“‹ é¡¯ç¤ºè©³ç´°çµæœ", key="show_detailed_results"):
            self._show_detailed_table(results)
        
        # è³‡æ–™åº«ç‹€æ…‹å’Œå‚™ç”¨ä¿å­˜
        if isinstance(realtime_results, dict):
            db_saved = realtime_results.get('database_saved', False)
            saved_count = realtime_results.get('database_saved_count', 0)
            if db_saved:
                st.success(f"âœ… å·²ä¿å­˜åˆ°è³‡æ–™åº« ({saved_count} å€‹è²¼æ–‡)")
            else:
                # é¡¯ç¤ºå‚™ç”¨ä¿å­˜é¸é …
                col_info, col_save = st.columns([3, 1])
                with col_info:
                    st.info("â„¹ï¸ çˆ¬èŸ²é€šå¸¸æœƒè‡ªå‹•ä¿å­˜åˆ°è³‡æ–™åº«ã€‚å¦‚æœçµ±è¨ˆä¸­æ²’æœ‰çœ‹åˆ°æ–°æ•¸æ“šï¼Œæ‚¨å¯ä»¥ä½¿ç”¨å‚™ç”¨ä¿å­˜åŠŸèƒ½")
                with col_save:
                    if st.button("ğŸ’¾ å‚™ç”¨ä¿å­˜", key="save_to_database", help="æ‰‹å‹•ä¿å­˜åˆ°è³‡æ–™åº«ï¼ˆå‚™ç”¨åŠŸèƒ½ï¼‰"):
                        self._save_results_to_database()
        else:
            st.info("ğŸ’¡ å¢é‡çˆ¬å–æ¨¡å¼æœƒè‡ªå‹•ä¿å­˜åˆ°è³‡æ–™åº«ä¸¦æ›´æ–°çµ±è¨ˆ")

        st.divider()
        
        # ä¸‹è¼‰å’Œå°å‡ºæŒ‰éˆ•
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            self._show_json_download_button(results_file)
        
        with col2:
            if st.button("ğŸ“Š å°å‡ºCSV", key="export_csv"):
                # åˆ‡æ›CSVå°å‡ºé¢æ¿çš„å¯è¦‹æ€§
                st.session_state.show_realtime_csv_export = not st.session_state.get('show_realtime_csv_export', False)
                st.rerun()
        
        with col3:
            if st.button("ğŸ“ˆ æ­·å²åˆ†æ", key="export_history"):
                # åˆ‡æ›æ­·å²åˆ†æé¢æ¿çš„å¯è¦‹æ€§
                st.session_state.show_realtime_history_analysis = not st.session_state.get('show_realtime_history_analysis', False)
                st.rerun()
            
        # é¡¯ç¤ºCSVå°å‡ºé¢æ¿ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
        if st.session_state.get('show_realtime_csv_export', False):
            self._show_csv_export_options(results_file)
            
        # é¡¯ç¤ºæ­·å²åˆ†æé¢æ¿ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
        if st.session_state.get('show_realtime_history_analysis', False):
            self._show_export_history_options()
        
        with col4:
            if st.button("ğŸ” æ›´å¤šå°å‡º", key="more_exports"):
                # åˆ‡æ›æ›´å¤šå°å‡ºé¢æ¿çš„å¯è¦‹æ€§
                st.session_state.show_realtime_advanced_exports = not st.session_state.get('show_realtime_advanced_exports', False)
                st.rerun()
        
        # é¡¯ç¤ºæ›´å¤šå°å‡ºé¢æ¿ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
        if st.session_state.get('show_realtime_advanced_exports', False):
            self._show_advanced_export_options()
    
    def _detect_duplicates(self):
        """æª¢æ¸¬é‡è¤‡è²¼æ–‡"""
        if 'realtime_results' not in st.session_state:
            st.error("âŒ æ²’æœ‰å¯æª¢æ¸¬çš„çµæœ")
            return
        
        results = st.session_state.realtime_results
        
        # æŒ‰ post_id åˆ†çµ„
        from collections import defaultdict
        grouped = defaultdict(list)
        for result in results:
            if result.get('post_id'):
                grouped[result['post_id']].append(result)
        
        # æ‰¾å‡ºé‡è¤‡é …
        duplicates = {k: v for k, v in grouped.items() if len(v) > 1}
        
        if not duplicates:
            st.success("âœ… æ²’æœ‰ç™¼ç¾é‡è¤‡è²¼æ–‡")
            return
        
        st.warning(f"âš ï¸ ç™¼ç¾ {len(duplicates)} çµ„é‡è¤‡è²¼æ–‡")
        
        for post_id, items in duplicates.items():
            with st.expander(f"ğŸ“‹ é‡è¤‡çµ„: {post_id} ({len(items)} å€‹ç‰ˆæœ¬)"):
                for i, item in enumerate(items):
                    views = item.get('views', 'N/A')
                    source = item.get('source', 'unknown')
                    content = item.get('content', 'N/A')[:100] + '...' if item.get('content') else 'N/A'
                    
                    col1, col2, col3 = st.columns([1, 1, 3])
                    with col1:
                        st.write(f"**ç‰ˆæœ¬ {i+1}**")
                        st.write(f"è§€çœ‹æ•¸: {views}")
                    with col2:
                        st.write(f"ä¾†æº: {source}")
                    with col3:
                        st.write(f"å…§å®¹: {content}")
    
    def _process_duplicates(self):
        """è™•ç†é‡è¤‡è²¼æ–‡"""
        if 'realtime_results' not in st.session_state:
            st.error("âŒ æ²’æœ‰å¯è™•ç†çš„çµæœ")
            return
        
        # èª¿ç”¨é‡è¤‡è™•ç†è…³æœ¬
        try:
            import subprocess
            import sys
            import os
            
            st.info("ğŸ”„ æ­£åœ¨è™•ç†é‡è¤‡è²¼æ–‡...")
            
            # åŸ·è¡Œé‡è¤‡è™•ç†è…³æœ¬
            script_path = "fix_duplicates_reextract.py"
            result = subprocess.run(
                [sys.executable, script_path],
                capture_output=True,
                text=True,
                encoding='utf-8',
                errors='replace',
                cwd=os.getcwd()
            )
            
            if result.returncode == 0:
                # æŸ¥æ‰¾è™•ç†å¾Œçš„æ–‡ä»¶
                import glob
                from pathlib import Path
                
                # æª¢æŸ¥æ–°çš„è³‡æ–™å¤¾ä½ç½®
                extraction_dir = Path("extraction_results")
                if extraction_dir.exists():
                    dedup_files = list(extraction_dir.glob("realtime_extraction_results_*_dedup.json"))
                else:
                    dedup_files = [Path(f) for f in glob.glob("realtime_extraction_results_*_dedup.json")]
                
                if dedup_files:
                    latest_dedup = max(dedup_files, key=lambda f: f.stat().st_mtime)
                    
                    # è®€å–è™•ç†å¾Œçš„çµæœ
                    import json
                    with open(latest_dedup, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    
                    # æ›´æ–°session_state
                    st.session_state.realtime_results = data.get('results', [])
                    st.session_state.realtime_results_file = latest_dedup
                    
                    duplicates_count = data.get('duplicates_processed', 0)
                    reextracted_count = data.get('reextracted_count', 0)
                    
                    st.success(f"âœ… é‡è¤‡è™•ç†å®Œæˆï¼")
                    st.info(f"ğŸ“Š è™•ç†äº† {duplicates_count} çµ„é‡è¤‡ï¼Œé‡æ–°æå– {reextracted_count} å€‹é …ç›®")
                    st.balloons()
                    
                    # è‡ªå‹•åˆ·æ–°é é¢ä»¥é¡¯ç¤ºæ›´æ–°çµæœ
                    st.rerun()
                else:
                    st.error("âŒ æœªæ‰¾åˆ°è™•ç†å¾Œçš„çµæœæ–‡ä»¶")
            else:
                st.error(f"âŒ è™•ç†å¤±æ•—ï¼š{result.stderr}")
                st.code(result.stdout)
                
        except Exception as e:
            st.error(f"âŒ è™•ç†éŒ¯èª¤ï¼š{str(e)}")
    
    def _show_csv_export_options(self, json_file_path: str):
        """é¡¯ç¤ºCSVå°å‡ºé¸é …"""
        with st.expander("ğŸ“Š CSVå°å‡ºé¸é …", expanded=True):
            # æ·»åŠ é—œé–‰æŒ‰éˆ•
            col_header1, col_header2 = st.columns([4, 1])
            with col_header1:
                st.write("**é¸æ“‡æ’åºæ–¹å¼ï¼ˆå»ºè­°æŒ‰è§€çœ‹æ•¸æ’åºï¼‰**")
            with col_header2:
                if st.button("âŒ é—œé–‰", key="close_realtime_csv_export"):
                    st.session_state.show_realtime_csv_export = False
                    st.rerun()
            
            sort_options = {
                "è§€çœ‹æ•¸ (é«˜â†’ä½)": "views",
                "æŒ‰è®šæ•¸ (é«˜â†’ä½)": "likes", 
                "ç•™è¨€æ•¸ (é«˜â†’ä½)": "comments",
                "è½‰ç™¼æ•¸ (é«˜â†’ä½)": "reposts",
                "åˆ†äº«æ•¸ (é«˜â†’ä½)": "shares",
                "è²¼æ–‡ID (Aâ†’Z)": "post_id",
                "åŸå§‹é †åº (ä¸æ’åº)": "none"
            }
            
            selected_sort = st.selectbox(
                "æ’åºæ–¹å¼",
                options=list(sort_options.keys()),
                index=0,  # é è¨­é¸æ“‡è§€çœ‹æ•¸æ’åº
                help="é¸æ“‡CSVæ–‡ä»¶ä¸­æ•¸æ“šçš„æ’åºæ–¹å¼ï¼Œå»ºè­°é¸æ“‡è§€çœ‹æ•¸ä»¥ä¾¿åˆ†ææœ€å—æ­¡è¿çš„è²¼æ–‡"
            )
            
            col1, col2 = st.columns(2)
            
            with col1:
                if st.button("ğŸ“¥ ç”ŸæˆCSV", key="export_csv_generate"):
                    sort_by = sort_options[selected_sort]
                    self._export_current_to_csv(json_file_path, sort_by)
                
                # æª¢æŸ¥æ˜¯å¦æœ‰ç”Ÿæˆå¥½çš„CSVå¯ä»¥ä¸‹è¼‰
                self._show_csv_download_if_available()
            
            with col2:
                st.info("ğŸ’¡ **CSVä½¿ç”¨æç¤ºï¼š**\n- ç”¨Excelæˆ–Google Sheetsæ‰“é–‹\n- å¯ä»¥é€²ä¸€æ­¥ç¯©é¸å’Œåˆ†æ\n- æ”¯æ´ä¸­æ–‡é¡¯ç¤º")
    
    def _export_current_to_csv(self, json_file_path: str, sort_by: str = 'views'):
        """å°å‡ºç•¶æ¬¡çµæœåˆ°CSV"""
        try:
            from common.csv_export_manager import CSVExportManager
            import os
            
            csv_manager = CSVExportManager()
            
            # ç¢ºä¿exportsç›®éŒ„å­˜åœ¨ï¼ˆä½¿ç”¨çµ•å°è·¯å¾‘ï¼Œé©åˆUbuntuéƒ¨ç½²ï¼‰
            import tempfile
            
            # åœ¨ç”Ÿç”¢ç’°å¢ƒä¸­ï¼Œå„ªå…ˆä½¿ç”¨ /app/exportsï¼Œé–‹ç™¼ç’°å¢ƒä½¿ç”¨ç›¸å°è·¯å¾‘
            if os.path.exists('/app'):  # Docker å®¹å™¨ç’°å¢ƒ
                exports_dir = "/app/exports"
            else:  # é–‹ç™¼ç’°å¢ƒ
                exports_dir = os.path.abspath("exports")
            
            if not os.path.exists(exports_dir):
                os.makedirs(exports_dir, mode=0o755)  # è¨­ç½®é©ç•¶æ¬Šé™
            
            # ç”Ÿæˆå®Œæ•´çš„è¼¸å‡ºè·¯å¾‘
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            with open(json_file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            username = data.get('target_username', 'unknown')
            
            csv_filename = f"export_current_{username}_{timestamp}.csv"
            csv_output_path = os.path.join(exports_dir, csv_filename)
            
            # èª¿ç”¨CSVç”Ÿæˆï¼ˆä½¿ç”¨çµ•å°è·¯å¾‘ï¼‰
            csv_file = csv_manager.export_current_session(json_file_path, output_path=csv_output_path, sort_by=sort_by)
            
            # é©—è­‰æ–‡ä»¶æ˜¯å¦çœŸçš„è¢«å‰µå»º
            if not os.path.exists(csv_file):
                raise FileNotFoundError(f"CSVæ–‡ä»¶å‰µå»ºå¤±æ•—: {csv_file}")
            
            # æª¢æŸ¥æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(csv_file)
            if file_size == 0:
                raise ValueError(f"CSVæ–‡ä»¶ç‚ºç©º: {csv_file}")
            
            # æª¢æŸ¥æ–‡ä»¶æ¬Šé™ï¼ˆUbuntuç’°å¢ƒé‡è¦ï¼‰
            if not os.access(csv_file, os.R_OK):
                raise PermissionError(f"CSVæ–‡ä»¶ç„¡è®€å–æ¬Šé™: {csv_file}")
            
            # é©—è­‰æ–‡ä»¶å…§å®¹çš„UTF-8ç·¨ç¢¼ï¼ˆUbuntuç’°å¢ƒé©—è­‰ï¼‰
            try:
                with open(csv_file, 'r', encoding='utf-8-sig') as test_f:
                    test_f.read(100)  # è®€å–å‰100å€‹å­—ç¬¦æ¸¬è©¦ç·¨ç¢¼
            except UnicodeDecodeError as e:
                raise ValueError(f"CSVæ–‡ä»¶ç·¨ç¢¼å•é¡Œ: {e}")
            
            # ä¿å­˜CSVæ–‡ä»¶è·¯å¾‘åˆ°æœƒè©±ç‹€æ…‹
            st.session_state.latest_csv_file = csv_file
            
            st.success(f"âœ… CSVç”ŸæˆæˆåŠŸï¼")
            st.info(f"ğŸ“ æ–‡ä»¶ä½ç½®: {csv_file}")
            st.info(f"ğŸ“ æ–‡ä»¶å¤§å°: {file_size} bytes")
            
        except Exception as e:
            import traceback
            error_details = traceback.format_exc()
            st.error(f"âŒ CSVç”Ÿæˆå¤±æ•—: {str(e)}")
            st.error(f"ğŸ” è©³ç´°éŒ¯èª¤: {error_details}")
            if 'latest_csv_file' in st.session_state:
                del st.session_state.latest_csv_file
    
    def _show_csv_download_if_available(self):
        """é¡¯ç¤ºCSVä¸‹è¼‰æŒ‰éˆ•ï¼ˆå¦‚æœæœ‰å¯ç”¨çš„CSVæ–‡ä»¶ï¼‰"""
        if 'latest_csv_file' in st.session_state:
            csv_file = st.session_state.latest_csv_file
            if csv_file and Path(csv_file).exists():
                try:
                    with open(csv_file, 'rb') as f:
                        csv_content = f.read()
                    
                    # ç”Ÿæˆæ™‚é–“æˆ³æ–‡ä»¶å
                    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                    download_filename = f"crawl_results_{timestamp}.csv"
                    
                    st.download_button(
                        label="ğŸ“¥ ä¸‹è¼‰CSVæ–‡ä»¶",
                        data=csv_content,
                        file_name=download_filename,
                        mime="text/csv",
                        help="ä¸‹è¼‰CSVæ–‡ä»¶åˆ°æ‚¨çš„ä¸‹è¼‰è³‡æ–™å¤¾",
                        key="download_csv_file_btn"
                    )
                    
                except Exception as e:
                    st.error(f"âŒ æº–å‚™CSVä¸‹è¼‰å¤±æ•—: {e}")
    
    def _show_export_history_options(self):
        """é¡¯ç¤ºæ­·å²å°å‡ºé¸é …"""
        # æ·»åŠ é—œé–‰æŒ‰éˆ•
        col_header1, col_header2 = st.columns([4, 1])
        with col_header1:
            st.write("**ğŸ“ˆ æ­·å²æ•¸æ“šåˆ†æ**")
        with col_header2:
            if st.button("âŒ é—œé–‰", key="close_realtime_history_analysis"):
                st.session_state.show_realtime_history_analysis = False
                st.rerun()
        
        if 'realtime_results' not in st.session_state:
            st.error("âŒ è«‹å…ˆåŸ·è¡Œçˆ¬å–ä»¥ç²å–å¸³è™Ÿä¿¡æ¯")
            return
        
        # ç²å–ç•¶å‰å¸³è™Ÿ
        results = st.session_state.realtime_results
        if not results:
            st.error("âŒ ç„¡æ³•ç²å–å¸³è™Ÿä¿¡æ¯")
            return
        
        # ğŸ”§ ä¿®å¾©ï¼šå¾çµæœä¸­æå–ç”¨æˆ¶åå’Œç”¨æˆ¶ID
        target_username = None
        target_user_id = None
        
        if 'realtime_results_file' in st.session_state:
            try:
                import json
                with open(st.session_state.realtime_results_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                target_username = data.get('target_username')
                target_user_id = data.get('target_user_id')
            except:
                pass
        
        # ğŸ”§ ä¿®å¾©ï¼šå¾ç•¶å‰çµæœä¸­å˜—è©¦ç²å–ç”¨æˆ¶ä¿¡æ¯
        if not target_username and isinstance(results, dict) and 'results' in results:
            first_result = results['results'][0] if results['results'] else None
            if first_result:
                target_username = first_result.get('username')
                target_user_id = first_result.get('user_id') or target_username
        elif not target_username and isinstance(results, list) and results:
            first_result = results[0] if results else None
            if first_result:
                target_username = first_result.get('username')
                target_user_id = first_result.get('user_id') or target_username
        
        if not target_username:
            st.error("âŒ ç„¡æ³•è­˜åˆ¥ç›®æ¨™å¸³è™Ÿ")
            return
        
        # å¦‚æœæ²’æœ‰user_idï¼Œä½¿ç”¨usernameä½œç‚ºfallback
        if not target_user_id:
            target_user_id = target_username
        
        # æ·»åŠ æ’åºè¨­å®š
        st.write("**ğŸ“Š æ’åºè¨­å®š**")
        col_sort1, col_sort2 = st.columns(2)
        
        with col_sort1:
            sort_by = st.selectbox(
                "æ’åºä¾æ“š",
                options=["fetched_at", "views_count", "likes_count", "comments_count", "calculated_score"],
                format_func=lambda x: {
                    "fetched_at": "çˆ¬å–æ™‚é–“",
                    "views_count": "è§€çœ‹æ•¸", 
                    "likes_count": "æŒ‰è®šæ•¸",
                    "comments_count": "ç•™è¨€æ•¸",
                    "calculated_score": "è¨ˆç®—åˆ†æ•¸"
                }.get(x, x),
                key="realtime_history_sort_by",
                help="é¸æ“‡æ’åºçš„ä¾æ“šæ¬„ä½"
            )
        
        with col_sort2:
            sort_order = st.selectbox(
                "æ’åºé †åº",
                options=["DESC", "ASC"],
                format_func=lambda x: "é™åº (é«˜åˆ°ä½)" if x == "DESC" else "å‡åº (ä½åˆ°é«˜)",
                key="realtime_history_sort_order",
                help="é¸æ“‡æ’åºé †åº"
            )
        
        # å°å‡ºé¡å‹é¸æ“‡
        export_type = st.radio(
            "é¸æ“‡å°å‡ºé¡å‹",
            options=["æœ€è¿‘æ•¸æ“š", "å…¨éƒ¨æ­·å²", "çµ±è¨ˆåˆ†æ"],
            help="é¸æ“‡è¦å°å‡ºçš„æ­·å²æ•¸æ“šç¯„åœ",
            key="realtime_export_type"
        )
        
        # æœ€å¤§è¨˜éŒ„æ•¸è¨­å®š
        max_records = st.number_input(
            "æœ€å¤§è¨˜éŒ„æ•¸",
            min_value=100,
            max_value=50000,
            value=5000,
            help="é™åˆ¶å°å‡ºçš„æœ€å¤§è¨˜éŒ„æ•¸",
            key="realtime_max_records"
        )
        
        # å°å‡ºæŒ‰éˆ•å’Œæ“ä½œ
        if export_type == "æœ€è¿‘æ•¸æ“š":
            col1, col2 = st.columns(2)
            with col1:
                days_back = st.number_input("å›æº¯å¤©æ•¸", min_value=1, max_value=365, value=7, key="realtime_days_back")
            
            if st.button("ğŸ“Š å°å‡ºæœ€è¿‘æ•¸æ“š", key="realtime_export_recent"):
                self._export_history_data(target_user_id, "recent", 
                                        days_back=days_back, limit=max_records, 
                                        sort_by=sort_by, sort_order=sort_order)
        
        elif export_type == "å…¨éƒ¨æ­·å²":
            if st.button("ğŸ“Š å°å‡ºå…¨éƒ¨æ­·å²", key="realtime_export_all"):
                self._export_history_data(target_user_id, "all", 
                                        limit=max_records, sort_by=sort_by, sort_order=sort_order)
        
        elif export_type == "çµ±è¨ˆåˆ†æ":
            st.info("æŒ‰æ—¥æœŸçµ±è¨ˆçš„åˆ†æå ±å‘Šï¼ŒåŒ…å«å¹³å‡è§€çœ‹æ•¸ã€æˆåŠŸç‡ç­‰æŒ‡æ¨™")
            
            if st.button("ğŸ“ˆ å°å‡ºçµ±è¨ˆåˆ†æ", key="realtime_export_analysis"):
                self._export_history_data(target_user_id, "analysis", 
                                        sort_by=sort_by, sort_order=sort_order)
    
    def _export_history_data(self, username: str, export_type: str, **kwargs):
        """å°å‡ºæ­·å²æ•¸æ“š"""
        try:
            import asyncio
            import json
            import pandas as pd
            from datetime import datetime
            
            # ç²å–æ’åºåƒæ•¸
            sort_by = kwargs.get('sort_by', 'fetched_at')
            sort_order = kwargs.get('sort_order', 'DESC')
            
            with st.spinner(f"ğŸ”„ æ­£åœ¨å¾è³‡æ–™åº«ç²å– @{username} çš„{export_type}æ•¸æ“š..."):
                # å¾è³‡æ–™åº«ç²å–æ•¸æ“š
                posts_data = asyncio.run(self._fetch_realtime_history_from_db(username, export_type, **kwargs))
            
            if not posts_data:
                st.warning(f"âš ï¸ æ²’æœ‰æ‰¾åˆ°ç”¨æˆ¶ @{username} çš„æ­·å²æ•¸æ“š")
                return
            
            # æ’åºæ•¸æ“š
            def get_sort_key(post):
                value = post.get(sort_by, 0)
                if value is None:
                    return 0
                if isinstance(value, str):
                    try:
                        return float(value)
                    except:
                        return 0
                return value
            
            posts_data.sort(key=get_sort_key, reverse=(sort_order == 'DESC'))
            
            # æ·»åŠ çµ±è¨ˆä¿¡æ¯
            summary = self._calculate_realtime_stats(posts_data)
            
            # é¡¯ç¤ºçµ±è¨ˆæ¦‚è¦½
            st.write("**ğŸ“Š æ•¸æ“šæ¦‚è¦½**")
            col_s1, col_s2, col_s3, col_s4 = st.columns(4)
            with col_s1:
                st.metric("ç¸½è¨˜éŒ„æ•¸", f"{len(posts_data):,}")
            with col_s2:
                st.metric("å¹³å‡è§€çœ‹æ•¸", f"{summary.get('avg_views', 0):,.0f}")
            with col_s3:
                st.metric("å¹³å‡æŒ‰è®šæ•¸", f"{summary.get('avg_likes', 0):,.0f}")
            with col_s4:
                st.metric("æœ€é«˜è§€çœ‹æ•¸", f"{summary.get('max_views', 0):,.0f}")
            
            # é¡¯ç¤ºå‰10ç­†æ•¸æ“šé è¦½
            if posts_data:
                st.write("**å‰10ç­†æ•¸æ“šï¼š**")
                preview_data = []
                for i, post in enumerate(posts_data[:10], 1):
                    content_preview = (post.get('content', '')[:40] + "...") if post.get('content') and len(post.get('content', '')) > 40 else post.get('content', 'N/A')
                    preview_data.append({
                        "#": i,
                        "è²¼æ–‡ID": post.get('post_id', 'N/A')[:20] + "..." if len(post.get('post_id', '')) > 20 else post.get('post_id', 'N/A'),
                        "å…§å®¹é è¦½": content_preview,
                        "è§€çœ‹æ•¸": f"{post.get('views_count', 0):,}",
                        "æŒ‰è®šæ•¸": f"{post.get('likes_count', 0):,}",
                        "çˆ¬å–æ™‚é–“": str(post.get('fetched_at', 'N/A'))[:19]
                    })
                st.dataframe(preview_data, use_container_width=True)
            
            st.success(f"âœ… {export_type}æ•¸æ“šå°å‡ºå®Œæˆï¼å…± {len(posts_data)} ç­†è¨˜éŒ„")
            
            # æº–å‚™ä¸‹è¼‰æ•¸æ“š
            data = {
                "username": username,
                "export_type": export_type,
                "exported_at": datetime.now().isoformat(),
                "sort_by": sort_by,
                "sort_order": sort_order,
                "total_records": len(posts_data),
                "summary": summary,
                "data": posts_data
            }
            
            # æä¾› JSON å’Œ CSV ä¸‹è¼‰
            col1, col2 = st.columns(2)
            
            with col1:
                # JSON ä¸‹è¼‰
                json_data = json.dumps(data, ensure_ascii=False, indent=2, default=str)
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                json_filename = f"realtime_history_{username}_{export_type}_{timestamp}.json"
                
                st.download_button(
                    label="ğŸ“¥ ä¸‹è¼‰ JSON",
                    data=json_data,
                    file_name=json_filename,
                    mime="application/json",
                    help="ä¸‹è¼‰å®Œæ•´çš„JSONæ ¼å¼æ•¸æ“š",
                    use_container_width=True
                )
            
            with col2:
                # CSV ä¸‹è¼‰
                df = pd.DataFrame(posts_data)
                csv_content = df.to_csv(index=False, encoding='utf-8-sig').encode('utf-8-sig')
                csv_filename = f"realtime_history_{username}_{export_type}_{timestamp}.csv"
                
                st.download_button(
                    label="ğŸ“¥ ä¸‹è¼‰ CSV",
                    data=csv_content,
                    file_name=csv_filename,
                    mime="text/csv",
                    help="ä¸‹è¼‰CSVæ ¼å¼æ•¸æ“šï¼ˆé©åˆExcelé–‹å•Ÿï¼‰",
                    use_container_width=True
                )
                
        except Exception as e:
            st.error(f"âŒ æ­·å²æ•¸æ“šå°å‡ºå¤±æ•—: {str(e)}")
            import traceback
            st.error(f"è©³ç´°éŒ¯èª¤: {traceback.format_exc()}")
    
    async def _fetch_realtime_history_from_db(self, user_identifier: str, export_type: str, **kwargs):
        """å¾è³‡æ–™åº«ç²å–å¯¦æ™‚çˆ¬èŸ²çš„æ­·å²æ•¸æ“š"""
        try:
            from common.db_client import DatabaseClient
            
            db = DatabaseClient()
            await db.init_pool()
            
            async with db.get_connection() as conn:
                # ğŸ”§ ä¿®å¾©ï¼šæ§‹å»ºæ›´æ™ºèƒ½çš„æŸ¥è©¢ï¼Œæ”¯æ´ç”¨æˆ¶IDå’Œç”¨æˆ¶åæŸ¥è©¢
                base_query = """
                    SELECT post_id, username, content, views_count, likes_count, comments_count, 
                           reposts_count, shares_count, calculated_score, tags, images, videos, url, 
                           created_at, fetched_at, post_published_at
                    FROM post_metrics_sql 
                    WHERE (username = $1 OR post_id LIKE $2)
                """
                
                # ğŸ”§ ä¿®å¾©ï¼šæ”¯æ´ç”¨æˆ¶IDæ¨¡å¼æŸ¥è©¢ (user_id_%)
                params = [user_identifier, f"{user_identifier}_%"]
                
                # æ ¹æ“šå°å‡ºé¡å‹æ·»åŠ æ¢ä»¶
                if export_type == "recent":
                    days_back = kwargs.get('days_back', 7)
                    base_query += f" AND fetched_at >= NOW() - INTERVAL '{days_back} days'"
                
                # æ·»åŠ æ’åºå’Œé™åˆ¶
                sort_by = kwargs.get('sort_by', 'fetched_at')
                sort_order = kwargs.get('sort_order', 'DESC')
                limit = kwargs.get('limit', 5000)
                
                base_query += f" ORDER BY {sort_by} {sort_order} LIMIT $" + str(len(params) + 1)
                params.append(limit)
                
                # åŸ·è¡ŒæŸ¥è©¢
                rows = await conn.fetch(base_query, *params)
                
                # è½‰æ›ç‚ºå­—å…¸åˆ—è¡¨
                posts = []
                for row in rows:
                    post_dict = dict(row)
                    # è™•ç†é™£åˆ—å­—æ®µ
                    for field in ['tags', 'images', 'videos']:
                        if isinstance(post_dict.get(field), str):
                            try:
                                post_dict[field] = json.loads(post_dict[field])
                            except:
                                post_dict[field] = []
                    posts.append(post_dict)
                
                return posts
                
        except Exception as e:
            st.error(f"è³‡æ–™åº«æŸ¥è©¢å¤±æ•—: {e}")
            return []
    
    def _calculate_realtime_stats(self, posts_data):
        """è¨ˆç®—å¯¦æ™‚çˆ¬èŸ²æ•¸æ“šçš„çµ±è¨ˆä¿¡æ¯"""
        if not posts_data:
            return {}
        
        views = [post.get('views_count', 0) for post in posts_data if post.get('views_count')]
        likes = [post.get('likes_count', 0) for post in posts_data if post.get('likes_count')]
        comments = [post.get('comments_count', 0) for post in posts_data if post.get('comments_count')]
        
        return {
            "total_posts": len(posts_data),
            "avg_views": sum(views) / len(views) if views else 0,
            "max_views": max(views) if views else 0,
            "min_views": min(views) if views else 0,
            "avg_likes": sum(likes) / len(likes) if likes else 0,
            "max_likes": max(likes) if likes else 0,
            "avg_comments": sum(comments) / len(comments) if comments else 0,
            "max_comments": max(comments) if comments else 0
        }
    
    def _show_advanced_export_options(self):
        """é¡¯ç¤ºé€²éšå°å‡ºé¸é …"""
        # æ·»åŠ é—œé–‰æŒ‰éˆ•
        col_header1, col_header2 = st.columns([4, 1])
        with col_header1:
            st.write("**ğŸ” é€²éšå°å‡ºåŠŸèƒ½**")
        with col_header2:
            if st.button("âŒ é—œé–‰", key="close_realtime_advanced_exports"):
                st.session_state.show_realtime_advanced_exports = False
                st.rerun()
        
        st.markdown("**æ›´å¤šå°å‡ºé¸é …å’Œæ‰¹é‡æ“ä½œ**")
        
        tab1, tab2, tab3 = st.tabs(["ğŸ“Š å°æ¯”å ±å‘Š", "ğŸ”„ æ‰¹é‡å°å‡º", "âš¡ å¿«é€Ÿå·¥å…·"])
        
        with tab1:
            st.subheader("ğŸ“Š å¤šæ¬¡çˆ¬å–å°æ¯”å ±å‘Š")
            st.info("æ¯”è¼ƒå¤šæ¬¡çˆ¬å–çµæœçš„æ•ˆèƒ½å’ŒæˆåŠŸç‡")
            
            # æŸ¥æ‰¾æ‰€æœ‰JSONæ–‡ä»¶
            import glob
            import os
            # æª¢æŸ¥æ–°çš„è³‡æ–™å¤¾ä½ç½®
            extraction_dir = Path("extraction_results")
            if extraction_dir.exists():
                json_files = list(extraction_dir.glob("realtime_extraction_results_*.json"))
            else:
                json_files = [Path(f) for f in glob.glob("realtime_extraction_results_*.json")]
            
            if len(json_files) >= 2:
                st.write(f"ğŸ” æ‰¾åˆ° {len(json_files)} å€‹çˆ¬å–çµæœæ–‡ä»¶ï¼š")
                
                st.info("ğŸ“Š å°æ¯”å ±å‘ŠåŠŸèƒ½ï¼šæ¯”è¼ƒå¤šæ¬¡çˆ¬å–çµæœçš„æ•ˆèƒ½å’ŒæˆåŠŸç‡")
                st.write(f"ğŸ” æ‰¾åˆ° {len(json_files)} å€‹çµæœæ–‡ä»¶")
                
                # ç°¡åŒ–çš„æ–‡ä»¶é¸æ“‡
                file_names = [f.name for f in sorted(json_files, reverse=True)[:5]]
                if file_names:
                    selected_file = st.selectbox(
                        "é¸æ“‡ä¸€å€‹æ–‡ä»¶æŸ¥çœ‹è©³æƒ…ï¼š",
                        options=file_names,
                        help="æŸ¥çœ‹æ–‡ä»¶çš„åŸºæœ¬ä¿¡æ¯"
                    )
                    
                    if selected_file:
                        st.success(f"âœ… é¸ä¸­æ–‡ä»¶: {selected_file}")
                        # é€™è£¡å¯ä»¥æ·»åŠ æ›´å¤šæ–‡ä»¶è©³æƒ…å±•ç¤º
            else:
                st.warning("âš ï¸ éœ€è¦è‡³å°‘2å€‹çˆ¬å–çµæœæ–‡ä»¶æ‰èƒ½é€²è¡Œå°æ¯”")
            
            with tab2:
                st.subheader("ğŸ”„ æ‰¹é‡å°å‡ºåŠŸèƒ½")
                
                col1, col2 = st.columns(2)
                
                with col1:
                    if st.button("ğŸ“¥ å°å‡ºæ‰€æœ‰æœ€æ–°çµæœ", key="export_all_latest"):
                        self._export_all_latest_results()
                
                with col2:
                    if st.button("ğŸ“ˆ å°å‡ºæ‰€æœ‰å¸³è™Ÿçµ±è¨ˆ", key="export_all_stats"):
                        self._export_all_account_stats()
                
                st.divider()
                
                # è‡ªå‹•åŒ–å°å‡ºè¨­å®š
                st.write("**è‡ªå‹•åŒ–å°å‡ºè¨­å®š**")
                auto_sort = st.selectbox(
                    "é è¨­æ’åºæ–¹å¼",
                    ["è§€çœ‹æ•¸", "æŒ‰è®šæ•¸", "ç•™è¨€æ•¸", "æ™‚é–“é †åº"],
                    help="æ‰¹é‡å°å‡ºæ™‚ä½¿ç”¨çš„é è¨­æ’åº"
                )
                
                if st.button("ğŸ’¾ ä¿å­˜è¨­å®š", key="save_export_settings"):
                    st.session_state.default_sort = auto_sort
                    st.success(f"âœ… å·²ä¿å­˜é è¨­æ’åº: {auto_sort}")
            
            with tab3:
                st.subheader("âš¡ å¿«é€Ÿå·¥å…·")
                
                # å¿«é€Ÿé è¦½
                st.write("**å¿«é€Ÿé è¦½CSVæ–‡ä»¶**")
                uploaded_csv = st.file_uploader(
                    "ä¸Šå‚³CSVæ–‡ä»¶é€²è¡Œé è¦½",
                    type=['csv'],
                    help="ä¸Šå‚³ä»»ä½•CSVæ–‡ä»¶ï¼Œå¿«é€ŸæŸ¥çœ‹å‰å¹¾è¡Œæ•¸æ“š"
                )
                
                if uploaded_csv:
                    try:
                        import pandas as pd
                        df = pd.read_csv(uploaded_csv, encoding='utf-8-sig')
                        
                        col1, col2 = st.columns(2)
                        with col1:
                            st.metric("ç¸½è¡Œæ•¸", len(df))
                        with col2:
                            st.metric("ç¸½æ¬„ä½", len(df.columns))
                        
                        st.write("**å‰10è¡Œé è¦½ï¼š**")
                        st.dataframe(df.head(10), use_container_width=True)
                        
                    except Exception as e:
                        st.error(f"âŒ é è¦½å¤±æ•—: {e}")
                
                st.divider()
                
                # æ¸…ç†å·¥å…·
                st.write("**æ¸…ç†å·¥å…·**")
                if st.button("ğŸ—‘ï¸ æ¸…ç†èˆŠçš„å°å‡ºæ–‡ä»¶", key="cleanup_exports"):
                    self._cleanup_old_exports()
    
    def _extract_time_from_filename(self, filename: str) -> str:
        """å¾æ–‡ä»¶åæå–æ™‚é–“"""
        try:
            import re
            match = re.search(r'_(\d{8}_\d{6})\.json$', filename)
            if match:
                time_str = match.group(1)
                # æ ¼å¼åŒ–ç‚ºå¯è®€æ™‚é–“
                date_part = time_str[:8]
                time_part = time_str[9:]
                return f"{date_part[:4]}-{date_part[4:6]}-{date_part[6:8]} {time_part[:2]}:{time_part[2:4]}:{time_part[4:6]}"
        except:
            pass
        return "æœªçŸ¥æ™‚é–“"
    
    def _generate_comparison_report(self, selected_files: List[str]):
        """ç”Ÿæˆå°æ¯”å ±å‘Š"""
        try:
            from common.csv_export_manager import CSVExportManager
            
            csv_manager = CSVExportManager()
            csv_file = csv_manager.export_comparison_report(selected_files)
            
            st.success("âœ… å°æ¯”å ±å‘Šç”ŸæˆæˆåŠŸï¼")
            st.info(f"ğŸ“ æ–‡ä»¶ä½ç½®: {csv_file}")
            
            # æä¾›ä¸‹è¼‰
            import os
            if os.path.exists(csv_file):
                with open(csv_file, 'r', encoding='utf-8-sig') as f:
                    csv_content = f.read()
                
                st.download_button(
                    label="ğŸ“¥ ä¸‹è¼‰å°æ¯”å ±å‘Š",
                    data=csv_content,
                    file_name=os.path.basename(csv_file),
                    mime="text/csv"
                )
                
                # é¡¯ç¤ºæ‘˜è¦
                st.write("**ğŸ“Š å°æ¯”æ‘˜è¦ï¼š**")
                import pandas as pd
                df = pd.read_csv(csv_file, encoding='utf-8-sig')
                
                # é¡¯ç¤ºå®Œæ•´è¡¨æ ¼
                st.dataframe(df, use_container_width=True)
                
                # é¡¯ç¤ºé—œéµæŒ‡æ¨™å°æ¯”
                if len(df) >= 2:
                    st.write("**ğŸ” é—œéµæŒ‡æ¨™åˆ†æï¼š**")
                    
                    col1, col2, col3 = st.columns(3)
                    
                    with col1:
                        avg_success_rate = df['æˆåŠŸç‡(%)'].mean()
                        max_success_rate = df['æˆåŠŸç‡(%)'].max()
                        min_success_rate = df['æˆåŠŸç‡(%)'].min()
                        st.metric("å¹³å‡æˆåŠŸç‡", f"{avg_success_rate:.1f}%", 
                                 f"{max_success_rate - min_success_rate:.1f}% å·®è·")
                    
                    with col2:
                        if 'ç¸½è€—æ™‚(ç§’)' in df.columns:
                            avg_time = df['ç¸½è€—æ™‚(ç§’)'].mean()
                            fastest = df['ç¸½è€—æ™‚(ç§’)'].min()
                            slowest = df['ç¸½è€—æ™‚(ç§’)'].max()
                            st.metric("å¹³å‡è€—æ™‚", f"{avg_time:.1f}s", 
                                     f"{slowest - fastest:.1f}s å·®è·")
                    
                    with col3:
                        if 'è§€çœ‹æ•¸æå–ç‡(%)' in df.columns:
                            avg_views_rate = df['è§€çœ‹æ•¸æå–ç‡(%)'].mean()
                            st.metric("å¹³å‡è§€çœ‹æ•¸æå–ç‡", f"{avg_views_rate:.1f}%")
                
                # é¡¯ç¤ºè¶¨å‹¢åˆ†æ
                if len(df) >= 3:
                    st.write("**ğŸ“ˆ è¶¨å‹¢åˆ†æï¼š**")
                    
                    # æŒ‰æ™‚é–“æ’åº
                    df_sorted = df.sort_values('çˆ¬å–æ™‚é–“') if 'çˆ¬å–æ™‚é–“' in df.columns else df
                    
                    # æˆåŠŸç‡è¶¨å‹¢
                    success_trend = df_sorted['æˆåŠŸç‡(%)'].diff().iloc[-1] if len(df_sorted) > 1 else 0
                    if success_trend > 0:
                        st.success(f"ğŸ“ˆ æˆåŠŸç‡å‘ˆä¸Šå‡è¶¨å‹¢ (+{success_trend:.1f}%)")
                    elif success_trend < 0:
                        st.error(f"ğŸ“‰ æˆåŠŸç‡å‘ˆä¸‹é™è¶¨å‹¢ ({success_trend:.1f}%)")
                    else:
                        st.info("ğŸ“Š æˆåŠŸç‡ä¿æŒç©©å®š")
                
        except Exception as e:
            st.error(f"âŒ ç”Ÿæˆå°æ¯”å ±å‘Šå¤±æ•—: {e}")
    
    def _export_all_latest_results(self):
        """å°å‡ºæ‰€æœ‰æœ€æ–°çµæœ"""
        try:
            import glob
            # æª¢æŸ¥æ–°çš„è³‡æ–™å¤¾ä½ç½®  
            extraction_dir = Path("extraction_results")
            if extraction_dir.exists():
                json_files = list(extraction_dir.glob("realtime_extraction_results_*.json"))
            else:
                json_files = [Path(f) for f in glob.glob("realtime_extraction_results_*.json")]
            
            if not json_files:
                st.warning("âš ï¸ æœªæ‰¾åˆ°ä»»ä½•çˆ¬å–çµæœæ–‡ä»¶")
                return
            
            # æ‰¾æœ€æ–°çš„æ–‡ä»¶
            latest_file = max(json_files, key=lambda f: f.stat().st_mtime)
            
            from common.csv_export_manager import CSVExportManager
            csv_manager = CSVExportManager()
            
            # ä½¿ç”¨é è¨­æ’åº
            default_sort = getattr(st.session_state, 'default_sort', 'è§€çœ‹æ•¸')
            sort_mapping = {"è§€çœ‹æ•¸": "views", "æŒ‰è®šæ•¸": "likes", "ç•™è¨€æ•¸": "comments", "æ™‚é–“é †åº": "none"}
            sort_by = sort_mapping.get(default_sort, "views")
            
            csv_file = csv_manager.export_current_session(latest_file, sort_by=sort_by)
            
            st.success("âœ… æœ€æ–°çµæœå°å‡ºæˆåŠŸï¼")
            st.info(f"ğŸ“ ä½¿ç”¨äº† {latest_file}")
            st.info(f"ğŸ“Š æŒ‰ {default_sort} æ’åº")
            
            # æä¾›ä¸‹è¼‰
            import os
            if os.path.exists(csv_file):
                with open(csv_file, 'r', encoding='utf-8-sig') as f:
                    csv_content = f.read()
                
                st.download_button(
                    label="ğŸ“¥ ä¸‹è¼‰æœ€æ–°çµæœCSV",
                    data=csv_content,
                    file_name=os.path.basename(csv_file),
                    mime="text/csv"
                )
                
        except Exception as e:
            st.error(f"âŒ å°å‡ºå¤±æ•—: {e}")
    
    def _export_all_account_stats(self):
        """å°å‡ºæ‰€æœ‰å¸³è™Ÿçµ±è¨ˆ"""
        try:
            from common.incremental_crawl_manager import IncrementalCrawlManager
            import asyncio
            
            # å‰µå»ºäº‹ä»¶å¾ªç’°
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            
            try:
                manager = IncrementalCrawlManager()
                
                # ç²å–æ‰€æœ‰å¸³è™Ÿ
                results = loop.run_until_complete(manager.db.fetch_all("""
                    SELECT DISTINCT username FROM crawl_state ORDER BY last_crawl_at DESC
                """))
                
                if not results:
                    st.warning("âš ï¸ æœªæ‰¾åˆ°ä»»ä½•çˆ¬å–è¨˜éŒ„")
                    return
                
                all_stats = []
                for row in results:
                    username = row['username']
                    summary = loop.run_until_complete(manager.get_crawl_summary(username))
                    
                    if 'error' not in summary:
                        checkpoint = summary['checkpoint']
                        stats = summary['statistics']
                        
                        all_stats.append({
                            'å¸³è™Ÿ': f"@{username}",
                            'æœ€æ–°è²¼æ–‡ID': checkpoint['latest_post_id'] or 'N/A',
                            'ç´¯è¨ˆçˆ¬å–': checkpoint['total_crawled'],
                            'è³‡æ–™åº«è²¼æ–‡æ•¸': stats['total_posts'],
                            'æœ‰è§€çœ‹æ•¸è²¼æ–‡': stats['posts_with_views'],
                            'å¹³å‡è§€çœ‹æ•¸': round(stats['avg_views'], 0),
                            'æœ€é«˜è§€çœ‹æ•¸': stats['max_views'],
                            'ä¸Šæ¬¡çˆ¬å–': checkpoint['last_crawl_at'].strftime('%Y-%m-%d %H:%M') if checkpoint['last_crawl_at'] else 'N/A'
                        })
                
                if all_stats:
                    # è½‰æ›ç‚ºCSV
                    import pandas as pd
                    df = pd.DataFrame(all_stats)
                    
                    from datetime import datetime
                    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                    csv_file = f"export_all_accounts_stats_{timestamp}.csv"
                    
                    # ä½¿ç”¨å­—ç¯€æµå°å‡ºä¸¦æä¾›ä¸‹è¼‰
                    import io
                    output = io.BytesIO()
                    df.to_csv(output, index=False, encoding='utf-8-sig')
                    csv_content = output.getvalue().encode('utf-8-sig')
                    
                    st.download_button(
                        label="ğŸ“¥ ä¸‹è¼‰æ‰€æœ‰å¸³è™Ÿçµ±è¨ˆ",
                        data=csv_content,
                        file_name=csv_file,
                        mime="text/csv",
                        help="ä¸‹è¼‰æ‰€æœ‰å¸³è™Ÿçš„çµ±è¨ˆæ•¸æ“š"
                    )
                    st.success("âœ… æ‰€æœ‰å¸³è™Ÿçµ±è¨ˆæº–å‚™å®Œæˆï¼")
                    
                    # é¡¯ç¤ºé è¦½
                    st.write("**çµ±è¨ˆé è¦½ï¼š**")
                    st.dataframe(df, use_container_width=True)
                    
                    # æä¾›ä¸‹è¼‰ - ä½¿ç”¨å­—ç¯€æµç¢ºä¿æ­£ç¢ºç·¨ç¢¼
                    import io
                    output = io.BytesIO()
                    df.to_csv(output, index=False, encoding='utf-8-sig')
                    csv_content = output.getvalue().encode('utf-8-sig')
                    st.download_button(
                        label="ğŸ“¥ ä¸‹è¼‰å¸³è™Ÿçµ±è¨ˆ",
                        data=csv_content,
                        file_name=csv_file,
                        mime="text/csv"
                    )
                else:
                    st.warning("âš ï¸ æœªæ‰¾åˆ°æœ‰æ•ˆçš„çµ±è¨ˆæ•¸æ“š")
                    
            finally:
                loop.close()
                
        except Exception as e:
            st.error(f"âŒ å°å‡ºå¸³è™Ÿçµ±è¨ˆå¤±æ•—: {e}")
    
    def _cleanup_old_exports(self):
        """æ¸…ç†èˆŠçš„å°å‡ºæ–‡ä»¶"""
        try:
            import glob
            import os
            from datetime import datetime, timedelta
            
            # æ‰¾åˆ°æ‰€æœ‰å°å‡ºæ–‡ä»¶
            export_patterns = [
                "export_current_*.csv",
                "export_history_*.csv", 
                "export_analysis_*.csv",
                "export_comparison_*.csv"
            ]
            
            old_files = []
            cutoff_date = datetime.now() - timedelta(days=7)  # 7å¤©å‰
            
            for pattern in export_patterns:
                files = glob.glob(pattern)
                for file in files:
                    file_time = datetime.fromtimestamp(os.path.getmtime(file))
                    if file_time < cutoff_date:
                        old_files.append(file)
            
            if old_files:
                st.write(f"ğŸ” æ‰¾åˆ° {len(old_files)} å€‹7å¤©å‰çš„å°å‡ºæ–‡ä»¶ï¼š")
                
                for file in old_files[:5]:  # åªé¡¯ç¤ºå‰5å€‹
                    st.text(f"- {file}")
                
                if len(old_files) > 5:
                    st.text(f"... ä»¥åŠå…¶ä»– {len(old_files) - 5} å€‹æ–‡ä»¶")
                
                if st.button("ğŸ—‘ï¸ ç¢ºèªåˆªé™¤", key="confirm_cleanup"):
                    deleted_count = 0
                    for file in old_files:
                        try:
                            os.remove(file)
                            deleted_count += 1
                        except:
                            pass
                    
                    st.success(f"âœ… å·²åˆªé™¤ {deleted_count} å€‹èˆŠæ–‡ä»¶")
            else:
                st.info("âœ¨ æ²’æœ‰æ‰¾åˆ°éœ€è¦æ¸…ç†çš„èˆŠæ–‡ä»¶")
                
        except Exception as e:
            st.error(f"âŒ æ¸…ç†å¤±æ•—: {e}")
    
    def _show_detailed_table(self, results: List[Dict]):
        """é¡¯ç¤ºè©³ç´°çµæœè¡¨æ ¼"""
        st.subheader("ğŸ“‹ è©³ç´°çµæœ")
        
        # æº–å‚™è¡¨æ ¼æ•¸æ“š
        table_data = []
        for r in results:
            table_data.append({
                "è²¼æ–‡ID": r.get('post_id', 'N/A'),
                "è§€çœ‹æ•¸": r.get('views', 'N/A'),
                "æŒ‰è®šæ•¸": r.get('likes', 'N/A'),
                "ç•™è¨€æ•¸": r.get('comments', 'N/A'),
                "è½‰ç™¼æ•¸": r.get('reposts', 'N/A'),
                "åˆ†äº«æ•¸": r.get('shares', 'N/A'),
                "å…§å®¹é è¦½": (r.get('content', '')[:50] + "...") if r.get('content') else 'N/A',
                "ä¾†æº": r.get('source', 'N/A'),
                "é‡æ–°æå–": "âœ…" if r.get('reextracted', False) else ""
            })
        
        # é¡¯ç¤ºè¡¨æ ¼
        st.dataframe(
            table_data,
            use_container_width=True,
            height=400
        )
        
        # äº’å‹•æ•¸æ“šåˆ†æ
        if st.checkbox("ğŸ“ˆ äº’å‹•æ•¸æ“šåˆ†æ", key="show_engagement_analysis"):
            self._show_engagement_analysis(results)
    
    def _show_engagement_analysis(self, results: List[Dict]):
        """é¡¯ç¤ºäº’å‹•æ•¸æ“šåˆ†æ"""
        st.subheader("ğŸ“ˆ äº’å‹•æ•¸æ“šåˆ†æ")
        
        # æ”¶é›†æœ‰æ•ˆçš„äº’å‹•æ•¸æ“š
        valid_results = [r for r in results if isinstance(r, dict) and r.get('has_views') and r.get('has_likes')]
        
        if not valid_results:
            st.warning("ç„¡è¶³å¤ çš„äº’å‹•æ•¸æ“šé€²è¡Œåˆ†æ")
            return
        
        # ç°¡å–®çµ±è¨ˆ
        avg_likes = []
        avg_comments = []
        for r in valid_results:
            if r.get('likes') and r['likes'] != 'N/A':
                try:
                    # ç°¡åŒ–çš„æ•¸å­—è½‰æ›
                    likes_str = str(r['likes']).replace('K', '000').replace('M', '000000')
                    if likes_str.replace('.', '').isdigit():
                        avg_likes.append(float(likes_str))
                except:
                    pass
        
        if avg_likes:
            col1, col2 = st.columns(2)
            with col1:
                st.metric("å¹³å‡æŒ‰è®šæ•¸", f"{sum(avg_likes)/len(avg_likes):.0f}")
            with col2:
                st.metric("æœ€é«˜æŒ‰è®šæ•¸", f"{max(avg_likes):.0f}")
    
    def _render_cached_stats(self, stats):
        """æ¸²æŸ“ç·©å­˜çš„çµ±è¨ˆä¿¡æ¯ï¼Œä¸¦æ•´åˆæ–°çš„ç”¨æˆ¶ç®¡ç† UI"""
        total_stats = stats.get("total_stats", {})
        if total_stats:
            st.info(f"""
            **ğŸ“ˆ ç¸½é«”çµ±è¨ˆ (Realtime)**
            - ğŸ“Š ç¸½è²¼æ–‡æ•¸: {total_stats.get('total_posts', 0):,}
            - ğŸ‘¥ å·²çˆ¬å–ç”¨æˆ¶: {total_stats.get('total_users', 0)} å€‹
            - â° æœ€å¾Œæ´»å‹•: {str(total_stats.get('latest_activity', 'N/A'))[:16] if total_stats.get('latest_activity') else 'N/A'}
            """)
        
        user_stats = stats.get("user_stats", [])
        if user_stats:
            st.write("**ğŸ‘¥ å„ç”¨æˆ¶çµ±è¨ˆ (Realtime):**")
            
            import pandas as pd
            df_data = [{
                "ç”¨æˆ¶å": f"@{user.get('username', 'N/A')}",
                "è²¼æ–‡æ•¸": f"{user.get('post_count', 0):,}",
                "æœ€å¾Œçˆ¬å–": str(user.get('latest_crawl', 'N/A'))[:16] if user.get('latest_crawl') else 'N/A'
            } for user in user_stats]

            st.dataframe(
                pd.DataFrame(df_data),
                use_container_width=True,
                hide_index=True,
                height=min(300, len(df_data) * 35 + 38)
            )
            
            # --- ç”¨æˆ¶è³‡æ–™ç®¡ç† ---
            st.markdown("---")
            with st.expander("ğŸ—‚ï¸ ç”¨æˆ¶è³‡æ–™ç®¡ç† (Realtime)", expanded=False):
                user_options = [user.get('username') for user in user_stats if user.get('username')]
                
                # ä½¿ç”¨ session state æŒä¹…åŒ–é¸æ“‡
                if 'realtime_selected_user' not in st.session_state or st.session_state.realtime_selected_user not in user_options:
                    st.session_state.realtime_selected_user = user_options[0] if user_options else None

                selected_user = st.selectbox(
                    "é¸æ“‡è¦ç®¡ç†çš„ç”¨æˆ¶:",
                    options=user_options,
                    key="realtime_user_selector",
                    index=user_options.index(st.session_state.realtime_selected_user) if st.session_state.realtime_selected_user in user_options else 0,
                )

                if selected_user and st.session_state.realtime_selected_user != selected_user:
                    st.session_state.realtime_selected_user = selected_user
                    if 'realtime_confirm_delete_user' in st.session_state:
                        del st.session_state['realtime_confirm_delete_user']
                    st.rerun()

                if selected_user:
                    selected_user_info = next((u for u in user_stats if u.get('username') == selected_user), None)
                    if selected_user_info:
                        st.info(f"""
                        **ğŸ“‹ ç”¨æˆ¶ @{selected_user} çš„è©³ç´°ä¿¡æ¯:**
                        - ğŸ“Š è²¼æ–‡ç¸½æ•¸: {selected_user_info.get('post_count', 0):,} å€‹
                        - â° æœ€å¾Œçˆ¬å–: {str(selected_user_info.get('latest_crawl', 'N/A'))[:16] if selected_user_info.get('latest_crawl') else 'N/A'}
                        """)
                    
                    col1, col2 = st.columns(2)
                    with col1:
                        if st.button("ğŸ“Š å°å‡ºCSV", key=f"realtime_export_csv_{selected_user}", use_container_width=True):
                            self._export_user_csv(selected_user)
                    with col2:
                        self.handle_delete_button(selected_user) # å‘¼å«æ–°çš„åˆªé™¤è™•ç†å™¨
        else:
            st.warning("ğŸ“ Realtime è³‡æ–™åº«ä¸­æš«ç„¡çˆ¬å–è¨˜éŒ„")
    
    def _save_results_to_database(self):
        """å°‡ç•¶å‰çˆ¬å–çµæœä¿å­˜åˆ°è³‡æ–™åº«"""
        if 'realtime_results' not in st.session_state:
            st.error("âŒ æ²’æœ‰å¯ä¿å­˜çš„çµæœ")
            return
        
        # å¾session stateç²å–çµæœ
        realtime_results = st.session_state.realtime_results
        
        # æª¢æŸ¥resultsçš„æ ¼å¼ï¼Œå¦‚æœæ˜¯å­—å…¸å‰‡æå–resultsåˆ—è¡¨
        if isinstance(realtime_results, dict):
            results = realtime_results.get('results', [])
            target_username = realtime_results.get('target_username', '')
        else:
            results = realtime_results if realtime_results else []
            target_username = results[0].get('username', '') if results else ''
        
        if not results:
            st.error("âŒ æ²’æœ‰æ‰¾åˆ°å¯ä¿å­˜çš„çµæœ")
            return
        
        if not target_username:
            st.error("âŒ ç„¡æ³•è­˜åˆ¥ç›®æ¨™ç”¨æˆ¶å")
            return
        
        try:
            import subprocess
            import json
            import sys
            import os
            import tempfile
            
            # å‰µå»ºä¿å­˜è…³æœ¬
            save_script_content = f'''
import asyncio
import sys
import os
import json
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from common.incremental_crawl_manager import IncrementalCrawlManager

async def save_to_database():
    crawl_manager = IncrementalCrawlManager()
    try:
        await crawl_manager.db.init_pool()
        
        # æº–å‚™çµæœæ•¸æ“š
        results = {json.dumps(results, ensure_ascii=False)}
        target_username = "{target_username}"
        
        # ä¿å­˜çµæœåˆ°è³‡æ–™åº«
        saved_count = await crawl_manager.save_quick_crawl_results(results, target_username)
        
        # æ›´æ–°æª¢æŸ¥é»ï¼ˆä½¿ç”¨æœ€æ–°çš„è²¼æ–‡IDï¼‰
        if results and saved_count > 0:
            latest_post_id = results[0].get('post_id')  # ç¬¬ä¸€å€‹æ˜¯æœ€æ–°çš„
            if latest_post_id:
                await crawl_manager.update_crawl_checkpoint(
                    target_username, 
                    latest_post_id, 
                    saved_count
                )
        
        result = {{
            "success": True,
            "saved_count": saved_count,
            "target_username": target_username
        }}
        
        print(json.dumps(result))
        
    except Exception as e:
        print(json.dumps({"success": False, "error": str(e)}))
    finally:
        await crawl_manager.db.close_pool()

if __name__ == "__main__":
    asyncio.run(save_to_database())
'''
            
            # å¯«å…¥è‡¨æ™‚æ–‡ä»¶
            with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False, encoding='utf-8') as f:
                f.write(save_script_content)
                temp_script = f.name
            
            try:
                # åŸ·è¡Œä¿å­˜è…³æœ¬
                with st.spinner(f"ğŸ’¾ æ­£åœ¨ä¿å­˜ {len(results)} å€‹è²¼æ–‡åˆ°è³‡æ–™åº«..."):
                    result = subprocess.run(
                        [sys.executable, temp_script],
                        capture_output=True,
                        text=True,
                        encoding='utf-8',
                        cwd=os.path.dirname(os.path.dirname(os.path.dirname(__file__))),
                        timeout=60
                    )
                
                if result.returncode == 0 and result.stdout.strip():
                    save_result = json.loads(result.stdout.strip())
                    
                    if save_result.get("success"):
                        saved_count = save_result.get("saved_count", 0)
                        
                        st.success(f"""
                        âœ… **ä¿å­˜æˆåŠŸï¼**
                        
                        å·²æˆåŠŸå°‡ @{target_username} çš„è²¼æ–‡ä¿å­˜åˆ°è³‡æ–™åº«ï¼š
                        - ğŸ’¾ ä¿å­˜è²¼æ–‡æ•¸: {saved_count} å€‹
                        - ğŸ”„ æª¢æŸ¥é»å·²æ›´æ–°
                        """)
                        
                        # æ›´æ–°session stateï¼Œæ¨™è¨˜ç‚ºå·²ä¿å­˜
                        if isinstance(st.session_state.realtime_results, dict):
                            st.session_state.realtime_results['database_saved'] = True
                            st.session_state.realtime_results['database_saved_count'] = saved_count
                        
                        # æ¸…ç†è³‡æ–™åº«çµ±è¨ˆç·©å­˜ï¼Œä¸‹æ¬¡æŸ¥çœ‹æœƒæ›´æ–°
                        if 'db_stats_cache' in st.session_state:
                            del st.session_state.db_stats_cache
                        
                        st.info("ğŸ“Š è³‡æ–™åº«çµ±è¨ˆå·²æ›´æ–°ï¼Œæ‚¨å¯ä»¥é»æ“Šåˆ·æ–°æŒ‰éˆ•æŸ¥çœ‹æœ€æ–°æ•¸æ“š")
                        
                    else:
                        st.error(f"âŒ ä¿å­˜å¤±æ•—: {save_result.get('error', 'æœªçŸ¥éŒ¯èª¤')}")
                else:
                    st.error(f"âŒ ä¿å­˜è…³æœ¬åŸ·è¡Œå¤±æ•—")
                    if result.stderr:
                        st.text(f"éŒ¯èª¤è©³æƒ…: {result.stderr}")
                        
            finally:
                # æ¸…ç†è‡¨æ™‚æ–‡ä»¶
                try:
                    os.unlink(temp_script)
                except:
                    pass
                    
        except Exception as e:
            st.error(f"âŒ ä¿å­˜æ“ä½œå¤±æ•—: {str(e)}")