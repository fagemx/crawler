"""
å¯¦æ™‚çˆ¬èŸ²çµ„ä»¶ - æ™ºèƒ½URLæ”¶é›† + è¼ªè¿´ç­–ç•¥æå–
åŒ…å«å®Œæ•´äº’å‹•æ•¸æ“šæå–åŠŸèƒ½
"""

import streamlit as st
import asyncio
import json
import time
import threading
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, Optional, List
import sys
import os

# æ·»åŠ å°ˆæ¡ˆæ ¹ç›®éŒ„åˆ° Python è·¯å¾‘
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

class RealtimeCrawlerComponent:
    def __init__(self):
        self.is_running = False
        self.current_task = None
        
    def render(self):
        """æ¸²æŸ“å¯¦æ™‚çˆ¬èŸ²çµ„ä»¶"""
        st.header("ğŸš€ å¯¦æ™‚æ™ºèƒ½çˆ¬èŸ²")
        st.markdown("**æ™ºèƒ½æ»¾å‹•æ”¶é›†URLs + è¼ªè¿´ç­–ç•¥å¿«é€Ÿæå– + å®Œæ•´äº’å‹•æ•¸æ“š**")
        
        # åƒæ•¸è¨­å®šå€åŸŸ
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("âš™ï¸ çˆ¬å–è¨­å®š")
            username = st.text_input(
                "ç›®æ¨™å¸³è™Ÿ", 
                value="gvmonthly",
                help="è¦çˆ¬å–çš„Threadså¸³è™Ÿç”¨æˆ¶å",
                key="realtime_username"
            )
            
            max_posts = st.number_input(
                "çˆ¬å–æ•¸é‡", 
                min_value=1, 
                max_value=500, 
                value=50,
                help="è¦çˆ¬å–çš„è²¼æ–‡æ•¸é‡",
                key="realtime_max_posts"
            )
            
            # å¢é‡çˆ¬å–æ¨¡å¼é¸é …
            crawl_mode = st.radio(
                "çˆ¬å–æ¨¡å¼",
                options=["å¢é‡çˆ¬å–", "å…¨é‡çˆ¬å–"],
                index=0,
                help="å¢é‡çˆ¬å–ï¼šåªæŠ“å–æ–°è²¼æ–‡ï¼Œé¿å…é‡è¤‡ï¼›å…¨é‡çˆ¬å–ï¼šæŠ“å–æ‰€æœ‰æ‰¾åˆ°çš„è²¼æ–‡",
                key="crawl_mode"
            )
            
            # é¡¯ç¤ºçˆ¬å–éç¨‹æ—¥èªŒï¼ˆç§»åˆ°é€™è£¡ï¼Œé¿å…é‡æ–°æ¸²æŸ“å½±éŸ¿ï¼‰
            if 'realtime_crawl_logs' in st.session_state and st.session_state.realtime_crawl_logs:
                with st.expander("ğŸ“‹ çˆ¬å–éç¨‹æ—¥èªŒ", expanded=False):
                    # é¡¯ç¤ºæœ€å¾Œ50è¡Œæ—¥èªŒ
                    log_lines = st.session_state.realtime_crawl_logs[-50:] if len(st.session_state.realtime_crawl_logs) > 50 else st.session_state.realtime_crawl_logs
                    st.code('\n'.join(log_lines), language='text')
            
        with col2:
            col_title, col_refresh = st.columns([3, 1])
            with col_title:
                st.subheader("ğŸ“Š è³‡æ–™åº«çµ±è¨ˆ")
            with col_refresh:
                if st.button("ğŸ”„ åˆ·æ–°", key="refresh_db_stats", help="åˆ·æ–°è³‡æ–™åº«çµ±è¨ˆä¿¡æ¯", type="secondary"):
                    # æ¸…ç†å¯èƒ½çš„ç·©å­˜ç‹€æ…‹
                    if 'db_stats_cache' in st.session_state:
                        del st.session_state.db_stats_cache
                    st.success("ğŸ”„ æ­£åœ¨åˆ·æ–°çµ±è¨ˆ...")
                    st.rerun()  # é‡æ–°é‹è¡Œé é¢ä¾†åˆ·æ–°çµ±è¨ˆ
            
            self._display_database_stats()
        
        # æ§åˆ¶æŒ‰éˆ•
        col1, col2, col3 = st.columns([1, 1, 2])
        
        with col1:
            if st.button("ğŸš€ é–‹å§‹çˆ¬å–", key="start_realtime"):
                with st.spinner("æ­£åœ¨åŸ·è¡Œçˆ¬å–..."):
                    is_incremental = crawl_mode == "å¢é‡çˆ¬å–"
                    self._execute_crawling_simple(username, max_posts, is_incremental)
                
        with col2:
            # è¼‰å…¥CSVæ–‡ä»¶åŠŸèƒ½
            uploaded_file = st.file_uploader(
                "ğŸ“ è¼‰å…¥CSVæ–‡ä»¶", 
                type=['csv'], 
                key="csv_uploader",
                help="ä¸Šå‚³ä¹‹å‰å°å‡ºçš„CSVæ–‡ä»¶ä¾†æŸ¥çœ‹çµæœ"
            )
            if uploaded_file is not None:
                self._load_csv_file(uploaded_file)
        
        with col3:
            # æ¸…é™¤çµæœæŒ‰éˆ• (åªåœ¨æœ‰çµæœæ™‚é¡¯ç¤º)
            if 'realtime_results' in st.session_state:
                if st.button("ğŸ—‘ï¸ æ¸…é™¤çµæœ", key="clear_results", help="æ¸…é™¤ç•¶å‰é¡¯ç¤ºçš„çµæœ"):
                    self._clear_results()
        
        # çµæœé¡¯ç¤º
        self._render_results_area()
    
    def _load_csv_file(self, uploaded_file):
        """è¼‰å…¥CSVæ–‡ä»¶ä¸¦è½‰æ›ç‚ºçµæœæ ¼å¼"""
        try:
            import pandas as pd
            import io
            
            # è®€å–CSVæ–‡ä»¶
            content = uploaded_file.getvalue()
            df = pd.read_csv(io.StringIO(content.decode('utf-8-sig')))
            
            # æª¢æŸ¥CSVæ ¼å¼æ˜¯å¦æ­£ç¢º
            required_columns = ['username', 'post_id', 'content', 'views']
            missing_columns = [col for col in required_columns if col not in df.columns]
            
            if missing_columns:
                st.error(f"âŒ CSVæ ¼å¼ä¸æ­£ç¢ºï¼Œç¼ºå°‘æ¬„ä½: {', '.join(missing_columns)}")
                return
            
            # è½‰æ›ç‚ºçµæœæ ¼å¼
            results = []
            for _, row in df.iterrows():
                # è½‰æ›æ•¸æ“šä¸¦è™•ç†ç©ºå€¼
                views = str(row.get('views', '')).strip()
                likes = str(row.get('likes', '')).strip()
                comments = str(row.get('comments', '')).strip()
                reposts = str(row.get('reposts', '')).strip()
                shares = str(row.get('shares', '')).strip()
                content = str(row.get('content', '')).strip()
                
                result = {
                    'username': str(row.get('username', '')),
                    'post_id': str(row.get('post_id', '')),
                    'content': content,
                    'views': views,
                    'likes': likes,
                    'comments': comments,
                    'reposts': reposts,
                    'shares': shares,
                    'url': str(row.get('url', '')),
                    'source': str(row.get('source', 'csv_import')),
                    'created_at': str(row.get('created_at', '')),
                    'fetched_at': str(row.get('fetched_at', '')),
                    'success': True,
                    # æ·»åŠ has_*æ¬„ä½ä»¥å…¼å®¹é¡¯ç¤ºé‚è¼¯
                    'has_views': bool(views and views != 'nan' and views != ''),
                    'has_content': bool(content and content != 'nan' and content != ''),
                    'has_likes': bool(likes and likes != 'nan' and likes != ''),
                    'has_comments': bool(comments and comments != 'nan' and comments != ''),
                    'has_reposts': bool(reposts and reposts != 'nan' and reposts != ''),
                    'has_shares': bool(shares and shares != 'nan' and shares != ''),
                    'content_length': len(content) if content else 0,
                    'extracted_at': datetime.now().isoformat()
                }
                results.append(result)
            
            # ä¿å­˜åˆ°æœƒè©±ç‹€æ…‹
            st.session_state.realtime_results = {
                'results': results,
                'total_count': len(results),
                'username': results[0]['username'] if results else '',
                'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'source': f"CSVæ–‡ä»¶: {uploaded_file.name}"
            }
            
            st.success(f"âœ… æˆåŠŸè¼‰å…¥ {len(results)} ç­†è¨˜éŒ„")
            st.info(f"ğŸ“Š ä¾†æº: {uploaded_file.name}")
            
        except Exception as e:
            st.error(f"âŒ è¼‰å…¥CSVæ–‡ä»¶å¤±æ•—: {str(e)}")
    
    def _execute_crawling_simple(self, username: str, max_posts: int, is_incremental: bool = True):
        """ç°¡åŒ–çš„çˆ¬å–åŸ·è¡Œæ–¹æ³• - ä½¿ç”¨åŒæ­¥ç‰ˆæœ¬é¿å…asyncioè¡çª"""
        if not username.strip():
            st.error("è«‹è¼¸å…¥ç›®æ¨™å¸³è™Ÿï¼")
            return
            
        try:
            mode_text = "å¢é‡çˆ¬å–" if is_incremental else "å…¨é‡çˆ¬å–"
            st.info(f"ğŸ”„ æ­£åœ¨åŸ·è¡Œ{mode_text}ï¼Œè«‹ç¨å€™...")
            
            # ä½¿ç”¨subprocessä¾†é¿å…asyncioè¡çª
            import subprocess
            import json
            import sys
            import os
            
            # æ§‹å»ºå‘½ä»¤
            script_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), '..', 'scripts', 'realtime_crawler_extractor.py')
            
            # ä¿®æ”¹è…³æœ¬ä»¥æ¥å—å‘½ä»¤è¡Œåƒæ•¸
            cmd = [
                sys.executable, 
                script_path,
                '--username', username,
                '--max_posts', str(max_posts)
            ]
            
            # æ·»åŠ çˆ¬å–æ¨¡å¼åƒæ•¸
            if is_incremental:
                cmd.append('--incremental')  # å¢é‡æ¨¡å¼
            else:
                cmd.append('--full')  # å…¨é‡æ¨¡å¼
            
            # åŸ·è¡Œè…³æœ¬ - è¨­ç½®UTF-8ç·¨ç¢¼
            import locale
            env = os.environ.copy()
            env['PYTHONIOENCODING'] = 'utf-8'
            env['PYTHONUTF8'] = '1'
            
            # å‰µå»ºä¸€å€‹æ—¥å¿—å®¹å™¨ä¾†å¯¦æ™‚é¡¯ç¤ºè¼¸å‡º
            log_container = st.empty()
            # å°‡æ—¥èªŒä¿å­˜åˆ°æœƒè©±ç‹€æ…‹ï¼Œé¿å…é é¢é‡æ–°æ¸²æŸ“æ™‚ä¸Ÿå¤±
            # æ¯æ¬¡æ–°çš„çˆ¬å–é–‹å§‹æ™‚æ¸…ç©ºä¹‹å‰çš„æ—¥èªŒ
            st.session_state.realtime_crawl_logs = []
            log_text = st.session_state.realtime_crawl_logs
            
            with st.expander("ğŸ“‹ çˆ¬å–éç¨‹æ—¥å¿—", expanded=True):
                log_placeholder = st.empty()
                
                # ä½¿ç”¨Popenä¾†å¯¦æ™‚æ•ç²è¼¸å‡º
                process = subprocess.Popen(
                    cmd,
                    stdout=subprocess.PIPE,
                    stderr=subprocess.STDOUT,  # åˆä½µstderråˆ°stdout
                    text=True,
                    encoding='utf-8',
                    errors='replace',
                    env=env,
                    cwd=os.path.dirname(os.path.dirname(os.path.dirname(__file__))),
                    bufsize=1,  # è¡Œç·©è¡
                    universal_newlines=True
                )
                
                # å¯¦æ™‚è®€å–è¼¸å‡º
                all_output = []
                while True:
                    output = process.stdout.readline()
                    if output == '' and process.poll() is not None:
                        break
                    if output:
                        line = output.strip()
                        all_output.append(line)
                        log_text.append(line)
                        
                        # åªé¡¯ç¤ºæœ€å¾Œ30è¡Œï¼Œé¿å…ç•Œé¢éé•·
                        display_lines = log_text[-30:] if len(log_text) > 30 else log_text
                        log_placeholder.code('\n'.join(display_lines), language='text')
                
                # ç­‰å¾…é€²ç¨‹å®Œæˆ
                return_code = process.poll()
                
            if return_code == 0:
                # æˆåŠŸåŸ·è¡Œï¼Œå°‹æ‰¾æœ€æ–°çš„çµæœæ–‡ä»¶
                import glob
                from pathlib import Path
                
                # å…ˆæª¢æŸ¥æ–°çš„è³‡æ–™å¤¾ä½ç½®
                extraction_dir = Path("extraction_results")
                if extraction_dir.exists():
                    results_files = list(extraction_dir.glob("realtime_extraction_results_*.json"))
                else:
                    # å›é€€åˆ°æ ¹ç›®éŒ„æŸ¥æ‰¾ï¼ˆå‘å¾Œå…¼å®¹ï¼‰
                    results_files = [Path(f) for f in glob.glob("realtime_extraction_results_*.json")]
                
                if results_files:
                    # å–æœ€æ–°çš„æ–‡ä»¶
                    latest_file = max(results_files, key=lambda f: f.stat().st_mtime)
                    
                    # è®€å–çµæœ
                    with open(latest_file, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    
                    # ä¿å­˜åˆ°session_state
                    st.session_state.realtime_results = data.get('results', [])
                    st.session_state.realtime_results_file = latest_file
                    
                    total_processed = len(st.session_state.realtime_results)
                    st.success(f"âœ… çˆ¬å–å®Œæˆï¼è™•ç†äº† {total_processed} ç¯‡è²¼æ–‡")
                    
                    # æ¸…ç†è³‡æ–™åº«çµ±è¨ˆç·©å­˜ï¼Œä¸‹æ¬¡æœƒè‡ªå‹•åˆ·æ–°
                    if 'db_stats_cache' in st.session_state:
                        del st.session_state.db_stats_cache
                    
                    st.info("ğŸ“Š å¢é‡çˆ¬å–å·²è‡ªå‹•ä¿å­˜åˆ°è³‡æ–™åº«ï¼Œæ‚¨å¯ä»¥é»æ“Šå³å´ã€ŒğŸ”„ åˆ·æ–°ã€æŸ¥çœ‹æ›´æ–°çš„çµ±è¨ˆ")
                    st.balloons()
                else:
                    st.error("âŒ æœªæ‰¾åˆ°çµæœæ–‡ä»¶")
            else:
                st.error(f"âŒ çˆ¬å–å¤±æ•— (è¿”å›ç¢¼: {return_code})")
                # é¡¯ç¤ºæœ€å¾Œçš„éŒ¯èª¤æ—¥å¿—
                if all_output:
                    error_lines = [line for line in all_output if 'âŒ' in line or 'Error' in line or 'Exception' in line]
                    if error_lines:
                        st.error("éŒ¯èª¤è©³æƒ…ï¼š")
                        for error_line in error_lines[-5:]:  # é¡¯ç¤ºæœ€å¾Œ5æ¢éŒ¯èª¤
                            st.text(error_line)
                
        except Exception as e:
            st.error(f"âŒ åŸ·è¡ŒéŒ¯èª¤ï¼š{str(e)}")
            st.session_state.realtime_error = str(e)
    
    def _display_database_stats(self):
        """é¡¯ç¤ºè³‡æ–™åº«çµ±è¨ˆä¿¡æ¯"""
        # æª¢æŸ¥æ˜¯å¦æœ‰ç·©å­˜çš„çµ±è¨ˆä¿¡æ¯
        if 'db_stats_cache' in st.session_state:
            self._render_cached_stats(st.session_state.db_stats_cache)
            return
        
        try:
            # ä½¿ç”¨ asyncio å’Œ subprocess ä¾†ç²å–è³‡æ–™åº«çµ±è¨ˆ
            import subprocess
            import json
            import sys
            import os
            
            # å‰µå»ºä¸€å€‹è‡¨æ™‚è…³æœ¬ä¾†ç²å–è³‡æ–™åº«çµ±è¨ˆ
            script_content = '''
import asyncio
import sys
import os
import json
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from common.incremental_crawl_manager import IncrementalCrawlManager

async def get_database_stats():
    crawl_manager = IncrementalCrawlManager()
    try:
        await crawl_manager.db.init_pool()
        
        # ç²å–æ‰€æœ‰ç”¨æˆ¶çš„çµ±è¨ˆä¿¡æ¯
        async with crawl_manager.db.get_connection() as conn:
            # çµ±è¨ˆæ¯å€‹ç”¨æˆ¶çš„è²¼æ–‡æ•¸é‡
            user_stats = await conn.fetch("""
                SELECT 
                    username,
                    COUNT(*) as post_count,
                    MAX(created_at) as latest_crawl,
                    MIN(created_at) as first_crawl
                FROM post_metrics_sql 
                GROUP BY username 
                ORDER BY post_count DESC, latest_crawl DESC
                LIMIT 20
            """)
            
            # ç¸½é«”çµ±è¨ˆ
            total_stats = await conn.fetchrow("""
                SELECT 
                    COUNT(*) as total_posts,
                    COUNT(DISTINCT username) as total_users,
                    MAX(created_at) as latest_activity
                FROM post_metrics_sql
            """)
            
            stats = {
                "total_stats": dict(total_stats) if total_stats else {},
                "user_stats": [dict(row) for row in user_stats] if user_stats else []
            }
            
            print(json.dumps(stats, default=str))
            
    except Exception as e:
        print(json.dumps({"error": str(e)}))
    finally:
        await crawl_manager.db.close_pool()

if __name__ == "__main__":
    asyncio.run(get_database_stats())
'''
            
            # å°‡è…³æœ¬å¯«å…¥è‡¨æ™‚æ–‡ä»¶
            import tempfile
            with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False, encoding='utf-8') as f:
                f.write(script_content)
                temp_script = f.name
            
            try:
                # åŸ·è¡Œè…³æœ¬ç²å–çµ±è¨ˆä¿¡æ¯
                result = subprocess.run(
                    [sys.executable, temp_script],
                    capture_output=True,
                    text=True,
                    encoding='utf-8',
                    cwd=os.path.dirname(os.path.dirname(os.path.dirname(__file__))),
                    timeout=10
                )
                
                if result.returncode == 0 and result.stdout.strip():
                    stats = json.loads(result.stdout.strip())
                    
                    if "error" in stats:
                        st.error(f"âŒ è³‡æ–™åº«éŒ¯èª¤: {stats['error']}")
                        return
                    
                    # ä¿å­˜åˆ°ç·©å­˜
                    st.session_state.db_stats_cache = stats
                    
                    # æ¸²æŸ“çµ±è¨ˆä¿¡æ¯
                    self._render_cached_stats(stats)
                    
                else:
                    st.warning("âš ï¸ ç„¡æ³•ç²å–è³‡æ–™åº«çµ±è¨ˆä¿¡æ¯")
                    if result.stderr:
                        st.text(f"éŒ¯èª¤: {result.stderr}")
                        
            finally:
                # æ¸…ç†è‡¨æ™‚æ–‡ä»¶
                try:
                    os.unlink(temp_script)
                except:
                    pass
                    
        except Exception as e:
            st.error(f"âŒ ç²å–çµ±è¨ˆä¿¡æ¯å¤±æ•—: {str(e)}")
    
    def _delete_user_data(self, username: str):
        """åˆªé™¤æŒ‡å®šç”¨æˆ¶çš„æ‰€æœ‰çˆ¬èŸ²è³‡æ–™"""
        if not username:
            st.error("âŒ è«‹é¸æ“‡ä¸€å€‹æœ‰æ•ˆçš„ç”¨æˆ¶")
            return
        
        # ä½¿ç”¨ç°¡åŒ–çš„ç¢ºèªé‚è¼¯ï¼Œé¿å…session stateè¤‡é›œæ€§
        import hashlib
        username_hash = hashlib.md5(username.encode()).hexdigest()[:8]
        
        # ç›´æ¥é¡¯ç¤ºç¢ºèªæŒ‰éˆ•ï¼Œä½¿ç”¨å”¯ä¸€çš„key
        st.error(f"âš ï¸ **å±éšªæ“ä½œç¢ºèª**")
        st.markdown(f"""
        æ‚¨å³å°‡åˆªé™¤ç”¨æˆ¶ **@{username}** çš„æ‰€æœ‰çˆ¬èŸ²è³‡æ–™ï¼ŒåŒ…æ‹¬ï¼š
        - æ‰€æœ‰è²¼æ–‡å…§å®¹  
        - è§€çœ‹æ•¸ã€æŒ‰è®šæ•¸ã€ç•™è¨€æ•¸ç­‰æŒ‡æ¨™
        - çˆ¬å–æ™‚é–“æˆ³è¨˜éŒ„
        - å¢é‡çˆ¬å–æª¢æŸ¥é»
        
        **æ­¤æ“ä½œç„¡æ³•å¾©åŸï¼**
        """)
        
        col1, col2 = st.columns(2)
        with col1:
            if st.button(f"âœ… ç¢ºèªåˆªé™¤ @{username}", type="primary", key=f"final_confirm_delete_{username_hash}"):
                # ç«‹å³åŸ·è¡Œåˆªé™¤
                self._execute_user_deletion(username)
        
        with col2:
            if st.button("âŒ å–æ¶ˆæ“ä½œ", key=f"cancel_delete_{username_hash}"):
                st.success("âœ… å·²å–æ¶ˆåˆªé™¤æ“ä½œ")
                return
    
    def _execute_user_deletion(self, username: str):
        """åŸ·è¡Œå¯¦éš›çš„ç”¨æˆ¶åˆªé™¤æ“ä½œ"""
        try:
            import subprocess
            import json
            import sys
            import os
            import tempfile
            import time
            
            # å‰µå»ºç°¡åŒ–çš„åˆªé™¤è…³æœ¬ï¼ˆåŸºæ–¼æ¸¬è©¦æˆåŠŸçš„é‚è¼¯ï¼‰
            delete_script_content = f'''
import asyncio
import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from common.incremental_crawl_manager import IncrementalCrawlManager

async def delete_user_data():
    crawl_manager = IncrementalCrawlManager()
    try:
        await crawl_manager.db.init_pool()
        
        async with crawl_manager.db.get_connection() as conn:
            # è¨ˆç®—è¦åˆªé™¤çš„æ•¸é‡
            posts_count = await conn.fetchval("""
                SELECT COUNT(*) FROM post_metrics_sql WHERE username = $1
            """, "{username}")
            
            crawl_state_count = await conn.fetchval("""
                SELECT COUNT(*) FROM crawl_state WHERE username = $1
            """, "{username}")
            
            # åŸ·è¡Œåˆªé™¤
            async with conn.transaction():
                await conn.execute("""
                    DELETE FROM post_metrics_sql WHERE username = $1
                """, "{username}")
                
                await conn.execute("""
                    DELETE FROM crawl_state WHERE username = $1
                """, "{username}")
            
            print(f"SUCCESS:{posts_count}:{crawl_state_count}")
            
    except Exception as e:
        print(f"ERROR:{str(e)}")
    finally:
        await crawl_manager.db.close_pool()

if __name__ == "__main__":
    asyncio.run(delete_user_data())
'''
            
            # å¯«å…¥è‡¨æ™‚æ–‡ä»¶
            with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False, encoding='utf-8') as f:
                f.write(delete_script_content)
                temp_script = f.name
            
            try:
                # åŸ·è¡Œåˆªé™¤è…³æœ¬
                with st.spinner(f"ğŸ—‘ï¸ æ­£åœ¨åˆªé™¤ç”¨æˆ¶ @{username} çš„è³‡æ–™..."):
                    result = subprocess.run(
                        [sys.executable, temp_script],
                        capture_output=True,
                        text=True,
                        encoding='utf-8',
                        cwd=os.path.dirname(os.path.dirname(os.path.dirname(__file__))),
                        timeout=30
                    )
                
                if result.returncode == 0 and result.stdout.strip():
                    output = result.stdout.strip()
                    if output.startswith("SUCCESS:"):
                        _, posts_count, crawl_state_count = output.split(":")
                        st.success(f"""
                        âœ… **åˆªé™¤æˆåŠŸï¼**
                        
                        ç”¨æˆ¶ @{username} çš„è³‡æ–™å·²è¢«å®Œå…¨åˆªé™¤ï¼š
                        - ğŸ—‘ï¸ åˆªé™¤è²¼æ–‡æ•¸: {posts_count} å€‹
                        - ğŸ—‘ï¸ åˆªé™¤çˆ¬å–è¨˜éŒ„: {crawl_state_count} å€‹
                        """)
                        
                        # åˆ·æ–°é é¢ä»¥æ›´æ–°çµ±è¨ˆ
                        st.info("ğŸ“Š æ­£åœ¨åˆ·æ–°çµ±è¨ˆè³‡æ–™...")
                        time.sleep(1)
                        st.rerun()
                    else:
                        st.error(f"âŒ åˆªé™¤å¤±æ•—: {output}")
                else:
                    st.error(f"âŒ åˆªé™¤è…³æœ¬åŸ·è¡Œå¤±æ•—")
                    if result.stderr:
                        st.text(f"éŒ¯èª¤è©³æƒ…: {result.stderr}")
                        
            finally:
                # æ¸…ç†è‡¨æ™‚æ–‡ä»¶
                try:
                    os.unlink(temp_script)
                except:
                    pass
                    
        except Exception as e:
            st.error(f"âŒ åˆªé™¤æ“ä½œå¤±æ•—: {str(e)}")
    
    def _export_user_csv(self, username: str):
        """å°å‡ºæŒ‡å®šç”¨æˆ¶çš„æ‰€æœ‰è²¼æ–‡ç‚ºCSVæ ¼å¼"""
        if not username:
            st.error("âŒ è«‹é¸æ“‡ä¸€å€‹æœ‰æ•ˆçš„ç”¨æˆ¶")
            return
        
        try:
            import subprocess
            import json
            import sys
            import os
            import tempfile
            from datetime import datetime
            
            # å‰µå»ºå°å‡ºè…³æœ¬
            export_script_content = f'''
import asyncio
import sys
import os
import json
import csv
from datetime import datetime
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from common.incremental_crawl_manager import IncrementalCrawlManager

async def export_user_csv(username):
    crawl_manager = IncrementalCrawlManager()
    try:
        await crawl_manager.db.init_pool()
        
        async with crawl_manager.db.get_connection() as conn:
            # æŸ¥è©¢ç”¨æˆ¶çš„æ‰€æœ‰è²¼æ–‡æ•¸æ“š
            posts = await conn.fetch("""
                SELECT 
                    post_id,
                    url,
                    content,
                    views_count,
                    likes_count,
                    comments_count,
                    reposts_count,
                    shares_count,
                    source,
                    created_at,
                    fetched_at
                FROM post_metrics_sql 
                WHERE username = $1
                ORDER BY created_at DESC
            """, username)
            
            if not posts:
                print(json.dumps({{"success": False, "error": "ç”¨æˆ¶æ²’æœ‰è²¼æ–‡è³‡æ–™"}}))
                return
            
            # æº–å‚™CSVæ–‡ä»¶è·¯å¾‘
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            csv_filename = f"user_posts_{{username}}_{{timestamp}}.csv"
            csv_filepath = os.path.join("exports", csv_filename)
            
            # ç¢ºä¿exportsç›®éŒ„å­˜åœ¨
            os.makedirs("exports", exist_ok=True)
            
            # å¯«å…¥CSVæ–‡ä»¶
            with open(csv_filepath, 'w', newline='', encoding='utf-8-sig') as csvfile:
                fieldnames = [
                    'username', 'post_id', 'url', 'content', 'views', 
                    'likes', 'comments', 'reposts', 'shares', 'source', 'created_at', 'fetched_at'
                ]
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                
                # å¯«å…¥æ¨™é¡Œè¡Œ
                writer.writeheader()
                
                # å¯«å…¥æ•¸æ“š
                for post in posts:
                    writer.writerow({{
                        'username': username,
                        'post_id': post['post_id'],
                        'url': post['url'],
                        'content': post['content'] or '',
                        'views': post['views_count'] or '',
                        'likes': post['likes_count'] or '',
                        'comments': post['comments_count'] or '',
                        'reposts': post['reposts_count'] or '',
                        'shares': post['shares_count'] or '',
                        'source': post['source'] or '',
                        'created_at': str(post['created_at']) if post['created_at'] else '',
                        'fetched_at': str(post['fetched_at']) if post['fetched_at'] else ''
                    }})
            
            result = {{
                "success": True,
                "csv_file": csv_filepath,
                "post_count": len(posts),
                "username": username
            }}
            
            print(json.dumps(result))
            
    except Exception as e:
        print(json.dumps({{"success": False, "error": str(e)}}))
    finally:
        await crawl_manager.db.close_pool()

if __name__ == "__main__":
    asyncio.run(export_user_csv("{username}"))
'''
            
            # å¯«å…¥è‡¨æ™‚æ–‡ä»¶
            with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False, encoding='utf-8') as f:
                f.write(export_script_content)
                temp_script = f.name
            
            try:
                # åŸ·è¡Œå°å‡ºè…³æœ¬
                with st.spinner(f"ğŸ“Š æ­£åœ¨å°å‡ºç”¨æˆ¶ @{username} çš„è²¼æ–‡è³‡æ–™..."):
                    result = subprocess.run(
                        [sys.executable, temp_script],
                        capture_output=True,
                        text=True,
                        encoding='utf-8',
                        cwd=os.path.dirname(os.path.dirname(os.path.dirname(__file__))),
                        timeout=60
                    )
                
                if result.returncode == 0 and result.stdout.strip():
                    export_result = json.loads(result.stdout.strip())
                    
                    if export_result.get("success"):
                        csv_file_path = export_result.get("csv_file")
                        post_count = export_result.get("post_count", 0)
                        
                        st.success(f"""
                        âœ… **å°å‡ºæˆåŠŸï¼**
                        
                        ç”¨æˆ¶ @{username} çš„è²¼æ–‡å·²å°å‡ºç‚ºCSVï¼š
                        - ğŸ“Š å°å‡ºè²¼æ–‡æ•¸: {post_count:,} å€‹
                        - ğŸ“ æ–‡ä»¶è·¯å¾‘: {csv_file_path}
                        """)
                        
                        # æä¾›ä¸‹è¼‰æŒ‰éˆ•
                        if os.path.exists(csv_file_path):
                            with open(csv_file_path, 'r', encoding='utf-8-sig') as f:
                                csv_content = f.read()
                            
                            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                            download_filename = f"user_posts_{username}_{timestamp}.csv"
                            
                            st.download_button(
                                label="ğŸ“¥ ä¸‹è¼‰CSVæ–‡ä»¶",
                                data=csv_content,
                                file_name=download_filename,
                                mime="text/csv",
                                key=f"download_user_csv_{username}"
                            )
                        
                    else:
                        st.error(f"âŒ å°å‡ºå¤±æ•—: {export_result.get('error', 'æœªçŸ¥éŒ¯èª¤')}")
                else:
                    st.error(f"âŒ å°å‡ºè…³æœ¬åŸ·è¡Œå¤±æ•—")
                    if result.stderr:
                        st.text(f"éŒ¯èª¤è©³æƒ…: {result.stderr}")
                        
            finally:
                # æ¸…ç†è‡¨æ™‚æ–‡ä»¶
                try:
                    os.unlink(temp_script)
                except:
                    pass
                    
        except Exception as e:
            st.error(f"âŒ å°å‡ºæ“ä½œå¤±æ•—: {str(e)}")
    
    def _show_json_download_button(self, results_file):
        """é¡¯ç¤ºJSONä¸‹è¼‰æŒ‰éˆ•"""
        if results_file and Path(results_file).exists():
            try:
                # è®€å–JSONæ–‡ä»¶å…§å®¹
                with open(results_file, 'r', encoding='utf-8') as f:
                    json_content = f.read()
                
                # ç”Ÿæˆä¸‹è¼‰æ–‡ä»¶åï¼ˆåŒ…å«æ™‚é–“æˆ³ï¼‰
                file_path = Path(results_file)
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                download_filename = f"crawl_results_{timestamp}.json"
                
                # ä½¿ç”¨ st.download_button æä¾›ä¸‹è¼‰
                st.download_button(
                    label="ğŸ’¾ ä¸‹è¼‰JSON",
                    data=json_content,
                    file_name=download_filename,
                    mime="application/json",
                    help="ä¸‹è¼‰çˆ¬å–çµæœJSONæ–‡ä»¶åˆ°æ‚¨çš„ä¸‹è¼‰è³‡æ–™å¤¾",
                    key="download_json_btn"
                )
                
            except Exception as e:
                st.error(f"âŒ æº–å‚™ä¸‹è¼‰æ–‡ä»¶å¤±æ•—: {e}")
        else:
            st.button("ğŸ’¾ ä¸‹è¼‰JSON", disabled=True, help="æš«ç„¡å¯ä¸‹è¼‰çš„çµæœæ–‡ä»¶")
    
    def _clear_results(self):
        """æ¸…é™¤ç•¶å‰çµæœ"""
        if 'realtime_results' in st.session_state:
            del st.session_state.realtime_results
        if 'realtime_results_file' in st.session_state:
            del st.session_state.realtime_results_file
        if 'realtime_error' in st.session_state:
            del st.session_state.realtime_error
        if 'latest_csv_file' in st.session_state:
            del st.session_state.latest_csv_file
        st.success("ğŸ—‘ï¸ çµæœå·²æ¸…é™¤")
        st.rerun()  # é‡æ–°é‹è¡Œé é¢ä¾†åˆ·æ–°UI
    
    def _render_results_area(self):
        """æ¸²æŸ“çµæœå€åŸŸ"""
        if 'realtime_results' in st.session_state:
            self._show_results()
        elif 'realtime_error' in st.session_state:
            st.error(f"âŒ çˆ¬å–éŒ¯èª¤ï¼š{st.session_state.realtime_error}")
        else:
            st.info("ğŸ‘† é»æ“Šã€Œé–‹å§‹çˆ¬å–ã€ä¾†é–‹å§‹ï¼Œæˆ–ä¸Šå‚³CSVæ–‡ä»¶æŸ¥çœ‹ä¹‹å‰çš„çµæœ")
    
    def _show_results(self):
        """é¡¯ç¤ºçˆ¬å–çµæœ"""
        # å¾session stateç²å–çµæœï¼ˆå¯èƒ½æ˜¯å­—å…¸æ ¼å¼ï¼‰
        realtime_results = st.session_state.realtime_results
        
        # æª¢æŸ¥resultsçš„æ ¼å¼ï¼Œå¦‚æœæ˜¯å­—å…¸å‰‡æå–resultsåˆ—è¡¨
        if isinstance(realtime_results, dict):
            results = realtime_results.get('results', [])
        else:
            results = realtime_results if realtime_results else []
        
        results_file = st.session_state.get('realtime_results_file', 'unknown.json')
        
        st.subheader("ğŸ“Š çˆ¬å–çµæœ")
        
        # ç¢ºä¿resultsæ˜¯åˆ—è¡¨
        if not isinstance(results, list):
            st.error("âŒ çµæœæ ¼å¼éŒ¯èª¤ï¼Œè«‹é‡æ–°è¼‰å…¥")
            return
        
        # åŸºæœ¬çµ±è¨ˆ
        total_posts = len(results)
        successful_views = len([r for r in results if isinstance(r, dict) and r.get('has_views')])
        successful_content = len([r for r in results if isinstance(r, dict) and r.get('has_content')])
        successful_likes = len([r for r in results if isinstance(r, dict) and r.get('has_likes')])
        successful_comments = len([r for r in results if isinstance(r, dict) and r.get('has_comments')])
        
        # çµ±è¨ˆæŒ‡æ¨™
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("ç¸½è²¼æ–‡æ•¸", total_posts)
        with col2:
            st.metric("è§€çœ‹æ•¸æˆåŠŸ", f"{successful_views}/{total_posts}")
        with col3:
            st.metric("å…§å®¹æˆåŠŸ", f"{successful_content}/{total_posts}")
        with col4:
            st.metric("äº’å‹•æ•¸æ“š", f"{successful_likes}/{total_posts}")
        
        # æˆåŠŸç‡æŒ‡æ¨™
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            view_rate = (successful_views / total_posts * 100) if total_posts > 0 else 0
            st.metric("è§€çœ‹æ•¸æˆåŠŸç‡", f"{view_rate:.1f}%")
        with col2:
            content_rate = (successful_content / total_posts * 100) if total_posts > 0 else 0
            st.metric("å…§å®¹æˆåŠŸç‡", f"{content_rate:.1f}%")
        with col3:
            like_rate = (successful_likes / total_posts * 100) if total_posts > 0 else 0
            st.metric("æŒ‰è®šæ•¸æˆåŠŸç‡", f"{like_rate:.1f}%")
        with col4:
            comment_rate = (successful_comments / total_posts * 100) if total_posts > 0 else 0
            st.metric("ç•™è¨€æ•¸æˆåŠŸç‡", f"{comment_rate:.1f}%")
        
        # é‡è¤‡è™•ç†åŠŸèƒ½
        st.divider()
        col1, col2, col3 = st.columns([2, 1, 1])
        with col1:
            st.write("**ğŸ”„ é‡è¤‡è™•ç†**")
            st.caption("æª¢æ¸¬é‡è¤‡è²¼æ–‡ï¼Œè§€çœ‹æ•¸ä½çš„ç”¨APIé‡æ–°æå–")
        with col2:
            if st.button("ğŸ” æª¢æ¸¬é‡è¤‡", key="detect_duplicates"):
                self._detect_duplicates()
        with col3:
            if st.button("ğŸ”„ è™•ç†é‡è¤‡", key="process_duplicates"):
                self._process_duplicates()
        
        # è©³ç´°çµæœè¡¨æ ¼
        if st.checkbox("ğŸ“‹ é¡¯ç¤ºè©³ç´°çµæœ", key="show_detailed_results"):
            self._show_detailed_table(results)
        
        # è³‡æ–™åº«ç‹€æ…‹å’Œå‚™ç”¨ä¿å­˜
        if isinstance(realtime_results, dict):
            db_saved = realtime_results.get('database_saved', False)
            saved_count = realtime_results.get('database_saved_count', 0)
            if db_saved:
                st.success(f"âœ… å·²ä¿å­˜åˆ°è³‡æ–™åº« ({saved_count} å€‹è²¼æ–‡)")
            else:
                # é¡¯ç¤ºå‚™ç”¨ä¿å­˜é¸é …
                col_info, col_save = st.columns([3, 1])
                with col_info:
                    st.info("â„¹ï¸ çˆ¬èŸ²é€šå¸¸æœƒè‡ªå‹•ä¿å­˜åˆ°è³‡æ–™åº«ã€‚å¦‚æœçµ±è¨ˆä¸­æ²’æœ‰çœ‹åˆ°æ–°æ•¸æ“šï¼Œæ‚¨å¯ä»¥ä½¿ç”¨å‚™ç”¨ä¿å­˜åŠŸèƒ½")
                with col_save:
                    if st.button("ğŸ’¾ å‚™ç”¨ä¿å­˜", key="save_to_database", help="æ‰‹å‹•ä¿å­˜åˆ°è³‡æ–™åº«ï¼ˆå‚™ç”¨åŠŸèƒ½ï¼‰"):
                        self._save_results_to_database()
        else:
            st.info("ğŸ’¡ å¢é‡çˆ¬å–æ¨¡å¼æœƒè‡ªå‹•ä¿å­˜åˆ°è³‡æ–™åº«ä¸¦æ›´æ–°çµ±è¨ˆ")

        st.divider()
        
        # ä¸‹è¼‰å’Œå°å‡ºæŒ‰éˆ•
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            self._show_json_download_button(results_file)
        
        with col2:
            if st.button("ğŸ“Š å°å‡ºCSV", key="export_csv"):
                self._show_csv_export_options(results_file)
        
        with col3:
            if st.button("ğŸ“ˆ æ­·å²åˆ†æ", key="export_history"):
                self._show_export_history_options()
        
        with col4:
            if st.button("ğŸ” æ›´å¤šå°å‡º", key="more_exports"):
                self._show_advanced_export_options()
    
    def _detect_duplicates(self):
        """æª¢æ¸¬é‡è¤‡è²¼æ–‡"""
        if 'realtime_results' not in st.session_state:
            st.error("âŒ æ²’æœ‰å¯æª¢æ¸¬çš„çµæœ")
            return
        
        results = st.session_state.realtime_results
        
        # æŒ‰ post_id åˆ†çµ„
        from collections import defaultdict
        grouped = defaultdict(list)
        for result in results:
            if result.get('post_id'):
                grouped[result['post_id']].append(result)
        
        # æ‰¾å‡ºé‡è¤‡é …
        duplicates = {k: v for k, v in grouped.items() if len(v) > 1}
        
        if not duplicates:
            st.success("âœ… æ²’æœ‰ç™¼ç¾é‡è¤‡è²¼æ–‡")
            return
        
        st.warning(f"âš ï¸ ç™¼ç¾ {len(duplicates)} çµ„é‡è¤‡è²¼æ–‡")
        
        for post_id, items in duplicates.items():
            with st.expander(f"ğŸ“‹ é‡è¤‡çµ„: {post_id} ({len(items)} å€‹ç‰ˆæœ¬)"):
                for i, item in enumerate(items):
                    views = item.get('views', 'N/A')
                    source = item.get('source', 'unknown')
                    content = item.get('content', 'N/A')[:100] + '...' if item.get('content') else 'N/A'
                    
                    col1, col2, col3 = st.columns([1, 1, 3])
                    with col1:
                        st.write(f"**ç‰ˆæœ¬ {i+1}**")
                        st.write(f"è§€çœ‹æ•¸: {views}")
                    with col2:
                        st.write(f"ä¾†æº: {source}")
                    with col3:
                        st.write(f"å…§å®¹: {content}")
    
    def _process_duplicates(self):
        """è™•ç†é‡è¤‡è²¼æ–‡"""
        if 'realtime_results' not in st.session_state:
            st.error("âŒ æ²’æœ‰å¯è™•ç†çš„çµæœ")
            return
        
        # èª¿ç”¨é‡è¤‡è™•ç†è…³æœ¬
        try:
            import subprocess
            import sys
            import os
            
            st.info("ğŸ”„ æ­£åœ¨è™•ç†é‡è¤‡è²¼æ–‡...")
            
            # åŸ·è¡Œé‡è¤‡è™•ç†è…³æœ¬
            script_path = "fix_duplicates_reextract.py"
            result = subprocess.run(
                [sys.executable, script_path],
                capture_output=True,
                text=True,
                encoding='utf-8',
                errors='replace',
                cwd=os.getcwd()
            )
            
            if result.returncode == 0:
                # æŸ¥æ‰¾è™•ç†å¾Œçš„æ–‡ä»¶
                import glob
                from pathlib import Path
                
                # æª¢æŸ¥æ–°çš„è³‡æ–™å¤¾ä½ç½®
                extraction_dir = Path("extraction_results")
                if extraction_dir.exists():
                    dedup_files = list(extraction_dir.glob("realtime_extraction_results_*_dedup.json"))
                else:
                    dedup_files = [Path(f) for f in glob.glob("realtime_extraction_results_*_dedup.json")]
                
                if dedup_files:
                    latest_dedup = max(dedup_files, key=lambda f: f.stat().st_mtime)
                    
                    # è®€å–è™•ç†å¾Œçš„çµæœ
                    import json
                    with open(latest_dedup, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    
                    # æ›´æ–°session_state
                    st.session_state.realtime_results = data.get('results', [])
                    st.session_state.realtime_results_file = latest_dedup
                    
                    duplicates_count = data.get('duplicates_processed', 0)
                    reextracted_count = data.get('reextracted_count', 0)
                    
                    st.success(f"âœ… é‡è¤‡è™•ç†å®Œæˆï¼")
                    st.info(f"ğŸ“Š è™•ç†äº† {duplicates_count} çµ„é‡è¤‡ï¼Œé‡æ–°æå– {reextracted_count} å€‹é …ç›®")
                    st.balloons()
                    
                    # è‡ªå‹•åˆ·æ–°é é¢ä»¥é¡¯ç¤ºæ›´æ–°çµæœ
                    st.rerun()
                else:
                    st.error("âŒ æœªæ‰¾åˆ°è™•ç†å¾Œçš„çµæœæ–‡ä»¶")
            else:
                st.error(f"âŒ è™•ç†å¤±æ•—ï¼š{result.stderr}")
                st.code(result.stdout)
                
        except Exception as e:
            st.error(f"âŒ è™•ç†éŒ¯èª¤ï¼š{str(e)}")
    
    def _show_csv_export_options(self, json_file_path: str):
        """é¡¯ç¤ºCSVå°å‡ºé¸é …"""
        with st.expander("ğŸ“Š CSVå°å‡ºé¸é …", expanded=True):
            st.write("**é¸æ“‡æ’åºæ–¹å¼ï¼ˆå»ºè­°æŒ‰è§€çœ‹æ•¸æ’åºï¼‰**")
            
            sort_options = {
                "è§€çœ‹æ•¸ (é«˜â†’ä½)": "views",
                "æŒ‰è®šæ•¸ (é«˜â†’ä½)": "likes", 
                "ç•™è¨€æ•¸ (é«˜â†’ä½)": "comments",
                "è½‰ç™¼æ•¸ (é«˜â†’ä½)": "reposts",
                "åˆ†äº«æ•¸ (é«˜â†’ä½)": "shares",
                "è²¼æ–‡ID (Aâ†’Z)": "post_id",
                "åŸå§‹é †åº (ä¸æ’åº)": "none"
            }
            
            selected_sort = st.selectbox(
                "æ’åºæ–¹å¼",
                options=list(sort_options.keys()),
                index=0,  # é è¨­é¸æ“‡è§€çœ‹æ•¸æ’åº
                help="é¸æ“‡CSVæ–‡ä»¶ä¸­æ•¸æ“šçš„æ’åºæ–¹å¼ï¼Œå»ºè­°é¸æ“‡è§€çœ‹æ•¸ä»¥ä¾¿åˆ†ææœ€å—æ­¡è¿çš„è²¼æ–‡"
            )
            
            col1, col2 = st.columns(2)
            
            with col1:
                if st.button("ğŸ“¥ ç”ŸæˆCSV", key="export_csv_generate"):
                    sort_by = sort_options[selected_sort]
                    self._export_current_to_csv(json_file_path, sort_by)
                
                # æª¢æŸ¥æ˜¯å¦æœ‰ç”Ÿæˆå¥½çš„CSVå¯ä»¥ä¸‹è¼‰
                self._show_csv_download_if_available()
            
            with col2:
                st.info("ğŸ’¡ **CSVä½¿ç”¨æç¤ºï¼š**\n- ç”¨Excelæˆ–Google Sheetsæ‰“é–‹\n- å¯ä»¥é€²ä¸€æ­¥ç¯©é¸å’Œåˆ†æ\n- æ”¯æ´ä¸­æ–‡é¡¯ç¤º")
    
    def _export_current_to_csv(self, json_file_path: str, sort_by: str = 'views'):
        """å°å‡ºç•¶æ¬¡çµæœåˆ°CSV"""
        try:
            from common.csv_export_manager import CSVExportManager
            
            csv_manager = CSVExportManager()
            csv_file = csv_manager.export_current_session(json_file_path, sort_by=sort_by)
            
            # ä¿å­˜CSVæ–‡ä»¶è·¯å¾‘åˆ°æœƒè©±ç‹€æ…‹
            st.session_state.latest_csv_file = csv_file
            
            st.success(f"âœ… CSVç”ŸæˆæˆåŠŸï¼")
            st.info(f"ğŸ“ æ–‡ä»¶ä½ç½®: {csv_file}")
            
        except Exception as e:
            st.error(f"âŒ CSVç”Ÿæˆå¤±æ•—: {str(e)}")
            if 'latest_csv_file' in st.session_state:
                del st.session_state.latest_csv_file
    
    def _show_csv_download_if_available(self):
        """é¡¯ç¤ºCSVä¸‹è¼‰æŒ‰éˆ•ï¼ˆå¦‚æœæœ‰å¯ç”¨çš„CSVæ–‡ä»¶ï¼‰"""
        if 'latest_csv_file' in st.session_state:
            csv_file = st.session_state.latest_csv_file
            if csv_file and Path(csv_file).exists():
                try:
                    with open(csv_file, 'r', encoding='utf-8-sig') as f:
                        csv_content = f.read()
                    
                    # ç”Ÿæˆæ™‚é–“æˆ³æ–‡ä»¶å
                    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                    download_filename = f"crawl_results_{timestamp}.csv"
                    
                    st.download_button(
                        label="ğŸ“¥ ä¸‹è¼‰CSVæ–‡ä»¶",
                        data=csv_content,
                        file_name=download_filename,
                        mime="text/csv",
                        help="ä¸‹è¼‰CSVæ–‡ä»¶åˆ°æ‚¨çš„ä¸‹è¼‰è³‡æ–™å¤¾",
                        key="download_csv_file_btn"
                    )
                    
                except Exception as e:
                    st.error(f"âŒ æº–å‚™CSVä¸‹è¼‰å¤±æ•—: {e}")
    
    def _show_export_history_options(self):
        """é¡¯ç¤ºæ­·å²å°å‡ºé¸é …"""
        if 'realtime_results' not in st.session_state:
            st.error("âŒ è«‹å…ˆåŸ·è¡Œçˆ¬å–ä»¥ç²å–å¸³è™Ÿä¿¡æ¯")
            return
        
        # ç²å–ç•¶å‰å¸³è™Ÿ
        results = st.session_state.realtime_results
        if not results:
            st.error("âŒ ç„¡æ³•ç²å–å¸³è™Ÿä¿¡æ¯")
            return
        
        # å‡è¨­å¾ç¬¬ä¸€å€‹çµæœä¸­æå–ç”¨æˆ¶å
        target_username = None
        if 'realtime_results_file' in st.session_state:
            try:
                import json
                with open(st.session_state.realtime_results_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                target_username = data.get('target_username')
            except:
                pass
        
        if not target_username:
            st.error("âŒ ç„¡æ³•è­˜åˆ¥ç›®æ¨™å¸³è™Ÿ")
            return
        
        with st.expander("ğŸ“ˆ æ­·å²æ•¸æ“šå°å‡ºé¸é …", expanded=True):
            export_type = st.radio(
                "é¸æ“‡å°å‡ºé¡å‹",
                options=["æœ€è¿‘æ•¸æ“š", "å…¨éƒ¨æ­·å²", "çµ±è¨ˆåˆ†æ"],
                help="é¸æ“‡è¦å°å‡ºçš„æ­·å²æ•¸æ“šç¯„åœ"
            )
            
            col1, col2 = st.columns(2)
            
            if export_type == "æœ€è¿‘æ•¸æ“š":
                with col1:
                    days_back = st.number_input("å›æº¯å¤©æ•¸", min_value=1, max_value=365, value=7)
                with col2:
                    limit = st.number_input("æœ€å¤§è¨˜éŒ„æ•¸", min_value=10, max_value=10000, value=1000)
                
                if st.button("ğŸ“Š å°å‡ºæœ€è¿‘æ•¸æ“š", key="export_recent"):
                    self._export_history_data(target_username, "recent", days_back=days_back, limit=limit)
            
            elif export_type == "å…¨éƒ¨æ­·å²":
                with col1:
                    limit = st.number_input("æœ€å¤§è¨˜éŒ„æ•¸", min_value=100, max_value=50000, value=5000)
                
                if st.button("ğŸ“Š å°å‡ºå…¨éƒ¨æ­·å²", key="export_all"):
                    self._export_history_data(target_username, "all", limit=limit)
            
            elif export_type == "çµ±è¨ˆåˆ†æ":
                st.info("æŒ‰æ—¥æœŸçµ±è¨ˆçš„åˆ†æå ±å‘Šï¼ŒåŒ…å«å¹³å‡è§€çœ‹æ•¸ã€æˆåŠŸç‡ç­‰æŒ‡æ¨™")
                
                if st.button("ğŸ“ˆ å°å‡ºçµ±è¨ˆåˆ†æ", key="export_analysis"):
                    self._export_history_data(target_username, "analysis")
    
    def _export_history_data(self, username: str, export_type: str, **kwargs):
        """å°å‡ºæ­·å²æ•¸æ“š"""
        try:
            from common.csv_export_manager import CSVExportManager
            import asyncio
            
            csv_manager = CSVExportManager()
            
            # å‰µå»ºæ–°çš„äº‹ä»¶å¾ªç’°ä¾†é¿å…è¡çª
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            
            try:
                if export_type == "recent":
                    csv_file = loop.run_until_complete(
                        csv_manager.export_database_history(
                            username, 
                            days_back=kwargs.get('days_back'),
                            limit=kwargs.get('limit')
                        )
                    )
                elif export_type == "all":
                    csv_file = loop.run_until_complete(
                        csv_manager.export_database_history(
                            username,
                            limit=kwargs.get('limit')
                        )
                    )
                elif export_type == "analysis":
                    csv_file = loop.run_until_complete(
                        csv_manager.export_combined_analysis(username)
                    )
                else:
                    raise ValueError(f"ä¸æ”¯æŒçš„å°å‡ºé¡å‹: {export_type}")
                
                st.success(f"âœ… æ­·å²æ•¸æ“šå°å‡ºæˆåŠŸï¼")
                st.info(f"ğŸ“ æ–‡ä»¶ä½ç½®: {csv_file}")
                
                # æä¾›ä¸‹è¼‰
                import os
                if os.path.exists(csv_file):
                    with open(csv_file, 'r', encoding='utf-8-sig') as f:
                        csv_content = f.read()
                    
                    st.download_button(
                        label="ğŸ“¥ ä¸‹è¼‰æ­·å²CSVæ–‡ä»¶",
                        data=csv_content,
                        file_name=os.path.basename(csv_file),
                        mime="text/csv"
                    )
                
            finally:
                loop.close()
                
        except Exception as e:
            st.error(f"âŒ æ­·å²æ•¸æ“šå°å‡ºå¤±æ•—: {str(e)}")
    
    def _show_advanced_export_options(self):
        """é¡¯ç¤ºé€²éšå°å‡ºé¸é …"""
        with st.expander("ğŸ” é€²éšå°å‡ºåŠŸèƒ½", expanded=True):
            st.markdown("**æ›´å¤šå°å‡ºé¸é …å’Œæ‰¹é‡æ“ä½œ**")
            
            tab1, tab2, tab3 = st.tabs(["ğŸ“Š å°æ¯”å ±å‘Š", "ğŸ”„ æ‰¹é‡å°å‡º", "âš¡ å¿«é€Ÿå·¥å…·"])
            
            with tab1:
                st.subheader("ğŸ“Š å¤šæ¬¡çˆ¬å–å°æ¯”å ±å‘Š")
                st.info("æ¯”è¼ƒå¤šæ¬¡çˆ¬å–çµæœçš„æ•ˆèƒ½å’ŒæˆåŠŸç‡")
                
                # æŸ¥æ‰¾æ‰€æœ‰JSONæ–‡ä»¶
                import glob
                import os
                # æª¢æŸ¥æ–°çš„è³‡æ–™å¤¾ä½ç½®
                extraction_dir = Path("extraction_results")
                if extraction_dir.exists():
                    json_files = list(extraction_dir.glob("realtime_extraction_results_*.json"))
                else:
                    json_files = [Path(f) for f in glob.glob("realtime_extraction_results_*.json")]
                
                if len(json_files) >= 2:
                    st.write(f"ğŸ” æ‰¾åˆ° {len(json_files)} å€‹çˆ¬å–çµæœæ–‡ä»¶ï¼š")
                    
                    # é¡¯ç¤ºæ–‡ä»¶åˆ—è¡¨ - ä½¿ç”¨ multiselect æ›´ç›´è§€
                    file_options = {}
                    for file in sorted(json_files, reverse=True)[:10]:  # æœ€æ–°çš„10å€‹
                        file_time = self._extract_time_from_filename(str(file))
                        display_name = f"{file.name} ({file_time})"
                        file_options[display_name] = str(file)
                    
                    # åˆå§‹åŒ–æœƒè©±ç‹€æ…‹
                    if "comparison_selected_files" not in st.session_state:
                        st.session_state.comparison_selected_files = []
                    
                    selected_displays = st.multiselect(
                        "é¸æ“‡è¦æ¯”å°çš„æ–‡ä»¶ï¼ˆè‡³å°‘2å€‹ï¼‰ï¼š",
                        options=list(file_options.keys()),
                        default=[],
                        help="é¸æ“‡å¤šå€‹æ–‡ä»¶é€²è¡Œæ¯”å°åˆ†æ",
                        key="comparison_file_selector"
                    )
                    
                    selected_files = [file_options[display] for display in selected_displays]
                    
                    # æ·»åŠ èª¿è©¦ä¿¡æ¯
                    if selected_displays:
                        st.text(f"ğŸ” èª¿è©¦: ç•¶å‰é¸ä¸­ {len(selected_displays)} å€‹é¡¯ç¤ºé …ç›®")
                        for i, display in enumerate(selected_displays):
                            st.text(f"   {i+1}. {display}")
                    
                    if len(selected_files) >= 2:
                        st.success(f"âœ… å·²é¸æ“‡ {len(selected_files)} å€‹æ–‡ä»¶é€²è¡Œæ¯”å°")
                        
                        # é¡¯ç¤ºé¸ä¸­çš„æ–‡ä»¶æ‘˜è¦
                        with st.expander("ğŸ“„ é¸ä¸­æ–‡ä»¶æ‘˜è¦", expanded=True):
                            for i, file_path in enumerate(selected_files):
                                try:
                                    with open(file_path, 'r', encoding='utf-8') as f:
                                        data = json.load(f)
                                    
                                    timestamp = data.get('timestamp', 'N/A')
                                    success_count = data.get('total_processed', 0)
                                    success_rate = data.get('overall_success_rate', 0)
                                    
                                    st.markdown(f"**ğŸ“ æ–‡ä»¶ {i+1}: {Path(file_path).name}**")
                                    col1, col2, col3 = st.columns(3)
                                    with col1:
                                        st.text(f"â° æ™‚é–“: {timestamp[:16] if timestamp != 'N/A' else 'N/A'}")
                                    with col2:
                                        st.text(f"âœ… æˆåŠŸ: {success_count} å€‹")
                                    with col3:
                                        st.text(f"ğŸ“Š æˆåŠŸç‡: {success_rate:.1f}%")
                                    st.divider()
                                except Exception as e:
                                    st.error(f"âŒ è®€å– {Path(file_path).name} å¤±æ•—: {e}")
                        
                        if st.button("ğŸ“Š ç”Ÿæˆå°æ¯”å ±å‘Š", key="generate_comparison", type="primary"):
                            with st.spinner("æ­£åœ¨ç”Ÿæˆå°æ¯”å ±å‘Š..."):
                                self._generate_comparison_report(selected_files)
                    elif len(selected_files) == 1:
                        st.warning("âš ï¸ å·²é¸æ“‡1å€‹æ–‡ä»¶ï¼Œè«‹å†é¸æ“‡è‡³å°‘1å€‹æ–‡ä»¶é€²è¡Œæ¯”å°")
                        st.info("ğŸ’¡ æç¤ºï¼šå¯ä»¥æŒ‰ä½ Ctrl éµé»æ“Šå…¶ä»–æ–‡ä»¶ä¾†å¤šé¸")
                    else:
                        st.info("ğŸ’¡ è«‹é¸æ“‡è‡³å°‘2å€‹æ–‡ä»¶é€²è¡Œæ¯”å°åˆ†æ")
                else:
                    st.warning("âš ï¸ éœ€è¦è‡³å°‘2å€‹çˆ¬å–çµæœæ–‡ä»¶æ‰èƒ½é€²è¡Œå°æ¯”")
            
            with tab2:
                st.subheader("ğŸ”„ æ‰¹é‡å°å‡ºåŠŸèƒ½")
                
                col1, col2 = st.columns(2)
                
                with col1:
                    if st.button("ğŸ“¥ å°å‡ºæ‰€æœ‰æœ€æ–°çµæœ", key="export_all_latest"):
                        self._export_all_latest_results()
                
                with col2:
                    if st.button("ğŸ“ˆ å°å‡ºæ‰€æœ‰å¸³è™Ÿçµ±è¨ˆ", key="export_all_stats"):
                        self._export_all_account_stats()
                
                st.divider()
                
                # è‡ªå‹•åŒ–å°å‡ºè¨­å®š
                st.write("**è‡ªå‹•åŒ–å°å‡ºè¨­å®š**")
                auto_sort = st.selectbox(
                    "é è¨­æ’åºæ–¹å¼",
                    ["è§€çœ‹æ•¸", "æŒ‰è®šæ•¸", "ç•™è¨€æ•¸", "æ™‚é–“é †åº"],
                    help="æ‰¹é‡å°å‡ºæ™‚ä½¿ç”¨çš„é è¨­æ’åº"
                )
                
                if st.button("ğŸ’¾ ä¿å­˜è¨­å®š", key="save_export_settings"):
                    st.session_state.default_sort = auto_sort
                    st.success(f"âœ… å·²ä¿å­˜é è¨­æ’åº: {auto_sort}")
            
            with tab3:
                st.subheader("âš¡ å¿«é€Ÿå·¥å…·")
                
                # å¿«é€Ÿé è¦½
                st.write("**å¿«é€Ÿé è¦½CSVæ–‡ä»¶**")
                uploaded_csv = st.file_uploader(
                    "ä¸Šå‚³CSVæ–‡ä»¶é€²è¡Œé è¦½",
                    type=['csv'],
                    help="ä¸Šå‚³ä»»ä½•CSVæ–‡ä»¶ï¼Œå¿«é€ŸæŸ¥çœ‹å‰å¹¾è¡Œæ•¸æ“š"
                )
                
                if uploaded_csv:
                    try:
                        import pandas as pd
                        df = pd.read_csv(uploaded_csv, encoding='utf-8-sig')
                        
                        col1, col2 = st.columns(2)
                        with col1:
                            st.metric("ç¸½è¡Œæ•¸", len(df))
                        with col2:
                            st.metric("ç¸½æ¬„ä½", len(df.columns))
                        
                        st.write("**å‰10è¡Œé è¦½ï¼š**")
                        st.dataframe(df.head(10), use_container_width=True)
                        
                    except Exception as e:
                        st.error(f"âŒ é è¦½å¤±æ•—: {e}")
                
                st.divider()
                
                # æ¸…ç†å·¥å…·
                st.write("**æ¸…ç†å·¥å…·**")
                if st.button("ğŸ—‘ï¸ æ¸…ç†èˆŠçš„å°å‡ºæ–‡ä»¶", key="cleanup_exports"):
                    self._cleanup_old_exports()
    
    def _extract_time_from_filename(self, filename: str) -> str:
        """å¾æ–‡ä»¶åæå–æ™‚é–“"""
        try:
            import re
            match = re.search(r'_(\d{8}_\d{6})\.json$', filename)
            if match:
                time_str = match.group(1)
                # æ ¼å¼åŒ–ç‚ºå¯è®€æ™‚é–“
                date_part = time_str[:8]
                time_part = time_str[9:]
                return f"{date_part[:4]}-{date_part[4:6]}-{date_part[6:8]} {time_part[:2]}:{time_part[2:4]}:{time_part[4:6]}"
        except:
            pass
        return "æœªçŸ¥æ™‚é–“"
    
    def _generate_comparison_report(self, selected_files: List[str]):
        """ç”Ÿæˆå°æ¯”å ±å‘Š"""
        try:
            from common.csv_export_manager import CSVExportManager
            
            csv_manager = CSVExportManager()
            csv_file = csv_manager.export_comparison_report(selected_files)
            
            st.success("âœ… å°æ¯”å ±å‘Šç”ŸæˆæˆåŠŸï¼")
            st.info(f"ğŸ“ æ–‡ä»¶ä½ç½®: {csv_file}")
            
            # æä¾›ä¸‹è¼‰
            import os
            if os.path.exists(csv_file):
                with open(csv_file, 'r', encoding='utf-8-sig') as f:
                    csv_content = f.read()
                
                st.download_button(
                    label="ğŸ“¥ ä¸‹è¼‰å°æ¯”å ±å‘Š",
                    data=csv_content,
                    file_name=os.path.basename(csv_file),
                    mime="text/csv"
                )
                
                # é¡¯ç¤ºæ‘˜è¦
                st.write("**ğŸ“Š å°æ¯”æ‘˜è¦ï¼š**")
                import pandas as pd
                df = pd.read_csv(csv_file, encoding='utf-8-sig')
                
                # é¡¯ç¤ºå®Œæ•´è¡¨æ ¼
                st.dataframe(df, use_container_width=True)
                
                # é¡¯ç¤ºé—œéµæŒ‡æ¨™å°æ¯”
                if len(df) >= 2:
                    st.write("**ğŸ” é—œéµæŒ‡æ¨™åˆ†æï¼š**")
                    
                    col1, col2, col3 = st.columns(3)
                    
                    with col1:
                        avg_success_rate = df['æˆåŠŸç‡(%)'].mean()
                        max_success_rate = df['æˆåŠŸç‡(%)'].max()
                        min_success_rate = df['æˆåŠŸç‡(%)'].min()
                        st.metric("å¹³å‡æˆåŠŸç‡", f"{avg_success_rate:.1f}%", 
                                 f"{max_success_rate - min_success_rate:.1f}% å·®è·")
                    
                    with col2:
                        if 'ç¸½è€—æ™‚(ç§’)' in df.columns:
                            avg_time = df['ç¸½è€—æ™‚(ç§’)'].mean()
                            fastest = df['ç¸½è€—æ™‚(ç§’)'].min()
                            slowest = df['ç¸½è€—æ™‚(ç§’)'].max()
                            st.metric("å¹³å‡è€—æ™‚", f"{avg_time:.1f}s", 
                                     f"{slowest - fastest:.1f}s å·®è·")
                    
                    with col3:
                        if 'è§€çœ‹æ•¸æå–ç‡(%)' in df.columns:
                            avg_views_rate = df['è§€çœ‹æ•¸æå–ç‡(%)'].mean()
                            st.metric("å¹³å‡è§€çœ‹æ•¸æå–ç‡", f"{avg_views_rate:.1f}%")
                
                # é¡¯ç¤ºè¶¨å‹¢åˆ†æ
                if len(df) >= 3:
                    st.write("**ğŸ“ˆ è¶¨å‹¢åˆ†æï¼š**")
                    
                    # æŒ‰æ™‚é–“æ’åº
                    df_sorted = df.sort_values('çˆ¬å–æ™‚é–“') if 'çˆ¬å–æ™‚é–“' in df.columns else df
                    
                    # æˆåŠŸç‡è¶¨å‹¢
                    success_trend = df_sorted['æˆåŠŸç‡(%)'].diff().iloc[-1] if len(df_sorted) > 1 else 0
                    if success_trend > 0:
                        st.success(f"ğŸ“ˆ æˆåŠŸç‡å‘ˆä¸Šå‡è¶¨å‹¢ (+{success_trend:.1f}%)")
                    elif success_trend < 0:
                        st.error(f"ğŸ“‰ æˆåŠŸç‡å‘ˆä¸‹é™è¶¨å‹¢ ({success_trend:.1f}%)")
                    else:
                        st.info("ğŸ“Š æˆåŠŸç‡ä¿æŒç©©å®š")
                
        except Exception as e:
            st.error(f"âŒ ç”Ÿæˆå°æ¯”å ±å‘Šå¤±æ•—: {e}")
    
    def _export_all_latest_results(self):
        """å°å‡ºæ‰€æœ‰æœ€æ–°çµæœ"""
        try:
            import glob
            # æª¢æŸ¥æ–°çš„è³‡æ–™å¤¾ä½ç½®  
            extraction_dir = Path("extraction_results")
            if extraction_dir.exists():
                json_files = list(extraction_dir.glob("realtime_extraction_results_*.json"))
            else:
                json_files = [Path(f) for f in glob.glob("realtime_extraction_results_*.json")]
            
            if not json_files:
                st.warning("âš ï¸ æœªæ‰¾åˆ°ä»»ä½•çˆ¬å–çµæœæ–‡ä»¶")
                return
            
            # æ‰¾æœ€æ–°çš„æ–‡ä»¶
            latest_file = max(json_files, key=lambda f: f.stat().st_mtime)
            
            from common.csv_export_manager import CSVExportManager
            csv_manager = CSVExportManager()
            
            # ä½¿ç”¨é è¨­æ’åº
            default_sort = getattr(st.session_state, 'default_sort', 'è§€çœ‹æ•¸')
            sort_mapping = {"è§€çœ‹æ•¸": "views", "æŒ‰è®šæ•¸": "likes", "ç•™è¨€æ•¸": "comments", "æ™‚é–“é †åº": "none"}
            sort_by = sort_mapping.get(default_sort, "views")
            
            csv_file = csv_manager.export_current_session(latest_file, sort_by=sort_by)
            
            st.success("âœ… æœ€æ–°çµæœå°å‡ºæˆåŠŸï¼")
            st.info(f"ğŸ“ ä½¿ç”¨äº† {latest_file}")
            st.info(f"ğŸ“Š æŒ‰ {default_sort} æ’åº")
            
            # æä¾›ä¸‹è¼‰
            import os
            if os.path.exists(csv_file):
                with open(csv_file, 'r', encoding='utf-8-sig') as f:
                    csv_content = f.read()
                
                st.download_button(
                    label="ğŸ“¥ ä¸‹è¼‰æœ€æ–°çµæœCSV",
                    data=csv_content,
                    file_name=os.path.basename(csv_file),
                    mime="text/csv"
                )
                
        except Exception as e:
            st.error(f"âŒ å°å‡ºå¤±æ•—: {e}")
    
    def _export_all_account_stats(self):
        """å°å‡ºæ‰€æœ‰å¸³è™Ÿçµ±è¨ˆ"""
        try:
            from common.incremental_crawl_manager import IncrementalCrawlManager
            import asyncio
            
            # å‰µå»ºäº‹ä»¶å¾ªç’°
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            
            try:
                manager = IncrementalCrawlManager()
                
                # ç²å–æ‰€æœ‰å¸³è™Ÿ
                results = loop.run_until_complete(manager.db.fetch_all("""
                    SELECT DISTINCT username FROM crawl_state ORDER BY last_crawl_at DESC
                """))
                
                if not results:
                    st.warning("âš ï¸ æœªæ‰¾åˆ°ä»»ä½•çˆ¬å–è¨˜éŒ„")
                    return
                
                all_stats = []
                for row in results:
                    username = row['username']
                    summary = loop.run_until_complete(manager.get_crawl_summary(username))
                    
                    if 'error' not in summary:
                        checkpoint = summary['checkpoint']
                        stats = summary['statistics']
                        
                        all_stats.append({
                            'å¸³è™Ÿ': f"@{username}",
                            'æœ€æ–°è²¼æ–‡ID': checkpoint['latest_post_id'] or 'N/A',
                            'ç´¯è¨ˆçˆ¬å–': checkpoint['total_crawled'],
                            'è³‡æ–™åº«è²¼æ–‡æ•¸': stats['total_posts'],
                            'æœ‰è§€çœ‹æ•¸è²¼æ–‡': stats['posts_with_views'],
                            'å¹³å‡è§€çœ‹æ•¸': round(stats['avg_views'], 0),
                            'æœ€é«˜è§€çœ‹æ•¸': stats['max_views'],
                            'ä¸Šæ¬¡çˆ¬å–': checkpoint['last_crawl_at'].strftime('%Y-%m-%d %H:%M') if checkpoint['last_crawl_at'] else 'N/A'
                        })
                
                if all_stats:
                    # è½‰æ›ç‚ºCSV
                    import pandas as pd
                    df = pd.DataFrame(all_stats)
                    
                    from datetime import datetime
                    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                    csv_file = f"export_all_accounts_stats_{timestamp}.csv"
                    df.to_csv(csv_file, index=False, encoding='utf-8-sig')
                    
                    st.success("âœ… æ‰€æœ‰å¸³è™Ÿçµ±è¨ˆå°å‡ºæˆåŠŸï¼")
                    st.info(f"ğŸ“ æ–‡ä»¶ä½ç½®: {csv_file}")
                    
                    # é¡¯ç¤ºé è¦½
                    st.write("**çµ±è¨ˆé è¦½ï¼š**")
                    st.dataframe(df, use_container_width=True)
                    
                    # æä¾›ä¸‹è¼‰
                    csv_content = df.to_csv(index=False, encoding='utf-8-sig')
                    st.download_button(
                        label="ğŸ“¥ ä¸‹è¼‰å¸³è™Ÿçµ±è¨ˆ",
                        data=csv_content,
                        file_name=csv_file,
                        mime="text/csv"
                    )
                else:
                    st.warning("âš ï¸ æœªæ‰¾åˆ°æœ‰æ•ˆçš„çµ±è¨ˆæ•¸æ“š")
                    
            finally:
                loop.close()
                
        except Exception as e:
            st.error(f"âŒ å°å‡ºå¸³è™Ÿçµ±è¨ˆå¤±æ•—: {e}")
    
    def _cleanup_old_exports(self):
        """æ¸…ç†èˆŠçš„å°å‡ºæ–‡ä»¶"""
        try:
            import glob
            import os
            from datetime import datetime, timedelta
            
            # æ‰¾åˆ°æ‰€æœ‰å°å‡ºæ–‡ä»¶
            export_patterns = [
                "export_current_*.csv",
                "export_history_*.csv", 
                "export_analysis_*.csv",
                "export_comparison_*.csv"
            ]
            
            old_files = []
            cutoff_date = datetime.now() - timedelta(days=7)  # 7å¤©å‰
            
            for pattern in export_patterns:
                files = glob.glob(pattern)
                for file in files:
                    file_time = datetime.fromtimestamp(os.path.getmtime(file))
                    if file_time < cutoff_date:
                        old_files.append(file)
            
            if old_files:
                st.write(f"ğŸ” æ‰¾åˆ° {len(old_files)} å€‹7å¤©å‰çš„å°å‡ºæ–‡ä»¶ï¼š")
                
                for file in old_files[:5]:  # åªé¡¯ç¤ºå‰5å€‹
                    st.text(f"- {file}")
                
                if len(old_files) > 5:
                    st.text(f"... ä»¥åŠå…¶ä»– {len(old_files) - 5} å€‹æ–‡ä»¶")
                
                if st.button("ğŸ—‘ï¸ ç¢ºèªåˆªé™¤", key="confirm_cleanup"):
                    deleted_count = 0
                    for file in old_files:
                        try:
                            os.remove(file)
                            deleted_count += 1
                        except:
                            pass
                    
                    st.success(f"âœ… å·²åˆªé™¤ {deleted_count} å€‹èˆŠæ–‡ä»¶")
            else:
                st.info("âœ¨ æ²’æœ‰æ‰¾åˆ°éœ€è¦æ¸…ç†çš„èˆŠæ–‡ä»¶")
                
        except Exception as e:
            st.error(f"âŒ æ¸…ç†å¤±æ•—: {e}")
    
    def _show_detailed_table(self, results: List[Dict]):
        """é¡¯ç¤ºè©³ç´°çµæœè¡¨æ ¼"""
        st.subheader("ğŸ“‹ è©³ç´°çµæœ")
        
        # æº–å‚™è¡¨æ ¼æ•¸æ“š
        table_data = []
        for r in results:
            table_data.append({
                "è²¼æ–‡ID": r.get('post_id', 'N/A'),
                "è§€çœ‹æ•¸": r.get('views', 'N/A'),
                "æŒ‰è®šæ•¸": r.get('likes', 'N/A'),
                "ç•™è¨€æ•¸": r.get('comments', 'N/A'),
                "è½‰ç™¼æ•¸": r.get('reposts', 'N/A'),
                "åˆ†äº«æ•¸": r.get('shares', 'N/A'),
                "å…§å®¹é è¦½": (r.get('content', '')[:50] + "...") if r.get('content') else 'N/A',
                "ä¾†æº": r.get('source', 'N/A'),
                "é‡æ–°æå–": "âœ…" if r.get('reextracted', False) else ""
            })
        
        # é¡¯ç¤ºè¡¨æ ¼
        st.dataframe(
            table_data,
            use_container_width=True,
            height=400
        )
        
        # äº’å‹•æ•¸æ“šåˆ†æ
        if st.checkbox("ğŸ“ˆ äº’å‹•æ•¸æ“šåˆ†æ", key="show_engagement_analysis"):
            self._show_engagement_analysis(results)
    
    def _show_engagement_analysis(self, results: List[Dict]):
        """é¡¯ç¤ºäº’å‹•æ•¸æ“šåˆ†æ"""
        st.subheader("ğŸ“ˆ äº’å‹•æ•¸æ“šåˆ†æ")
        
        # æ”¶é›†æœ‰æ•ˆçš„äº’å‹•æ•¸æ“š
        valid_results = [r for r in results if isinstance(r, dict) and r.get('has_views') and r.get('has_likes')]
        
        if not valid_results:
            st.warning("ç„¡è¶³å¤ çš„äº’å‹•æ•¸æ“šé€²è¡Œåˆ†æ")
            return
        
        # ç°¡å–®çµ±è¨ˆ
        avg_likes = []
        avg_comments = []
        for r in valid_results:
            if r.get('likes') and r['likes'] != 'N/A':
                try:
                    # ç°¡åŒ–çš„æ•¸å­—è½‰æ›
                    likes_str = str(r['likes']).replace('K', '000').replace('M', '000000')
                    if likes_str.replace('.', '').isdigit():
                        avg_likes.append(float(likes_str))
                except:
                    pass
        
        if avg_likes:
            col1, col2 = st.columns(2)
            with col1:
                st.metric("å¹³å‡æŒ‰è®šæ•¸", f"{sum(avg_likes)/len(avg_likes):.0f}")
            with col2:
                st.metric("æœ€é«˜æŒ‰è®šæ•¸", f"{max(avg_likes):.0f}")
    
    def _render_cached_stats(self, stats):
        """æ¸²æŸ“ç·©å­˜çš„çµ±è¨ˆä¿¡æ¯"""
        # é¡¯ç¤ºç¸½é«”çµ±è¨ˆ
        total_stats = stats.get("total_stats", {})
        if total_stats:
            st.info(f"""
            **ğŸ“ˆ ç¸½é«”çµ±è¨ˆ**
            - ğŸ“Š ç¸½è²¼æ–‡æ•¸: {total_stats.get('total_posts', 0):,}
            - ğŸ‘¥ å·²çˆ¬å–ç”¨æˆ¶: {total_stats.get('total_users', 0)} å€‹
            - â° æœ€å¾Œæ´»å‹•: {str(total_stats.get('latest_activity', 'N/A'))[:16] if total_stats.get('latest_activity') else 'N/A'}
            """)
        
        # é¡¯ç¤ºç”¨æˆ¶çµ±è¨ˆ
        user_stats = stats.get("user_stats", [])
        if user_stats:
            st.write("**ğŸ‘¥ å„ç”¨æˆ¶çµ±è¨ˆ:**")
            
            # ä½¿ç”¨è¡¨æ ¼é¡¯ç¤º
            import pandas as pd
            df_data = []
            for user in user_stats:
                latest = str(user.get('latest_crawl', 'N/A'))[:16] if user.get('latest_crawl') else 'N/A'
                df_data.append({
                    "ç”¨æˆ¶å": f"@{user.get('username', 'N/A')}",
                    "è²¼æ–‡æ•¸": f"{user.get('post_count', 0):,}",
                    "æœ€å¾Œçˆ¬å–": latest
                })
            
            if df_data:
                df = pd.DataFrame(df_data)
                st.dataframe(
                    df, 
                    use_container_width=True,
                    hide_index=True,
                    height=min(300, len(df_data) * 35 + 38)  # å‹•æ…‹é«˜åº¦
                )
                
                # æ·»åŠ ç”¨æˆ¶è³‡æ–™ç®¡ç†åŠŸèƒ½ï¼ˆæŠ˜ç–Šå½¢å¼ï¼‰
                st.markdown("---")
                with st.expander("ğŸ—‚ï¸ ç”¨æˆ¶è³‡æ–™ç®¡ç†", expanded=False):
                    # ç”¨æˆ¶é¸æ“‡
                    user_options = [user.get('username', 'N/A') for user in user_stats]
                    selected_user = st.selectbox(
                        "é¸æ“‡è¦ç®¡ç†çš„ç”¨æˆ¶:",
                        options=user_options,
                        index=0 if user_options else None,
                        help="é¸æ“‡ä¸€å€‹ç”¨æˆ¶ä¾†ç®¡ç†å…¶çˆ¬èŸ²è³‡æ–™"
                    )
                    
                    # æ“ä½œæŒ‰éˆ•
                    if selected_user:
                        col1, col2 = st.columns(2)
                        
                        with col1:
                            # å°å‡ºç”¨æˆ¶CSVæŒ‰éˆ•
                            if st.button(
                                "ğŸ“Š å°å‡ºCSV", 
                                key="export_user_csv_btn",
                                help="å°å‡ºæ‰€é¸ç”¨æˆ¶çš„æ‰€æœ‰è²¼æ–‡ç‚ºCSVæ ¼å¼",
                                use_container_width=True
                            ):
                                self._export_user_csv(selected_user)
                        
                        with col2:
                            # ä½¿ç”¨JavaScriptä¾†ç²¾ç¢ºå®šä½ä¸¦è¨­ç½®æŒ‰éˆ•æ¨£å¼
                            st.markdown("""
                            <script>
                            setTimeout(function() {
                                // æŸ¥æ‰¾å…·æœ‰ç‰¹å®šæ–‡æœ¬çš„æŒ‰éˆ•
                                const buttons = document.querySelectorAll('button');
                                buttons.forEach(button => {
                                    if (button.textContent.includes('ğŸ—‘ï¸ åˆªé™¤ç”¨æˆ¶è³‡æ–™')) {
                                        button.style.backgroundColor = '#ff4b4b';
                                        button.style.color = 'white';
                                        button.style.borderColor = '#ff4b4b';
                                        
                                        button.addEventListener('mouseenter', function() {
                                            this.style.backgroundColor = '#ff2b2b';
                                            this.style.borderColor = '#ff2b2b';
                                        });
                                        
                                        button.addEventListener('mouseleave', function() {
                                            if (!this.disabled) {
                                                this.style.backgroundColor = '#ff4b4b';
                                                this.style.borderColor = '#ff4b4b';
                                            }
                                        });
                                    }
                                });
                            }, 100);
                            </script>
                            """, unsafe_allow_html=True)
                            
                            # åˆªé™¤ç”¨æˆ¶è³‡æ–™æŒ‰éˆ•
                            if st.button(
                                "ğŸ—‘ï¸ åˆªé™¤ç”¨æˆ¶è³‡æ–™", 
                                key="delete_user_data_btn",
                                help="åˆªé™¤æ‰€é¸ç”¨æˆ¶çš„æ‰€æœ‰çˆ¬èŸ²è³‡æ–™",
                                use_container_width=True
                            ):
                                self._delete_user_data(selected_user)
                    
                    if selected_user:
                        # é¡¯ç¤ºé¸ä¸­ç”¨æˆ¶çš„è©³ç´°ä¿¡æ¯
                        selected_user_info = next((u for u in user_stats if u.get('username') == selected_user), None)
                        if selected_user_info:
                            st.info(f"""
                            **ğŸ“‹ ç”¨æˆ¶ @{selected_user} çš„è©³ç´°ä¿¡æ¯:**
                            - ğŸ“Š è²¼æ–‡ç¸½æ•¸: {selected_user_info.get('post_count', 0):,} å€‹
                            - â° æœ€å¾Œçˆ¬å–: {str(selected_user_info.get('latest_crawl', 'N/A'))[:16] if selected_user_info.get('latest_crawl') else 'N/A'}
                            - ğŸ• é¦–æ¬¡çˆ¬å–: {str(selected_user_info.get('first_crawl', 'N/A'))[:16] if selected_user_info.get('first_crawl') else 'N/A'}
                            """)
                            
                            st.warning("âš ï¸ **æ³¨æ„**: åˆªé™¤æ“ä½œå°‡æ°¸ä¹…ç§»é™¤è©²ç”¨æˆ¶çš„æ‰€æœ‰çˆ¬èŸ²è³‡æ–™ï¼ŒåŒ…æ‹¬è²¼æ–‡å…§å®¹ã€è§€çœ‹æ•¸ç­‰ï¼Œæ­¤æ“ä½œç„¡æ³•å¾©åŸï¼")
        else:
            st.warning("ğŸ“ è³‡æ–™åº«ä¸­æš«ç„¡çˆ¬å–è¨˜éŒ„")
    
    def _save_results_to_database(self):
        """å°‡ç•¶å‰çˆ¬å–çµæœä¿å­˜åˆ°è³‡æ–™åº«"""
        if 'realtime_results' not in st.session_state:
            st.error("âŒ æ²’æœ‰å¯ä¿å­˜çš„çµæœ")
            return
        
        # å¾session stateç²å–çµæœ
        realtime_results = st.session_state.realtime_results
        
        # æª¢æŸ¥resultsçš„æ ¼å¼ï¼Œå¦‚æœæ˜¯å­—å…¸å‰‡æå–resultsåˆ—è¡¨
        if isinstance(realtime_results, dict):
            results = realtime_results.get('results', [])
            target_username = realtime_results.get('target_username', '')
        else:
            results = realtime_results if realtime_results else []
            target_username = results[0].get('username', '') if results else ''
        
        if not results:
            st.error("âŒ æ²’æœ‰æ‰¾åˆ°å¯ä¿å­˜çš„çµæœ")
            return
        
        if not target_username:
            st.error("âŒ ç„¡æ³•è­˜åˆ¥ç›®æ¨™ç”¨æˆ¶å")
            return
        
        try:
            import subprocess
            import json
            import sys
            import os
            import tempfile
            
            # å‰µå»ºä¿å­˜è…³æœ¬
            save_script_content = f'''
import asyncio
import sys
import os
import json
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from common.incremental_crawl_manager import IncrementalCrawlManager

async def save_to_database():
    crawl_manager = IncrementalCrawlManager()
    try:
        await crawl_manager.db.init_pool()
        
        # æº–å‚™çµæœæ•¸æ“š
        results = {json.dumps(results, ensure_ascii=False)}
        target_username = "{target_username}"
        
        # ä¿å­˜çµæœåˆ°è³‡æ–™åº«
        saved_count = await crawl_manager.save_quick_crawl_results(results, target_username)
        
        # æ›´æ–°æª¢æŸ¥é»ï¼ˆä½¿ç”¨æœ€æ–°çš„è²¼æ–‡IDï¼‰
        if results and saved_count > 0:
            latest_post_id = results[0].get('post_id')  # ç¬¬ä¸€å€‹æ˜¯æœ€æ–°çš„
            if latest_post_id:
                await crawl_manager.update_crawl_checkpoint(
                    target_username, 
                    latest_post_id, 
                    saved_count
                )
        
        result = {{
            "success": True,
            "saved_count": saved_count,
            "target_username": target_username
        }}
        
        print(json.dumps(result))
        
    except Exception as e:
        print(json.dumps({{"success": False, "error": str(e)}}))
    finally:
        await crawl_manager.db.close_pool()

if __name__ == "__main__":
    asyncio.run(save_to_database())
'''
            
            # å¯«å…¥è‡¨æ™‚æ–‡ä»¶
            with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False, encoding='utf-8') as f:
                f.write(save_script_content)
                temp_script = f.name
            
            try:
                # åŸ·è¡Œä¿å­˜è…³æœ¬
                with st.spinner(f"ğŸ’¾ æ­£åœ¨ä¿å­˜ {len(results)} å€‹è²¼æ–‡åˆ°è³‡æ–™åº«..."):
                    result = subprocess.run(
                        [sys.executable, temp_script],
                        capture_output=True,
                        text=True,
                        encoding='utf-8',
                        cwd=os.path.dirname(os.path.dirname(os.path.dirname(__file__))),
                        timeout=60
                    )
                
                if result.returncode == 0 and result.stdout.strip():
                    save_result = json.loads(result.stdout.strip())
                    
                    if save_result.get("success"):
                        saved_count = save_result.get("saved_count", 0)
                        
                        st.success(f"""
                        âœ… **ä¿å­˜æˆåŠŸï¼**
                        
                        å·²æˆåŠŸå°‡ @{target_username} çš„è²¼æ–‡ä¿å­˜åˆ°è³‡æ–™åº«ï¼š
                        - ğŸ’¾ ä¿å­˜è²¼æ–‡æ•¸: {saved_count} å€‹
                        - ğŸ”„ æª¢æŸ¥é»å·²æ›´æ–°
                        """)
                        
                        # æ›´æ–°session stateï¼Œæ¨™è¨˜ç‚ºå·²ä¿å­˜
                        if isinstance(st.session_state.realtime_results, dict):
                            st.session_state.realtime_results['database_saved'] = True
                            st.session_state.realtime_results['database_saved_count'] = saved_count
                        
                        # æ¸…ç†è³‡æ–™åº«çµ±è¨ˆç·©å­˜ï¼Œä¸‹æ¬¡æŸ¥çœ‹æœƒæ›´æ–°
                        if 'db_stats_cache' in st.session_state:
                            del st.session_state.db_stats_cache
                        
                        st.info("ğŸ“Š è³‡æ–™åº«çµ±è¨ˆå·²æ›´æ–°ï¼Œæ‚¨å¯ä»¥é»æ“Šåˆ·æ–°æŒ‰éˆ•æŸ¥çœ‹æœ€æ–°æ•¸æ“š")
                        
                    else:
                        st.error(f"âŒ ä¿å­˜å¤±æ•—: {save_result.get('error', 'æœªçŸ¥éŒ¯èª¤')}")
                else:
                    st.error(f"âŒ ä¿å­˜è…³æœ¬åŸ·è¡Œå¤±æ•—")
                    if result.stderr:
                        st.text(f"éŒ¯èª¤è©³æƒ…: {result.stderr}")
                        
            finally:
                # æ¸…ç†è‡¨æ™‚æ–‡ä»¶
                try:
                    os.unlink(temp_script)
                except:
                    pass
                    
        except Exception as e:
            st.error(f"âŒ ä¿å­˜æ“ä½œå¤±æ•—: {str(e)}")