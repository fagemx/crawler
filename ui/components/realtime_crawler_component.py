"""
å¯¦æ™‚çˆ¬èŸ²çµ„ä»¶ - æ™ºèƒ½URLæ”¶é›† + è¼ªè¿´ç­–ç•¥æå–
åŒ…å«å®Œæ•´äº’å‹•æ•¸æ“šæå–åŠŸèƒ½
"""

import streamlit as st
import asyncio
import json
import time
import threading
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, Optional, List
import sys
import os

# æ·»åŠ å°ˆæ¡ˆæ ¹ç›®éŒ„åˆ° Python è·¯å¾‘
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

class RealtimeCrawlerComponent:
    def __init__(self):
        self.is_running = False
        self.current_task = None
        
    def render(self):
        """æ¸²æŸ“å¯¦æ™‚çˆ¬èŸ²çµ„ä»¶"""
        st.header("ğŸš€ å¯¦æ™‚æ™ºèƒ½çˆ¬èŸ²")
        st.markdown("**æ™ºèƒ½æ»¾å‹•æ”¶é›†URLs + è¼ªè¿´ç­–ç•¥å¿«é€Ÿæå– + å®Œæ•´äº’å‹•æ•¸æ“š**")
        
        # åƒæ•¸è¨­å®šå€åŸŸ
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("âš™ï¸ çˆ¬å–è¨­å®š")
            username = st.text_input(
                "ç›®æ¨™å¸³è™Ÿ", 
                value="gvmonthly",
                help="è¦çˆ¬å–çš„Threadså¸³è™Ÿç”¨æˆ¶å",
                key="realtime_username"
            )
            
            max_posts = st.number_input(
                "çˆ¬å–æ•¸é‡", 
                min_value=1, 
                max_value=500, 
                value=50,
                help="è¦çˆ¬å–çš„è²¼æ–‡æ•¸é‡",
                key="realtime_max_posts"
            )
            
            # å¢é‡çˆ¬å–æ¨¡å¼é¸é …
            crawl_mode = st.radio(
                "çˆ¬å–æ¨¡å¼",
                options=["å¢é‡çˆ¬å–", "å…¨é‡çˆ¬å–"],
                index=0,
                help="å¢é‡çˆ¬å–ï¼šåªæŠ“å–æ–°è²¼æ–‡ï¼Œé¿å…é‡è¤‡ï¼›å…¨é‡çˆ¬å–ï¼šæŠ“å–æ‰€æœ‰æ‰¾åˆ°çš„è²¼æ–‡",
                key="crawl_mode"
            )
            
        with col2:
            col_title, col_refresh = st.columns([3, 1])
            with col_title:
                st.subheader("ğŸ“Š è³‡æ–™åº«çµ±è¨ˆ")
            with col_refresh:
                if st.button("ğŸ”„", key="refresh_db_stats", help="åˆ·æ–°çµ±è¨ˆä¿¡æ¯"):
                    st.rerun()  # é‡æ–°é‹è¡Œé é¢ä¾†åˆ·æ–°çµ±è¨ˆ
            
            self._display_database_stats()
        
        # æ§åˆ¶æŒ‰éˆ•
        col1, col2, col3 = st.columns([1, 1, 2])
        
        with col1:
            if st.button("ğŸš€ é–‹å§‹çˆ¬å–", key="start_realtime"):
                with st.spinner("æ­£åœ¨åŸ·è¡Œçˆ¬å–..."):
                    is_incremental = crawl_mode == "å¢é‡çˆ¬å–"
                    self._execute_crawling_simple(username, max_posts, is_incremental)
                
        with col2:
            if st.button("ğŸ”„ é‡ç½®", key="reset_realtime"):
                self._reset_results()
        
        # çµæœé¡¯ç¤º
        self._render_results_area()
    
    def _execute_crawling_simple(self, username: str, max_posts: int, is_incremental: bool = True):
        """ç°¡åŒ–çš„çˆ¬å–åŸ·è¡Œæ–¹æ³• - ä½¿ç”¨åŒæ­¥ç‰ˆæœ¬é¿å…asyncioè¡çª"""
        if not username.strip():
            st.error("è«‹è¼¸å…¥ç›®æ¨™å¸³è™Ÿï¼")
            return
            
        try:
            mode_text = "å¢é‡çˆ¬å–" if is_incremental else "å…¨é‡çˆ¬å–"
            st.info(f"ğŸ”„ æ­£åœ¨åŸ·è¡Œ{mode_text}ï¼Œè«‹ç¨å€™...")
            
            # ä½¿ç”¨subprocessä¾†é¿å…asyncioè¡çª
            import subprocess
            import json
            import sys
            import os
            
            # æ§‹å»ºå‘½ä»¤
            script_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), '..', 'scripts', 'realtime_crawler_extractor.py')
            
            # ä¿®æ”¹è…³æœ¬ä»¥æ¥å—å‘½ä»¤è¡Œåƒæ•¸
            cmd = [
                sys.executable, 
                script_path,
                '--username', username,
                '--max_posts', str(max_posts)
            ]
            
            # æ·»åŠ çˆ¬å–æ¨¡å¼åƒæ•¸
            if is_incremental:
                cmd.append('--incremental')  # å¢é‡æ¨¡å¼
            else:
                cmd.append('--full')  # å…¨é‡æ¨¡å¼
            
            # åŸ·è¡Œè…³æœ¬ - è¨­ç½®UTF-8ç·¨ç¢¼
            import locale
            env = os.environ.copy()
            env['PYTHONIOENCODING'] = 'utf-8'
            env['PYTHONUTF8'] = '1'
            
            # å‰µå»ºä¸€å€‹æ—¥å¿—å®¹å™¨ä¾†å¯¦æ™‚é¡¯ç¤ºè¼¸å‡º
            log_container = st.empty()
            log_text = []
            
            with st.expander("ğŸ“‹ çˆ¬å–éç¨‹æ—¥å¿—", expanded=True):
                log_placeholder = st.empty()
                
                # ä½¿ç”¨Popenä¾†å¯¦æ™‚æ•ç²è¼¸å‡º
                process = subprocess.Popen(
                    cmd,
                    stdout=subprocess.PIPE,
                    stderr=subprocess.STDOUT,  # åˆä½µstderråˆ°stdout
                    text=True,
                    encoding='utf-8',
                    errors='replace',
                    env=env,
                    cwd=os.path.dirname(os.path.dirname(os.path.dirname(__file__))),
                    bufsize=1,  # è¡Œç·©è¡
                    universal_newlines=True
                )
                
                # å¯¦æ™‚è®€å–è¼¸å‡º
                all_output = []
                while True:
                    output = process.stdout.readline()
                    if output == '' and process.poll() is not None:
                        break
                    if output:
                        line = output.strip()
                        all_output.append(line)
                        log_text.append(line)
                        
                        # åªé¡¯ç¤ºæœ€å¾Œ30è¡Œï¼Œé¿å…ç•Œé¢éé•·
                        display_lines = log_text[-30:] if len(log_text) > 30 else log_text
                        log_placeholder.code('\n'.join(display_lines), language='text')
                
                # ç­‰å¾…é€²ç¨‹å®Œæˆ
                return_code = process.poll()
                
            if return_code == 0:
                # æˆåŠŸåŸ·è¡Œï¼Œå°‹æ‰¾æœ€æ–°çš„çµæœæ–‡ä»¶
                import glob
                from pathlib import Path
                
                # å…ˆæª¢æŸ¥æ–°çš„è³‡æ–™å¤¾ä½ç½®
                extraction_dir = Path("extraction_results")
                if extraction_dir.exists():
                    results_files = list(extraction_dir.glob("realtime_extraction_results_*.json"))
                else:
                    # å›é€€åˆ°æ ¹ç›®éŒ„æŸ¥æ‰¾ï¼ˆå‘å¾Œå…¼å®¹ï¼‰
                    results_files = [Path(f) for f in glob.glob("realtime_extraction_results_*.json")]
                
                if results_files:
                    # å–æœ€æ–°çš„æ–‡ä»¶
                    latest_file = max(results_files, key=lambda f: f.stat().st_mtime)
                    
                    # è®€å–çµæœ
                    with open(latest_file, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    
                    # ä¿å­˜åˆ°session_state
                    st.session_state.realtime_results = data.get('results', [])
                    st.session_state.realtime_results_file = latest_file
                    
                    total_processed = len(st.session_state.realtime_results)
                    st.success(f"âœ… çˆ¬å–å®Œæˆï¼è™•ç†äº† {total_processed} ç¯‡è²¼æ–‡")
                    st.balloons()
                else:
                    st.error("âŒ æœªæ‰¾åˆ°çµæœæ–‡ä»¶")
            else:
                st.error(f"âŒ çˆ¬å–å¤±æ•— (è¿”å›ç¢¼: {return_code})")
                # é¡¯ç¤ºæœ€å¾Œçš„éŒ¯èª¤æ—¥å¿—
                if all_output:
                    error_lines = [line for line in all_output if 'âŒ' in line or 'Error' in line or 'Exception' in line]
                    if error_lines:
                        st.error("éŒ¯èª¤è©³æƒ…ï¼š")
                        for error_line in error_lines[-5:]:  # é¡¯ç¤ºæœ€å¾Œ5æ¢éŒ¯èª¤
                            st.text(error_line)
                
        except Exception as e:
            st.error(f"âŒ åŸ·è¡ŒéŒ¯èª¤ï¼š{str(e)}")
            st.session_state.realtime_error = str(e)
    
    def _display_database_stats(self):
        """é¡¯ç¤ºè³‡æ–™åº«çµ±è¨ˆä¿¡æ¯"""
        try:
            # ä½¿ç”¨ asyncio å’Œ subprocess ä¾†ç²å–è³‡æ–™åº«çµ±è¨ˆ
            import subprocess
            import json
            import sys
            import os
            
            # å‰µå»ºä¸€å€‹è‡¨æ™‚è…³æœ¬ä¾†ç²å–è³‡æ–™åº«çµ±è¨ˆ
            script_content = '''
import asyncio
import sys
import os
import json
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from common.incremental_crawl_manager import IncrementalCrawlManager

async def get_database_stats():
    crawl_manager = IncrementalCrawlManager()
    try:
        await crawl_manager.db.init_pool()
        
        # ç²å–æ‰€æœ‰ç”¨æˆ¶çš„çµ±è¨ˆä¿¡æ¯
        async with crawl_manager.db.get_connection() as conn:
            # çµ±è¨ˆæ¯å€‹ç”¨æˆ¶çš„è²¼æ–‡æ•¸é‡
            user_stats = await conn.fetch("""
                SELECT 
                    username,
                    COUNT(*) as post_count,
                    MAX(created_at) as latest_crawl,
                    MIN(created_at) as first_crawl
                FROM post_metrics_sql 
                GROUP BY username 
                ORDER BY post_count DESC, latest_crawl DESC
                LIMIT 20
            """)
            
            # ç¸½é«”çµ±è¨ˆ
            total_stats = await conn.fetchrow("""
                SELECT 
                    COUNT(*) as total_posts,
                    COUNT(DISTINCT username) as total_users,
                    MAX(created_at) as latest_activity
                FROM post_metrics_sql
            """)
            
            stats = {
                "total_stats": dict(total_stats) if total_stats else {},
                "user_stats": [dict(row) for row in user_stats] if user_stats else []
            }
            
            print(json.dumps(stats, default=str))
            
    except Exception as e:
        print(json.dumps({"error": str(e)}))
    finally:
        await crawl_manager.db.close_pool()

if __name__ == "__main__":
    asyncio.run(get_database_stats())
'''
            
            # å°‡è…³æœ¬å¯«å…¥è‡¨æ™‚æ–‡ä»¶
            import tempfile
            with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False, encoding='utf-8') as f:
                f.write(script_content)
                temp_script = f.name
            
            try:
                # åŸ·è¡Œè…³æœ¬ç²å–çµ±è¨ˆä¿¡æ¯
                result = subprocess.run(
                    [sys.executable, temp_script],
                    capture_output=True,
                    text=True,
                    encoding='utf-8',
                    cwd=os.path.dirname(os.path.dirname(os.path.dirname(__file__))),
                    timeout=10
                )
                
                if result.returncode == 0 and result.stdout.strip():
                    stats = json.loads(result.stdout.strip())
                    
                    if "error" in stats:
                        st.error(f"âŒ è³‡æ–™åº«éŒ¯èª¤: {stats['error']}")
                        return
                    
                    # é¡¯ç¤ºç¸½é«”çµ±è¨ˆ
                    total_stats = stats.get("total_stats", {})
                    if total_stats:
                        st.info(f"""
                        **ğŸ“ˆ ç¸½é«”çµ±è¨ˆ**
                        - ğŸ“Š ç¸½è²¼æ–‡æ•¸: {total_stats.get('total_posts', 0):,}
                        - ğŸ‘¥ å·²çˆ¬å–ç”¨æˆ¶: {total_stats.get('total_users', 0)} å€‹
                        - â° æœ€å¾Œæ´»å‹•: {str(total_stats.get('latest_activity', 'N/A'))[:16] if total_stats.get('latest_activity') else 'N/A'}
                        """)
                    
                    # é¡¯ç¤ºç”¨æˆ¶çµ±è¨ˆ
                    user_stats = stats.get("user_stats", [])
                    if user_stats:
                        st.write("**ğŸ‘¥ å„ç”¨æˆ¶çµ±è¨ˆ:**")
                        
                        # ä½¿ç”¨è¡¨æ ¼é¡¯ç¤º
                        import pandas as pd
                        df_data = []
                        for user in user_stats:
                            latest = str(user.get('latest_crawl', 'N/A'))[:16] if user.get('latest_crawl') else 'N/A'
                            df_data.append({
                                "ç”¨æˆ¶å": f"@{user.get('username', 'N/A')}",
                                "è²¼æ–‡æ•¸": f"{user.get('post_count', 0):,}",
                                "æœ€å¾Œçˆ¬å–": latest
                            })
                        
                        if df_data:
                            df = pd.DataFrame(df_data)
                            st.dataframe(
                                df, 
                                use_container_width=True,
                                hide_index=True,
                                height=min(300, len(df_data) * 35 + 38)  # å‹•æ…‹é«˜åº¦
                            )
                    else:
                        st.warning("ğŸ“ è³‡æ–™åº«ä¸­æš«ç„¡çˆ¬å–è¨˜éŒ„")
                else:
                    st.warning("âš ï¸ ç„¡æ³•ç²å–è³‡æ–™åº«çµ±è¨ˆä¿¡æ¯")
                    if result.stderr:
                        st.text(f"éŒ¯èª¤: {result.stderr}")
                        
            finally:
                # æ¸…ç†è‡¨æ™‚æ–‡ä»¶
                try:
                    os.unlink(temp_script)
                except:
                    pass
                    
        except Exception as e:
            st.error(f"âŒ ç²å–çµ±è¨ˆä¿¡æ¯å¤±æ•—: {str(e)}")
    
    def _show_json_download_button(self, results_file):
        """é¡¯ç¤ºJSONä¸‹è¼‰æŒ‰éˆ•"""
        if results_file and Path(results_file).exists():
            try:
                # è®€å–JSONæ–‡ä»¶å…§å®¹
                with open(results_file, 'r', encoding='utf-8') as f:
                    json_content = f.read()
                
                # ç”Ÿæˆä¸‹è¼‰æ–‡ä»¶åï¼ˆåŒ…å«æ™‚é–“æˆ³ï¼‰
                file_path = Path(results_file)
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                download_filename = f"crawl_results_{timestamp}.json"
                
                # ä½¿ç”¨ st.download_button æä¾›ä¸‹è¼‰
                st.download_button(
                    label="ğŸ’¾ ä¸‹è¼‰JSON",
                    data=json_content,
                    file_name=download_filename,
                    mime="application/json",
                    help="ä¸‹è¼‰çˆ¬å–çµæœJSONæ–‡ä»¶åˆ°æ‚¨çš„ä¸‹è¼‰è³‡æ–™å¤¾",
                    key="download_json_btn"
                )
                
            except Exception as e:
                st.error(f"âŒ æº–å‚™ä¸‹è¼‰æ–‡ä»¶å¤±æ•—: {e}")
        else:
            st.button("ğŸ’¾ ä¸‹è¼‰JSON", disabled=True, help="æš«ç„¡å¯ä¸‹è¼‰çš„çµæœæ–‡ä»¶")
    
    def _reset_results(self):
        """é‡ç½®çµæœ"""
        if 'realtime_results' in st.session_state:
            del st.session_state.realtime_results
        if 'realtime_results_file' in st.session_state:
            del st.session_state.realtime_results_file
        if 'realtime_error' in st.session_state:
            del st.session_state.realtime_error
        st.success("ğŸ”„ çµæœå·²é‡ç½®")
    
    def _render_results_area(self):
        """æ¸²æŸ“çµæœå€åŸŸ"""
        if 'realtime_results' in st.session_state:
            self._show_results()
        elif 'realtime_error' in st.session_state:
            st.error(f"âŒ çˆ¬å–éŒ¯èª¤ï¼š{st.session_state.realtime_error}")
        else:
            st.info("ğŸ‘† é»æ“Šã€Œé–‹å§‹çˆ¬å–ã€ä¾†é–‹å§‹")
    
    def _show_results(self):
        """é¡¯ç¤ºçˆ¬å–çµæœ"""
        results = st.session_state.realtime_results
        results_file = st.session_state.get('realtime_results_file', 'unknown.json')
        
        st.subheader("ğŸ“Š çˆ¬å–çµæœ")
        
        # åŸºæœ¬çµ±è¨ˆ
        total_posts = len(results)
        successful_views = len([r for r in results if r.get('has_views')])
        successful_content = len([r for r in results if r.get('has_content')])
        successful_likes = len([r for r in results if r.get('has_likes')])
        successful_comments = len([r for r in results if r.get('has_comments')])
        
        # çµ±è¨ˆæŒ‡æ¨™
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("ç¸½è²¼æ–‡æ•¸", total_posts)
        with col2:
            st.metric("è§€çœ‹æ•¸æˆåŠŸ", f"{successful_views}/{total_posts}")
        with col3:
            st.metric("å…§å®¹æˆåŠŸ", f"{successful_content}/{total_posts}")
        with col4:
            st.metric("äº’å‹•æ•¸æ“š", f"{successful_likes}/{total_posts}")
        
        # æˆåŠŸç‡æŒ‡æ¨™
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            view_rate = (successful_views / total_posts * 100) if total_posts > 0 else 0
            st.metric("è§€çœ‹æ•¸æˆåŠŸç‡", f"{view_rate:.1f}%")
        with col2:
            content_rate = (successful_content / total_posts * 100) if total_posts > 0 else 0
            st.metric("å…§å®¹æˆåŠŸç‡", f"{content_rate:.1f}%")
        with col3:
            like_rate = (successful_likes / total_posts * 100) if total_posts > 0 else 0
            st.metric("æŒ‰è®šæ•¸æˆåŠŸç‡", f"{like_rate:.1f}%")
        with col4:
            comment_rate = (successful_comments / total_posts * 100) if total_posts > 0 else 0
            st.metric("ç•™è¨€æ•¸æˆåŠŸç‡", f"{comment_rate:.1f}%")
        
        # é‡è¤‡è™•ç†åŠŸèƒ½
        st.divider()
        col1, col2, col3 = st.columns([2, 1, 1])
        with col1:
            st.write("**ğŸ”„ é‡è¤‡è™•ç†**")
            st.caption("æª¢æ¸¬é‡è¤‡è²¼æ–‡ï¼Œè§€çœ‹æ•¸ä½çš„ç”¨APIé‡æ–°æå–")
        with col2:
            if st.button("ğŸ” æª¢æ¸¬é‡è¤‡", key="detect_duplicates"):
                self._detect_duplicates()
        with col3:
            if st.button("ğŸ”„ è™•ç†é‡è¤‡", key="process_duplicates"):
                self._process_duplicates()
        
        # è©³ç´°çµæœè¡¨æ ¼
        if st.checkbox("ğŸ“‹ é¡¯ç¤ºè©³ç´°çµæœ", key="show_detailed_results"):
            self._show_detailed_table(results)
        
        # ä¸‹è¼‰å’Œå°å‡ºæŒ‰éˆ•
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            self._show_json_download_button(results_file)
        
        with col2:
            if st.button("ğŸ“Š å°å‡ºCSV", key="export_csv"):
                self._show_csv_export_options(results_file)
        
        with col3:
            if st.button("ğŸ“ˆ æ­·å²åˆ†æ", key="export_history"):
                self._show_export_history_options()
        
        with col4:
            if st.button("ğŸ” æ›´å¤šå°å‡º", key="more_exports"):
                self._show_advanced_export_options()
    
    def _detect_duplicates(self):
        """æª¢æ¸¬é‡è¤‡è²¼æ–‡"""
        if 'realtime_results' not in st.session_state:
            st.error("âŒ æ²’æœ‰å¯æª¢æ¸¬çš„çµæœ")
            return
        
        results = st.session_state.realtime_results
        
        # æŒ‰ post_id åˆ†çµ„
        from collections import defaultdict
        grouped = defaultdict(list)
        for result in results:
            if result.get('post_id'):
                grouped[result['post_id']].append(result)
        
        # æ‰¾å‡ºé‡è¤‡é …
        duplicates = {k: v for k, v in grouped.items() if len(v) > 1}
        
        if not duplicates:
            st.success("âœ… æ²’æœ‰ç™¼ç¾é‡è¤‡è²¼æ–‡")
            return
        
        st.warning(f"âš ï¸ ç™¼ç¾ {len(duplicates)} çµ„é‡è¤‡è²¼æ–‡")
        
        for post_id, items in duplicates.items():
            with st.expander(f"ğŸ“‹ é‡è¤‡çµ„: {post_id} ({len(items)} å€‹ç‰ˆæœ¬)"):
                for i, item in enumerate(items):
                    views = item.get('views', 'N/A')
                    source = item.get('source', 'unknown')
                    content = item.get('content', 'N/A')[:100] + '...' if item.get('content') else 'N/A'
                    
                    col1, col2, col3 = st.columns([1, 1, 3])
                    with col1:
                        st.write(f"**ç‰ˆæœ¬ {i+1}**")
                        st.write(f"è§€çœ‹æ•¸: {views}")
                    with col2:
                        st.write(f"ä¾†æº: {source}")
                    with col3:
                        st.write(f"å…§å®¹: {content}")
    
    def _process_duplicates(self):
        """è™•ç†é‡è¤‡è²¼æ–‡"""
        if 'realtime_results' not in st.session_state:
            st.error("âŒ æ²’æœ‰å¯è™•ç†çš„çµæœ")
            return
        
        # èª¿ç”¨é‡è¤‡è™•ç†è…³æœ¬
        try:
            import subprocess
            import sys
            import os
            
            st.info("ğŸ”„ æ­£åœ¨è™•ç†é‡è¤‡è²¼æ–‡...")
            
            # åŸ·è¡Œé‡è¤‡è™•ç†è…³æœ¬
            script_path = "fix_duplicates_reextract.py"
            result = subprocess.run(
                [sys.executable, script_path],
                capture_output=True,
                text=True,
                encoding='utf-8',
                errors='replace',
                cwd=os.getcwd()
            )
            
            if result.returncode == 0:
                # æŸ¥æ‰¾è™•ç†å¾Œçš„æ–‡ä»¶
                import glob
                from pathlib import Path
                
                # æª¢æŸ¥æ–°çš„è³‡æ–™å¤¾ä½ç½®
                extraction_dir = Path("extraction_results")
                if extraction_dir.exists():
                    dedup_files = list(extraction_dir.glob("realtime_extraction_results_*_dedup.json"))
                else:
                    dedup_files = [Path(f) for f in glob.glob("realtime_extraction_results_*_dedup.json")]
                
                if dedup_files:
                    latest_dedup = max(dedup_files, key=lambda f: f.stat().st_mtime)
                    
                    # è®€å–è™•ç†å¾Œçš„çµæœ
                    import json
                    with open(latest_dedup, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    
                    # æ›´æ–°session_state
                    st.session_state.realtime_results = data.get('results', [])
                    st.session_state.realtime_results_file = latest_dedup
                    
                    duplicates_count = data.get('duplicates_processed', 0)
                    reextracted_count = data.get('reextracted_count', 0)
                    
                    st.success(f"âœ… é‡è¤‡è™•ç†å®Œæˆï¼")
                    st.info(f"ğŸ“Š è™•ç†äº† {duplicates_count} çµ„é‡è¤‡ï¼Œé‡æ–°æå– {reextracted_count} å€‹é …ç›®")
                    st.balloons()
                    
                    # è‡ªå‹•åˆ·æ–°é é¢ä»¥é¡¯ç¤ºæ›´æ–°çµæœ
                    st.rerun()
                else:
                    st.error("âŒ æœªæ‰¾åˆ°è™•ç†å¾Œçš„çµæœæ–‡ä»¶")
            else:
                st.error(f"âŒ è™•ç†å¤±æ•—ï¼š{result.stderr}")
                st.code(result.stdout)
                
        except Exception as e:
            st.error(f"âŒ è™•ç†éŒ¯èª¤ï¼š{str(e)}")
    
    def _show_csv_export_options(self, json_file_path: str):
        """é¡¯ç¤ºCSVå°å‡ºé¸é …"""
        with st.expander("ğŸ“Š CSVå°å‡ºé¸é …", expanded=True):
            st.write("**é¸æ“‡æ’åºæ–¹å¼ï¼ˆå»ºè­°æŒ‰è§€çœ‹æ•¸æ’åºï¼‰**")
            
            sort_options = {
                "è§€çœ‹æ•¸ (é«˜â†’ä½)": "views",
                "æŒ‰è®šæ•¸ (é«˜â†’ä½)": "likes", 
                "ç•™è¨€æ•¸ (é«˜â†’ä½)": "comments",
                "è½‰ç™¼æ•¸ (é«˜â†’ä½)": "reposts",
                "åˆ†äº«æ•¸ (é«˜â†’ä½)": "shares",
                "è²¼æ–‡ID (Aâ†’Z)": "post_id",
                "åŸå§‹é †åº (ä¸æ’åº)": "none"
            }
            
            selected_sort = st.selectbox(
                "æ’åºæ–¹å¼",
                options=list(sort_options.keys()),
                index=0,  # é è¨­é¸æ“‡è§€çœ‹æ•¸æ’åº
                help="é¸æ“‡CSVæ–‡ä»¶ä¸­æ•¸æ“šçš„æ’åºæ–¹å¼ï¼Œå»ºè­°é¸æ“‡è§€çœ‹æ•¸ä»¥ä¾¿åˆ†ææœ€å—æ­¡è¿çš„è²¼æ–‡"
            )
            
            col1, col2 = st.columns(2)
            
            with col1:
                if st.button("ğŸ“¥ ç”ŸæˆCSV", key="export_csv_generate"):
                    sort_by = sort_options[selected_sort]
                    self._export_current_to_csv(json_file_path, sort_by)
                
                # æª¢æŸ¥æ˜¯å¦æœ‰ç”Ÿæˆå¥½çš„CSVå¯ä»¥ä¸‹è¼‰
                self._show_csv_download_if_available()
            
            with col2:
                st.info("ğŸ’¡ **CSVä½¿ç”¨æç¤ºï¼š**\n- ç”¨Excelæˆ–Google Sheetsæ‰“é–‹\n- å¯ä»¥é€²ä¸€æ­¥ç¯©é¸å’Œåˆ†æ\n- æ”¯æ´ä¸­æ–‡é¡¯ç¤º")
    
    def _export_current_to_csv(self, json_file_path: str, sort_by: str = 'views'):
        """å°å‡ºç•¶æ¬¡çµæœåˆ°CSV"""
        try:
            from common.csv_export_manager import CSVExportManager
            
            csv_manager = CSVExportManager()
            csv_file = csv_manager.export_current_session(json_file_path, sort_by=sort_by)
            
            # ä¿å­˜CSVæ–‡ä»¶è·¯å¾‘åˆ°æœƒè©±ç‹€æ…‹
            st.session_state.latest_csv_file = csv_file
            
            st.success(f"âœ… CSVç”ŸæˆæˆåŠŸï¼")
            st.info(f"ğŸ“ æ–‡ä»¶ä½ç½®: {csv_file}")
            
        except Exception as e:
            st.error(f"âŒ CSVç”Ÿæˆå¤±æ•—: {str(e)}")
            if 'latest_csv_file' in st.session_state:
                del st.session_state.latest_csv_file
    
    def _show_csv_download_if_available(self):
        """é¡¯ç¤ºCSVä¸‹è¼‰æŒ‰éˆ•ï¼ˆå¦‚æœæœ‰å¯ç”¨çš„CSVæ–‡ä»¶ï¼‰"""
        if 'latest_csv_file' in st.session_state:
            csv_file = st.session_state.latest_csv_file
            if csv_file and Path(csv_file).exists():
                try:
                    with open(csv_file, 'r', encoding='utf-8-sig') as f:
                        csv_content = f.read()
                    
                    # ç”Ÿæˆæ™‚é–“æˆ³æ–‡ä»¶å
                    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                    download_filename = f"crawl_results_{timestamp}.csv"
                    
                    st.download_button(
                        label="ğŸ“¥ ä¸‹è¼‰CSVæ–‡ä»¶",
                        data=csv_content,
                        file_name=download_filename,
                        mime="text/csv",
                        help="ä¸‹è¼‰CSVæ–‡ä»¶åˆ°æ‚¨çš„ä¸‹è¼‰è³‡æ–™å¤¾",
                        key="download_csv_file_btn"
                    )
                    
                except Exception as e:
                    st.error(f"âŒ æº–å‚™CSVä¸‹è¼‰å¤±æ•—: {e}")
    
    def _show_export_history_options(self):
        """é¡¯ç¤ºæ­·å²å°å‡ºé¸é …"""
        if 'realtime_results' not in st.session_state:
            st.error("âŒ è«‹å…ˆåŸ·è¡Œçˆ¬å–ä»¥ç²å–å¸³è™Ÿä¿¡æ¯")
            return
        
        # ç²å–ç•¶å‰å¸³è™Ÿ
        results = st.session_state.realtime_results
        if not results:
            st.error("âŒ ç„¡æ³•ç²å–å¸³è™Ÿä¿¡æ¯")
            return
        
        # å‡è¨­å¾ç¬¬ä¸€å€‹çµæœä¸­æå–ç”¨æˆ¶å
        target_username = None
        if 'realtime_results_file' in st.session_state:
            try:
                import json
                with open(st.session_state.realtime_results_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                target_username = data.get('target_username')
            except:
                pass
        
        if not target_username:
            st.error("âŒ ç„¡æ³•è­˜åˆ¥ç›®æ¨™å¸³è™Ÿ")
            return
        
        with st.expander("ğŸ“ˆ æ­·å²æ•¸æ“šå°å‡ºé¸é …", expanded=True):
            export_type = st.radio(
                "é¸æ“‡å°å‡ºé¡å‹",
                options=["æœ€è¿‘æ•¸æ“š", "å…¨éƒ¨æ­·å²", "çµ±è¨ˆåˆ†æ"],
                help="é¸æ“‡è¦å°å‡ºçš„æ­·å²æ•¸æ“šç¯„åœ"
            )
            
            col1, col2 = st.columns(2)
            
            if export_type == "æœ€è¿‘æ•¸æ“š":
                with col1:
                    days_back = st.number_input("å›æº¯å¤©æ•¸", min_value=1, max_value=365, value=7)
                with col2:
                    limit = st.number_input("æœ€å¤§è¨˜éŒ„æ•¸", min_value=10, max_value=10000, value=1000)
                
                if st.button("ğŸ“Š å°å‡ºæœ€è¿‘æ•¸æ“š", key="export_recent"):
                    self._export_history_data(target_username, "recent", days_back=days_back, limit=limit)
            
            elif export_type == "å…¨éƒ¨æ­·å²":
                with col1:
                    limit = st.number_input("æœ€å¤§è¨˜éŒ„æ•¸", min_value=100, max_value=50000, value=5000)
                
                if st.button("ğŸ“Š å°å‡ºå…¨éƒ¨æ­·å²", key="export_all"):
                    self._export_history_data(target_username, "all", limit=limit)
            
            elif export_type == "çµ±è¨ˆåˆ†æ":
                st.info("æŒ‰æ—¥æœŸçµ±è¨ˆçš„åˆ†æå ±å‘Šï¼ŒåŒ…å«å¹³å‡è§€çœ‹æ•¸ã€æˆåŠŸç‡ç­‰æŒ‡æ¨™")
                
                if st.button("ğŸ“ˆ å°å‡ºçµ±è¨ˆåˆ†æ", key="export_analysis"):
                    self._export_history_data(target_username, "analysis")
    
    def _export_history_data(self, username: str, export_type: str, **kwargs):
        """å°å‡ºæ­·å²æ•¸æ“š"""
        try:
            from common.csv_export_manager import CSVExportManager
            import asyncio
            
            csv_manager = CSVExportManager()
            
            # å‰µå»ºæ–°çš„äº‹ä»¶å¾ªç’°ä¾†é¿å…è¡çª
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            
            try:
                if export_type == "recent":
                    csv_file = loop.run_until_complete(
                        csv_manager.export_database_history(
                            username, 
                            days_back=kwargs.get('days_back'),
                            limit=kwargs.get('limit')
                        )
                    )
                elif export_type == "all":
                    csv_file = loop.run_until_complete(
                        csv_manager.export_database_history(
                            username,
                            limit=kwargs.get('limit')
                        )
                    )
                elif export_type == "analysis":
                    csv_file = loop.run_until_complete(
                        csv_manager.export_combined_analysis(username)
                    )
                else:
                    raise ValueError(f"ä¸æ”¯æŒçš„å°å‡ºé¡å‹: {export_type}")
                
                st.success(f"âœ… æ­·å²æ•¸æ“šå°å‡ºæˆåŠŸï¼")
                st.info(f"ğŸ“ æ–‡ä»¶ä½ç½®: {csv_file}")
                
                # æä¾›ä¸‹è¼‰
                import os
                if os.path.exists(csv_file):
                    with open(csv_file, 'r', encoding='utf-8-sig') as f:
                        csv_content = f.read()
                    
                    st.download_button(
                        label="ğŸ“¥ ä¸‹è¼‰æ­·å²CSVæ–‡ä»¶",
                        data=csv_content,
                        file_name=os.path.basename(csv_file),
                        mime="text/csv"
                    )
                
            finally:
                loop.close()
                
        except Exception as e:
            st.error(f"âŒ æ­·å²æ•¸æ“šå°å‡ºå¤±æ•—: {str(e)}")
    
    def _show_advanced_export_options(self):
        """é¡¯ç¤ºé€²éšå°å‡ºé¸é …"""
        with st.expander("ğŸ” é€²éšå°å‡ºåŠŸèƒ½", expanded=True):
            st.markdown("**æ›´å¤šå°å‡ºé¸é …å’Œæ‰¹é‡æ“ä½œ**")
            
            tab1, tab2, tab3 = st.tabs(["ğŸ“Š å°æ¯”å ±å‘Š", "ğŸ”„ æ‰¹é‡å°å‡º", "âš¡ å¿«é€Ÿå·¥å…·"])
            
            with tab1:
                st.subheader("ğŸ“Š å¤šæ¬¡çˆ¬å–å°æ¯”å ±å‘Š")
                st.info("æ¯”è¼ƒå¤šæ¬¡çˆ¬å–çµæœçš„æ•ˆèƒ½å’ŒæˆåŠŸç‡")
                
                # æŸ¥æ‰¾æ‰€æœ‰JSONæ–‡ä»¶
                import glob
                import os
                # æª¢æŸ¥æ–°çš„è³‡æ–™å¤¾ä½ç½®
                extraction_dir = Path("extraction_results")
                if extraction_dir.exists():
                    json_files = list(extraction_dir.glob("realtime_extraction_results_*.json"))
                else:
                    json_files = [Path(f) for f in glob.glob("realtime_extraction_results_*.json")]
                
                if len(json_files) >= 2:
                    st.write(f"ğŸ” æ‰¾åˆ° {len(json_files)} å€‹çˆ¬å–çµæœæ–‡ä»¶ï¼š")
                    
                    # é¡¯ç¤ºæ–‡ä»¶åˆ—è¡¨ - ä½¿ç”¨ multiselect æ›´ç›´è§€
                    file_options = {}
                    for file in sorted(json_files, reverse=True)[:10]:  # æœ€æ–°çš„10å€‹
                        file_time = self._extract_time_from_filename(str(file))
                        display_name = f"{file.name} ({file_time})"
                        file_options[display_name] = str(file)
                    
                    # åˆå§‹åŒ–æœƒè©±ç‹€æ…‹
                    if "comparison_selected_files" not in st.session_state:
                        st.session_state.comparison_selected_files = []
                    
                    selected_displays = st.multiselect(
                        "é¸æ“‡è¦æ¯”å°çš„æ–‡ä»¶ï¼ˆè‡³å°‘2å€‹ï¼‰ï¼š",
                        options=list(file_options.keys()),
                        default=[],
                        help="é¸æ“‡å¤šå€‹æ–‡ä»¶é€²è¡Œæ¯”å°åˆ†æ",
                        key="comparison_file_selector"
                    )
                    
                    selected_files = [file_options[display] for display in selected_displays]
                    
                    # æ·»åŠ èª¿è©¦ä¿¡æ¯
                    if selected_displays:
                        st.text(f"ğŸ” èª¿è©¦: ç•¶å‰é¸ä¸­ {len(selected_displays)} å€‹é¡¯ç¤ºé …ç›®")
                        for i, display in enumerate(selected_displays):
                            st.text(f"   {i+1}. {display}")
                    
                    if len(selected_files) >= 2:
                        st.success(f"âœ… å·²é¸æ“‡ {len(selected_files)} å€‹æ–‡ä»¶é€²è¡Œæ¯”å°")
                        
                        # é¡¯ç¤ºé¸ä¸­çš„æ–‡ä»¶æ‘˜è¦
                        with st.expander("ğŸ“„ é¸ä¸­æ–‡ä»¶æ‘˜è¦", expanded=True):
                            for i, file_path in enumerate(selected_files):
                                try:
                                    with open(file_path, 'r', encoding='utf-8') as f:
                                        data = json.load(f)
                                    
                                    timestamp = data.get('timestamp', 'N/A')
                                    success_count = data.get('total_processed', 0)
                                    success_rate = data.get('overall_success_rate', 0)
                                    
                                    st.markdown(f"**ğŸ“ æ–‡ä»¶ {i+1}: {Path(file_path).name}**")
                                    col1, col2, col3 = st.columns(3)
                                    with col1:
                                        st.text(f"â° æ™‚é–“: {timestamp[:16] if timestamp != 'N/A' else 'N/A'}")
                                    with col2:
                                        st.text(f"âœ… æˆåŠŸ: {success_count} å€‹")
                                    with col3:
                                        st.text(f"ğŸ“Š æˆåŠŸç‡: {success_rate:.1f}%")
                                    st.divider()
                                except Exception as e:
                                    st.error(f"âŒ è®€å– {Path(file_path).name} å¤±æ•—: {e}")
                        
                        if st.button("ğŸ“Š ç”Ÿæˆå°æ¯”å ±å‘Š", key="generate_comparison", type="primary"):
                            with st.spinner("æ­£åœ¨ç”Ÿæˆå°æ¯”å ±å‘Š..."):
                                self._generate_comparison_report(selected_files)
                    elif len(selected_files) == 1:
                        st.warning("âš ï¸ å·²é¸æ“‡1å€‹æ–‡ä»¶ï¼Œè«‹å†é¸æ“‡è‡³å°‘1å€‹æ–‡ä»¶é€²è¡Œæ¯”å°")
                        st.info("ğŸ’¡ æç¤ºï¼šå¯ä»¥æŒ‰ä½ Ctrl éµé»æ“Šå…¶ä»–æ–‡ä»¶ä¾†å¤šé¸")
                    else:
                        st.info("ğŸ’¡ è«‹é¸æ“‡è‡³å°‘2å€‹æ–‡ä»¶é€²è¡Œæ¯”å°åˆ†æ")
                else:
                    st.warning("âš ï¸ éœ€è¦è‡³å°‘2å€‹çˆ¬å–çµæœæ–‡ä»¶æ‰èƒ½é€²è¡Œå°æ¯”")
            
            with tab2:
                st.subheader("ğŸ”„ æ‰¹é‡å°å‡ºåŠŸèƒ½")
                
                col1, col2 = st.columns(2)
                
                with col1:
                    if st.button("ğŸ“¥ å°å‡ºæ‰€æœ‰æœ€æ–°çµæœ", key="export_all_latest"):
                        self._export_all_latest_results()
                
                with col2:
                    if st.button("ğŸ“ˆ å°å‡ºæ‰€æœ‰å¸³è™Ÿçµ±è¨ˆ", key="export_all_stats"):
                        self._export_all_account_stats()
                
                st.divider()
                
                # è‡ªå‹•åŒ–å°å‡ºè¨­å®š
                st.write("**è‡ªå‹•åŒ–å°å‡ºè¨­å®š**")
                auto_sort = st.selectbox(
                    "é è¨­æ’åºæ–¹å¼",
                    ["è§€çœ‹æ•¸", "æŒ‰è®šæ•¸", "ç•™è¨€æ•¸", "æ™‚é–“é †åº"],
                    help="æ‰¹é‡å°å‡ºæ™‚ä½¿ç”¨çš„é è¨­æ’åº"
                )
                
                if st.button("ğŸ’¾ ä¿å­˜è¨­å®š", key="save_export_settings"):
                    st.session_state.default_sort = auto_sort
                    st.success(f"âœ… å·²ä¿å­˜é è¨­æ’åº: {auto_sort}")
            
            with tab3:
                st.subheader("âš¡ å¿«é€Ÿå·¥å…·")
                
                # å¿«é€Ÿé è¦½
                st.write("**å¿«é€Ÿé è¦½CSVæ–‡ä»¶**")
                uploaded_csv = st.file_uploader(
                    "ä¸Šå‚³CSVæ–‡ä»¶é€²è¡Œé è¦½",
                    type=['csv'],
                    help="ä¸Šå‚³ä»»ä½•CSVæ–‡ä»¶ï¼Œå¿«é€ŸæŸ¥çœ‹å‰å¹¾è¡Œæ•¸æ“š"
                )
                
                if uploaded_csv:
                    try:
                        import pandas as pd
                        df = pd.read_csv(uploaded_csv, encoding='utf-8-sig')
                        
                        col1, col2 = st.columns(2)
                        with col1:
                            st.metric("ç¸½è¡Œæ•¸", len(df))
                        with col2:
                            st.metric("ç¸½æ¬„ä½", len(df.columns))
                        
                        st.write("**å‰10è¡Œé è¦½ï¼š**")
                        st.dataframe(df.head(10), use_container_width=True)
                        
                    except Exception as e:
                        st.error(f"âŒ é è¦½å¤±æ•—: {e}")
                
                st.divider()
                
                # æ¸…ç†å·¥å…·
                st.write("**æ¸…ç†å·¥å…·**")
                if st.button("ğŸ—‘ï¸ æ¸…ç†èˆŠçš„å°å‡ºæ–‡ä»¶", key="cleanup_exports"):
                    self._cleanup_old_exports()
    
    def _extract_time_from_filename(self, filename: str) -> str:
        """å¾æ–‡ä»¶åæå–æ™‚é–“"""
        try:
            import re
            match = re.search(r'_(\d{8}_\d{6})\.json$', filename)
            if match:
                time_str = match.group(1)
                # æ ¼å¼åŒ–ç‚ºå¯è®€æ™‚é–“
                date_part = time_str[:8]
                time_part = time_str[9:]
                return f"{date_part[:4]}-{date_part[4:6]}-{date_part[6:8]} {time_part[:2]}:{time_part[2:4]}:{time_part[4:6]}"
        except:
            pass
        return "æœªçŸ¥æ™‚é–“"
    
    def _generate_comparison_report(self, selected_files: List[str]):
        """ç”Ÿæˆå°æ¯”å ±å‘Š"""
        try:
            from common.csv_export_manager import CSVExportManager
            
            csv_manager = CSVExportManager()
            csv_file = csv_manager.export_comparison_report(selected_files)
            
            st.success("âœ… å°æ¯”å ±å‘Šç”ŸæˆæˆåŠŸï¼")
            st.info(f"ğŸ“ æ–‡ä»¶ä½ç½®: {csv_file}")
            
            # æä¾›ä¸‹è¼‰
            import os
            if os.path.exists(csv_file):
                with open(csv_file, 'r', encoding='utf-8-sig') as f:
                    csv_content = f.read()
                
                st.download_button(
                    label="ğŸ“¥ ä¸‹è¼‰å°æ¯”å ±å‘Š",
                    data=csv_content,
                    file_name=os.path.basename(csv_file),
                    mime="text/csv"
                )
                
                # é¡¯ç¤ºæ‘˜è¦
                st.write("**ğŸ“Š å°æ¯”æ‘˜è¦ï¼š**")
                import pandas as pd
                df = pd.read_csv(csv_file, encoding='utf-8-sig')
                
                # é¡¯ç¤ºå®Œæ•´è¡¨æ ¼
                st.dataframe(df, use_container_width=True)
                
                # é¡¯ç¤ºé—œéµæŒ‡æ¨™å°æ¯”
                if len(df) >= 2:
                    st.write("**ğŸ” é—œéµæŒ‡æ¨™åˆ†æï¼š**")
                    
                    col1, col2, col3 = st.columns(3)
                    
                    with col1:
                        avg_success_rate = df['æˆåŠŸç‡(%)'].mean()
                        max_success_rate = df['æˆåŠŸç‡(%)'].max()
                        min_success_rate = df['æˆåŠŸç‡(%)'].min()
                        st.metric("å¹³å‡æˆåŠŸç‡", f"{avg_success_rate:.1f}%", 
                                 f"{max_success_rate - min_success_rate:.1f}% å·®è·")
                    
                    with col2:
                        if 'ç¸½è€—æ™‚(ç§’)' in df.columns:
                            avg_time = df['ç¸½è€—æ™‚(ç§’)'].mean()
                            fastest = df['ç¸½è€—æ™‚(ç§’)'].min()
                            slowest = df['ç¸½è€—æ™‚(ç§’)'].max()
                            st.metric("å¹³å‡è€—æ™‚", f"{avg_time:.1f}s", 
                                     f"{slowest - fastest:.1f}s å·®è·")
                    
                    with col3:
                        if 'è§€çœ‹æ•¸æå–ç‡(%)' in df.columns:
                            avg_views_rate = df['è§€çœ‹æ•¸æå–ç‡(%)'].mean()
                            st.metric("å¹³å‡è§€çœ‹æ•¸æå–ç‡", f"{avg_views_rate:.1f}%")
                
                # é¡¯ç¤ºè¶¨å‹¢åˆ†æ
                if len(df) >= 3:
                    st.write("**ğŸ“ˆ è¶¨å‹¢åˆ†æï¼š**")
                    
                    # æŒ‰æ™‚é–“æ’åº
                    df_sorted = df.sort_values('çˆ¬å–æ™‚é–“') if 'çˆ¬å–æ™‚é–“' in df.columns else df
                    
                    # æˆåŠŸç‡è¶¨å‹¢
                    success_trend = df_sorted['æˆåŠŸç‡(%)'].diff().iloc[-1] if len(df_sorted) > 1 else 0
                    if success_trend > 0:
                        st.success(f"ğŸ“ˆ æˆåŠŸç‡å‘ˆä¸Šå‡è¶¨å‹¢ (+{success_trend:.1f}%)")
                    elif success_trend < 0:
                        st.error(f"ğŸ“‰ æˆåŠŸç‡å‘ˆä¸‹é™è¶¨å‹¢ ({success_trend:.1f}%)")
                    else:
                        st.info("ğŸ“Š æˆåŠŸç‡ä¿æŒç©©å®š")
                
        except Exception as e:
            st.error(f"âŒ ç”Ÿæˆå°æ¯”å ±å‘Šå¤±æ•—: {e}")
    
    def _export_all_latest_results(self):
        """å°å‡ºæ‰€æœ‰æœ€æ–°çµæœ"""
        try:
            import glob
            # æª¢æŸ¥æ–°çš„è³‡æ–™å¤¾ä½ç½®  
            extraction_dir = Path("extraction_results")
            if extraction_dir.exists():
                json_files = list(extraction_dir.glob("realtime_extraction_results_*.json"))
            else:
                json_files = [Path(f) for f in glob.glob("realtime_extraction_results_*.json")]
            
            if not json_files:
                st.warning("âš ï¸ æœªæ‰¾åˆ°ä»»ä½•çˆ¬å–çµæœæ–‡ä»¶")
                return
            
            # æ‰¾æœ€æ–°çš„æ–‡ä»¶
            latest_file = max(json_files, key=lambda f: f.stat().st_mtime)
            
            from common.csv_export_manager import CSVExportManager
            csv_manager = CSVExportManager()
            
            # ä½¿ç”¨é è¨­æ’åº
            default_sort = getattr(st.session_state, 'default_sort', 'è§€çœ‹æ•¸')
            sort_mapping = {"è§€çœ‹æ•¸": "views", "æŒ‰è®šæ•¸": "likes", "ç•™è¨€æ•¸": "comments", "æ™‚é–“é †åº": "none"}
            sort_by = sort_mapping.get(default_sort, "views")
            
            csv_file = csv_manager.export_current_session(latest_file, sort_by=sort_by)
            
            st.success("âœ… æœ€æ–°çµæœå°å‡ºæˆåŠŸï¼")
            st.info(f"ğŸ“ ä½¿ç”¨äº† {latest_file}")
            st.info(f"ğŸ“Š æŒ‰ {default_sort} æ’åº")
            
            # æä¾›ä¸‹è¼‰
            import os
            if os.path.exists(csv_file):
                with open(csv_file, 'r', encoding='utf-8-sig') as f:
                    csv_content = f.read()
                
                st.download_button(
                    label="ğŸ“¥ ä¸‹è¼‰æœ€æ–°çµæœCSV",
                    data=csv_content,
                    file_name=os.path.basename(csv_file),
                    mime="text/csv"
                )
                
        except Exception as e:
            st.error(f"âŒ å°å‡ºå¤±æ•—: {e}")
    
    def _export_all_account_stats(self):
        """å°å‡ºæ‰€æœ‰å¸³è™Ÿçµ±è¨ˆ"""
        try:
            from common.incremental_crawl_manager import IncrementalCrawlManager
            import asyncio
            
            # å‰µå»ºäº‹ä»¶å¾ªç’°
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            
            try:
                manager = IncrementalCrawlManager()
                
                # ç²å–æ‰€æœ‰å¸³è™Ÿ
                results = loop.run_until_complete(manager.db.fetch_all("""
                    SELECT DISTINCT username FROM crawl_state ORDER BY last_crawl_at DESC
                """))
                
                if not results:
                    st.warning("âš ï¸ æœªæ‰¾åˆ°ä»»ä½•çˆ¬å–è¨˜éŒ„")
                    return
                
                all_stats = []
                for row in results:
                    username = row['username']
                    summary = loop.run_until_complete(manager.get_crawl_summary(username))
                    
                    if 'error' not in summary:
                        checkpoint = summary['checkpoint']
                        stats = summary['statistics']
                        
                        all_stats.append({
                            'å¸³è™Ÿ': f"@{username}",
                            'æœ€æ–°è²¼æ–‡ID': checkpoint['latest_post_id'] or 'N/A',
                            'ç´¯è¨ˆçˆ¬å–': checkpoint['total_crawled'],
                            'è³‡æ–™åº«è²¼æ–‡æ•¸': stats['total_posts'],
                            'æœ‰è§€çœ‹æ•¸è²¼æ–‡': stats['posts_with_views'],
                            'å¹³å‡è§€çœ‹æ•¸': round(stats['avg_views'], 0),
                            'æœ€é«˜è§€çœ‹æ•¸': stats['max_views'],
                            'ä¸Šæ¬¡çˆ¬å–': checkpoint['last_crawl_at'].strftime('%Y-%m-%d %H:%M') if checkpoint['last_crawl_at'] else 'N/A'
                        })
                
                if all_stats:
                    # è½‰æ›ç‚ºCSV
                    import pandas as pd
                    df = pd.DataFrame(all_stats)
                    
                    from datetime import datetime
                    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                    csv_file = f"export_all_accounts_stats_{timestamp}.csv"
                    df.to_csv(csv_file, index=False, encoding='utf-8-sig')
                    
                    st.success("âœ… æ‰€æœ‰å¸³è™Ÿçµ±è¨ˆå°å‡ºæˆåŠŸï¼")
                    st.info(f"ğŸ“ æ–‡ä»¶ä½ç½®: {csv_file}")
                    
                    # é¡¯ç¤ºé è¦½
                    st.write("**çµ±è¨ˆé è¦½ï¼š**")
                    st.dataframe(df, use_container_width=True)
                    
                    # æä¾›ä¸‹è¼‰
                    csv_content = df.to_csv(index=False, encoding='utf-8-sig')
                    st.download_button(
                        label="ğŸ“¥ ä¸‹è¼‰å¸³è™Ÿçµ±è¨ˆ",
                        data=csv_content,
                        file_name=csv_file,
                        mime="text/csv"
                    )
                else:
                    st.warning("âš ï¸ æœªæ‰¾åˆ°æœ‰æ•ˆçš„çµ±è¨ˆæ•¸æ“š")
                    
            finally:
                loop.close()
                
        except Exception as e:
            st.error(f"âŒ å°å‡ºå¸³è™Ÿçµ±è¨ˆå¤±æ•—: {e}")
    
    def _cleanup_old_exports(self):
        """æ¸…ç†èˆŠçš„å°å‡ºæ–‡ä»¶"""
        try:
            import glob
            import os
            from datetime import datetime, timedelta
            
            # æ‰¾åˆ°æ‰€æœ‰å°å‡ºæ–‡ä»¶
            export_patterns = [
                "export_current_*.csv",
                "export_history_*.csv", 
                "export_analysis_*.csv",
                "export_comparison_*.csv"
            ]
            
            old_files = []
            cutoff_date = datetime.now() - timedelta(days=7)  # 7å¤©å‰
            
            for pattern in export_patterns:
                files = glob.glob(pattern)
                for file in files:
                    file_time = datetime.fromtimestamp(os.path.getmtime(file))
                    if file_time < cutoff_date:
                        old_files.append(file)
            
            if old_files:
                st.write(f"ğŸ” æ‰¾åˆ° {len(old_files)} å€‹7å¤©å‰çš„å°å‡ºæ–‡ä»¶ï¼š")
                
                for file in old_files[:5]:  # åªé¡¯ç¤ºå‰5å€‹
                    st.text(f"- {file}")
                
                if len(old_files) > 5:
                    st.text(f"... ä»¥åŠå…¶ä»– {len(old_files) - 5} å€‹æ–‡ä»¶")
                
                if st.button("ğŸ—‘ï¸ ç¢ºèªåˆªé™¤", key="confirm_cleanup"):
                    deleted_count = 0
                    for file in old_files:
                        try:
                            os.remove(file)
                            deleted_count += 1
                        except:
                            pass
                    
                    st.success(f"âœ… å·²åˆªé™¤ {deleted_count} å€‹èˆŠæ–‡ä»¶")
            else:
                st.info("âœ¨ æ²’æœ‰æ‰¾åˆ°éœ€è¦æ¸…ç†çš„èˆŠæ–‡ä»¶")
                
        except Exception as e:
            st.error(f"âŒ æ¸…ç†å¤±æ•—: {e}")
    
    def _show_detailed_table(self, results: List[Dict]):
        """é¡¯ç¤ºè©³ç´°çµæœè¡¨æ ¼"""
        st.subheader("ğŸ“‹ è©³ç´°çµæœ")
        
        # æº–å‚™è¡¨æ ¼æ•¸æ“š
        table_data = []
        for r in results:
            table_data.append({
                "è²¼æ–‡ID": r.get('post_id', 'N/A'),
                "è§€çœ‹æ•¸": r.get('views', 'N/A'),
                "æŒ‰è®šæ•¸": r.get('likes', 'N/A'),
                "ç•™è¨€æ•¸": r.get('comments', 'N/A'),
                "è½‰ç™¼æ•¸": r.get('reposts', 'N/A'),
                "åˆ†äº«æ•¸": r.get('shares', 'N/A'),
                "å…§å®¹é è¦½": (r.get('content', '')[:50] + "...") if r.get('content') else 'N/A',
                "ä¾†æº": r.get('source', 'N/A'),
                "é‡æ–°æå–": "âœ…" if r.get('reextracted', False) else ""
            })
        
        # é¡¯ç¤ºè¡¨æ ¼
        st.dataframe(
            table_data,
            use_container_width=True,
            height=400
        )
        
        # äº’å‹•æ•¸æ“šåˆ†æ
        if st.checkbox("ğŸ“ˆ äº’å‹•æ•¸æ“šåˆ†æ", key="show_engagement_analysis"):
            self._show_engagement_analysis(results)
    
    def _show_engagement_analysis(self, results: List[Dict]):
        """é¡¯ç¤ºäº’å‹•æ•¸æ“šåˆ†æ"""
        st.subheader("ğŸ“ˆ äº’å‹•æ•¸æ“šåˆ†æ")
        
        # æ”¶é›†æœ‰æ•ˆçš„äº’å‹•æ•¸æ“š
        valid_results = [r for r in results if r.get('has_views') and r.get('has_likes')]
        
        if not valid_results:
            st.warning("ç„¡è¶³å¤ çš„äº’å‹•æ•¸æ“šé€²è¡Œåˆ†æ")
            return
        
        # ç°¡å–®çµ±è¨ˆ
        avg_likes = []
        avg_comments = []
        for r in valid_results:
            if r.get('likes') and r['likes'] != 'N/A':
                try:
                    # ç°¡åŒ–çš„æ•¸å­—è½‰æ›
                    likes_str = str(r['likes']).replace('K', '000').replace('M', '000000')
                    if likes_str.replace('.', '').isdigit():
                        avg_likes.append(float(likes_str))
                except:
                    pass
        
        if avg_likes:
            col1, col2 = st.columns(2)
            with col1:
                st.metric("å¹³å‡æŒ‰è®šæ•¸", f"{sum(avg_likes)/len(avg_likes):.0f}")
            with col2:
                st.metric("æœ€é«˜æŒ‰è®šæ•¸", f"{max(avg_likes):.0f}")