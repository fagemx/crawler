"""
å¯¦æ™‚çˆ¬èŸ²çµ„ä»¶ - æ™ºèƒ½URLæ”¶é›† + è¼ªè¿´ç­–ç•¥æå–
åŒ…å«å®Œæ•´äº’å‹•æ•¸æ“šæå–åŠŸèƒ½
"""

import streamlit as st
import asyncio
import json
import time
import threading
from pathlib import Path
from typing import Dict, Any, Optional, List
import sys
import os

# æ·»åŠ å°ˆæ¡ˆæ ¹ç›®éŒ„åˆ° Python è·¯å¾‘
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

class RealtimeCrawlerComponent:
    def __init__(self):
        self.is_running = False
        self.current_task = None
        
    def render(self):
        """æ¸²æŸ“å¯¦æ™‚çˆ¬èŸ²çµ„ä»¶"""
        st.header("ğŸš€ å¯¦æ™‚æ™ºèƒ½çˆ¬èŸ²")
        st.markdown("**æ™ºèƒ½æ»¾å‹•æ”¶é›†URLs + è¼ªè¿´ç­–ç•¥å¿«é€Ÿæå– + å®Œæ•´äº’å‹•æ•¸æ“š**")
        
        # åƒæ•¸è¨­å®šå€åŸŸ
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("âš™ï¸ çˆ¬å–è¨­å®š")
            username = st.text_input(
                "ç›®æ¨™å¸³è™Ÿ", 
                value="gvmonthly",
                help="è¦çˆ¬å–çš„Threadså¸³è™Ÿç”¨æˆ¶å",
                key="realtime_username"
            )
            
            max_posts = st.number_input(
                "çˆ¬å–æ•¸é‡", 
                min_value=1, 
                max_value=500, 
                value=50,
                help="è¦çˆ¬å–çš„è²¼æ–‡æ•¸é‡",
                key="realtime_max_posts"
            )
            
            # å¢é‡çˆ¬å–æ¨¡å¼é¸é …
            crawl_mode = st.radio(
                "çˆ¬å–æ¨¡å¼",
                options=["å¢é‡çˆ¬å–", "å…¨é‡çˆ¬å–"],
                index=0,
                help="å¢é‡çˆ¬å–ï¼šåªæŠ“å–æ–°è²¼æ–‡ï¼Œé¿å…é‡è¤‡ï¼›å…¨é‡çˆ¬å–ï¼šæŠ“å–æ‰€æœ‰æ‰¾åˆ°çš„è²¼æ–‡",
                key="crawl_mode"
            )
            
        with col2:
            st.subheader("ğŸ“Š æå–ç­–ç•¥")
            strategy_info = st.info("""
            **ğŸ”„ è¼ªè¿´ç­–ç•¥ï¼š**
            - 10å€‹APIè«‹æ±‚ â†’ 20å€‹æœ¬åœ°Reader
            - é¿å…API 429é˜»æ“‹
            - è‡ªå‹•å›é€€æ©Ÿåˆ¶
            
            **ğŸ“ˆ æå–æ•¸æ“šï¼š**
            - è§€çœ‹æ•¸ã€æ–‡å­—å…§å®¹
            - æŒ‰è®šã€ç•™è¨€ã€è½‰ç™¼ã€åˆ†äº«
            """)
        
        # æ§åˆ¶æŒ‰éˆ•
        col1, col2, col3 = st.columns([1, 1, 2])
        
        with col1:
            if st.button("ğŸš€ é–‹å§‹çˆ¬å–", key="start_realtime"):
                with st.spinner("æ­£åœ¨åŸ·è¡Œçˆ¬å–..."):
                    is_incremental = crawl_mode == "å¢é‡çˆ¬å–"
                    self._execute_crawling_simple(username, max_posts, is_incremental)
                
        with col2:
            if st.button("ğŸ”„ é‡ç½®", key="reset_realtime"):
                self._reset_results()
        
        # çµæœé¡¯ç¤º
        self._render_results_area()
    
    def _execute_crawling_simple(self, username: str, max_posts: int, is_incremental: bool = True):
        """ç°¡åŒ–çš„çˆ¬å–åŸ·è¡Œæ–¹æ³• - ä½¿ç”¨åŒæ­¥ç‰ˆæœ¬é¿å…asyncioè¡çª"""
        if not username.strip():
            st.error("è«‹è¼¸å…¥ç›®æ¨™å¸³è™Ÿï¼")
            return
            
        try:
            mode_text = "å¢é‡çˆ¬å–" if is_incremental else "å…¨é‡çˆ¬å–"
            st.info(f"ğŸ”„ æ­£åœ¨åŸ·è¡Œ{mode_text}ï¼Œè«‹ç¨å€™...")
            
            # ä½¿ç”¨subprocessä¾†é¿å…asyncioè¡çª
            import subprocess
            import json
            import sys
            import os
            
            # æ§‹å»ºå‘½ä»¤
            script_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), '..', 'scripts', 'realtime_crawler_extractor.py')
            
            # ä¿®æ”¹è…³æœ¬ä»¥æ¥å—å‘½ä»¤è¡Œåƒæ•¸
            cmd = [
                sys.executable, 
                script_path,
                '--username', username,
                '--max_posts', str(max_posts)
            ]
            
            # æ·»åŠ çˆ¬å–æ¨¡å¼åƒæ•¸
            if not is_incremental:
                cmd.append('--full')  # å…¨é‡æ¨¡å¼
            
            # åŸ·è¡Œè…³æœ¬ - è¨­ç½®UTF-8ç·¨ç¢¼
            import locale
            env = os.environ.copy()
            env['PYTHONIOENCODING'] = 'utf-8'
            env['PYTHONUTF8'] = '1'
            
            result = subprocess.run(
                cmd, 
                capture_output=True, 
                text=True, 
                encoding='utf-8',
                errors='replace',
                env=env,
                cwd=os.path.dirname(os.path.dirname(os.path.dirname(__file__)))
            )
            
            if result.returncode == 0:
                # æˆåŠŸåŸ·è¡Œï¼Œå°‹æ‰¾æœ€æ–°çš„çµæœæ–‡ä»¶
                import glob
                results_pattern = "realtime_extraction_results_*.json"
                results_files = glob.glob(results_pattern)
                
                if results_files:
                    # å–æœ€æ–°çš„æ–‡ä»¶
                    latest_file = max(results_files, key=os.path.getctime)
                    
                    # è®€å–çµæœ
                    with open(latest_file, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    
                    # ä¿å­˜åˆ°session_state
                    st.session_state.realtime_results = data.get('results', [])
                    st.session_state.realtime_results_file = latest_file
                    
                    total_processed = len(st.session_state.realtime_results)
                    st.success(f"âœ… çˆ¬å–å®Œæˆï¼è™•ç†äº† {total_processed} ç¯‡è²¼æ–‡")
                    st.balloons()
                else:
                    st.error("âŒ æœªæ‰¾åˆ°çµæœæ–‡ä»¶")
            else:
                st.error(f"âŒ çˆ¬å–å¤±æ•—ï¼š{result.stderr}")
                
        except Exception as e:
            st.error(f"âŒ åŸ·è¡ŒéŒ¯èª¤ï¼š{str(e)}")
            st.session_state.realtime_error = str(e)
    
    def _reset_results(self):
        """é‡ç½®çµæœ"""
        if 'realtime_results' in st.session_state:
            del st.session_state.realtime_results
        if 'realtime_results_file' in st.session_state:
            del st.session_state.realtime_results_file
        if 'realtime_error' in st.session_state:
            del st.session_state.realtime_error
        st.success("ğŸ”„ çµæœå·²é‡ç½®")
    
    def _render_results_area(self):
        """æ¸²æŸ“çµæœå€åŸŸ"""
        if 'realtime_results' in st.session_state:
            self._show_results()
        elif 'realtime_error' in st.session_state:
            st.error(f"âŒ çˆ¬å–éŒ¯èª¤ï¼š{st.session_state.realtime_error}")
        else:
            st.info("ğŸ‘† é»æ“Šã€Œé–‹å§‹çˆ¬å–ã€ä¾†é–‹å§‹")
    
    def _show_results(self):
        """é¡¯ç¤ºçˆ¬å–çµæœ"""
        results = st.session_state.realtime_results
        results_file = st.session_state.get('realtime_results_file', 'unknown.json')
        
        st.subheader("ğŸ“Š çˆ¬å–çµæœ")
        
        # åŸºæœ¬çµ±è¨ˆ
        total_posts = len(results)
        successful_views = len([r for r in results if r.get('has_views')])
        successful_content = len([r for r in results if r.get('has_content')])
        successful_likes = len([r for r in results if r.get('has_likes')])
        successful_comments = len([r for r in results if r.get('has_comments')])
        
        # çµ±è¨ˆæŒ‡æ¨™
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("ç¸½è²¼æ–‡æ•¸", total_posts)
        with col2:
            st.metric("è§€çœ‹æ•¸æˆåŠŸ", f"{successful_views}/{total_posts}")
        with col3:
            st.metric("å…§å®¹æˆåŠŸ", f"{successful_content}/{total_posts}")
        with col4:
            st.metric("äº’å‹•æ•¸æ“š", f"{successful_likes}/{total_posts}")
        
        # æˆåŠŸç‡æŒ‡æ¨™
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            view_rate = (successful_views / total_posts * 100) if total_posts > 0 else 0
            st.metric("è§€çœ‹æ•¸æˆåŠŸç‡", f"{view_rate:.1f}%")
        with col2:
            content_rate = (successful_content / total_posts * 100) if total_posts > 0 else 0
            st.metric("å…§å®¹æˆåŠŸç‡", f"{content_rate:.1f}%")
        with col3:
            like_rate = (successful_likes / total_posts * 100) if total_posts > 0 else 0
            st.metric("æŒ‰è®šæ•¸æˆåŠŸç‡", f"{like_rate:.1f}%")
        with col4:
            comment_rate = (successful_comments / total_posts * 100) if total_posts > 0 else 0
            st.metric("ç•™è¨€æ•¸æˆåŠŸç‡", f"{comment_rate:.1f}%")
        
        # é‡è¤‡è™•ç†åŠŸèƒ½
        st.divider()
        col1, col2, col3 = st.columns([2, 1, 1])
        with col1:
            st.write("**ğŸ”„ é‡è¤‡è™•ç†**")
            st.caption("æª¢æ¸¬é‡è¤‡è²¼æ–‡ï¼Œè§€çœ‹æ•¸ä½çš„ç”¨APIé‡æ–°æå–")
        with col2:
            if st.button("ğŸ” æª¢æ¸¬é‡è¤‡", key="detect_duplicates"):
                self._detect_duplicates()
        with col3:
            if st.button("ğŸ”„ è™•ç†é‡è¤‡", key="process_duplicates"):
                self._process_duplicates()
        
        # è©³ç´°çµæœè¡¨æ ¼
        if st.checkbox("ğŸ“‹ é¡¯ç¤ºè©³ç´°çµæœ", key="show_detailed_results"):
            self._show_detailed_table(results)
        
        # ä¸‹è¼‰å’Œå°å‡ºæŒ‰éˆ•
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            if st.button("ğŸ’¾ ä¸‹è¼‰JSON", key="download_json"):
                st.success(f"JSONçµæœå·²ä¿å­˜åˆ°: {results_file}")
                st.json({"message": f"è«‹æŸ¥çœ‹é …ç›®æ ¹ç›®éŒ„ä¸‹çš„ {results_file} æ–‡ä»¶"})
        
        with col2:
            if st.button("ğŸ“Š å°å‡ºCSV", key="export_csv"):
                self._show_csv_export_options(results_file)
        
        with col3:
            if st.button("ğŸ“ˆ æ­·å²åˆ†æ", key="export_history"):
                self._show_export_history_options()
        
        with col4:
            if st.button("ğŸ” æ›´å¤šå°å‡º", key="more_exports"):
                self._show_advanced_export_options()
    
    def _detect_duplicates(self):
        """æª¢æ¸¬é‡è¤‡è²¼æ–‡"""
        if 'realtime_results' not in st.session_state:
            st.error("âŒ æ²’æœ‰å¯æª¢æ¸¬çš„çµæœ")
            return
        
        results = st.session_state.realtime_results
        
        # æŒ‰ post_id åˆ†çµ„
        from collections import defaultdict
        grouped = defaultdict(list)
        for result in results:
            if result.get('post_id'):
                grouped[result['post_id']].append(result)
        
        # æ‰¾å‡ºé‡è¤‡é …
        duplicates = {k: v for k, v in grouped.items() if len(v) > 1}
        
        if not duplicates:
            st.success("âœ… æ²’æœ‰ç™¼ç¾é‡è¤‡è²¼æ–‡")
            return
        
        st.warning(f"âš ï¸ ç™¼ç¾ {len(duplicates)} çµ„é‡è¤‡è²¼æ–‡")
        
        for post_id, items in duplicates.items():
            with st.expander(f"ğŸ“‹ é‡è¤‡çµ„: {post_id} ({len(items)} å€‹ç‰ˆæœ¬)"):
                for i, item in enumerate(items):
                    views = item.get('views', 'N/A')
                    source = item.get('source', 'unknown')
                    content = item.get('content', 'N/A')[:100] + '...' if item.get('content') else 'N/A'
                    
                    col1, col2, col3 = st.columns([1, 1, 3])
                    with col1:
                        st.write(f"**ç‰ˆæœ¬ {i+1}**")
                        st.write(f"è§€çœ‹æ•¸: {views}")
                    with col2:
                        st.write(f"ä¾†æº: {source}")
                    with col3:
                        st.write(f"å…§å®¹: {content}")
    
    def _process_duplicates(self):
        """è™•ç†é‡è¤‡è²¼æ–‡"""
        if 'realtime_results' not in st.session_state:
            st.error("âŒ æ²’æœ‰å¯è™•ç†çš„çµæœ")
            return
        
        # èª¿ç”¨é‡è¤‡è™•ç†è…³æœ¬
        try:
            import subprocess
            import sys
            import os
            
            st.info("ğŸ”„ æ­£åœ¨è™•ç†é‡è¤‡è²¼æ–‡...")
            
            # åŸ·è¡Œé‡è¤‡è™•ç†è…³æœ¬
            script_path = "fix_duplicates_reextract.py"
            result = subprocess.run(
                [sys.executable, script_path],
                capture_output=True,
                text=True,
                encoding='utf-8',
                errors='replace',
                cwd=os.getcwd()
            )
            
            if result.returncode == 0:
                # æŸ¥æ‰¾è™•ç†å¾Œçš„æ–‡ä»¶
                import glob
                dedup_files = glob.glob("realtime_extraction_results_*_dedup.json")
                if dedup_files:
                    latest_dedup = max(dedup_files, key=os.path.getctime)
                    
                    # è®€å–è™•ç†å¾Œçš„çµæœ
                    import json
                    with open(latest_dedup, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    
                    # æ›´æ–°session_state
                    st.session_state.realtime_results = data.get('results', [])
                    st.session_state.realtime_results_file = latest_dedup
                    
                    duplicates_count = data.get('duplicates_processed', 0)
                    reextracted_count = data.get('reextracted_count', 0)
                    
                    st.success(f"âœ… é‡è¤‡è™•ç†å®Œæˆï¼")
                    st.info(f"ğŸ“Š è™•ç†äº† {duplicates_count} çµ„é‡è¤‡ï¼Œé‡æ–°æå– {reextracted_count} å€‹é …ç›®")
                    st.balloons()
                    
                    # è‡ªå‹•åˆ·æ–°é é¢ä»¥é¡¯ç¤ºæ›´æ–°çµæœ
                    st.rerun()
                else:
                    st.error("âŒ æœªæ‰¾åˆ°è™•ç†å¾Œçš„çµæœæ–‡ä»¶")
            else:
                st.error(f"âŒ è™•ç†å¤±æ•—ï¼š{result.stderr}")
                st.code(result.stdout)
                
        except Exception as e:
            st.error(f"âŒ è™•ç†éŒ¯èª¤ï¼š{str(e)}")
    
    def _show_csv_export_options(self, json_file_path: str):
        """é¡¯ç¤ºCSVå°å‡ºé¸é …"""
        with st.expander("ğŸ“Š CSVå°å‡ºé¸é …", expanded=True):
            st.write("**é¸æ“‡æ’åºæ–¹å¼ï¼ˆå»ºè­°æŒ‰è§€çœ‹æ•¸æ’åºï¼‰**")
            
            sort_options = {
                "è§€çœ‹æ•¸ (é«˜â†’ä½)": "views",
                "æŒ‰è®šæ•¸ (é«˜â†’ä½)": "likes", 
                "ç•™è¨€æ•¸ (é«˜â†’ä½)": "comments",
                "è½‰ç™¼æ•¸ (é«˜â†’ä½)": "reposts",
                "åˆ†äº«æ•¸ (é«˜â†’ä½)": "shares",
                "è²¼æ–‡ID (Aâ†’Z)": "post_id",
                "åŸå§‹é †åº (ä¸æ’åº)": "none"
            }
            
            selected_sort = st.selectbox(
                "æ’åºæ–¹å¼",
                options=list(sort_options.keys()),
                index=0,  # é è¨­é¸æ“‡è§€çœ‹æ•¸æ’åº
                help="é¸æ“‡CSVæ–‡ä»¶ä¸­æ•¸æ“šçš„æ’åºæ–¹å¼ï¼Œå»ºè­°é¸æ“‡è§€çœ‹æ•¸ä»¥ä¾¿åˆ†ææœ€å—æ­¡è¿çš„è²¼æ–‡"
            )
            
            col1, col2 = st.columns(2)
            
            with col1:
                if st.button("ğŸ“¥ å°å‡ºä¸¦ä¸‹è¼‰", key="export_csv_download"):
                    sort_by = sort_options[selected_sort]
                    self._export_current_to_csv(json_file_path, sort_by)
            
            with col2:
                st.info("ğŸ’¡ **CSVä½¿ç”¨æç¤ºï¼š**\n- ç”¨Excelæˆ–Google Sheetsæ‰“é–‹\n- å¯ä»¥é€²ä¸€æ­¥ç¯©é¸å’Œåˆ†æ\n- æ”¯æ´ä¸­æ–‡é¡¯ç¤º")
    
    def _export_current_to_csv(self, json_file_path: str, sort_by: str = 'views'):
        """å°å‡ºç•¶æ¬¡çµæœåˆ°CSV"""
        try:
            from common.csv_export_manager import CSVExportManager
            
            csv_manager = CSVExportManager()
            csv_file = csv_manager.export_current_session(json_file_path, sort_by=sort_by)
            
            st.success(f"âœ… CSVå°å‡ºæˆåŠŸï¼")
            st.info(f"ğŸ“ æ–‡ä»¶ä½ç½®: {csv_file}")
            
            # é¡¯ç¤ºä¸‹è¼‰é€£çµï¼ˆå¦‚æœå¯èƒ½ï¼‰
            import os
            if os.path.exists(csv_file):
                with open(csv_file, 'r', encoding='utf-8-sig') as f:
                    csv_content = f.read()
                
                st.download_button(
                    label="ğŸ“¥ ä¸‹è¼‰CSVæ–‡ä»¶",
                    data=csv_content,
                    file_name=os.path.basename(csv_file),
                    mime="text/csv"
                )
            
        except Exception as e:
            st.error(f"âŒ CSVå°å‡ºå¤±æ•—: {str(e)}")
    
    def _show_export_history_options(self):
        """é¡¯ç¤ºæ­·å²å°å‡ºé¸é …"""
        if 'realtime_results' not in st.session_state:
            st.error("âŒ è«‹å…ˆåŸ·è¡Œçˆ¬å–ä»¥ç²å–å¸³è™Ÿä¿¡æ¯")
            return
        
        # ç²å–ç•¶å‰å¸³è™Ÿ
        results = st.session_state.realtime_results
        if not results:
            st.error("âŒ ç„¡æ³•ç²å–å¸³è™Ÿä¿¡æ¯")
            return
        
        # å‡è¨­å¾ç¬¬ä¸€å€‹çµæœä¸­æå–ç”¨æˆ¶å
        target_username = None
        if 'realtime_results_file' in st.session_state:
            try:
                import json
                with open(st.session_state.realtime_results_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                target_username = data.get('target_username')
            except:
                pass
        
        if not target_username:
            st.error("âŒ ç„¡æ³•è­˜åˆ¥ç›®æ¨™å¸³è™Ÿ")
            return
        
        with st.expander("ğŸ“ˆ æ­·å²æ•¸æ“šå°å‡ºé¸é …", expanded=True):
            export_type = st.radio(
                "é¸æ“‡å°å‡ºé¡å‹",
                options=["æœ€è¿‘æ•¸æ“š", "å…¨éƒ¨æ­·å²", "çµ±è¨ˆåˆ†æ"],
                help="é¸æ“‡è¦å°å‡ºçš„æ­·å²æ•¸æ“šç¯„åœ"
            )
            
            col1, col2 = st.columns(2)
            
            if export_type == "æœ€è¿‘æ•¸æ“š":
                with col1:
                    days_back = st.number_input("å›æº¯å¤©æ•¸", min_value=1, max_value=365, value=7)
                with col2:
                    limit = st.number_input("æœ€å¤§è¨˜éŒ„æ•¸", min_value=10, max_value=10000, value=1000)
                
                if st.button("ğŸ“Š å°å‡ºæœ€è¿‘æ•¸æ“š", key="export_recent"):
                    self._export_history_data(target_username, "recent", days_back=days_back, limit=limit)
            
            elif export_type == "å…¨éƒ¨æ­·å²":
                with col1:
                    limit = st.number_input("æœ€å¤§è¨˜éŒ„æ•¸", min_value=100, max_value=50000, value=5000)
                
                if st.button("ğŸ“Š å°å‡ºå…¨éƒ¨æ­·å²", key="export_all"):
                    self._export_history_data(target_username, "all", limit=limit)
            
            elif export_type == "çµ±è¨ˆåˆ†æ":
                st.info("æŒ‰æ—¥æœŸçµ±è¨ˆçš„åˆ†æå ±å‘Šï¼ŒåŒ…å«å¹³å‡è§€çœ‹æ•¸ã€æˆåŠŸç‡ç­‰æŒ‡æ¨™")
                
                if st.button("ğŸ“ˆ å°å‡ºçµ±è¨ˆåˆ†æ", key="export_analysis"):
                    self._export_history_data(target_username, "analysis")
    
    def _export_history_data(self, username: str, export_type: str, **kwargs):
        """å°å‡ºæ­·å²æ•¸æ“š"""
        try:
            from common.csv_export_manager import CSVExportManager
            import asyncio
            
            csv_manager = CSVExportManager()
            
            # å‰µå»ºæ–°çš„äº‹ä»¶å¾ªç’°ä¾†é¿å…è¡çª
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            
            try:
                if export_type == "recent":
                    csv_file = loop.run_until_complete(
                        csv_manager.export_database_history(
                            username, 
                            days_back=kwargs.get('days_back'),
                            limit=kwargs.get('limit')
                        )
                    )
                elif export_type == "all":
                    csv_file = loop.run_until_complete(
                        csv_manager.export_database_history(
                            username,
                            limit=kwargs.get('limit')
                        )
                    )
                elif export_type == "analysis":
                    csv_file = loop.run_until_complete(
                        csv_manager.export_combined_analysis(username)
                    )
                else:
                    raise ValueError(f"ä¸æ”¯æŒçš„å°å‡ºé¡å‹: {export_type}")
                
                st.success(f"âœ… æ­·å²æ•¸æ“šå°å‡ºæˆåŠŸï¼")
                st.info(f"ğŸ“ æ–‡ä»¶ä½ç½®: {csv_file}")
                
                # æä¾›ä¸‹è¼‰
                import os
                if os.path.exists(csv_file):
                    with open(csv_file, 'r', encoding='utf-8-sig') as f:
                        csv_content = f.read()
                    
                    st.download_button(
                        label="ğŸ“¥ ä¸‹è¼‰æ­·å²CSVæ–‡ä»¶",
                        data=csv_content,
                        file_name=os.path.basename(csv_file),
                        mime="text/csv"
                    )
                
            finally:
                loop.close()
                
        except Exception as e:
            st.error(f"âŒ æ­·å²æ•¸æ“šå°å‡ºå¤±æ•—: {str(e)}")
    
    def _show_advanced_export_options(self):
        """é¡¯ç¤ºé€²éšå°å‡ºé¸é …"""
        with st.expander("ğŸ” é€²éšå°å‡ºåŠŸèƒ½", expanded=True):
            st.markdown("**æ›´å¤šå°å‡ºé¸é …å’Œæ‰¹é‡æ“ä½œ**")
            
            tab1, tab2, tab3 = st.tabs(["ğŸ“Š å°æ¯”å ±å‘Š", "ğŸ”„ æ‰¹é‡å°å‡º", "âš¡ å¿«é€Ÿå·¥å…·"])
            
            with tab1:
                st.subheader("ğŸ“Š å¤šæ¬¡çˆ¬å–å°æ¯”å ±å‘Š")
                st.info("æ¯”è¼ƒå¤šæ¬¡çˆ¬å–çµæœçš„æ•ˆèƒ½å’ŒæˆåŠŸç‡")
                
                # æŸ¥æ‰¾æ‰€æœ‰JSONæ–‡ä»¶
                import glob
                import os
                json_files = glob.glob("realtime_extraction_results_*.json")
                
                if len(json_files) >= 2:
                    st.write(f"ğŸ” æ‰¾åˆ° {len(json_files)} å€‹çˆ¬å–çµæœæ–‡ä»¶ï¼š")
                    
                    # é¡¯ç¤ºæ–‡ä»¶åˆ—è¡¨
                    selected_files = []
                    for i, file in enumerate(sorted(json_files, reverse=True)[:10]):  # æœ€æ–°çš„10å€‹
                        file_time = self._extract_time_from_filename(file)
                        if st.checkbox(f"{os.path.basename(file)} ({file_time})", key=f"compare_file_{i}"):
                            selected_files.append(file)
                    
                    if len(selected_files) >= 2:
                        if st.button("ğŸ“Š ç”Ÿæˆå°æ¯”å ±å‘Š", key="generate_comparison"):
                            self._generate_comparison_report(selected_files)
                    else:
                        st.warning("âš ï¸ è«‹è‡³å°‘é¸æ“‡2å€‹æ–‡ä»¶é€²è¡Œå°æ¯”")
                else:
                    st.warning("âš ï¸ éœ€è¦è‡³å°‘2å€‹çˆ¬å–çµæœæ–‡ä»¶æ‰èƒ½é€²è¡Œå°æ¯”")
            
            with tab2:
                st.subheader("ğŸ”„ æ‰¹é‡å°å‡ºåŠŸèƒ½")
                
                col1, col2 = st.columns(2)
                
                with col1:
                    if st.button("ğŸ“¥ å°å‡ºæ‰€æœ‰æœ€æ–°çµæœ", key="export_all_latest"):
                        self._export_all_latest_results()
                
                with col2:
                    if st.button("ğŸ“ˆ å°å‡ºæ‰€æœ‰å¸³è™Ÿçµ±è¨ˆ", key="export_all_stats"):
                        self._export_all_account_stats()
                
                st.divider()
                
                # è‡ªå‹•åŒ–å°å‡ºè¨­å®š
                st.write("**è‡ªå‹•åŒ–å°å‡ºè¨­å®š**")
                auto_sort = st.selectbox(
                    "é è¨­æ’åºæ–¹å¼",
                    ["è§€çœ‹æ•¸", "æŒ‰è®šæ•¸", "ç•™è¨€æ•¸", "æ™‚é–“é †åº"],
                    help="æ‰¹é‡å°å‡ºæ™‚ä½¿ç”¨çš„é è¨­æ’åº"
                )
                
                if st.button("ğŸ’¾ ä¿å­˜è¨­å®š", key="save_export_settings"):
                    st.session_state.default_sort = auto_sort
                    st.success(f"âœ… å·²ä¿å­˜é è¨­æ’åº: {auto_sort}")
            
            with tab3:
                st.subheader("âš¡ å¿«é€Ÿå·¥å…·")
                
                # å¿«é€Ÿé è¦½
                st.write("**å¿«é€Ÿé è¦½CSVæ–‡ä»¶**")
                uploaded_csv = st.file_uploader(
                    "ä¸Šå‚³CSVæ–‡ä»¶é€²è¡Œé è¦½",
                    type=['csv'],
                    help="ä¸Šå‚³ä»»ä½•CSVæ–‡ä»¶ï¼Œå¿«é€ŸæŸ¥çœ‹å‰å¹¾è¡Œæ•¸æ“š"
                )
                
                if uploaded_csv:
                    try:
                        import pandas as pd
                        df = pd.read_csv(uploaded_csv, encoding='utf-8-sig')
                        
                        col1, col2 = st.columns(2)
                        with col1:
                            st.metric("ç¸½è¡Œæ•¸", len(df))
                        with col2:
                            st.metric("ç¸½æ¬„ä½", len(df.columns))
                        
                        st.write("**å‰10è¡Œé è¦½ï¼š**")
                        st.dataframe(df.head(10), use_container_width=True)
                        
                    except Exception as e:
                        st.error(f"âŒ é è¦½å¤±æ•—: {e}")
                
                st.divider()
                
                # æ¸…ç†å·¥å…·
                st.write("**æ¸…ç†å·¥å…·**")
                if st.button("ğŸ—‘ï¸ æ¸…ç†èˆŠçš„å°å‡ºæ–‡ä»¶", key="cleanup_exports"):
                    self._cleanup_old_exports()
    
    def _extract_time_from_filename(self, filename: str) -> str:
        """å¾æ–‡ä»¶åæå–æ™‚é–“"""
        try:
            import re
            match = re.search(r'_(\d{8}_\d{6})\.json$', filename)
            if match:
                time_str = match.group(1)
                # æ ¼å¼åŒ–ç‚ºå¯è®€æ™‚é–“
                date_part = time_str[:8]
                time_part = time_str[9:]
                return f"{date_part[:4]}-{date_part[4:6]}-{date_part[6:8]} {time_part[:2]}:{time_part[2:4]}:{time_part[4:6]}"
        except:
            pass
        return "æœªçŸ¥æ™‚é–“"
    
    def _generate_comparison_report(self, selected_files: List[str]):
        """ç”Ÿæˆå°æ¯”å ±å‘Š"""
        try:
            from common.csv_export_manager import CSVExportManager
            
            csv_manager = CSVExportManager()
            csv_file = csv_manager.export_comparison_report(selected_files)
            
            st.success("âœ… å°æ¯”å ±å‘Šç”ŸæˆæˆåŠŸï¼")
            st.info(f"ğŸ“ æ–‡ä»¶ä½ç½®: {csv_file}")
            
            # æä¾›ä¸‹è¼‰
            import os
            if os.path.exists(csv_file):
                with open(csv_file, 'r', encoding='utf-8-sig') as f:
                    csv_content = f.read()
                
                st.download_button(
                    label="ğŸ“¥ ä¸‹è¼‰å°æ¯”å ±å‘Š",
                    data=csv_content,
                    file_name=os.path.basename(csv_file),
                    mime="text/csv"
                )
                
                # é¡¯ç¤ºæ‘˜è¦
                st.write("**å°æ¯”æ‘˜è¦ï¼š**")
                import pandas as pd
                df = pd.read_csv(csv_file, encoding='utf-8-sig')
                st.dataframe(df, use_container_width=True)
                
        except Exception as e:
            st.error(f"âŒ ç”Ÿæˆå°æ¯”å ±å‘Šå¤±æ•—: {e}")
    
    def _export_all_latest_results(self):
        """å°å‡ºæ‰€æœ‰æœ€æ–°çµæœ"""
        try:
            import glob
            json_files = glob.glob("realtime_extraction_results_*.json")
            
            if not json_files:
                st.warning("âš ï¸ æœªæ‰¾åˆ°ä»»ä½•çˆ¬å–çµæœæ–‡ä»¶")
                return
            
            # æ‰¾æœ€æ–°çš„æ–‡ä»¶
            latest_file = max(json_files, key=lambda f: Path(f).stat().st_mtime)
            
            from common.csv_export_manager import CSVExportManager
            csv_manager = CSVExportManager()
            
            # ä½¿ç”¨é è¨­æ’åº
            default_sort = getattr(st.session_state, 'default_sort', 'è§€çœ‹æ•¸')
            sort_mapping = {"è§€çœ‹æ•¸": "views", "æŒ‰è®šæ•¸": "likes", "ç•™è¨€æ•¸": "comments", "æ™‚é–“é †åº": "none"}
            sort_by = sort_mapping.get(default_sort, "views")
            
            csv_file = csv_manager.export_current_session(latest_file, sort_by=sort_by)
            
            st.success("âœ… æœ€æ–°çµæœå°å‡ºæˆåŠŸï¼")
            st.info(f"ğŸ“ ä½¿ç”¨äº† {latest_file}")
            st.info(f"ğŸ“Š æŒ‰ {default_sort} æ’åº")
            
            # æä¾›ä¸‹è¼‰
            import os
            if os.path.exists(csv_file):
                with open(csv_file, 'r', encoding='utf-8-sig') as f:
                    csv_content = f.read()
                
                st.download_button(
                    label="ğŸ“¥ ä¸‹è¼‰æœ€æ–°çµæœCSV",
                    data=csv_content,
                    file_name=os.path.basename(csv_file),
                    mime="text/csv"
                )
                
        except Exception as e:
            st.error(f"âŒ å°å‡ºå¤±æ•—: {e}")
    
    def _export_all_account_stats(self):
        """å°å‡ºæ‰€æœ‰å¸³è™Ÿçµ±è¨ˆ"""
        try:
            from common.incremental_crawl_manager import IncrementalCrawlManager
            import asyncio
            
            # å‰µå»ºäº‹ä»¶å¾ªç’°
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            
            try:
                manager = IncrementalCrawlManager()
                
                # ç²å–æ‰€æœ‰å¸³è™Ÿ
                results = loop.run_until_complete(manager.db.fetch_all("""
                    SELECT DISTINCT username FROM crawl_state ORDER BY last_crawl_at DESC
                """))
                
                if not results:
                    st.warning("âš ï¸ æœªæ‰¾åˆ°ä»»ä½•çˆ¬å–è¨˜éŒ„")
                    return
                
                all_stats = []
                for row in results:
                    username = row['username']
                    summary = loop.run_until_complete(manager.get_crawl_summary(username))
                    
                    if 'error' not in summary:
                        checkpoint = summary['checkpoint']
                        stats = summary['statistics']
                        
                        all_stats.append({
                            'å¸³è™Ÿ': f"@{username}",
                            'æœ€æ–°è²¼æ–‡ID': checkpoint['latest_post_id'] or 'N/A',
                            'ç´¯è¨ˆçˆ¬å–': checkpoint['total_crawled'],
                            'è³‡æ–™åº«è²¼æ–‡æ•¸': stats['total_posts'],
                            'æœ‰è§€çœ‹æ•¸è²¼æ–‡': stats['posts_with_views'],
                            'å¹³å‡è§€çœ‹æ•¸': round(stats['avg_views'], 0),
                            'æœ€é«˜è§€çœ‹æ•¸': stats['max_views'],
                            'ä¸Šæ¬¡çˆ¬å–': checkpoint['last_crawl_at'].strftime('%Y-%m-%d %H:%M') if checkpoint['last_crawl_at'] else 'N/A'
                        })
                
                if all_stats:
                    # è½‰æ›ç‚ºCSV
                    import pandas as pd
                    df = pd.DataFrame(all_stats)
                    
                    from datetime import datetime
                    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                    csv_file = f"export_all_accounts_stats_{timestamp}.csv"
                    df.to_csv(csv_file, index=False, encoding='utf-8-sig')
                    
                    st.success("âœ… æ‰€æœ‰å¸³è™Ÿçµ±è¨ˆå°å‡ºæˆåŠŸï¼")
                    st.info(f"ğŸ“ æ–‡ä»¶ä½ç½®: {csv_file}")
                    
                    # é¡¯ç¤ºé è¦½
                    st.write("**çµ±è¨ˆé è¦½ï¼š**")
                    st.dataframe(df, use_container_width=True)
                    
                    # æä¾›ä¸‹è¼‰
                    csv_content = df.to_csv(index=False, encoding='utf-8-sig')
                    st.download_button(
                        label="ğŸ“¥ ä¸‹è¼‰å¸³è™Ÿçµ±è¨ˆ",
                        data=csv_content,
                        file_name=csv_file,
                        mime="text/csv"
                    )
                else:
                    st.warning("âš ï¸ æœªæ‰¾åˆ°æœ‰æ•ˆçš„çµ±è¨ˆæ•¸æ“š")
                    
            finally:
                loop.close()
                
        except Exception as e:
            st.error(f"âŒ å°å‡ºå¸³è™Ÿçµ±è¨ˆå¤±æ•—: {e}")
    
    def _cleanup_old_exports(self):
        """æ¸…ç†èˆŠçš„å°å‡ºæ–‡ä»¶"""
        try:
            import glob
            import os
            from datetime import datetime, timedelta
            
            # æ‰¾åˆ°æ‰€æœ‰å°å‡ºæ–‡ä»¶
            export_patterns = [
                "export_current_*.csv",
                "export_history_*.csv", 
                "export_analysis_*.csv",
                "export_comparison_*.csv"
            ]
            
            old_files = []
            cutoff_date = datetime.now() - timedelta(days=7)  # 7å¤©å‰
            
            for pattern in export_patterns:
                files = glob.glob(pattern)
                for file in files:
                    file_time = datetime.fromtimestamp(os.path.getmtime(file))
                    if file_time < cutoff_date:
                        old_files.append(file)
            
            if old_files:
                st.write(f"ğŸ” æ‰¾åˆ° {len(old_files)} å€‹7å¤©å‰çš„å°å‡ºæ–‡ä»¶ï¼š")
                
                for file in old_files[:5]:  # åªé¡¯ç¤ºå‰5å€‹
                    st.text(f"- {file}")
                
                if len(old_files) > 5:
                    st.text(f"... ä»¥åŠå…¶ä»– {len(old_files) - 5} å€‹æ–‡ä»¶")
                
                if st.button("ğŸ—‘ï¸ ç¢ºèªåˆªé™¤", key="confirm_cleanup"):
                    deleted_count = 0
                    for file in old_files:
                        try:
                            os.remove(file)
                            deleted_count += 1
                        except:
                            pass
                    
                    st.success(f"âœ… å·²åˆªé™¤ {deleted_count} å€‹èˆŠæ–‡ä»¶")
            else:
                st.info("âœ¨ æ²’æœ‰æ‰¾åˆ°éœ€è¦æ¸…ç†çš„èˆŠæ–‡ä»¶")
                
        except Exception as e:
            st.error(f"âŒ æ¸…ç†å¤±æ•—: {e}")
    
    def _show_detailed_table(self, results: List[Dict]):
        """é¡¯ç¤ºè©³ç´°çµæœè¡¨æ ¼"""
        st.subheader("ğŸ“‹ è©³ç´°çµæœ")
        
        # æº–å‚™è¡¨æ ¼æ•¸æ“š
        table_data = []
        for r in results:
            table_data.append({
                "è²¼æ–‡ID": r.get('post_id', 'N/A'),
                "è§€çœ‹æ•¸": r.get('views', 'N/A'),
                "æŒ‰è®šæ•¸": r.get('likes', 'N/A'),
                "ç•™è¨€æ•¸": r.get('comments', 'N/A'),
                "è½‰ç™¼æ•¸": r.get('reposts', 'N/A'),
                "åˆ†äº«æ•¸": r.get('shares', 'N/A'),
                "å…§å®¹é è¦½": (r.get('content', '')[:50] + "...") if r.get('content') else 'N/A',
                "ä¾†æº": r.get('source', 'N/A'),
                "é‡æ–°æå–": "âœ…" if r.get('reextracted', False) else ""
            })
        
        # é¡¯ç¤ºè¡¨æ ¼
        st.dataframe(
            table_data,
            use_container_width=True,
            height=400
        )
        
        # äº’å‹•æ•¸æ“šåˆ†æ
        if st.checkbox("ğŸ“ˆ äº’å‹•æ•¸æ“šåˆ†æ", key="show_engagement_analysis"):
            self._show_engagement_analysis(results)
    
    def _show_engagement_analysis(self, results: List[Dict]):
        """é¡¯ç¤ºäº’å‹•æ•¸æ“šåˆ†æ"""
        st.subheader("ğŸ“ˆ äº’å‹•æ•¸æ“šåˆ†æ")
        
        # æ”¶é›†æœ‰æ•ˆçš„äº’å‹•æ•¸æ“š
        valid_results = [r for r in results if r.get('has_views') and r.get('has_likes')]
        
        if not valid_results:
            st.warning("ç„¡è¶³å¤ çš„äº’å‹•æ•¸æ“šé€²è¡Œåˆ†æ")
            return
        
        # ç°¡å–®çµ±è¨ˆ
        avg_likes = []
        avg_comments = []
        for r in valid_results:
            if r.get('likes') and r['likes'] != 'N/A':
                try:
                    # ç°¡åŒ–çš„æ•¸å­—è½‰æ›
                    likes_str = str(r['likes']).replace('K', '000').replace('M', '000000')
                    if likes_str.replace('.', '').isdigit():
                        avg_likes.append(float(likes_str))
                except:
                    pass
        
        if avg_likes:
            col1, col2 = st.columns(2)
            with col1:
                st.metric("å¹³å‡æŒ‰è®šæ•¸", f"{sum(avg_likes)/len(avg_likes):.0f}")
            with col2:
                st.metric("æœ€é«˜æŒ‰è®šæ•¸", f"{max(avg_likes):.0f}")