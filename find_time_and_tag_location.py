"""
ä½ç½®æŸ¥æ‰¾å™¨ - æ‰¾å‡ºç™¼æ–‡æ™‚é–“å’Œä¸»é¡Œtagçš„ä½ç½®
ç›®æ¨™è²¼æ–‡: https://www.threads.com/@chnyu._12/post/DM4gtYYybr-
æœŸæœ›æ•¸æ“š:
- ç™¼æ–‡æ™‚é–“: 2025å¹´8æœˆ3æ—¥ä¸‹åˆ 2:36
- ä¸»é¡Œtag: é›²æ—
"""

import asyncio
import json
import re
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Optional, List

import sys
sys.path.append(str(Path(__file__).parent))

from playwright.async_api import async_playwright
from common.config import get_auth_file_path

# ç›®æ¨™è²¼æ–‡
TARGET_POST_URL = "https://www.threads.com/@chnyu._12/post/DM4gtYYybr-"
TARGET_USERNAME = "chnyu._12"
TARGET_CODE = "DM4gtYYybr-"

# æœŸæœ›çš„æ•¸æ“š
EXPECTED_TIME = "2025å¹´8æœˆ3æ—¥ä¸‹åˆ 2:36"
EXPECTED_TAG = "é›²æ—"

class TimeAndTagLocationFinder:
    """ç™¼æ–‡æ™‚é–“å’Œä¸»é¡Œtagä½ç½®æŸ¥æ‰¾å™¨"""
    
    def __init__(self):
        self.captured_responses = []
        self.potential_matches = []
    
    async def intercept_all_responses(self):
        """æ””æˆªæ‰€æœ‰çš„ GraphQL å›æ‡‰ï¼ˆæ¨¡ä»¿ analyze_all_graphql.py çš„æˆåŠŸæ¨¡å¼ï¼‰"""
        print(f"ğŸ¯ æ””æˆªç›®æ¨™è²¼æ–‡çš„æ‰€æœ‰å›æ‡‰...")
        print(f"   ğŸ“ URL: {TARGET_POST_URL}")
        print(f"   â° æœŸæœ›æ™‚é–“: {EXPECTED_TIME}")
        print(f"   ğŸ·ï¸ æœŸæœ›æ¨™ç±¤: {EXPECTED_TAG}")
        
        auth_file_path = get_auth_file_path()
        
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=False)
            context = await browser.new_context(
                storage_state=str(auth_file_path),
                user_agent="Mozilla/5.0 (iPhone; CPU iPhone OS 15_0 like Mac OS X) AppleWebKit/605.1.15",
                viewport={"width": 375, "height": 812},
                locale="zh-TW"
            )
            
            page = await context.new_page()
            
            async def response_handler(response):
                url = response.url.lower()
                if "/graphql" in url and response.status == 200:
                    friendly_name = response.request.headers.get("x-fb-friendly-name", "Unknown")
                    root_field = response.request.headers.get("x-root-field-name", "")
                    
                    print(f"   ğŸ“¡ {friendly_name}")
                    if root_field:
                        print(f"      ğŸ” Root field: {root_field}")
                    
                    try:
                        data = await response.json()
                        
                        # åˆ†æéŸ¿æ‡‰çµæ§‹
                        content_indicators = []
                        target_post_found = False
                        contains_yunlin = False
                        contains_time_info = False
                        
                        if "data" in data and data["data"]:
                            # æª¢æŸ¥æ˜¯å¦åŒ…å«ç›®æ¨™ç”¨æˆ¶åæˆ–ä»£ç¢¼
                            data_str = json.dumps(data, ensure_ascii=False)
                            if TARGET_USERNAME in data_str or TARGET_CODE in data_str:
                                target_post_found = True
                                content_indicators.append("HAS_TARGET_POST")
                            
                            # æª¢æŸ¥æ˜¯å¦åŒ…å«é›²æ—
                            if "é›²æ—" in data_str:
                                contains_yunlin = True
                                content_indicators.append("HAS_YUNLIN")
                            
                            # æª¢æŸ¥æ™‚é–“ç›¸é—œä¿¡æ¯
                            time_keywords = ["2025", "8æœˆ", "3æ—¥", "taken_at", "timestamp", "created_time"]
                            for keyword in time_keywords:
                                if keyword in data_str:
                                    contains_time_info = True
                                    content_indicators.append(f"HAS_TIME({keyword})")
                                    break
                            
                            # æª¢æŸ¥å…¶ä»–å…§å®¹æŒ‡æ¨™
                            if "caption" in data_str:
                                content_indicators.append("has_caption")
                            if "like_count" in data_str:
                                content_indicators.append("has_likes")
                            if "text_post_app_info" in data_str:
                                content_indicators.append("has_text_info")
                            if len(data_str) > 10000:
                                content_indicators.append("large_response")
                        
                        indicators_text = ", ".join(content_indicators) if content_indicators else "no_content"
                        print(f"      ğŸ“Š æŒ‡æ¨™: {indicators_text}")
                        
                        # è¨˜éŒ„æŸ¥è©¢ä¿¡æ¯
                        query_info = {
                            "friendly_name": friendly_name,
                            "root_field": root_field,
                            "url": response.url,
                            "has_target_post": target_post_found,
                            "contains_yunlin": contains_yunlin,
                            "contains_time_info": contains_time_info,
                            "content_indicators": content_indicators,
                            "request_headers": dict(response.request.headers),
                            "request_data": response.request.post_data,
                            "response_size": len(json.dumps(data)) if data else 0,
                            "timestamp": datetime.now().isoformat()
                        }
                        
                        self.captured_responses.append(query_info)
                        
                        # å¦‚æœæ‰¾åˆ°ç›®æ¨™è²¼æ–‡æˆ–åŒ…å«é›²æ—ï¼Œè©³ç´°åˆ†æä¸¦ä¿å­˜
                        if target_post_found or contains_yunlin or contains_time_info:
                            print(f"      ğŸ¯ æ‰¾åˆ°é‡è¦å›æ‡‰ï¼")
                            
                            # ä¿å­˜å®Œæ•´éŸ¿æ‡‰
                            detail_file = Path(f"time_tag_found_{friendly_name}_{datetime.now().strftime('%H%M%S')}.json")
                            with open(detail_file, 'w', encoding='utf-8') as f:
                                json.dump({
                                    "query_info": query_info,
                                    "full_response": data
                                }, f, indent=2, ensure_ascii=False)
                            print(f"      ğŸ“ è©³ç´°æ•¸æ“šå·²ä¿å­˜: {detail_file}")
                            
                            # ç«‹å³åˆ†ææ™‚é–“å’Œæ¨™ç±¤
                            await self.analyze_time_and_tag(query_info, data)
                    
                    except Exception as e:
                        print(f"      âŒ è§£æå¤±æ•—: {e}")
            
            page.on("response", response_handler)
            
            # å°èˆªåˆ°é é¢
            print(f"   ğŸŒ å°èˆªåˆ°ç›®æ¨™é é¢...")
            await page.goto(TARGET_POST_URL, wait_until="networkidle", timeout=60000)
            
            # ç­‰å¾…åˆå§‹è¼‰å…¥
            await asyncio.sleep(5)
            
            # å˜—è©¦ä¸€äº›æ“ä½œä¾†è§¸ç™¼æ›´å¤šæŸ¥è©¢
            print(f"   ğŸ–±ï¸ å˜—è©¦ç”¨æˆ¶æ“ä½œ...")
            
            # æ»¾å‹•é é¢
            await page.evaluate("window.scrollTo(0, 300)")
            await asyncio.sleep(2)
            
            # å˜—è©¦é»æ“Šè²¼æ–‡å€åŸŸ
            try:
                await page.click('article', timeout=5000)
                await asyncio.sleep(2)
            except:
                pass
            
            # å˜—è©¦åˆ·æ–°é é¢
            print(f"   ğŸ”„ åˆ·æ–°é é¢...")
            await page.reload(wait_until="networkidle")
            await asyncio.sleep(5)
            
            # å†æ¬¡æ»¾å‹•
            await page.evaluate("window.scrollTo(0, 600)")
            await asyncio.sleep(2)
            
            await browser.close()
        
        print(f"\nğŸ“Š æ””æˆªå®Œæˆï¼Œå…± {len(self.captured_responses)} å€‹å›æ‡‰")
        return len(self.captured_responses) > 0
    
    async def analyze_time_and_tag(self, query_info: Dict[str, Any], data: Dict[str, Any]):
        """åˆ†æå–®å€‹å›æ‡‰ä¸­çš„æ™‚é–“å’Œæ¨™ç±¤ä¿¡æ¯"""
        friendly_name = query_info["friendly_name"]
        
        # è½‰æ›ç‚ºå­—ç¬¦ä¸²ä¾¿æ–¼æœç´¢
        data_str = json.dumps(data, ensure_ascii=False, indent=2)
        
        # æŸ¥æ‰¾æ™‚é–“ç›¸é—œçš„æ¬„ä½
        time_patterns = [
            r'"taken_at[^"]*":\s*(\d+)',           # taken_at æ™‚é–“æˆ³
            r'"device_timestamp[^"]*":\s*(\d+)',   # device_timestamp
            r'"created_time[^"]*":\s*(\d+)',       # created_time
            r'"upload_time[^"]*":\s*(\d+)',        # upload_time
            r'"timestamp[^"]*":\s*(\d+)',          # ä¸€èˆ¬ timestamp
            r'"time[^"]*":\s*"([^"]+)"',           # æ–‡å­—æ ¼å¼æ™‚é–“
            r'"date[^"]*":\s*"([^"]+)"',           # æ—¥æœŸæ ¼å¼
            r'"published_time[^"]*":\s*(\d+)',     # ç™¼å¸ƒæ™‚é–“
        ]
        
        # æŸ¥æ‰¾æ¨™ç±¤ç›¸é—œçš„æ¬„ä½
        tag_patterns = [
            r'"hashtags?"[^:]*:\s*\[([^\]]+)\]',                    # hashtags æ•¸çµ„
            r'"tags?"[^:]*:\s*\[([^\]]+)\]',                        # tags æ•¸çµ„
            r'"location[^"]*":\s*"([^"]*é›²æ—[^"]*)"',               # åœ°é»åŒ…å«é›²æ—
            r'"place[^"]*":\s*"([^"]*é›²æ—[^"]*)"',                  # place åŒ…å«é›²æ—
            r'"location_name[^"]*":\s*"([^"]*é›²æ—[^"]*)"',          # location_name
            r'"venue[^"]*":\s*\{[^}]*"name":\s*"([^"]*é›²æ—[^"]*)"', # venue name
            r'"city[^"]*":\s*"([^"]*é›²æ—[^"]*)"',                   # city
            r'"region[^"]*":\s*"([^"]*é›²æ—[^"]*)"',                 # region
            r'"categories?"[^:]*:\s*\[([^\]]*é›²æ—[^\]]*)\]',        # categories
        ]
        
        print(f"\nğŸ” åˆ†æå›æ‡‰: {friendly_name}")
        
        # æŸ¥æ‰¾æ™‚é–“
        found_times = []
        for pattern in time_patterns:
            matches = re.findall(pattern, data_str, re.IGNORECASE)
            for match in matches:
                if match.isdigit():
                    # æ™‚é–“æˆ³è½‰æ›
                    try:
                        timestamp = int(match)
                        if timestamp > 1000000000:  # æœ‰æ•ˆçš„æ™‚é–“æˆ³
                            from datetime import datetime
                            dt = datetime.fromtimestamp(timestamp)
                            time_str = dt.strftime("%Yå¹´%mæœˆ%dæ—¥")
                            found_times.append({
                                "pattern": pattern,
                                "raw_value": match,
                                "timestamp": timestamp,
                                "formatted": time_str,
                                "match_expected": "2025å¹´8æœˆ3æ—¥" in time_str
                            })
                    except:
                        pass
                else:
                    # æ–‡å­—æ ¼å¼æ™‚é–“
                    found_times.append({
                        "pattern": pattern,
                        "raw_value": match,
                        "timestamp": None,
                        "formatted": match,
                        "match_expected": "2025" in match or "8æœˆ" in match or "é›²æ—" in match
                    })
        
        # æŸ¥æ‰¾æ¨™ç±¤
        found_tags = []
        for pattern in tag_patterns:
            matches = re.findall(pattern, data_str, re.IGNORECASE)
            for match in matches:
                found_tags.append({
                    "pattern": pattern,
                    "raw_value": match,
                    "match_expected": "é›²æ—" in match
                })
        
        # ç›´æ¥æœç´¢ "é›²æ—" å’Œæ™‚é–“ç›¸é—œå­—ç¬¦ä¸²
        if "é›²æ—" in data_str:
            print(f"   ğŸ·ï¸ ç™¼ç¾ 'é›²æ—' å­—ç¬¦ä¸²ï¼")
            
            # æå–åŒ…å«é›²æ—çš„å®Œæ•´å­—æ®µ
            yunlin_contexts = []
            lines = data_str.split('\n')
            for i, line in enumerate(lines):
                if "é›²æ—" in line:
                    # ç²å–ä¸Šä¸‹æ–‡
                    start = max(0, i-2)
                    end = min(len(lines), i+3)
                    context = '\n'.join(lines[start:end])
                    yunlin_contexts.append({
                        "line_number": i,
                        "line": line.strip(),
                        "context": context
                    })
            
            for ctx in yunlin_contexts:
                print(f"      ğŸ“ ç¬¬ {ctx['line_number']} è¡Œ: {ctx['line']}")
        
        # æœç´¢æ™‚é–“ç›¸é—œ
        time_keywords = ["2025", "8æœˆ", "3æ—¥", "ä¸‹åˆ", "2:36"]
        for keyword in time_keywords:
            if keyword in data_str:
                print(f"   â° ç™¼ç¾æ™‚é–“é—œéµå­— '{keyword}'")
        
        # ä¿å­˜æœ‰åƒ¹å€¼çš„ç™¼ç¾
        if found_times or found_tags or "é›²æ—" in data_str:
            match_info = {
                "friendly_name": friendly_name,
                "found_times": found_times,
                "found_tags": found_tags,
                "contains_yunlin": "é›²æ—" in data_str,
                "contains_time_keywords": any(kw in data_str for kw in time_keywords),
                "payload": query_info["request_data"],
                "headers": query_info["request_headers"]
            }
            self.potential_matches.append(match_info)
    
    def save_detailed_analysis(self):
        """ä¿å­˜è©³ç´°åˆ†æçµæœ"""
        if not self.captured_responses:
            print("âŒ æ²’æœ‰æ””æˆªåˆ°ä»»ä½•å›æ‡‰")
            return
        
        # åˆ†æçµæœ
        print(f"\nğŸ“Š åˆ†æçµæœ:")
        print(f"   ç¸½æŸ¥è©¢æ•¸: {len(self.captured_responses)}")
        
        # æ‰¾åˆ°åŒ…å«ç›®æ¨™è²¼æ–‡çš„æŸ¥è©¢
        target_queries = [q for q in self.captured_responses if q["has_target_post"]]
        yunlin_queries = [q for q in self.captured_responses if q["contains_yunlin"]]
        time_queries = [q for q in self.captured_responses if q["contains_time_info"]]
        
        print(f"   åŒ…å«ç›®æ¨™è²¼æ–‡çš„æŸ¥è©¢: {len(target_queries)}")
        print(f"   åŒ…å«é›²æ—çš„æŸ¥è©¢: {len(yunlin_queries)}")
        print(f"   åŒ…å«æ™‚é–“ä¿¡æ¯çš„æŸ¥è©¢: {len(time_queries)}")
        
        # é¡¯ç¤ºé‡è¦æŸ¥è©¢
        important_queries = [q for q in self.captured_responses 
                           if q["has_target_post"] or q["contains_yunlin"] or q["contains_time_info"]]
        
        if important_queries:
            print(f"\nğŸ¯ é‡è¦æŸ¥è©¢:")
            for i, query in enumerate(important_queries):
                print(f"   {i+1}. {query['friendly_name']}")
                print(f"      Root field: {query['root_field']}")
                print(f"      æŒ‡æ¨™: {', '.join(query['content_indicators'])}")
                print(f"      éŸ¿æ‡‰å¤§å°: {query['response_size']:,} å­—ç¬¦")
        
        # æŒ‰æŸ¥è©¢åç¨±åˆ†çµ„çµ±è¨ˆ
        query_stats = {}
        for query in self.captured_responses:
            name = query["friendly_name"]
            if name not in query_stats:
                query_stats[name] = {"count": 0, "has_target": 0, "has_yunlin": 0, "has_time": 0, "avg_size": 0}
            query_stats[name]["count"] += 1
            if query["has_target_post"]:
                query_stats[name]["has_target"] += 1
            if query["contains_yunlin"]:
                query_stats[name]["has_yunlin"] += 1
            if query["contains_time_info"]:
                query_stats[name]["has_time"] += 1
            query_stats[name]["avg_size"] += query["response_size"]
        
        for name, stats in query_stats.items():
            stats["avg_size"] = stats["avg_size"] // stats["count"] if stats["count"] > 0 else 0
        
        print(f"\nğŸ“‹ æŸ¥è©¢çµ±è¨ˆ:")
        for name, stats in sorted(query_stats.items(), 
                                key=lambda x: (x[1]["has_target"], x[1]["has_yunlin"], x[1]["has_time"]), 
                                reverse=True):
            print(f"   {name}:")
            print(f"      æ¬¡æ•¸: {stats['count']}, ç›®æ¨™: {stats['has_target']}, é›²æ—: {stats['has_yunlin']}, æ™‚é–“: {stats['has_time']}, å¹³å‡å¤§å°: {stats['avg_size']:,}")
        
        # ä¿å­˜å®Œæ•´åˆ†æçµæœ
        analysis_file = Path(f"time_tag_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
        with open(analysis_file, 'w', encoding='utf-8') as f:
            json.dump({
                "target_url": TARGET_POST_URL,
                "target_username": TARGET_USERNAME,
                "target_code": TARGET_CODE,
                "expected_time": EXPECTED_TIME,
                "expected_tag": EXPECTED_TAG,
                "all_queries": self.captured_responses,
                "summary": {
                    "total_queries": len(self.captured_responses),
                    "target_queries_count": len(target_queries),
                    "yunlin_queries_count": len(yunlin_queries),
                    "time_queries_count": len(time_queries),
                    "query_stats": query_stats
                }
            }, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"\nğŸ“ å®Œæ•´åˆ†æå·²ä¿å­˜: {analysis_file}")
        
        # æ¨è–¦æœ€ä½³æŸ¥è©¢
        if important_queries:
            # é¸æ“‡åŒ…å«æœ€å¤šé‡è¦æŒ‡æ¨™çš„æŸ¥è©¢
            best_query = max(important_queries, key=lambda q: (
                q["has_target_post"],
                q["contains_yunlin"],
                q["contains_time_info"],
                len(q["content_indicators"])
            ))
            print(f"\nğŸ’¡ æ¨è–¦æŸ¥è©¢: {best_query['friendly_name']}")
            print(f"   Root field: {best_query['root_field']}")
            print(f"   å…§å®¹æŒ‡æ¨™: {', '.join(best_query['content_indicators'])}")
            print(f"   éŸ¿æ‡‰å¤§å°: {best_query['response_size']:,} å­—ç¬¦")
            
            return analysis_file, best_query
        else:
            print(f"\nğŸ˜ æœªæ‰¾åˆ°åŒ…å«é‡è¦ä¿¡æ¯çš„æŸ¥è©¢")
            print(f"ğŸ’¡ å¯èƒ½éœ€è¦:")
            print(f"   1. æª¢æŸ¥è²¼æ–‡ URL æ˜¯å¦æ­£ç¢º")
            print(f"   2. å˜—è©¦ä¸åŒçš„ç”¨æˆ¶æ“ä½œ")
            print(f"   3. æª¢æŸ¥èªè­‰ç‹€æ…‹")
            
            return analysis_file, None

async def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸ¯ ç™¼æ–‡æ™‚é–“å’Œä¸»é¡Œtagä½ç½®æŸ¥æ‰¾å™¨")
    print("===============================")
    
    auth_file = get_auth_file_path()
    if not auth_file.exists():
        print(f"âŒ èªè­‰æª”æ¡ˆ {auth_file} ä¸å­˜åœ¨ã€‚è«‹å…ˆåŸ·è¡Œ save_auth.pyã€‚")
        return
    
    finder = TimeAndTagLocationFinder()
    
    # æ””æˆªå›æ‡‰
    success = await finder.intercept_all_responses()
    
    if success:
        print(f"\nğŸ“Š åˆ†æå®Œæˆï¼")
        result = finder.save_detailed_analysis()
        
        if result:
            analysis_file, best_query = result
            if best_query:
                print(f"\nğŸ‰ æ‰¾åˆ°æœ€ä½³æŸ¥è©¢ï¼")
                print(f"ğŸ’¡ è«‹æª¢æŸ¥ä¿å­˜çš„ JSON æª”æ¡ˆä¾†æ‰¾åˆ°å…·é«”çš„å­—æ®µè·¯å¾‘")
            else:
                print(f"\nğŸ˜ æœªæ‰¾åˆ°åŒ…å«é‡è¦ä¿¡æ¯çš„æŸ¥è©¢")
        
        print(f"\nğŸ’¡ ä¸‹ä¸€æ­¥:")
        print(f"   1. æª¢æŸ¥ä¿å­˜çš„ JSON æª”æ¡ˆ")
        print(f"   2. åœ¨ JSON ä¸­æœç´¢åŒ…å« 'é›²æ—' å’Œæ™‚é–“æ•¸æ“šçš„å­—æ®µ")
        print(f"   3. ç¢ºå®šå­—æ®µçš„å®Œæ•´è·¯å¾‘")
        print(f"   4. æ•´åˆåˆ°ä¸»çˆ¬èŸ²ä¸­")
    else:
        print(f"\nğŸ˜ æœªèƒ½æ””æˆªåˆ°ç›¸é—œå›æ‡‰")

if __name__ == "__main__":
    asyncio.run(main())