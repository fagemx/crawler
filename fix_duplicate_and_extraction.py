#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¿®æ­£é‡è¤‡è²¼æ–‡å’Œå…§å®¹æå–å•é¡Œ
1. è™•ç†é‡è¤‡è²¼æ–‡ï¼Œä¿ç•™è§€çœ‹æ•¸é«˜çš„
2. ä¿®æ­£å…§å®¹æå–é‚è¼¯
"""

import json
import re
from typing import Dict, List, Optional
from collections import defaultdict

def convert_views_to_number(views_str: str) -> int:
    """å°‡è§€çœ‹æ•¸å­—ç¬¦ä¸²è½‰æ›ç‚ºæ•¸å­—ä»¥ä¾¿æ¯”è¼ƒ"""
    if not views_str:
        return 0
    
    views_str = views_str.upper().replace(',', '')
    
    if views_str.endswith('K'):
        return int(float(views_str[:-1]) * 1000)
    elif views_str.endswith('M'):
        return int(float(views_str[:-1]) * 1000000)
    elif views_str.endswith('B'):
        return int(float(views_str[:-1]) * 1000000000)
    else:
        try:
            return int(views_str)
        except:
            return 0

def extract_post_content_fixed(content: str) -> Optional[str]:
    """ä¿®æ­£å¾Œçš„å…§å®¹æå–é‚è¼¯"""
    lines = content.split('\n')
    
    # ç­–ç•¥1: æ‰¾åˆ°ç¬¬ä¸€è¡Œå¦‚æœæ˜¯å›è¦†ï¼Œä½¿ç”¨å®ƒ
    if lines and lines[0].strip().startswith('>>>'):
        reply_content = lines[0].strip()
        if len(reply_content) > 10:
            return reply_content
    
    # ç­–ç•¥2: æŸ¥æ‰¾ä¸»è²¼æ–‡å…§å®¹ï¼ˆæ›´æ™ºèƒ½çš„è­˜åˆ¥ï¼‰
    for i, line in enumerate(lines):
        stripped = line.strip()
        
        # è·³éæ˜é¡¯çš„éå…§å®¹è¡Œ
        if (not stripped or
            stripped.startswith('[') or
            stripped.startswith('![') or
            stripped.startswith('http') or
            stripped.startswith('Log in') or
            stripped.startswith('Thread') or
            stripped.startswith('gvmonthly') or
            stripped.isdigit() or
            re.match(r'^\d+/\d+/\d+$', stripped) or  # æ—¥æœŸæ ¼å¼
            stripped in ['Translate', 'views', '===============']):
            continue
        
        # æª¢æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆçš„è²¼æ–‡å…§å®¹
        if (len(stripped) > 15 and
            not stripped.startswith('>>>') and
            ('ã€‚' in stripped or 'ï¼Œ' in stripped or '!' in stripped or '?' in stripped or
             stripped.endswith('ğŸ˜†') or stripped.endswith('ğŸ˜…') or 'è­·ç…§' in stripped)):
            
            # æª¢æŸ¥å¾ŒçºŒæ˜¯å¦æœ‰ Translate æ¨™è­˜
            for j in range(i + 1, min(i + 3, len(lines))):
                if 'Translate' in lines[j]:
                    return stripped
            
            # å¦‚æœå…§å®¹åˆç†ä¸”é•·åº¦è¶³å¤ ï¼Œä¹Ÿè¿”å›
            if len(stripped) > 20:
                return stripped
    
    return None

def process_duplicates_and_fix_content(filename: str):
    """è™•ç†é‡è¤‡è²¼æ–‡å’Œä¿®æ­£å…§å®¹æå–"""
    
    print(f"ğŸ“‚ è™•ç†æ–‡ä»¶: {filename}")
    
    # è®€å–çµæœ
    with open(filename, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    results = data.get('results', [])
    print(f"ğŸ“Š åŸå§‹é …ç›®æ•¸: {len(results)}")
    
    # 1. æŒ‰ post_id åˆ†çµ„ï¼Œæ‰¾å‡ºé‡è¤‡é …
    grouped = defaultdict(list)
    for result in results:
        if result.get('post_id'):
            grouped[result['post_id']].append(result)
    
    # 2. è™•ç†é‡è¤‡é …ï¼šä¿ç•™è§€çœ‹æ•¸æœ€é«˜çš„
    processed_results = []
    duplicates_found = 0
    content_fixed = 0
    
    for post_id, items in grouped.items():
        if len(items) > 1:
            duplicates_found += 1
            print(f"ğŸ” ç™¼ç¾é‡è¤‡: {post_id} ({len(items)} å€‹ç‰ˆæœ¬)")
            
            # æ‰“å°è§€çœ‹æ•¸æ¯”è¼ƒ
            for i, item in enumerate(items):
                views = item.get('views', 'N/A')
                source = item.get('source', 'unknown')
                print(f"   ç‰ˆæœ¬{i+1}: {views} è§€çœ‹ (ä¾†æº: {source})")
            
            # é¸æ“‡è§€çœ‹æ•¸æœ€é«˜çš„
            best_item = max(items, key=lambda x: convert_views_to_number(x.get('views', '0')))
            print(f"   âœ… ä¿ç•™: {best_item.get('views')} è§€çœ‹")
            
            processed_results.append(best_item)
        else:
            processed_results.append(items[0])
        
        # 3. ä¿®æ­£å…§å®¹æå–
        current_item = processed_results[-1]
        if current_item.get('content'):
            # æª¢æŸ¥æ˜¯å¦éœ€è¦é‡æ–°æå–å…§å®¹
            original_content = current_item.get('content', '')
            
            # å¦‚æœæå–åˆ°çš„æ˜¯å›è¦†å…§å®¹ä½†é€™ä¸æ˜¯å›è¦†è²¼æ–‡ï¼Œå˜—è©¦é‡æ–°æå–
            if (original_content.startswith('>>>') and 
                not current_item.get('post_id', '').endswith('reply')):  # å‡è¨­å›è¦†IDæœ‰ç‰¹æ®Šæ¨™è­˜
                
                # å˜—è©¦å¾åŸå§‹å…§å®¹é‡æ–°æå–
                raw_content = current_item.get('raw_content', '')  # å¦‚æœæœ‰ä¿å­˜åŸå§‹å…§å®¹
                if raw_content:
                    fixed_content = extract_post_content_fixed(raw_content)
                    if fixed_content and fixed_content != original_content:
                        print(f"ğŸ”§ ä¿®æ­£å…§å®¹æå–: {post_id}")
                        print(f"   åŸå§‹: {original_content[:50]}...")
                        print(f"   ä¿®æ­£: {fixed_content[:50]}...")
                        current_item['content'] = fixed_content
                        content_fixed += 1
    
    print(f"\nğŸ“Š è™•ç†çµæœ:")
    print(f"   ğŸ”„ è™•ç†é‡è¤‡: {duplicates_found} çµ„")
    print(f"   ğŸ”§ ä¿®æ­£å…§å®¹: {content_fixed} å€‹")
    print(f"   ğŸ“ æœ€çµ‚é …ç›®: {len(processed_results)} å€‹")
    
    # 4. ä¿å­˜ä¿®æ­£å¾Œçš„çµæœ
    data['results'] = processed_results
    data['total_processed'] = len(processed_results)
    
    # é‡æ–°è¨ˆç®—çµ±è¨ˆ
    successful = [r for r in processed_results if r.get('success', False)]
    data['overall_success_rate'] = len(successful) / len(processed_results) * 100 if processed_results else 0
    
    output_filename = filename.replace('.json', '_fixed.json')
    with open(output_filename, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ’¾ ä¿®æ­£çµæœå·²ä¿å­˜åˆ°: {output_filename}")
    
    # 5. é¡¯ç¤ºä¿®æ­£å¾Œçš„å‰å¹¾å€‹é …ç›®
    print(f"\nğŸ¯ ä¿®æ­£å¾Œçš„å‰5å€‹é …ç›®:")
    for i, result in enumerate(processed_results[:5]):
        post_id = result.get('post_id', 'N/A')
        views = result.get('views', 'N/A')
        content = result.get('content', 'N/A')
        source = result.get('source', 'N/A')
        content_preview = content[:60] + '...' if len(content) > 60 else content
        print(f"   {i+1}. {post_id} | {views} | {source}")
        print(f"      ğŸ“ {content_preview}")

if __name__ == "__main__":
    # è™•ç†æœ€æ–°çš„çµæœæ–‡ä»¶
    import glob
    result_files = glob.glob("realtime_extraction_results_*.json")
    if result_files:
        latest_file = max(result_files, key=lambda x: x.split('_')[-1])
        print(f"ğŸ¯ è™•ç†æœ€æ–°æ–‡ä»¶: {latest_file}")
        process_duplicates_and_fix_content(latest_file)
    else:
        print("âŒ æœªæ‰¾åˆ°çµæœæ–‡ä»¶")