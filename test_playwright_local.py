import sys, asyncio
if sys.platform.startswith("win"):
    asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())

import json
import random
from pathlib import Path
from datetime import datetime
from playwright.async_api import async_playwright

# --- è¨­å®šè·¯å¾‘ ---
try:
    PROJECT_ROOT = Path(__file__).resolve().parent
    if str(PROJECT_ROOT) not in sys.path:
        sys.path.insert(0, str(PROJECT_ROOT))
    from common.config import get_auth_file_path
except ImportError:
    def get_auth_file_path():
        return Path("agents/playwright_crawler/auth.json")

# --- æ¸¬è©¦è¨­å®š ---
TARGET_USERNAME = "wuyiju28"  # è¦çˆ¬å–çš„ä½¿ç”¨è€…åç¨±
MAX_POSTS_TO_FETCH = 2       # è¦çˆ¬å–çš„è²¼æ–‡æ•¸é‡ï¼ˆå…ˆæ¸¬è©¦å°‘é‡ï¼‰

UA = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 " \
     "(KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36"

def parse_views_text(text: str) -> int:
    """å°‡ 'ä¸²æ–‡ 6,803æ¬¡ç€è¦½' æˆ– '1,234 views' è½‰æ›ç‚ºæ•´æ•¸"""
    import re
    if not text:
        return 0
    
    # ç§»é™¤ä¸å¿…è¦çš„æ–‡å­—ï¼Œä¿ç•™æ•¸å­—å’Œå–®ä½
    text = re.sub(r'ä¸²æ–‡\s*', '', text)  # ç§»é™¤ "ä¸²æ–‡"
    
    # è™•ç†ä¸­æ–‡æ ¼å¼ï¼š1.2è¬ã€4 è¬æ¬¡ç€è¦½ã€5000æ¬¡ç€è¦½
    if 'è¬' in text:
        match = re.search(r'([\d.]+)\s*è¬', text)  # å…è¨±æ•¸å­—å’Œè¬ä¹‹é–“æœ‰ç©ºæ ¼
        if match:
            return int(float(match.group(1)) * 10000)
    elif 'å„„' in text:
        match = re.search(r'([\d.]+)\s*å„„', text)  # å…è¨±æ•¸å­—å’Œå„„ä¹‹é–“æœ‰ç©ºæ ¼
        if match:
            return int(float(match.group(1)) * 100000000)
    
    # è™•ç†è‹±æ–‡æ ¼å¼ï¼š1.2M views, 500K views
    text_upper = text.upper()
    if 'M' in text_upper:
        match = re.search(r'([\d.]+)M', text_upper)
        if match:
            return int(float(match.group(1)) * 1000000)
    elif 'K' in text_upper:
        match = re.search(r'([\d.]+)K', text_upper)
        if match:
            return int(float(match.group(1)) * 1000)
    
    # è™•ç†ç´”æ•¸å­—æ ¼å¼ï¼ˆå¯èƒ½åŒ…å«é€—è™Ÿï¼‰
    match = re.search(r'[\d,]+', text)
    if match:
        return int(match.group().replace(',', ''))
    
    return 0

async def extract_post_views(page, post_url: str) -> dict:
    """æå–å–®å€‹è²¼æ–‡çš„ç€è¦½æ•¸å’ŒåŸºæœ¬è³‡è¨Š"""
    try:
        print(f"ğŸ“„ æ­£åœ¨è™•ç†: {post_url}")
        
        # å°èˆªåˆ°è²¼æ–‡é é¢
        await page.goto(post_url, wait_until="networkidle", timeout=30000)
        
        # æª¢æŸ¥é é¢æ˜¯å¦æ­£ç¢ºè¼‰å…¥ï¼ˆä½†ä¸è¦ç›´æ¥è·³é Gate é é¢ï¼‰
        page_content = await page.content()
        is_gate_page = "__NEXT_DATA__" not in page_content
        if is_gate_page:
            print(f"   âš ï¸ æª¢æ¸¬åˆ°è¨ªå®¢ Gate é é¢ï¼Œä½†ä»å˜—è©¦æå–åŸºæœ¬æ•¸æ“š...")
        
        # æ–¹æ³• 1: å˜—è©¦æ””æˆª GraphQL API å›æ‡‰ï¼ˆåªåœ¨é Gate é é¢æ™‚ï¼‰
        views_count = 0
        if not is_gate_page:
            try:
                response = await page.wait_for_response(
                    lambda r: "containing_thread" in r.url and r.status == 200, 
                    timeout=8000
                )
                data = await response.json()
                thread_items = data["data"]["containing_thread"]["thread_items"]
                post_data = thread_items[0]["post"]
                views_count = (post_data.get("feedback_info", {}).get("view_count") or
                              post_data.get("video_info", {}).get("play_count") or 0)
                
                if views_count > 0:
                    print(f"   âœ… GraphQL API ç²å–ç€è¦½æ•¸: {views_count:,}")
                    return {
                        "url": post_url,
                        "views_count": views_count,
                        "extraction_method": "graphql_api",
                        "status": "success"
                    }
            except Exception as e:
                print(f"   âš ï¸ GraphQL æ””æˆªå¤±æ•—: {str(e)[:100]}...")
        else:
            print("   âš ï¸ Gate é é¢ç„¡æ³•æ””æˆª GraphQLï¼Œç›´æ¥å˜—è©¦ DOM é¸æ“‡å™¨...")
        
        # æ–¹æ³• 2: DOM é¸æ“‡å™¨ç­–ç•¥
        selectors = [
            "a:has-text(' æ¬¡ç€è¦½'), a:has-text(' views')",
            "*:has-text('æ¬¡ç€è¦½'), *:has-text('views')",
            "span:has-text('æ¬¡ç€è¦½'), span:has-text('views')",
            "text=/\\d+.*æ¬¡ç€è¦½/, text=/\\d+.*views?/",
        ]
        
        for i, sel in enumerate(selectors):
            try:
                element = await page.wait_for_selector(sel, timeout=3000)
                text = await element.inner_text()
                views_count = parse_views_text(text)
                
                if views_count > 0:
                    print(f"   âœ… DOM é¸æ“‡å™¨ç²å–ç€è¦½æ•¸: {views_count:,} (é¸æ“‡å™¨ {i+1})")
                    return {
                        "url": post_url,
                        "views_count": views_count,
                        "extraction_method": f"dom_selector_{i+1}",
                        "raw_text": text.strip(),
                        "status": "success"
                    }
            except:
                continue
        
        print(f"   âŒ æ‰€æœ‰æ–¹æ³•éƒ½å¤±æ•—ï¼Œç„¡æ³•ç²å–ç€è¦½æ•¸")
        return {
            "url": post_url,
            "views_count": 0,
            "extraction_method": "gate_page" if is_gate_page else "failed",
            "is_gate_page": is_gate_page,
            "status": "failed"
        }
        
    except Exception as e:
        print(f"   âŒ è™•ç†è²¼æ–‡æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return {
            "url": post_url,
            "views_count": 0,
            "extraction_method": "error",
            "error": str(e),
            "status": "error"
        }

async def get_user_posts_urls(page, username: str, max_posts: int) -> list:
    """ç²å–ç”¨æˆ¶çš„è²¼æ–‡ URLs"""
    user_url = f"https://www.threads.com/@{username}"
    print(f"ğŸ” æ­£åœ¨ç²å– @{username} çš„è²¼æ–‡ URLs...")
    
    await page.goto(user_url, wait_until="networkidle")
    
    # ç­‰å¾…è²¼æ–‡è¼‰å…¥ä¸¦æ»¾å‹•ä»¥è¼‰å…¥æ›´å¤š
    await asyncio.sleep(3)
    
    # æ»¾å‹•å¹¾æ¬¡ä»¥è¼‰å…¥æ›´å¤šè²¼æ–‡
    for i in range(3):
        await page.mouse.wheel(0, 1000)
        await asyncio.sleep(2)
    
    # æå–è²¼æ–‡ URLs
    post_urls = await page.evaluate("""
        () => {
            const links = Array.from(document.querySelectorAll('a[href*="/post/"]'));
            const urls = links.map(link => link.href).filter(url => url.includes('/post/'));
            // å»é‡
            return [...new Set(urls)];
        }
    """)
    
    # é™åˆ¶æ•¸é‡
    post_urls = post_urls[:max_posts]
    print(f"   âœ… æ‰¾åˆ° {len(post_urls)} å€‹è²¼æ–‡ URLs")
    
    return post_urls

async def main():
    """ä¸»å‡½æ•¸ï¼šçˆ¬å–æŒ‡å®šç”¨æˆ¶çš„è²¼æ–‡ç€è¦½æ•¸"""
    auth_file = get_auth_file_path()
    if not auth_file.exists():
        print(f"âŒ æ‰¾ä¸åˆ°èªè­‰æª”æ¡ˆ: {auth_file}")
        print("   è«‹å…ˆåŸ·è¡Œ: python agents/playwright_crawler/save_auth.py")
        return

    print(f"ğŸš€ é–‹å§‹çˆ¬å– @{TARGET_USERNAME} çš„è²¼æ–‡ç€è¦½æ•¸")
    print(f"   ç›®æ¨™æ•¸é‡: {MAX_POSTS_TO_FETCH}")
    print(f"   èªè­‰æª”æ¡ˆ: {auth_file}")

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)  # ä½¿ç”¨ç„¡é ­æ¨¡å¼
        ctx = await browser.new_context(
            storage_state=str(auth_file),
            user_agent=UA,
            bypass_csp=True,
            locale="zh-TW",
        )
        await ctx.add_init_script(
            "Object.defineProperty(navigator,'webdriver',{get:()=>undefined})")

        page = await ctx.new_page()
        
        try:
            # æ­¥é©Ÿ 0: å…ˆè¨ªå• Threads é¦–é å»ºç«‹æœƒè©±
            print("ğŸ  æ­£åœ¨è¨ªå• Threads é¦–é å»ºç«‹æœƒè©±...")
            await page.goto("https://www.threads.com/", wait_until="networkidle")
            
            # æª¢æŸ¥æ˜¯å¦æˆåŠŸç™»å…¥ï¼ˆä½†ä¸è¦å› æ­¤åœæ­¢ï¼Œå› ç‚ºå€‹åˆ¥è²¼æ–‡ä»å¯èƒ½æå–åˆ°æ•¸æ“šï¼‰
            page_content = await page.content()
            if "__NEXT_DATA__" not in page_content:
                print("âš ï¸ é¦–é é¡¯ç¤ºç‚ºè¨ªå®¢æ¨¡å¼ï¼Œä½†ä»å˜—è©¦è™•ç†å€‹åˆ¥è²¼æ–‡")
                print("   æç¤ºï¼šå¦‚æœæ‰€æœ‰è²¼æ–‡éƒ½å¤±æ•—ï¼Œè«‹é‡æ–°åŸ·è¡Œ: python agents/playwright_crawler/save_auth.py")
            else:
                print("âœ… é¦–é èªè­‰æˆåŠŸ")
            
            # çŸ­æš«æš–èº«
            await page.mouse.wheel(0, 500)
            await asyncio.sleep(2)
            
            # æ­¥é©Ÿ 1: ç²å–è²¼æ–‡ URLs
            post_urls = await get_user_posts_urls(page, TARGET_USERNAME, MAX_POSTS_TO_FETCH)
            
            if not post_urls:
                print("âŒ æ²’æœ‰æ‰¾åˆ°ä»»ä½•è²¼æ–‡ URLs")
                return
            
            # æ­¥é©Ÿ 2: æå–æ¯å€‹è²¼æ–‡çš„ç€è¦½æ•¸
            results = []
            successful_count = 0
            gate_page_count = 0
            
            for i, post_url in enumerate(post_urls, 1):
                print(f"\n--- è™•ç†è²¼æ–‡ {i}/{len(post_urls)} ---")
                
                result = await extract_post_views(page, post_url)
                if result:
                    results.append(result)
                    if result.get("views_count", 0) > 0:
                        successful_count += 1
                        # é‡ç½® gate_page_count å¦‚æœæˆåŠŸç²å–æ•¸æ“š
                        gate_page_count = 0
                    elif result.get("status") == "failed" and result.get("extraction_method") == "gate_page":
                        # Gate é é¢ä½†æ²’æœ‰ç²å–åˆ°æ•¸æ“š
                        gate_page_count += 1
                        print(f"   âš ï¸ Gate é é¢ç„¡æ³•ç²å–æ•¸æ“š ({gate_page_count}/3)")
                        
                        # å¦‚æœé€£çºŒé‡åˆ°å¤ªå¤šç„¡æ•¸æ“šçš„ Gate é é¢ï¼Œæ‰é‡æ–°å»ºç«‹æœƒè©±
                        if gate_page_count >= 3:
                            print("ğŸ”„ é€£çºŒç„¡æ³•å¾ Gate é é¢ç²å–æ•¸æ“šï¼Œé‡æ–°å»ºç«‹æœƒè©±...")
                            await page.goto("https://www.threads.com/", wait_until="networkidle")
                            await asyncio.sleep(3)
                            gate_page_count = 0
                
                # æ›´é•·çš„éš¨æ©Ÿå»¶é²é¿å…è§¸ç™¼åçˆ¬èŸ²
                delay = random.uniform(3, 6)
                print(f"   â±ï¸ ç­‰å¾… {delay:.1f} ç§’...")
                await asyncio.sleep(delay)
            
            # æ­¥é©Ÿ 3: æ•´ç†çµæœä¸¦ä¿å­˜
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = f"post_views_results_{TARGET_USERNAME}_{timestamp}.json"
            
            final_results = {
                "username": TARGET_USERNAME,
                "timestamp": timestamp,
                "total_posts_found": len(post_urls),
                "total_posts_processed": len(results),
                "successful_extractions": successful_count,
                "posts": results
            }
            
            # ä¿å­˜çµæœ
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(final_results, f, ensure_ascii=False, indent=2)
            
            # é¡¯ç¤ºæ‘˜è¦
            print(f"\n{'='*50}")
            print(f"ğŸ‰ çˆ¬å–å®Œæˆï¼")
            print(f"   è™•ç†è²¼æ–‡: {len(results)}/{len(post_urls)}")
            print(f"   æˆåŠŸæå–: {successful_count}")
            print(f"   çµæœå·²ä¿å­˜è‡³: {output_file}")
            
            # é¡¯ç¤ºå‰å¹¾å€‹çµæœ
            print(f"\n--- å‰ 3 å€‹çµæœé è¦½ ---")
            for i, result in enumerate(results[:3], 1):
                status_icon = "âœ…" if result.get("views_count", 0) > 0 else "âŒ"
                print(f"{i}. {status_icon} ç€è¦½æ•¸: {result.get('views_count', 0):,}")
                print(f"   URL: {result.get('url', 'N/A')}")
                print(f"   æ–¹æ³•: {result.get('extraction_method', 'N/A')}")
                if result.get('raw_text'):
                    print(f"   åŸå§‹æ–‡å­—: '{result['raw_text']}'")
                print()
            
        except Exception as e:
            print(f"âŒ åŸ·è¡Œéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
        finally:
            await browser.close()
            print("ğŸšª ç€è¦½å™¨å·²é—œé–‰")

if __name__ == "__main__":
    asyncio.run(main())