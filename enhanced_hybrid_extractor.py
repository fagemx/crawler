"""
å¢å¼·ç‰ˆæ··åˆå…§å®¹æå–å™¨ - æ·»åŠ ç™¼æ–‡æ™‚é–“å’Œä¸»é¡Œtagæå–
åŸºæ–¼ hybrid_content_extractor.py çš„æˆåŠŸæ¨¡å¼
"""

import asyncio
import json
import httpx
import urllib.parse
import re
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, Any, Optional, List, Tuple

import sys
sys.path.append(str(Path(__file__).parent))

from playwright.async_api import async_playwright
from common.config import get_auth_file_path

# æ¸¬è©¦è²¼æ–‡
TEST_POST_URL = "https://www.threads.com/@ttshow.tw/post/DMegHD-S3xR"
TARGET_USERNAME = "ttshow.tw"
TARGET_CODE = "DMegHD-S3xR"

# æœŸæœ›æ•¸æ“š
EXPECTED_TIME = "2025-7-24 ä¸‹åˆ 6:55"
EXPECTED_TAG = "é ‘ç«¥MJ116"

class EnhancedHybridExtractor:
    """å¢å¼·ç‰ˆæ··åˆå…§å®¹æå–å™¨ - æ·»åŠ æ™‚é–“å’Œæ¨™ç±¤æå–"""
    
    def __init__(self):
        self.counts_captured = False
        self.counts_headers = {}
        self.counts_payload = ""
        self.auth_header = ""
        self.lsd_token = ""
    
    async def intercept_counts_query(self):
        """æ””æˆªè¨ˆæ•¸æŸ¥è©¢ï¼ˆæ²¿ç”¨æˆåŠŸçš„æ–¹æ³•ï¼‰"""
        print("ğŸ“Š æ””æˆªè¨ˆæ•¸æŸ¥è©¢...")
        
        auth_file_path = get_auth_file_path()
        
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            context = await browser.new_context(
                storage_state=str(auth_file_path),
                user_agent="Mozilla/5.0 (iPhone; CPU iPhone OS 15_0 like Mac OS X) AppleWebKit/605.1.15",
                viewport={"width": 375, "height": 812}
            )
            
            page = await context.new_page()
            
            async def response_handler(response):
                friendly_name = response.request.headers.get("x-fb-friendly-name", "")
                
                if "useBarcelonaBatchedDynamicPostCountsSubscriptionQuery" in friendly_name:
                    try:
                        data = await response.json()
                        # æª¢æŸ¥æ˜¯å¦åŒ…å«ä»»ä½•å¯ç”¨çš„PK
                        data_str = json.dumps(data, ensure_ascii=False)
                        if "pk" in data_str:  # æ›´å¯¬é¬†çš„åŒ¹é…
                            print(f"   âœ… æ””æˆªåˆ°è¨ˆæ•¸æŸ¥è©¢")
                            
                            self.counts_headers = dict(response.request.headers)
                            self.counts_payload = response.request.post_data
                            self.auth_header = self.counts_headers.get("authorization", "")
                            self.lsd_token = self.counts_headers.get("x-fb-lsd", "")
                            
                            self.counts_captured = True
                    except:
                        pass
            
            page.on("response", response_handler)
            await page.goto(TEST_POST_URL, wait_until="networkidle")
            await asyncio.sleep(3)
            await browser.close()
        
        return self.counts_captured
    
    async def extract_time_from_dom(self, page) -> Optional[datetime]:
        """å¾DOMæå–ç™¼æ–‡æ™‚é–“ï¼ˆä¸‰ç¨®æ–¹æ³•ï¼‰"""
        print(f"      â° æå–ç™¼æ–‡æ™‚é–“...")
        
        try:
            # æ–¹æ³•A: ç›´æ¥æŠ“ <time> çš„ datetime å±¬æ€§
            time_elements = page.locator('time[datetime]')
            count = await time_elements.count()
            
            if count > 0:
                print(f"         æ‰¾åˆ° {count} å€‹ time å…ƒç´ ")
                
                for i in range(min(count, 5)):  # æª¢æŸ¥å‰5å€‹
                    try:
                        time_el = time_elements.nth(i)
                        
                        # æ–¹æ³•A: datetime å±¬æ€§
                        iso_time = await time_el.get_attribute('datetime')
                        if iso_time:
                            print(f"         âœ… datetime: {iso_time}")
                            from dateutil import parser
                            return parser.parse(iso_time)
                        
                        # æ–¹æ³•B: title æˆ– aria-label å±¬æ€§  
                        title_time = (await time_el.get_attribute('title') or 
                                    await time_el.get_attribute('aria-label'))
                        if title_time:
                            print(f"         âœ… title/aria-label: {title_time}")
                            # å˜—è©¦è§£æä¸­æ–‡æ™‚é–“æ ¼å¼
                            parsed_time = self.parse_chinese_time(title_time)
                            if parsed_time:
                                return parsed_time
                    except Exception as e:
                        continue
            
            # æ–¹æ³•C: è§£æ __NEXT_DATA__
            print(f"         ğŸ” å˜—è©¦ __NEXT_DATA__ æ–¹æ³•...")
            try:
                script_el = page.locator('#__NEXT_DATA__')
                if await script_el.count() > 0:
                    script_content = await script_el.text_content()
                    data = json.loads(script_content)
                    
                    # å°‹æ‰¾ taken_at æ™‚é–“æˆ³
                    taken_at = self.find_taken_at(data)
                    if taken_at:
                        print(f"         âœ… __NEXT_DATA__ taken_at: {taken_at}")
                        return datetime.fromtimestamp(taken_at)
                        
            except Exception as e:
                print(f"         âŒ __NEXT_DATA__ è§£æå¤±æ•—: {e}")
            
            # æ–¹æ³•D: è§£æç›¸å°æ™‚é–“ï¼ˆå…œåº•ï¼‰
            print(f"         ğŸ” å˜—è©¦ç›¸å°æ™‚é–“è§£æ...")
            time_elements_all = page.locator('time')
            time_count = await time_elements_all.count()
            
            for i in range(min(time_count, 10)):
                try:
                    time_el = time_elements_all.nth(i)
                    text = await time_el.inner_text()
                    if text:
                        parsed_time = self.parse_relative_time(text)
                        if parsed_time:
                            print(f"         âœ… ç›¸å°æ™‚é–“: {text} -> {parsed_time}")
                            return parsed_time
                except:
                    continue
            
        except Exception as e:
            print(f"         âŒ æ™‚é–“æå–å¤±æ•—: {e}")
        
        return None
    
    def parse_chinese_time(self, time_str: str) -> Optional[datetime]:
        """è§£æä¸­æ–‡æ™‚é–“æ ¼å¼"""
        try:
            # è™•ç† "2025å¹´8æœˆ3æ—¥ä¸‹åˆ 2:36" æ ¼å¼
            if "å¹´" in time_str and "æœˆ" in time_str and "æ—¥" in time_str:
                # æå–æ•¸å­—éƒ¨åˆ†
                import re
                match = re.search(r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥.*?(\d{1,2}):(\d{2})', time_str)
                if match:
                    year, month, day, hour, minute = map(int, match.groups())
                    
                    # è™•ç†ä¸‹åˆ/ä¸Šåˆ
                    if "ä¸‹åˆ" in time_str and hour < 12:
                        hour += 12
                    elif "ä¸Šåˆ" in time_str and hour == 12:
                        hour = 0
                    
                    return datetime(year, month, day, hour, minute)
        except:
            pass
        return None
    
    def parse_relative_time(self, text: str) -> Optional[datetime]:
        """è§£æç›¸å°æ™‚é–“"""
        try:
            import re
            pattern = re.compile(r'(\d+)\s*(ç§’|åˆ†é˜|åˆ†|å°æ™‚|å¤©|é€±|æœˆ|å¹´)')
            match = pattern.search(text)
            
            if match:
                num = int(match.group(1))
                unit = match.group(2)
                
                multiplier = {
                    "ç§’": 1,
                    "åˆ†é˜": 60, "åˆ†": 60,
                    "å°æ™‚": 3600,
                    "å¤©": 86400,
                    "é€±": 604800,
                    "æœˆ": 2592000,
                    "å¹´": 31536000
                }
                
                if unit in multiplier:
                    seconds_ago = num * multiplier[unit]
                    return datetime.now() - timedelta(seconds=seconds_ago)
        except:
            pass
        return None
    
    def find_taken_at(self, data: Any, path: str = "") -> Optional[int]:
        """éæ­¸æœç´¢ taken_at æ™‚é–“æˆ³"""
        if isinstance(data, dict):
            for key, value in data.items():
                if key == "taken_at" and isinstance(value, int) and value > 1000000000:
                    return value
                result = self.find_taken_at(value, f"{path}.{key}")
                if result:
                    return result
        elif isinstance(data, list):
            for i, item in enumerate(data):
                result = self.find_taken_at(item, f"{path}[{i}]")
                if result:
                    return result
        return None
    
    async def extract_tags_from_dom(self, page) -> List[str]:
        """å¾DOMæå–ä¸»æ–‡ç« çš„æ¨™ç±¤é€£çµï¼ˆå°ˆé–€æœç´¢Threadsæ¨™ç±¤é€£çµï¼‰"""
        print(f"      ğŸ·ï¸ æå–ä¸»æ–‡ç« æ¨™ç±¤é€£çµ...")
        tags = []
        
        try:
            # ç­–ç•¥1: æœç´¢æ¨™ç±¤é€£çµï¼ˆå„ªå…ˆç´šæœ€é«˜ï¼‰
            tag_link_selectors = [
                'a[href*="/search?q="][href*="serp_type=tags"]',  # æ¨™ç±¤æœç´¢é€£çµ
                'a[href*="/search"][href*="tag_id="]',  # åŒ…å«tag_idçš„é€£çµ
                'a[href*="serp_type=tags"]',  # æ¨™ç±¤é¡å‹é€£çµ
            ]
            
            for selector in tag_link_selectors:
                try:
                    tag_links = page.locator(selector)
                    count = await tag_links.count()
                    
                    if count > 0:
                        print(f"         ğŸ”— æ‰¾åˆ° {count} å€‹æ¨™ç±¤é€£çµ ({selector})")
                        
                        # åªæª¢æŸ¥å‰3å€‹ï¼ˆé¿å…å›å¾©ä¸­çš„æ¨™ç±¤ï¼‰
                        for i in range(min(count, 3)):
                            try:
                                link = tag_links.nth(i)
                                href = await link.get_attribute('href')
                                text = await link.inner_text()
                                
                                if href and text:
                                    # è§£ææ¨™ç±¤åç¨±
                                    tag_name = self.extract_tag_name_from_link(href, text)
                                    if tag_name and tag_name not in tags:
                                        tags.append(tag_name)
                                        print(f"         âœ… æ¨™ç±¤é€£çµ: {tag_name} -> {href}")
                                        return tags  # æ‰¾åˆ°ä¸€å€‹å°±è¿”å›ï¼Œå› ç‚ºé€šå¸¸åªæœ‰ä¸€å€‹ä¸»è¦æ¨™ç±¤
                                        
                            except Exception as e:
                                continue
                                
                except Exception as e:
                    continue
            
            # ç­–ç•¥2: æœç´¢ä¸»æ–‡ç« å€åŸŸå…§çš„æ¨™ç±¤å…ƒç´ 
            main_post_selectors = [
                'article:first-of-type',  # ç¬¬ä¸€å€‹æ–‡ç« å…ƒç´ 
                '[role="article"]:first-of-type',  # ç¬¬ä¸€å€‹æ–‡ç« è§’è‰²å…ƒç´ 
                'div[data-pressable-container]:first-of-type',  # ç¬¬ä¸€å€‹å¯é»æ“Šå®¹å™¨
            ]
            
            for main_selector in main_post_selectors:
                try:
                    main_element = page.locator(main_selector)
                    if await main_element.count() > 0:
                        print(f"         ğŸ¯ åœ¨ä¸»æ–‡ç« å€åŸŸæœç´¢æ¨™ç±¤: {main_selector}")
                        
                        # åœ¨ä¸»æ–‡ç« å…§æœç´¢æ¨™ç±¤é€£çµ
                        main_tag_links = main_element.locator('a[href*="/search"]')
                        main_count = await main_tag_links.count()
                        
                        if main_count > 0:
                            print(f"         ğŸ”— ä¸»æ–‡ç« å…§æ‰¾åˆ° {main_count} å€‹æœç´¢é€£çµ")
                            
                            for i in range(min(main_count, 2)):
                                try:
                                    link = main_tag_links.nth(i)
                                    href = await link.get_attribute('href')
                                    text = await link.inner_text()
                                    
                                    if href and text:
                                        tag_name = self.extract_tag_name_from_link(href, text)
                                        if tag_name and tag_name not in tags:
                                            tags.append(tag_name)
                                            print(f"         âœ… ä¸»æ–‡ç« æ¨™ç±¤: {tag_name}")
                                            return tags
                                            
                                except Exception as e:
                                    continue
                        
                        # å‚™ç”¨ï¼šæœç´¢hashtagæ–‡æœ¬
                        hashtag_in_main = await self.search_hashtags_in_element(main_element)
                        if hashtag_in_main:
                            tags.extend(hashtag_in_main[:1])  # åªå–ç¬¬ä¸€å€‹
                            print(f"         âœ… ä¸»æ–‡ç« hashtag: {hashtag_in_main[:1]}")
                            return tags
                            
                except Exception as e:
                    continue
            
            # ç­–ç•¥3: å‚™ç”¨æ–¹æ¡ˆ - æœç´¢ __NEXT_DATA__ ä¸­çš„æ¨™ç±¤ä¿¡æ¯
            try:
                script_el = page.locator('#__NEXT_DATA__')
                if await script_el.count() > 0:
                    script_content = await script_el.text_content()
                    data = json.loads(script_content)
                    
                    # æœç´¢æ¨™ç±¤ç›¸é—œçš„æ•¸æ“š
                    topic_tags = self.find_tags_in_data(data)
                    if topic_tags:
                        tags.extend(topic_tags)
                        print(f"         âœ… __NEXT_DATA__ æ¨™ç±¤: {topic_tags}")
                    
            except Exception as e:
                print(f"         âŒ __NEXT_DATA__ æ¨™ç±¤è§£æå¤±æ•—: {e}")
            
        except Exception as e:
            print(f"         âŒ æ¨™ç±¤æå–å¤±æ•—: {e}")
        
        # æ¸…ç†å’Œè¿”å›æ¨™ç±¤
        print(f"         ğŸ”„ æ‰¾åˆ°çš„æ¨™ç±¤: {tags}")
        cleaned_tags = self.clean_tag_list(tags)
        print(f"         ğŸ“‹ æœ€çµ‚æ¨™ç±¤: {cleaned_tags}")
        return cleaned_tags
    
    def extract_tag_name_from_link(self, href: str, text: str) -> Optional[str]:
        """å¾æ¨™ç±¤é€£çµä¸­æå–æ¨™ç±¤åç¨±"""
        try:
            # æ–¹æ³•1: å¾URLçš„qåƒæ•¸ä¸­è§£æ
            if "q=" in href:
                # è§£æURLç·¨ç¢¼çš„æ¨™ç±¤åç¨±
                import urllib.parse
                parsed_url = urllib.parse.urlparse(href)
                query_params = urllib.parse.parse_qs(parsed_url.query)
                
                if 'q' in query_params:
                    tag_name = query_params['q'][0]
                    # URLè§£ç¢¼
                    tag_name = urllib.parse.unquote(tag_name)
                    print(f"         ğŸ¯ å¾URLè§£ææ¨™ç±¤: {tag_name}")
                    return tag_name
            
            # æ–¹æ³•2: å¾é€£çµæ–‡æœ¬ä¸­å–å¾—ï¼ˆå‚™ç”¨ï¼‰
            if text and len(text.strip()) > 0 and len(text.strip()) <= 20:
                # æ¸…ç†æ–‡æœ¬
                clean_text = text.strip()
                # ç§»é™¤å¯èƒ½çš„å‰å¾Œç¶´
                if clean_text.startswith('#'):
                    clean_text = clean_text[1:]
                
                print(f"         ğŸ“ å¾æ–‡æœ¬è§£ææ¨™ç±¤: {clean_text}")
                return clean_text
                
        except Exception as e:
            print(f"         âŒ æ¨™ç±¤è§£æå¤±æ•—: {e}")
        
        return None
    
    def find_tags_in_data(self, data: Any, path: str = "") -> List[str]:
        """åœ¨__NEXT_DATA__ä¸­æœç´¢æ¨™ç±¤ä¿¡æ¯"""
        tags = []
        
        try:
            if isinstance(data, dict):
                for key, value in data.items():
                    # æœç´¢å¯èƒ½åŒ…å«æ¨™ç±¤çš„å­—æ®µ
                    if any(tag_key in key.lower() for tag_key in 
                          ['tag', 'hashtag', 'topic', 'category', 'label']):
                        if isinstance(value, str) and len(value) > 0 and len(value) <= 20:
                            tags.append(value)
                        elif isinstance(value, list):
                            for item in value:
                                if isinstance(item, str) and len(item) > 0 and len(item) <= 20:
                                    tags.append(item)
                    
                    # éæ­¸æœç´¢ï¼ˆé™åˆ¶æ·±åº¦ï¼‰
                    if path.count('.') < 2:
                        sub_tags = self.find_tags_in_data(value, f"{path}.{key}")
                        tags.extend(sub_tags)
                        
            elif isinstance(data, list):
                for i, item in enumerate(data[:3]):  # åªæª¢æŸ¥å‰3å€‹
                    sub_tags = self.find_tags_in_data(item, f"{path}[{i}]")
                    tags.extend(sub_tags)
                    
        except Exception as e:
            pass
        
        return tags[:3]  # åªè¿”å›å‰3å€‹
    
    def clean_tag_list(self, raw_tags: List[str]) -> List[str]:
        """æ¸…ç†æ¨™ç±¤åˆ—è¡¨"""
        if not raw_tags:
            return []
        
        cleaned_tags = []
        
        for tag in raw_tags:
            if tag and isinstance(tag, str):
                # åŸºæœ¬æ¸…ç†
                clean_tag = tag.strip()
                
                # ç§»é™¤URLç·¨ç¢¼çš„å­—ç¬¦
                if '%' in clean_tag:
                    try:
                        import urllib.parse
                        clean_tag = urllib.parse.unquote(clean_tag)
                    except:
                        pass
                
                # ç§»é™¤å¯èƒ½çš„å‰å¾Œç¶´
                if clean_tag.startswith('#'):
                    clean_tag = clean_tag[1:]
                
                # åŸºæœ¬é©—è­‰
                if (clean_tag and 
                    len(clean_tag) > 0 and 
                    len(clean_tag) <= 30 and 
                    clean_tag not in cleaned_tags):
                    cleaned_tags.append(clean_tag)
        
        # åªè¿”å›ç¬¬ä¸€å€‹æ¨™ç±¤ï¼ˆä¸»è¦æ¨™ç±¤ï¼‰
        return cleaned_tags[:1] if cleaned_tags else []
    
    def extract_main_topic_from_text(self, text: str) -> List[str]:
        """å¾ä¸»æ–‡ç« æ–‡æœ¬ä¸­æå–ä¸»è¦ä¸»é¡Œæ¨™ç±¤"""
        tags = []
        
        # æ“´å±•çš„ä¸»é¡Œæ¨¡å¼ï¼ˆæ›´å…¨é¢çš„é—œéµè©ï¼‰
        topic_patterns = [
            # æŠ€è¡“/å®‰å…¨ç›¸é—œ
            r'é›¶æ—¥æ”»æ“Š',
            r'0[- ]?day',
            r'zero[- ]?day',
            r'æ¼æ´',
            r'ç¶²è·¯å®‰å…¨',
            r'è³‡å®‰',
            r'é§­å®¢',
            r'æ”»æ“Š',
            r'å®‰å…¨',
            r'å¼±é»',
            r'æ»²é€',
            r'exploit',
            r'hack',
            r'security',
            r'vulnerability',
            r'cybersecurity',
            r'infosec',
            
            # æ”¿æ²»/ç¤¾æœƒç›¸é—œï¼ˆæ ¹æ“šå¯¦éš›å…§å®¹ï¼‰
            r'æ”¿æ²»',
            r'æ°‘ä¸»',
            r'é¸èˆ‰',
            r'æ”¿é»¨',
            r'å¹´è¼•äºº',
            r'ç¤¾æœƒ',
            r'å°ç£',
            r'å…¬æ°‘',
            
            # å…¶ä»–å¯èƒ½çš„ä¸»é¡Œ
            r'ç§‘æŠ€',
            r'AI',
            r'äººå·¥æ™ºæ…§',
            r'å€å¡Šéˆ',
            r'åŠ å¯†',
            r'æ•¸ä½',
        ]
        
        print(f"         ğŸ” æœç´¢æ–‡æœ¬: {text[:200]}...")
        
        for pattern in topic_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            if matches:
                print(f"         ğŸ¯ æ‰¾åˆ°é—œéµè©: {pattern} -> {matches}")
                # æ¨™æº–åŒ–æ¨™ç±¤åç¨±
                if any(keyword in pattern for keyword in ['é›¶æ—¥', '0day', 'zero']):
                    tags.append('é›¶æ—¥æ”»æ“Š')
                elif 'æ¼æ´' in pattern or 'vulnerability' in pattern:
                    tags.append('æ¼æ´')
                elif any(keyword in pattern for keyword in ['ç¶²è·¯å®‰å…¨', 'è³‡å®‰', 'security', 'cybersecurity', 'infosec']):
                    tags.append('è³‡å®‰')
                elif any(keyword in pattern for keyword in ['æ”¿æ²»', 'æ”¿é»¨', 'æ°‘ä¸»', 'é¸èˆ‰']):
                    tags.append('æ”¿æ²»')
                elif 'å¹´è¼•äºº' in pattern:
                    tags.append('å¹´è¼•äºº')
                else:
                    tags.append(matches[0])
                break  # åªè¦æ‰¾åˆ°ä¸€å€‹ä¸»è¦ä¸»é¡Œå°±åœæ­¢
        
        if not tags:
            print(f"         âŒ æœªæ‰¾åˆ°ä»»ä½•é—œéµè©åŒ¹é…")
        
        return tags
    
    def find_main_topic_in_data(self, data: Any, path: str = "") -> List[str]:
        """åœ¨__NEXT_DATA__ä¸­æœç´¢ä¸»è¦ä¸»é¡Œ"""
        tags = []
        
        if isinstance(data, dict):
            for key, value in data.items():
                # æª¢æŸ¥å¯èƒ½åŒ…å«ä¸»é¡Œçš„å­—æ®µå
                if any(topic_key in key.lower() for topic_key in 
                      ['title', 'caption', 'text', 'content', 'description']):
                    if isinstance(value, str):
                        topic_tags = self.extract_main_topic_from_text(value)
                        if topic_tags:
                            tags.extend(topic_tags)
                            return tags  # æ‰¾åˆ°å°±è¿”å›ï¼Œé¿å…éå¤šæœç´¢
                
                # éæ­¸æœç´¢ï¼ˆé™åˆ¶æ·±åº¦é¿å…å›å¾©ï¼‰
                if path.count('.') < 3:  # é™åˆ¶æœç´¢æ·±åº¦
                    sub_tags = self.find_main_topic_in_data(value, f"{path}.{key}")
                    if sub_tags:
                        tags.extend(sub_tags)
                        return tags
                        
        elif isinstance(data, list) and len(data) > 0:
            # åªæª¢æŸ¥ç¬¬ä¸€å€‹å…ƒç´ ï¼ˆé€šå¸¸æ˜¯ä¸»æ–‡ç« ï¼‰
            sub_tags = self.find_main_topic_in_data(data[0], f"{path}[0]")
            if sub_tags:
                tags.extend(sub_tags)
                        
        elif isinstance(data, str):
            topic_tags = self.extract_main_topic_from_text(data)
            if topic_tags:
                tags.extend(topic_tags)
        
        return tags
    
    async def search_hashtags_in_element(self, element) -> List[str]:
        """åœ¨æŒ‡å®šå…ƒç´ å…§æœç´¢hashtag"""
        hashtags = []
        try:
            # æœç´¢åŒ…å«#çš„å…ƒç´ 
            hash_elements = element.locator('*:has-text("#")')
            count = await hash_elements.count()
            
            for i in range(min(count, 5)):
                try:
                    hash_elem = hash_elements.nth(i)
                    text = await hash_elem.inner_text()
                    if text and "#" in text:
                        found_hashtags = re.findall(r'#([^#\s\n]+)', text)
                        for tag in found_hashtags:
                            if len(tag) <= 15 and tag not in hashtags:
                                hashtags.append(tag)
                except:
                    continue
                    
            # ä¹Ÿæœç´¢linkå…ƒç´ 
            link_elements = element.locator('a[href*="#"]')
            link_count = await link_elements.count()
            
            for i in range(min(link_count, 5)):
                try:
                    link_elem = link_elements.nth(i)
                    href = await link_elem.get_attribute('href')
                    text = await link_elem.inner_text()
                    
                    if href and "#" in href:
                        # å¾hrefä¸­æå–hashtag
                        href_tags = re.findall(r'#([^#\s&]+)', href)
                        for tag in href_tags:
                            if len(tag) <= 15 and tag not in hashtags:
                                hashtags.append(tag)
                    
                    if text and "#" in text:
                        # å¾æ–‡æœ¬ä¸­æå–hashtag
                        text_tags = re.findall(r'#([^#\s\n]+)', text)
                        for tag in text_tags:
                            if len(tag) <= 15 and tag not in hashtags:
                                hashtags.append(tag)
                except:
                    continue
                    
        except Exception as e:
            print(f"         âŒ hashtagæœç´¢å¤±æ•—: {e}")
        
        return hashtags
    
    def extract_location_tags(self, text: str) -> List[str]:
        """å¾æ–‡æœ¬ä¸­æå–åœ°é»æ¨™ç±¤"""
        tags = []
        
        # ç›´æ¥åŒ…å«é›²æ—çš„æƒ…æ³
        if "é›²æ—" in text:
            # åªæœ‰åœ¨é›²æ—æ˜¯ç¨ç«‹è©å½™æˆ–è€…æ˜¯çŸ­èªæ™‚æ‰æ·»åŠ 
            if self.is_valid_tag("é›²æ—"):
                tags.append("é›²æ—")
        
        # å¯èƒ½çš„åœ°é»æ ¼å¼
        location_patterns = [
            r'ä½æ–¼\s*([^ï¼Œã€‚\s]+)',
            r'åœ¨\s*([^ï¼Œã€‚\s]+)', 
            r'@\s*([^ï¼Œã€‚\s]+)',
            r'åœ°é»[ï¼š:]\s*([^ï¼Œã€‚\s]+)',
            r'([^ï¼Œã€‚\s]*é›²æ—[^ï¼Œã€‚\s]*)'
        ]
        
        for pattern in location_patterns:
            matches = re.findall(pattern, text)
            for match in matches:
                cleaned_tag = match.strip()
                if cleaned_tag and self.is_valid_tag(cleaned_tag) and cleaned_tag not in tags:
                    tags.append(cleaned_tag)
        
        return tags
    
    def is_valid_tag(self, tag: str) -> bool:
        """åˆ¤æ–·æ˜¯å¦ç‚ºæœ‰æ•ˆçš„æ¨™ç±¤"""
        if not tag or len(tag.strip()) == 0:
            return False
        
        tag = tag.strip()
        
        # éæ¿¾æ¢ä»¶
        # 1. é•·åº¦é™åˆ¶ï¼ˆæ¨™ç±¤é€šå¸¸ä¸æœƒå¤ªé•·ï¼‰
        if len(tag) > 15:
            return False
        
        # 2. éæ¿¾å®Œæ•´å¥å­ï¼ˆåŒ…å«æ¨™é»ç¬¦è™Ÿï¼‰
        if any(punct in tag for punct in ['ã€‚', 'ï¼', 'ï¼Ÿ', 'ï¼Œ', 'ã€', '\n', '  ']):
            return False
        
        # 3. éæ¿¾åŒ…å«å¤ªå¤šè¡¨æƒ…ç¬¦è™Ÿçš„
        emoji_count = len(re.findall(r'[ğŸ¥°ğŸ˜ŠğŸ˜‚â¤ï¸ğŸ‘ğŸ‰ğŸ”¥ğŸ’¯ğŸ™ˆğŸ™‰ğŸ™Š]', tag))
        if emoji_count > 2:
            return False
        
        # 4. éæ¿¾ç¶²å€æˆ–ç‰¹æ®Šæ ¼å¼
        if any(pattern in tag.lower() for pattern in ['http', 'www', '.com', '.net', 'libertytimes']):
            return False
        
        # 5. éæ¿¾ç´”æ•¸å­—æˆ–ç‰¹æ®Šç¬¦è™Ÿé–‹é ­
        if tag.isdigit() or tag.startswith(('@', '#', 'im')):
            return False
        
        # 6. éæ¿¾å•å¥
        if 'å—' in tag or tag.endswith('?'):
            return False
        
        # 7. éæ¿¾å¤ªé•·çš„æè¿°æ€§æ–‡å­—
        descriptive_words = ['æ‰€ä»¥æˆ‘éƒ½èªª', 'çµ•å°ä¸æœƒæ˜¯', 'é€™åº§', 'é€ åƒ¹', 'å„„', 'å¤§é›¨ä¸­', 'æŒçºŒé€²è¡Œ', 'ç©æ°´çš„', 'è·‘é“ä¸Š']
        if any(word in tag for word in descriptive_words):
            return False
        
        return True
    
    def filter_and_clean_tags(self, raw_tags: List[str]) -> List[str]:
        """éæ¿¾å’Œæ¸…ç†æ¨™ç±¤åˆ—è¡¨"""
        cleaned_tags = []
        
        for tag in raw_tags:
            if self.is_valid_tag(tag):
                # é€²ä¸€æ­¥æ¸…ç†
                cleaned_tag = tag.strip()
                
                # å»é™¤å¤šé¤˜çš„è¡¨æƒ…ç¬¦è™Ÿ
                cleaned_tag = re.sub(r'[ğŸ¥°ğŸ˜ŠğŸ˜‚â¤ï¸ğŸ‘ğŸ‰ğŸ”¥ğŸ’¯ğŸ™ˆğŸ™‰ğŸ™Š]{2,}', '', cleaned_tag)
                
                # å¦‚æœæ¸…ç†å¾Œä»ç„¶æœ‰æ•ˆä¸”ä¸é‡è¤‡ï¼Œå‰‡æ·»åŠ 
                if cleaned_tag and len(cleaned_tag) > 1 and cleaned_tag not in cleaned_tags:
                    cleaned_tags.append(cleaned_tag)
        
        # ç¢ºä¿é›²æ—åœ¨åˆ—è¡¨ä¸­
        if any('é›²æ—' in tag for tag in raw_tags) and 'é›²æ—' not in cleaned_tags:
            cleaned_tags.insert(0, 'é›²æ—')
        
        return cleaned_tags
    
    def filter_to_single_main_tag(self, raw_tags: List[str]) -> List[str]:
        """éæ¿¾åˆ°å–®ä¸€ä¸»è¦æ¨™ç±¤"""
        if not raw_tags:
            return []
        
        # æ“´å±•çš„å„ªå…ˆç´šè¦å‰‡
        priority_keywords = [
            'é›¶æ—¥æ”»æ“Š',  # æœ€é«˜å„ªå…ˆç´š - é‡å°ç•¶å‰æ¸¬è©¦
            '0day',
            'zero-day',
            'zero day',
            'è³‡å®‰',
            'ç¶²è·¯å®‰å…¨', 
            'æ¼æ´',
            'é§­å®¢',
            'æ”»æ“Š',
            'æ”¿æ²»',     # æ ¹æ“šå¯¦éš›å…§å®¹èª¿æ•´
            'å¹´è¼•äºº',
            'æ”¿é»¨',
            'ç§‘æŠ€',
            'AI',
            'å€å¡Šéˆ'
        ]
        
        # é¦–å…ˆæª¢æŸ¥æ˜¯å¦æœ‰é«˜å„ªå…ˆç´šçš„æ¨™ç±¤
        for keyword in priority_keywords:
            for tag in raw_tags:
                if keyword.lower() in tag.lower():
                    return [keyword if keyword == 'é›¶æ—¥æ”»æ“Š' else tag]
        
        # å¦‚æœæ²’æœ‰æ‰¾åˆ°é«˜å„ªå…ˆç´šæ¨™ç±¤ï¼Œä½¿ç”¨ä¸€èˆ¬éæ¿¾è¦å‰‡
        valid_tags = []
        for tag in raw_tags:
            if self.is_valid_single_tag(tag):
                valid_tags.append(tag)
        
        # åªè¿”å›ç¬¬ä¸€å€‹æœ‰æ•ˆæ¨™ç±¤
        return valid_tags[:1] if valid_tags else []
    
    def is_valid_single_tag(self, tag: str) -> bool:
        """åˆ¤æ–·æ˜¯å¦ç‚ºæœ‰æ•ˆçš„å–®ä¸€æ¨™ç±¤"""
        if not tag or len(tag.strip()) == 0:
            return False
        
        tag = tag.strip()
        
        # åŸºæœ¬éæ¿¾ï¼ˆæ¯”ä¹‹å‰æ›´åš´æ ¼ï¼‰
        if len(tag) > 8:  # æ¨™ç±¤é•·åº¦é™åˆ¶æ›´åš´æ ¼
            return False
        
        # éæ¿¾å®Œæ•´å¥å­å’Œæè¿°æ€§æ–‡å­—
        if any(punct in tag for punct in ['ã€‚', 'ï¼', 'ï¼Ÿ', 'ï¼Œ', 'ã€', '\n', 'çš„', 'æ˜¯', 'åœ¨']):
            return False
        
        # éæ¿¾è¡¨æƒ…ç¬¦è™Ÿ
        if re.search(r'[ğŸ¥°ğŸ˜ŠğŸ˜‚â¤ï¸ğŸ‘ğŸ‰ğŸ”¥ğŸ’¯ğŸ™ˆğŸ™‰ğŸ™ŠğŸ˜ ]', tag):
            return False
        
        # éæ¿¾æ•¸å­—å’Œç‰¹æ®Šç¬¦è™Ÿ
        if tag.isdigit() or tag.startswith(('@', '#', 'im')):
            return False
        
        return True
    
    def find_location_in_data(self, data: Any, path: str = "") -> List[str]:
        """éæ­¸æœç´¢æ•¸æ“šä¸­çš„åœ°é»ä¿¡æ¯"""
        tags = []
        
        if isinstance(data, dict):
            for key, value in data.items():
                # æª¢æŸ¥å¯èƒ½åŒ…å«åœ°é»çš„å­—æ®µå
                if any(location_key in key.lower() for location_key in 
                      ['location', 'place', 'venue', 'city', 'region', 'address']):
                    if isinstance(value, str) and "é›²æ—" in value:
                        tags.append(value)
                
                # éæ­¸æœç´¢
                sub_tags = self.find_location_in_data(value, f"{path}.{key}")
                tags.extend(sub_tags)
                
        elif isinstance(data, list):
            for i, item in enumerate(data):
                sub_tags = self.find_location_in_data(item, f"{path}[{i}]")
                tags.extend(sub_tags)
                
        elif isinstance(data, str) and "é›²æ—" in data:
            tags.append(data)
        
        return tags
    
    async def check_auth_status(self, page):
        """æª¢æŸ¥èªè­‰ç‹€æ…‹"""
        try:
            # æª¢æŸ¥æ˜¯å¦æœ‰ç™»å…¥æŒ‡ç¤ºå™¨
            login_indicators = [
                'button:has-text("ç™»å…¥")',
                'button:has-text("Log in")',
                'a:has-text("ç™»å…¥")',
                'text="ç™»å…¥"'
            ]
            
            is_logged_in = True
            for indicator in login_indicators:
                if await page.locator(indicator).count() > 0:
                    is_logged_in = False
                    break
            
            if is_logged_in:
                print(f"      âœ… èªè­‰ç‹€æ…‹ï¼šå·²ç™»å…¥")
            else:
                print(f"      âš ï¸ èªè­‰ç‹€æ…‹ï¼šæœªç™»å…¥ - å¯èƒ½å½±éŸ¿æ¨™ç±¤æå–")
            
            # æª¢æŸ¥æ˜¯å¦ç‚ºGateé é¢
            page_content = await page.content()
            is_gate_page = "__NEXT_DATA__" not in page_content
            
            if is_gate_page:
                print(f"      âš ï¸ æª¢æ¸¬åˆ°Gateé é¢ - åŠŸèƒ½å¯èƒ½å—é™")
            else:
                print(f"      âœ… å®Œæ•´é é¢ - åŠŸèƒ½æ­£å¸¸")
                
        except Exception as e:
            print(f"      âŒ èªè­‰ç‹€æ…‹æª¢æŸ¥å¤±æ•—: {e}")
    
    async def get_enhanced_content_and_media_from_dom(self, post_url: str) -> Optional[Dict[str, Any]]:
        """å¢å¼·ç‰ˆDOMè§£æï¼ˆæ·»åŠ æ™‚é–“å’Œæ¨™ç±¤ï¼‰"""
        print(f"   ğŸŒ å¾ DOM è§£æå…§å®¹ã€åª’é«”ã€æ™‚é–“å’Œæ¨™ç±¤...")
        
        auth_file_path = get_auth_file_path()
        
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            context = await browser.new_context(
                storage_state=str(auth_file_path),
                user_agent="Mozilla/5.0 (iPhone; CPU iPhone OS 15_0 like Mac OS X) AppleWebKit/605.1.15",
                viewport={"width": 375, "height": 812}
            )
            
            page = await context.new_page()
            
            try:
                # è¼‰å…¥é é¢
                await page.goto(post_url, wait_until="networkidle", timeout=60000)
                await asyncio.sleep(3)
                
                # æª¢æŸ¥èªè­‰ç‹€æ…‹
                await self.check_auth_status(page)
                
                # åŸºæ–¼ hybrid_content_extractor.py çš„æˆåŠŸé‚è¼¯ - æå–åŸºæœ¬ä¿¡æ¯
                
                # æå–ç”¨æˆ¶å
                username = ""
                try:
                    url_match = re.search(r'/@([^/]+)/', post_url)
                    if url_match:
                        username = url_match.group(1)
                except:
                    pass
                
                # æå–å…§å®¹æ–‡å­—
                content = ""
                try:
                    content_selectors = [
                        'div[data-pressable-container] span',
                        '[data-testid="thread-text"]',
                        'article div[dir="auto"]',
                        'div[role="article"] div[dir="auto"]',
                        'span[style*="text-overflow"]'
                    ]
                    
                    for selector in content_selectors:
                        try:
                            elements = page.locator(selector)
                            count = await elements.count()
                            
                            if count > 0:
                                for i in range(min(count, 20)):
                                    try:
                                        text = await elements.nth(i).inner_text()
                                        if (text and len(text.strip()) > 10 and 
                                            not text.strip().isdigit() and
                                            "å°æ™‚" not in text and "åˆ†é˜" not in text and
                                            not text.startswith("@")):
                                            content = text.strip()
                                            break
                                    except:
                                        continue
                                
                                if content:
                                    break
                        except:
                            continue
                except:
                    pass
                
                # æå–åœ–ç‰‡ï¼ˆæ²¿ç”¨æˆåŠŸçš„é‚è¼¯ï¼‰
                images = []
                try:
                    img_elements = page.locator('img')
                    img_count = await img_elements.count()
                    
                    for i in range(min(img_count, 50)):
                        try:
                            img_elem = img_elements.nth(i)
                            img_src = await img_elem.get_attribute("src")
                            
                            if not img_src or not ("fbcdn" in img_src or "cdninstagram" in img_src):
                                continue
                            
                            if ("rsrc.php" in img_src or "static.cdninstagram.com" in img_src):
                                continue
                            
                            try:
                                width = int(await img_elem.get_attribute("width") or 0)
                                height = int(await img_elem.get_attribute("height") or 0)
                                max_size = max(width, height)
                                
                                if max_size > 150 and img_src not in images:
                                    images.append(img_src)
                            except:
                                if ("t51.2885-15" in img_src or "scontent" in img_src) and img_src not in images:
                                    images.append(img_src)
                        except:
                            continue
                            
                    print(f"      âœ… æ‰¾åˆ° {len(images)} å€‹æœ‰æ•ˆåœ–ç‰‡")
                except Exception as e:
                    print(f"      âŒ åœ–ç‰‡æå–å¤±æ•—: {e}")
                
                # ===== æ–°å¢ï¼šæå–ç™¼æ–‡æ™‚é–“ =====
                created_at = await self.extract_time_from_dom(page)
                
                # ===== æ–°å¢ï¼šæå–ä¸»é¡Œæ¨™ç±¤ =====
                tags = await self.extract_tags_from_dom(page)
                
                await browser.close()
                
                return {
                    "username": username,
                    "content": content,
                    "images": images,
                    "videos": [],  # æš«æ™‚ä¿æŒåŸæœ‰çµæ§‹
                    "created_at": created_at.isoformat() if created_at else None,
                    "tags": tags
                }
            
            except Exception as e:
                print(f"   âŒ DOM è§£æå¤±æ•—: {e}")
                await browser.close()
                return None
    
    async def extract_complete_post_with_time_and_tags(self, post_url: str = TEST_POST_URL) -> Optional[Dict[str, Any]]:
        """æå–å®Œæ•´çš„è²¼æ–‡æ•¸æ“šï¼ˆåŒ…å«æ™‚é–“å’Œæ¨™ç±¤ï¼‰"""
        print(f"ğŸ¯ æå–å®Œæ•´è²¼æ–‡æ•¸æ“šï¼ˆåŒ…å«æ™‚é–“å’Œæ¨™ç±¤ï¼‰: {post_url}")
        
        # ç²å–å…§å®¹å’Œåª’é«”æ•¸æ“šï¼ˆç¾åœ¨åŒ…å«æ™‚é–“å’Œæ¨™ç±¤ï¼‰
        content_data = await self.get_enhanced_content_and_media_from_dom(post_url)
        if not content_data:
            print(f"   âš ï¸ ç„¡æ³•ç²å–å…§å®¹æ•¸æ“š")
            content_data = {
                "username": "", "content": "", "images": [], "videos": [],
                "created_at": None, "tags": []
            }
        else:
            print(f"   âœ… å…§å®¹æ•¸æ“š: @{content_data['username']}, {len(content_data['content'])}å­—ç¬¦")
            print(f"   âœ… åª’é«”: {len(content_data['images'])}åœ–ç‰‡, {len(content_data['videos'])}å½±ç‰‡")
            if content_data['created_at']:
                print(f"   âœ… ç™¼æ–‡æ™‚é–“: {content_data['created_at']}")
            if content_data['tags']:
                print(f"   âœ… æ¨™ç±¤: {content_data['tags']}")
        
        # åˆä½µæ•¸æ“š
        code = re.search(r'/post/([A-Za-z0-9_-]+)', post_url)
        code = code.group(1) if code else ""
        
        result = {
            "pk": "",  # å¦‚æœæœ‰è¨ˆæ•¸æŸ¥è©¢æœƒå¡«å…¥
            "code": code,
            "username": content_data["username"],
            "content": content_data["content"],
            "like_count": 0,  # å¦‚æœæœ‰è¨ˆæ•¸æŸ¥è©¢æœƒå¡«å…¥
            "comment_count": 0,
            "repost_count": 0, 
            "share_count": 0,
            "images": content_data["images"],
            "videos": content_data["videos"],
            "created_at": content_data["created_at"],  # æ–°å¢
            "tags": content_data["tags"],  # æ–°å¢
            "url": post_url,
            "extracted_at": datetime.now().isoformat(),
            "extraction_method": "enhanced_dom"
        }
        
        # é¡¯ç¤ºæœ€çµ‚çµæœ
        print(f"\nğŸ“‹ æœ€çµ‚çµæœ:")
        print(f"   ğŸ‘¤ ç”¨æˆ¶: @{result['username']}")
        print(f"   ğŸ“ å…§å®¹: {len(result['content'])} å­—ç¬¦")
        print(f"   ğŸ–¼ï¸ åœ–ç‰‡: {len(result['images'])} å€‹")
        print(f"   ğŸ¥ å½±ç‰‡: {len(result['videos'])} å€‹")
        print(f"   â° ç™¼æ–‡æ™‚é–“: {result['created_at']}")
        print(f"   ğŸ·ï¸ æ¨™ç±¤: {result['tags']}")
        
        if result['content']:
            print(f"   ğŸ“„ å…§å®¹é è¦½: {result['content'][:100]}...")
        
        # ä¿å­˜çµæœ
        result_file = Path(f"enhanced_extraction_result_{datetime.now().strftime('%H%M%S')}.json")
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, indent=2, ensure_ascii=False)
        print(f"   ğŸ“ å®Œæ•´çµæœå·²ä¿å­˜: {result_file}")
        
        return result

async def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸš€ å¢å¼·ç‰ˆæ··åˆå…§å®¹æå–å™¨ - æ·»åŠ æ™‚é–“å’Œæ¨™ç±¤")
    print("åŸºæ–¼ hybrid_content_extractor.py çš„æˆåŠŸæ¨¡å¼")
    print("=" * 50)
    
    auth_file = get_auth_file_path()
    if not auth_file.exists():
        print(f"âŒ èªè­‰æª”æ¡ˆ {auth_file} ä¸å­˜åœ¨ã€‚è«‹å…ˆåŸ·è¡Œ save_auth.pyã€‚")
        return
    
    extractor = EnhancedHybridExtractor()
    
    # å¯é¸ï¼šå˜—è©¦æ””æˆªè¨ˆæ•¸æŸ¥è©¢ï¼ˆå¦‚æœå¤±æ•—ä¸å½±éŸ¿ä¸»è¦åŠŸèƒ½ï¼‰
    print(f"\nğŸ“¡ ç¬¬ä¸€æ­¥ï¼šå˜—è©¦æ””æˆªè¨ˆæ•¸æŸ¥è©¢...")
    captured = await extractor.intercept_counts_query()
    
    if captured:
        print(f"   âœ… æˆåŠŸæ””æˆªè¨ˆæ•¸æŸ¥è©¢")
    else:
        print(f"   âš ï¸ æœªæ””æˆªåˆ°è¨ˆæ•¸æŸ¥è©¢ï¼Œç¹¼çºŒä½¿ç”¨ DOM è§£æ")
    
    # ç¬¬äºŒæ­¥ï¼šæå–å®Œæ•´æ•¸æ“šï¼ˆåŒ…å«æ™‚é–“å’Œæ¨™ç±¤ï¼‰
    print(f"\nğŸ¯ ç¬¬äºŒæ­¥ï¼šå¢å¼·DOMæå–...")
    result = await extractor.extract_complete_post_with_time_and_tags()
    
    if result:
        print(f"\nğŸ‰ å¢å¼·æå–æˆåŠŸï¼")
        print(f"ğŸ’¡ æ–°å¢åŠŸèƒ½:")
        print(f"   âœ… ç™¼æ–‡æ™‚é–“æå–: {result['created_at'] or 'æœªæ‰¾åˆ°'}")
        print(f"   âœ… ä¸»é¡Œæ¨™ç±¤æå–: {result['tags'] or 'æœªæ‰¾åˆ°'}")
        print(f"   âœ… åŸºæ–¼æˆåŠŸçš„ hybrid_content_extractor.py é‚è¼¯")
        print(f"   âœ… ä¸‰é‡æ™‚é–“æå–ç­–ç•¥ï¼ˆdatetime, title/aria-label, __NEXT_DATA__ï¼‰")
        print(f"   âœ… å¤šé‡æ¨™ç±¤æœç´¢ç­–ç•¥")
    else:
        print(f"\nğŸ˜ å¢å¼·æå–å¤±æ•—")

if __name__ == "__main__":
    asyncio.run(main())