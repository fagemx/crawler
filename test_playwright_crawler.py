#!/usr/bin/env python3
"""
ç¨ç«‹æ¸¬è©¦ Playwright Crawler æ ¸å¿ƒé‚è¼¯ (playwright_logic.py)

æ­¤è…³æœ¬å°ˆæ³¨æ–¼é©—è­‰çˆ¬èŸ²æ˜¯å¦èƒ½ï¼š
1. æ­£ç¢ºç™»å…¥ä¸¦å°èˆª
2. æ””æˆªä¸¦è§£æ GraphQL API å›æ‡‰
3. å¾è§£æçš„è³‡æ–™ä¸­å»ºç«‹æ ¼å¼æ­£ç¢ºçš„ PostMetrics ç‰©ä»¶ (ç‰¹åˆ¥æ˜¯ URL)
"""

import asyncio
import json
import logging
import sys
import os
from pathlib import Path
import re # Added for regex validation

# --- æ—¥èªŒè¨­å®š ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# --- è·¯å¾‘è¨­å®š ---
# ä¿®æ­£ project_root çš„è¨ˆç®—æ–¹å¼
# ç•¶è…³æœ¬åœ¨æ ¹ç›®éŒ„æ™‚ï¼Œos.path.dirname(__file__) å°±æ˜¯å°ˆæ¡ˆæ ¹ç›®éŒ„
project_root = os.path.abspath(os.path.dirname(__file__))
if 'pyproject.toml' not in os.listdir(project_root):
    # å¦‚æœç•¶å‰ç›®éŒ„æ²’æœ‰ pyproject.tomlï¼Œå¯èƒ½æ˜¯åœ¨å­ç›®éŒ„åŸ·è¡Œï¼Œå˜—è©¦å¾€ä¸Šä¸€å±¤
    project_root = os.path.abspath(os.path.join(project_root, '..'))

if project_root not in sys.path:
    sys.path.insert(0, project_root)

try:
    from agents.playwright_crawler.playwright_logic import PlaywrightLogic
    from common.models import PostMetricsBatch
    from datetime import datetime
except ModuleNotFoundError as e:
    logging.error(f"âŒ æ¨¡çµ„å°å…¥å¤±æ•—: {e}")
    logging.error("è«‹ç¢ºèªæ‚¨æ˜¯åœ¨å°ˆæ¡ˆæ ¹ç›®éŒ„ä¸‹åŸ·è¡Œæ­¤è…³æœ¬ï¼")
    sys.exit(1)

# --- æ¸¬è©¦åƒæ•¸ ---
TARGET_USERNAME = "natgeo"
MAX_POSTS_TO_CRAWL = 5 # <--- æš«æ™‚æ”¹å›è¼ƒå°çš„æ•¸é‡ä»¥é™ä½ API å£“åŠ›
AUTH_FILE_PATH = Path(project_root) / "agents" / "playwright_crawler" / "auth.json"


def save_results_to_json(batch: PostMetricsBatch, sorted_posts: list) -> str:
    """
    å°‡æ¸¬è©¦çµæœä¿å­˜ç‚º JSON æ–‡ä»¶ï¼ˆä»¿ç…§ pipeline_service.py çš„æ ¼å¼ï¼‰
    """
    try:
        # å‰µå»ºè¼¸å‡ºç›®éŒ„
        output_dir = Path(project_root) / "test_results"
        output_dir.mkdir(exist_ok=True)
        
        # ç”Ÿæˆæª”æ¡ˆåï¼ˆåŒ…å«æ™‚é–“æˆ³å’Œç”¨æˆ¶åï¼‰
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"test_crawler_results_{batch.username}_{timestamp}.json"
        file_path = output_dir / filename
        
        # æº–å‚™ JSON æ•¸æ“šï¼ˆèˆ‡ pipeline_service.py ç›¸åŒçš„æ ¼å¼ï¼‰
        result_data = {
            "batch_info": {
                "batch_id": batch.batch_id,
                "username": batch.username,
                "total_posts": len(batch.posts),
                "processing_stage": batch.processing_stage,
                "timestamp": timestamp,
                "test_mode": True  # æ¨™è¨˜é€™æ˜¯æ¸¬è©¦çµæœ
            },
            "posts": []
        }
        
        # æ·»åŠ æ¯å€‹è²¼æ–‡çš„è©³ç´°è³‡æ–™ï¼ˆä½¿ç”¨æ’åºå¾Œçš„é †åºï¼‰
        for rank, (post, score) in enumerate(sorted_posts, 1):
            post_data = {
                "rank": rank,  # æ·»åŠ æ’å
                "url": post.url,
                "post_id": post.post_id,
                "username": post.username,
                "metrics": {
                    "views_count": post.views_count,
                    "likes_count": post.likes_count,
                    "comments_count": post.comments_count,
                    "reposts_count": post.reposts_count,
                    "shares_count": post.shares_count,
                    "calculated_score": score
                },
                "content": {
                    "text": post.content[:200] + "..." if post.content and len(post.content) > 200 else post.content,
                    "images": post.images,  # åŒ…å«åœ–ç‰‡ URL
                    "videos": post.videos   # åŒ…å«å½±ç‰‡ URL
                },
                "metadata": {
                    "source": post.source,
                    "processing_stage": post.processing_stage,
                    "is_complete": post.is_complete,
                    "last_updated": post.last_updated.isoformat() if post.last_updated else None,
                    "created_at": post.created_at.isoformat() if post.created_at else None,
                    "views_fetched_at": post.views_fetched_at.isoformat() if post.views_fetched_at else None
                }
            }
            result_data["posts"].append(post_data)
        
        # å¯«å…¥ JSON æª”æ¡ˆ
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(result_data, f, ensure_ascii=False, indent=2)
        
        return str(file_path.absolute())
        
    except Exception as e:
        logging.error(f"âŒ ä¿å­˜ JSON æ–‡ä»¶å¤±æ•—: {e}")
        return "JSON save failed"


async def run_crawler_test():
    """åŸ·è¡Œçˆ¬èŸ²æ¸¬è©¦"""
    print("ğŸ§ª === Playwright Crawler æ ¸å¿ƒé‚è¼¯æ¸¬è©¦ ===")

    # 1. æª¢æŸ¥èªè­‰æª”æ¡ˆæ˜¯å¦å­˜åœ¨
    if not AUTH_FILE_PATH.exists():
        print(f"âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°èªè­‰æª”æ¡ˆ {AUTH_FILE_PATH}")
        print("è«‹å…ˆåŸ·è¡Œ `python agents/playwright_crawler/save_auth.py` ä¾†ç”¢ç”Ÿèªè­‰æª”æ¡ˆã€‚")
        return
        
    try:
        with open(AUTH_FILE_PATH, 'r', encoding='utf-8') as f:
            auth_json_content = json.load(f)
        print(f"âœ… æˆåŠŸè®€å–èªè­‰æª”æ¡ˆ: {AUTH_FILE_PATH}")
    except Exception as e:
        print(f"âŒ è®€å–æˆ–è§£æèªè­‰æª”æ¡ˆå¤±æ•—: {e}")
        return

    # 2. åˆå§‹åŒ– PlaywrightLogic
    crawler = PlaywrightLogic()
    print("âœ… PlaywrightLogic åˆå§‹åŒ–å®Œæˆã€‚")

    # 3. åŸ·è¡Œçˆ¬å–
    print(f"ğŸš€ é–‹å§‹çˆ¬å–ä½¿ç”¨è€… '{TARGET_USERNAME}' çš„æœ€è¿‘ {MAX_POSTS_TO_CRAWL} ç¯‡è²¼æ–‡...")
    
    start_time = asyncio.get_event_loop().time()
    
    try:
        result_batch: PostMetricsBatch = await crawler.fetch_posts(
            username=TARGET_USERNAME,
            max_posts=MAX_POSTS_TO_CRAWL,
            auth_json_content=auth_json_content,
            task_id="test_crawler_logic"
        )
        
        end_time = asyncio.get_event_loop().time()
        duration = end_time - start_time
        
        print("\nâœ… === çˆ¬å–å®Œæˆ ===")
        print(f"â±ï¸  ç¸½è€—æ™‚: {duration:.2f} ç§’")

        # 4. é©—è­‰çµæœ
        if not result_batch or not result_batch.posts:
            print("âŒ çµæœç‚ºç©ºï¼Œæ²’æœ‰çˆ¬å–åˆ°ä»»ä½•è²¼æ–‡ã€‚")
            return
            
        print(f"ğŸ“Š å…±çˆ¬å–åˆ° {len(result_batch.posts)}/{result_batch.total_count} ç¯‡è²¼æ–‡ã€‚")
        
        # ğŸ†• æ–°å¢ï¼šè¨ˆç®—åˆ†æ•¸ä¸¦æ’åº
        print("\nğŸ† === åˆ†æ•¸è¨ˆç®—èˆ‡æ’åº ===")
        
        # å‰µå»ºåŒ…å«åˆ†æ•¸çš„å…ƒçµ„åˆ—è¡¨ï¼Œé¿å…ä¿®æ”¹ Pydantic ç‰©ä»¶
        posts_with_scores = [(post, post.calculate_score()) for post in result_batch.posts]
        
        # æŒ‰åˆ†æ•¸é™åºæ’åˆ—ï¼ˆèˆ‡ pipeline_service.py ç›¸åŒçš„æ’åºé‚è¼¯ï¼‰
        sorted_posts_with_scores = sorted(posts_with_scores, key=lambda x: x[1], reverse=True)
        
        print("ğŸ“Š æ’åºçµæœï¼ˆæŒ‰åˆ†æ•¸é™åºï¼‰ï¼š")
        for i, (post, score) in enumerate(sorted_posts_with_scores[:5], 1):  # é¡¯ç¤ºå‰5å
            print(f"  {i}. åˆ†æ•¸: {score:.1f} | è§€çœ‹æ•¸: {post.views_count} | URL: {post.url.split('/')[-1]}")
        
        if len(sorted_posts_with_scores) > 5:
            print(f"  ... é‚„æœ‰ {len(sorted_posts_with_scores) - 5} ç¯‡è²¼æ–‡")
        
        # ğŸ†• æ–°å¢ï¼šä¿å­˜ JSON æ–‡ä»¶ï¼ˆä»¿ç…§ pipeline_service.pyï¼‰
        json_file_path = save_results_to_json(result_batch, sorted_posts_with_scores)
        print(f"ğŸ“„ çµæœå·²ä¿å­˜è‡³: {json_file_path}")
        
        print("\nğŸ“„ === è‡ªå‹•åŒ–æ•¸æ“šä¸€è‡´æ€§é©—è­‰ ===")
        validation_errors = 0
        for i, post in enumerate(result_batch.posts, 1):
            
            # å¾ URL ä¸­ç”¨æ­£å‰‡è¡¨é”å¼æå– code
            match = re.search(r"/post/([^/]+)", post.url)
            url_code = match.group(1) if match else None
            
            # post.post_id æ˜¯å¾ GraphQL çš„ 'pk' æˆ– 'id' ä¾†çš„
            # url_code æ˜¯å¾ GraphQL çš„ 'code' çµ„åˆæˆçš„ URL ä¸­æå–çš„
            
            print(f"--- è²¼æ–‡ {i}: {post.url.split('/')[-1]} ---")
            print(f"  å¾ API æå–çš„ Post ID: {post.post_id}")
            print(f"  å¾ URL æå–çš„ Code:    {url_code}")
            
            if url_code != post.url.split('/')[-1]: # ç°¡å–®é©—è­‰ä¸€ä¸‹æ­£å‰‡
                 print("  URL Code vs URL: âŒ æ­£å‰‡æå–èˆ‡åˆ†å‰²ä¸ç¬¦ï¼")
                 validation_errors += 1
            elif post.post_id: # ç¢ºä¿ post_id å­˜åœ¨
                print("  æ•¸æ“šä¸€è‡´æ€§: âœ… Post ID èˆ‡ URL Code åŒ¹é… (æˆ–ç„¡éœ€åŒ¹é…)")
            else:
                print("  æ•¸æ“šä¸€è‡´æ€§: âŒ ç¼ºå°‘ Post IDï¼")
                validation_errors += 1

            # --- æ–°å¢ï¼šåª’é«”æ¬„ä½é©—è­‰ ---
            print(f"  Images: {post.images}")
            print(f"  Videos: {post.videos}")
            if post.images or post.videos:
                print("  åª’é«”æ¬„ä½: âœ… æˆåŠŸæŠ“å–åˆ°åª’é«” URL")
            else:
                # é€™ä¸ä¸€å®šæ˜¯éŒ¯èª¤ï¼Œå¯èƒ½è²¼æ–‡æœ¬ä¾†å°±æ²’æœ‰åª’é«”
                print("  åª’é«”æ¬„ä½: âšªï¸ æœªç™¼ç¾åª’é«” URL (å¯èƒ½è²¼æ–‡æœ¬ä¾†å°±æ²’æœ‰)")

        print("\n" + "="*30)
        if validation_errors == 0:
            print("âœ…âœ…âœ… **é©—è­‰é€šé**ï¼šæ‰€æœ‰çˆ¬å–çš„è²¼æ–‡æ•¸æ“šä¸€è‡´æ€§è‰¯å¥½ï¼")
        else:
            print(f"âŒâŒâŒ **é©—è­‰å¤±æ•—**ï¼šç™¼ç¾ {validation_errors} å€‹æ•¸æ“šä¸ä¸€è‡´çš„è²¼æ–‡ï¼")
        print("="*30)

    except Exception as e:
        logging.error(f"âŒ æ¸¬è©¦éç¨‹ä¸­ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}", exc_info=True)


if __name__ == "__main__":
    asyncio.run(run_crawler_test()) 