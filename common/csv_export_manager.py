"""
CSVå°å‡ºç®¡ç†å™¨
æ”¯æŒå¤šç¨®æ•¸æ“šæºå’Œå°å‡ºæ¨¡å¼çš„éˆæ´»CSVå°å‡ºç³»çµ±
"""

import csv
import json
import asyncio
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Union
from pathlib import Path
import pandas as pd

from .db_client import DatabaseClient
from .incremental_crawl_manager import IncrementalCrawlManager

class CSVExportManager:
    """CSVå°å‡ºç®¡ç†å™¨"""
    
    def __init__(self):
        self.db = DatabaseClient()
        self.crawl_manager = IncrementalCrawlManager()
    
    def export_current_session(self, json_file_path: str, output_path: str = None, sort_by: str = 'views') -> str:
        """
        å°å‡ºç•¶æ¬¡çˆ¬å–çµæœåˆ°CSV
        
        Args:
            json_file_path: JSONçµæœæ–‡ä»¶è·¯å¾„
            output_path: è¼¸å‡ºCSVæ–‡ä»¶è·¯å¾„ï¼ˆå¯é¸ï¼‰
            sort_by: æ’åºæ¬„ä½ ('views', 'likes', 'comments', 'post_id', 'none')
        
        Returns:
            ç”Ÿæˆçš„CSVæ–‡ä»¶è·¯å¾„
        """
        try:
            # è®€å–JSONçµæœ
            with open(json_file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            results = data.get('results', [])
            if not results:
                raise ValueError("JSONæ–‡ä»¶ä¸­æ²’æœ‰æ‰¾åˆ°çµæœæ•¸æ“š")
            
            # ç”Ÿæˆè¼¸å‡ºæ–‡ä»¶å
            if not output_path:
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                username = data.get('target_username', 'unknown')
                output_path = f"export_current_{username}_{timestamp}.csv"
            
            # æº–å‚™CSVæ•¸æ“š
            csv_data = []
            for item in results:
                csv_row = {
                    'è²¼æ–‡ID': item.get('post_id', ''),
                    'URL': item.get('url', ''),
                    'è§€çœ‹æ•¸': item.get('views', ''),
                    'æŒ‰è®šæ•¸': item.get('likes', ''),
                    'ç•™è¨€æ•¸': item.get('comments', ''),
                    'è½‰ç™¼æ•¸': item.get('reposts', ''),
                    'åˆ†äº«æ•¸': item.get('shares', ''),
                    'å…§å®¹': item.get('content', ''),
                    'æ•¸æ“šä¾†æº': item.get('source', ''),
                    'æˆåŠŸç‹€æ…‹': 'æˆåŠŸ' if item.get('success', False) else 'å¤±æ•—',
                    'å…§å®¹é•·åº¦': item.get('content_length', 0),
                    'æå–æ™‚é–“': item.get('extracted_at', '')
                }
                csv_data.append(csv_row)
            
            # æ’åºè™•ç†
            if sort_by and sort_by != 'none' and csv_data:
                sort_mapping = {
                    'views': 'è§€çœ‹æ•¸',
                    'likes': 'æŒ‰è®šæ•¸', 
                    'comments': 'ç•™è¨€æ•¸',
                    'reposts': 'è½‰ç™¼æ•¸',
                    'shares': 'åˆ†äº«æ•¸',
                    'post_id': 'è²¼æ–‡ID'
                }
                
                if sort_by in sort_mapping:
                    sort_column = sort_mapping[sort_by]
                    
                    # æ•¸å­—æ¬„ä½éœ€è¦ç‰¹æ®Šè™•ç†ï¼ˆç©ºå€¼è½‰ç‚º0ï¼‰
                    if sort_by in ['views', 'likes', 'comments', 'reposts', 'shares']:
                        def sort_key(row):
                            value = row[sort_column]
                            try:
                                # è™•ç†æ•¸å­—æ ¼å¼ï¼ˆå¦‚ "1.2K" -> 1200ï¼‰
                                if isinstance(value, str):
                                    if value.upper().endswith('K'):
                                        return float(value[:-1]) * 1000
                                    elif value.upper().endswith('M'):
                                        return float(value[:-1]) * 1000000
                                    elif value == '' or value == 'N/A':
                                        return 0
                                    else:
                                        return float(value)
                                elif isinstance(value, (int, float)):
                                    return value
                                else:
                                    return 0
                            except:
                                return 0
                        
                        csv_data.sort(key=sort_key, reverse=True)  # æ•¸å­—é™åº
                    else:
                        csv_data.sort(key=lambda x: x[sort_column] or '', reverse=False)  # æ–‡å­—å‡åº
                    
                    print(f"ğŸ“Š å·²æŒ‰ {sort_column} æ’åº")
                else:
                    print(f"âš ï¸ ä¸æ”¯æŒçš„æ’åºæ¬„ä½: {sort_by}")
            else:
                print("ğŸ“‹ ä½¿ç”¨åŸå§‹é †åºï¼ˆæœªæ’åºï¼‰")
            
            # å¯«å…¥CSV
            with open(output_path, 'w', newline='', encoding='utf-8-sig') as f:
                if csv_data:
                    writer = csv.DictWriter(f, fieldnames=csv_data[0].keys())
                    writer.writeheader()
                    writer.writerows(csv_data)
            
            print(f"âœ… ç•¶æ¬¡çµæœå·²å°å‡ºåˆ°: {output_path}")
            return output_path
            
        except Exception as e:
            print(f"âŒ å°å‡ºç•¶æ¬¡çµæœå¤±æ•—: {e}")
            raise
    
    async def export_database_history(self, username: str, output_path: str = None, 
                                     days_back: int = None, limit: int = None) -> str:
        """
        å°å‡ºè³‡æ–™åº«æ­·å²æ•¸æ“šåˆ°CSV
        
        Args:
            username: å¸³è™Ÿåç¨±
            output_path: è¼¸å‡ºCSVæ–‡ä»¶è·¯å¾„ï¼ˆå¯é¸ï¼‰
            days_back: å›æº¯å¤©æ•¸ï¼ˆå¯é¸ï¼‰
            limit: é™åˆ¶è¨˜éŒ„æ•¸ï¼ˆå¯é¸ï¼‰
        
        Returns:
            ç”Ÿæˆçš„CSVæ–‡ä»¶è·¯å¾„
        """
        try:
            # ç”Ÿæˆè¼¸å‡ºæ–‡ä»¶å
            if not output_path:
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                suffix = f"_{days_back}days" if days_back else f"_top{limit}" if limit else "_all"
                output_path = f"export_history_{username}{suffix}_{timestamp}.csv"
            
            # æ§‹å»ºæŸ¥è©¢æ¢ä»¶
            where_conditions = ["username = $1"]
            params = [username]
            
            if days_back:
                where_conditions.append("fetched_at >= $2")
                params.append(datetime.now() - timedelta(days=days_back))
            
            where_clause = " AND ".join(where_conditions)
            limit_clause = f" LIMIT {limit}" if limit else ""
            
            # æŸ¥è©¢è³‡æ–™åº«
            query = f"""
                SELECT 
                    post_id,
                    url,
                    content,
                    likes_count,
                    comments_count,
                    reposts_count,
                    shares_count,
                    views_count,
                    source,
                    processing_stage,
                    is_complete,
                    created_at,
                    fetched_at,
                    views_fetched_at,
                    calculated_score
                FROM post_metrics_sql 
                WHERE {where_clause}
                ORDER BY fetched_at DESC
                {limit_clause}
            """
            
            results = await self.db.fetch_all(query, *params)
            
            if not results:
                raise ValueError(f"æœªæ‰¾åˆ°å¸³è™Ÿ @{username} çš„æ­·å²æ•¸æ“š")
            
            # æº–å‚™CSVæ•¸æ“š
            csv_data = []
            for row in results:
                csv_row = {
                    'è²¼æ–‡ID': row['post_id'],
                    'URL': row['url'],
                    'è§€çœ‹æ•¸': row['views_count'] or '',
                    'æŒ‰è®šæ•¸': row['likes_count'] or '',
                    'ç•™è¨€æ•¸': row['comments_count'] or '',
                    'è½‰ç™¼æ•¸': row['reposts_count'] or '',
                    'åˆ†äº«æ•¸': row['shares_count'] or '',
                    'å…§å®¹': row['content'] or '',
                    'æ•¸æ“šä¾†æº': row['source'] or '',
                    'è™•ç†éšæ®µ': row['processing_stage'] or '',
                    'æ•¸æ“šå®Œæ•´': 'æ˜¯' if row['is_complete'] else 'å¦',
                    'è¨ˆç®—åˆ†æ•¸': row['calculated_score'] or '',
                    'å‰µå»ºæ™‚é–“': row['created_at'].strftime('%Y-%m-%d %H:%M:%S') if row['created_at'] else '',
                    'çˆ¬å–æ™‚é–“': row['fetched_at'].strftime('%Y-%m-%d %H:%M:%S') if row['fetched_at'] else '',
                    'è§€çœ‹æ•¸æ›´æ–°æ™‚é–“': row['views_fetched_at'].strftime('%Y-%m-%d %H:%M:%S') if row['views_fetched_at'] else ''
                }
                csv_data.append(csv_row)
            
            # å¯«å…¥CSV
            with open(output_path, 'w', newline='', encoding='utf-8-sig') as f:
                if csv_data:
                    writer = csv.DictWriter(f, fieldnames=csv_data[0].keys())
                    writer.writeheader()
                    writer.writerows(csv_data)
            
            print(f"âœ… æ­·å²æ•¸æ“šå·²å°å‡ºåˆ°: {output_path} ({len(csv_data)} æ¢è¨˜éŒ„)")
            return output_path
            
        except Exception as e:
            print(f"âŒ å°å‡ºæ­·å²æ•¸æ“šå¤±æ•—: {e}")
            raise
    
    async def export_combined_analysis(self, username: str, output_path: str = None) -> str:
        """
        å°å‡ºçµ„åˆåˆ†ææ•¸æ“šåˆ°CSVï¼ˆçµ±è¨ˆæ‘˜è¦ï¼‰
        
        Args:
            username: å¸³è™Ÿåç¨±
            output_path: è¼¸å‡ºCSVæ–‡ä»¶è·¯å¾„ï¼ˆå¯é¸ï¼‰
        
        Returns:
            ç”Ÿæˆçš„CSVæ–‡ä»¶è·¯å¾„
        """
        try:
            # ç”Ÿæˆè¼¸å‡ºæ–‡ä»¶å
            if not output_path:
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                output_path = f"export_analysis_{username}_{timestamp}.csv"
            
            # ç²å–çµ±è¨ˆæ•¸æ“š
            stats_query = """
                SELECT 
                    DATE(fetched_at) as date,
                    COUNT(*) as posts_count,
                    COUNT(CASE WHEN views_count > 0 THEN 1 END) as posts_with_views,
                    COUNT(CASE WHEN content IS NOT NULL AND content != '' THEN 1 END) as posts_with_content,
                    AVG(views_count) as avg_views,
                    MAX(views_count) as max_views,
                    AVG(likes_count) as avg_likes,
                    MAX(likes_count) as max_likes,
                    AVG(comments_count) as avg_comments,
                    MAX(comments_count) as max_comments,
                    COUNT(DISTINCT source) as source_types
                FROM post_metrics_sql 
                WHERE username = $1
                GROUP BY DATE(fetched_at)
                ORDER BY DATE(fetched_at) DESC
            """
            
            results = await self.db.fetch_all(stats_query, username)
            
            if not results:
                raise ValueError(f"æœªæ‰¾åˆ°å¸³è™Ÿ @{username} çš„çµ±è¨ˆæ•¸æ“š")
            
            # æº–å‚™CSVæ•¸æ“š
            csv_data = []
            for row in results:
                csv_row = {
                    'æ—¥æœŸ': row['date'].strftime('%Y-%m-%d'),
                    'çˆ¬å–è²¼æ–‡æ•¸': row['posts_count'],
                    'æœ‰è§€çœ‹æ•¸çš„è²¼æ–‡': row['posts_with_views'],
                    'æœ‰å…§å®¹çš„è²¼æ–‡': row['posts_with_content'],
                    'å¹³å‡è§€çœ‹æ•¸': round(row['avg_views'], 2) if row['avg_views'] else 0,
                    'æœ€é«˜è§€çœ‹æ•¸': row['max_views'] or 0,
                    'å¹³å‡æŒ‰è®šæ•¸': round(row['avg_likes'], 2) if row['avg_likes'] else 0,
                    'æœ€é«˜æŒ‰è®šæ•¸': row['max_likes'] or 0,
                    'å¹³å‡ç•™è¨€æ•¸': round(row['avg_comments'], 2) if row['avg_comments'] else 0,
                    'æœ€é«˜ç•™è¨€æ•¸': row['max_comments'] or 0,
                    'æ•¸æ“šä¾†æºç¨®é¡': row['source_types'],
                    'è§€çœ‹æ•¸æˆåŠŸç‡': f"{(row['posts_with_views'] / row['posts_count'] * 100):.1f}%" if row['posts_count'] > 0 else "0%",
                    'å…§å®¹æˆåŠŸç‡': f"{(row['posts_with_content'] / row['posts_count'] * 100):.1f}%" if row['posts_count'] > 0 else "0%"
                }
                csv_data.append(csv_row)
            
            # å¯«å…¥CSV
            with open(output_path, 'w', newline='', encoding='utf-8-sig') as f:
                if csv_data:
                    writer = csv.DictWriter(f, fieldnames=csv_data[0].keys())
                    writer.writeheader()
                    writer.writerows(csv_data)
            
            print(f"âœ… åˆ†ææ•¸æ“šå·²å°å‡ºåˆ°: {output_path} ({len(csv_data)} å¤©æ•¸æ“š)")
            return output_path
            
        except Exception as e:
            print(f"âŒ å°å‡ºåˆ†ææ•¸æ“šå¤±æ•—: {e}")
            raise
    
    def export_comparison_report(self, json_files: List[str], output_path: str = None) -> str:
        """
        å°å‡ºå¤šæ¬¡çˆ¬å–çš„å°æ¯”å ±å‘Š
        
        Args:
            json_files: å¤šå€‹JSONçµæœæ–‡ä»¶è·¯å¾„
            output_path: è¼¸å‡ºCSVæ–‡ä»¶è·¯å¾„ï¼ˆå¯é¸ï¼‰
        
        Returns:
            ç”Ÿæˆçš„CSVæ–‡ä»¶è·¯å¾„
        """
        try:
            # ç”Ÿæˆè¼¸å‡ºæ–‡ä»¶å
            if not output_path:
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                output_path = f"export_comparison_{timestamp}.csv"
            
            csv_data = []
            
            for i, json_file in enumerate(json_files):
                try:
                    with open(json_file, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    
                    # æå–æ‘˜è¦ä¿¡æ¯
                    csv_row = {
                        'æ–‡ä»¶å': Path(json_file).name,
                        'é †åº': i + 1,
                        'ç›®æ¨™å¸³è™Ÿ': data.get('target_username', ''),
                        'çˆ¬å–æ™‚é–“': data.get('timestamp', ''),
                        'ç¸½è™•ç†æ•¸': data.get('total_processed', 0),
                        'APIæˆåŠŸæ•¸': data.get('api_success_count', 0),
                        'æœ¬åœ°æˆåŠŸæ•¸': data.get('local_success_count', 0),
                        'æ•´é«”æˆåŠŸç‡': f"{data.get('overall_success_rate', 0):.1f}%",
                        'è§€çœ‹æ•¸æå–ç‡': f"{data.get('views_extraction_rate', 0):.1f}%",
                        'å…§å®¹æå–ç‡': f"{data.get('content_extraction_rate', 0):.1f}%",
                        'URLæ”¶é›†æ™‚é–“': f"{data.get('timing', {}).get('url_collection_time', 0):.1f}s",
                        'å…§å®¹æå–æ™‚é–“': f"{data.get('timing', {}).get('content_extraction_time', 0):.1f}s",
                        'ç¸½è€—æ™‚': f"{data.get('timing', {}).get('total_time', 0):.1f}s",
                        'æ•´é«”é€Ÿåº¦': f"{data.get('timing', {}).get('overall_speed', 0):.2f} ç¯‡/ç§’"
                    }
                    csv_data.append(csv_row)
                    
                except Exception as e:
                    print(f"âš ï¸ è™•ç†æ–‡ä»¶ {json_file} æ™‚å‡ºéŒ¯: {e}")
                    continue
            
            if not csv_data:
                raise ValueError("æ²’æœ‰æˆåŠŸè™•ç†ä»»ä½•JSONæ–‡ä»¶")
            
            # å¯«å…¥CSV
            with open(output_path, 'w', newline='', encoding='utf-8-sig') as f:
                writer = csv.DictWriter(f, fieldnames=csv_data[0].keys())
                writer.writeheader()
                writer.writerows(csv_data)
            
            print(f"âœ… å°æ¯”å ±å‘Šå·²å°å‡ºåˆ°: {output_path} ({len(csv_data)} æ¬¡çˆ¬å–)")
            return output_path
            
        except Exception as e:
            print(f"âŒ å°å‡ºå°æ¯”å ±å‘Šå¤±æ•—: {e}")
            raise