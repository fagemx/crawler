#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ­£å¼ç‰ˆæœ¬çš„è¼ªè¿´ç­–ç•¥è®€å–å™¨
å¾ test_reader_rotation.py é·ç§»è€Œä¾†ï¼ŒåŒ…å«ä¿®æ­£å¾Œçš„å…§å®¹æå–é‚è¼¯
"""

import requests
import re
import time
import random
import json
from typing import List, Dict, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed

class RotationPipelineReader:
    """
    è¼ªè¿´ç­–ç•¥è®€å–å™¨ - 10å€‹API â†’ 20å€‹æœ¬åœ° â†’ è¼ªè¿´
    åŒ…å«ä¿®æ­£å¾Œçš„æ™ºèƒ½å…§å®¹æå–é‚è¼¯
    """
    
    def __init__(self):
        self.api_batch_size = 10
        self.local_batch_size = 20
        self.local_reader_url = "http://localhost:8880"
        
        # æœ€ä½³åŒ–çš„æœ¬åœ°Headers
        self.local_headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'x-return-format': 'markdown',
            'x-wait-for-selector': 'body',
            'x-timeout': '60',
        }
        
        # çµ±è¨ˆè¨˜éŒ„
        self.batch_stats = {}
    
    def normalize_content(self, content: str) -> str:
        """æ­£è¦åŒ–å…§å®¹ - è™•ç†NBSPç­‰ç‰¹æ®Šå­—ç¬¦"""
        if not content:
            return content
        
        # è™•ç†NBSP (Non-breaking space) - é€™æ˜¯é—œéµä¿®æ­£
        content = content.replace('\u00a0', ' ')  # U+00A0 â†’ æ™®é€šç©ºæ ¼
        content = content.replace('\xa0', ' ')    # \xa0 â†’ æ™®é€šç©ºæ ¼
        
        # æ¨™æº–åŒ–è¡ŒçµæŸç¬¦
        content = content.replace('\r\n', '\n').replace('\r', '\n')
        
        # ç§»é™¤å¤šé¤˜ç©ºç™½ä½†ä¿ç•™çµæ§‹
        lines = content.split('\n')
        normalized_lines = [line.rstrip() for line in lines]
        
        return '\n'.join(normalized_lines)
    
    def extract_views_count(self, content: str, post_id: str) -> Optional[str]:
        """æå–è§€çœ‹æ•¸ - ä½¿ç”¨åŠ å¼·çš„æ­£å‰‡è¡¨é”å¼æ¨¡å¼"""
        normalized_content = self.normalize_content(content)
        
        # å¢å¼·çš„è§€çœ‹æ•¸æ¨¡å¼ - é‡å°NBSPå•é¡Œ
        view_patterns = [
            r'(\d+(?:\.\d+)?[KMB]?)\s*views',  # æ¨™æº–æ ¼å¼
            r'(\d+(?:\.\d+)?[KMB]?)\s*è§€çœ‹',   # ä¸­æ–‡
            r'Thread\s*=+\s*(\d+(?:\.\d+)?[KMB]?)\s*views',  # Threadæ¨™é¡Œæ ¼å¼
            r'views\s*(\d+(?:\.\d+)?[KMB]?)',  # å€’åº
        ]
        
        for i, pattern in enumerate(view_patterns):
            matches = re.findall(pattern, normalized_content, re.IGNORECASE)
            if matches:
                return matches[0]
        
        return None
    
    def extract_post_content(self, content: str) -> Optional[str]:
        """æ™ºèƒ½æå–ä¸»è²¼æ–‡å…§å®¹ - å€åˆ†ä¸»è²¼æ–‡å’Œå›è¦† (ä¿®æ­£ç‰ˆæœ¬)"""
        lines = content.split('\n')
        
        # ç­–ç•¥1: æŸ¥æ‰¾ä¸»è²¼æ–‡ï¼ˆç¬¬ä¸€å€‹å‡ºç¾çš„å¯¦è³ªå…§å®¹ï¼‰
        main_post_content = self._extract_main_post_from_structure(lines)
        if main_post_content:
            return main_post_content
        
        # ç­–ç•¥2: å›åˆ°åŸå§‹æ–¹æ³•ä½œç‚ºå‚™é¸
        return self._extract_content_fallback(lines)
    
    def _extract_main_post_from_structure(self, lines: List[str]) -> Optional[str]:
        """å¾çµæ§‹åŒ–å…§å®¹ä¸­æå–ä¸»è²¼æ–‡ - å„ªå…ˆæå–ç•¶å‰é é¢çš„ä¸»è¦å…§å®¹"""
        main_content_candidates = []
        
        # ç­–ç•¥1: å¦‚æœç¬¬ä¸€è¡Œå°±æ˜¯å›è¦†å…§å®¹ï¼Œå„ªå…ˆä½¿ç”¨å®ƒ
        if lines and lines[0].strip().startswith('>>>'):
            reply_content = lines[0].strip()
            if len(reply_content) > 10:  # ç¢ºä¿æœ‰å¯¦è³ªå…§å®¹
                return reply_content
        
        for i, line in enumerate(lines):
            stripped = line.strip()
            
            # è·³éæ˜é¡¯çš„å›è¦†æ¨™è­˜ï¼ˆä½†å¦‚æœæ˜¯ç¬¬ä¸€è¡Œå·²ç¶“è™•ç†éäº†ï¼‰
            if i > 0 and (stripped.startswith('>>>') or stripped.startswith('å›è¦†') or stripped.startswith('Â·Author')):
                continue
            
            # å°‹æ‰¾ä¸»è²¼æ–‡å…§å®¹çš„æ¨¡å¼
            if (stripped and 
                not stripped.startswith('[') and  # è·³éé€£çµ
                not stripped.startswith('![') and  # è·³éåœ–ç‰‡
                not stripped.startswith('http') and  # è·³éURL
                not stripped.startswith('Log in') and  # è·³éç™»å…¥æç¤º
                not stripped.startswith('Thread') and  # è·³éThreadæ¨™é¡Œ
                not stripped.startswith('gvmonthly') and  # è·³éç”¨æˆ¶å
                not stripped.isdigit() and  # è·³éç´”æ•¸å­—
                not re.match(r'^\d+[dhm]$', stripped) and  # è·³éæ™‚é–“æ ¼å¼
                not stripped in ['Translate', 'views'] and  # è·³éç‰¹æ®Šè©
                len(stripped) > 8):  # å…§å®¹è¦æœ‰ä¸€å®šé•·åº¦
                
                # æª¢æŸ¥é€™æ˜¯å¦å¯èƒ½æ˜¯ä¸»è²¼æ–‡å…§å®¹
                if self._is_likely_main_post_content(stripped, lines, i):
                    main_content_candidates.append(stripped)
        
        # è¿”å›ç¬¬ä¸€å€‹åˆç†çš„ä¸»è²¼æ–‡å€™é¸
        if main_content_candidates:
            return main_content_candidates[0]
        
        return None
    
    def _is_likely_main_post_content(self, content: str, lines: List[str], index: int) -> bool:
        """åˆ¤æ–·å…§å®¹æ˜¯å¦å¯èƒ½æ˜¯ä¸»è²¼æ–‡"""
        # æª¢æŸ¥å¾ŒçºŒæ˜¯å¦æœ‰ "Translate" æ¨™è­˜ï¼ˆä¸»è²¼æ–‡çš„å…¸å‹çµæ§‹ï¼‰
        has_translate = False
        for j in range(index + 1, min(index + 3, len(lines))):
            if 'Translate' in lines[j]:
                has_translate = True
                break
        
        # æª¢æŸ¥æ˜¯å¦åŒ…å«å¸¸è¦‹çš„ä¸»è²¼æ–‡ç‰¹å¾µ
        has_content_features = (
            len(content) > 15 and  # æœ‰ä¸€å®šé•·åº¦
            not content.startswith('>>>') and  # ä¸æ˜¯å›è¦†
            not content.startswith('Â·') and  # ä¸æ˜¯å…ƒæ•¸æ“š
            not content.startswith('[') and  # ä¸æ˜¯é€£çµ
            ('!' in content or '?' in content or 'ã€‚' in content or 'ï¼Œ' in content or
             'ğŸ˜†' in content or 'ğŸ˜…' in content or 'è­·ç…§' in content or 'å°ç£' in content)  # åŒ…å«æ¨™é»ç¬¦è™Ÿæˆ–è¡¨æƒ…
        )
        
        # å¿…é ˆæœ‰ Translate æ¨™è­˜ AND æœ‰å…§å®¹ç‰¹å¾µ
        return has_translate and has_content_features
    
    def _extract_content_fallback(self, lines: List[str]) -> Optional[str]:
        """å‚™é¸å…§å®¹æå–æ–¹æ³•"""
        content_start = -1
        for i, line in enumerate(lines):
            if 'Markdown Content:' in line:
                content_start = i + 1
                break
        
        if content_start == -1:
            return None
        
        content_lines = []
        for i in range(content_start, min(content_start + 15, len(lines))):
            line = lines[i].strip()
            if (line and 
                not line.startswith('[![Image') and 
                not line.startswith('[Image') and
                not line.startswith('>>>')):  # æ’é™¤å›è¦†
                content_lines.append(line)
                
                # å¦‚æœæ‰¾åˆ°äº†åˆç†çš„å…§å®¹å°±åœæ­¢
                if len(content_lines) >= 2 and len(line) > 10:
                    break
        
        return '\n'.join(content_lines) if content_lines else None
    
    def extract_engagement_numbers(self, markdown_content: str) -> List[str]:
        """æå–æ‰€æœ‰çµ±è¨ˆæ•¸å­—åºåˆ—ï¼ˆæŒ‰è®šã€ç•™è¨€ã€è½‰ç™¼ã€åˆ†äº«ï¼‰"""
        lines = markdown_content.split('\n')
        
        # ç­–ç•¥1: æŸ¥æ‰¾è²¼æ–‡å…§å®¹å¾Œçš„ç¬¬ä¸€å€‹åœ–ç‰‡ï¼Œç„¶å¾Œæå–å¾ŒçºŒæ•¸å­—
        for i, line in enumerate(lines):
            stripped = line.strip()
            # æ‰¾åˆ°è²¼æ–‡åœ–ç‰‡ï¼ˆé€šå¸¸åœ¨Translateä¹‹å¾Œï¼‰
            if stripped.startswith('![Image') and not 'profile picture' in stripped:
                numbers = []
                # åœ¨é€™å€‹åœ–ç‰‡å¾ŒæŸ¥æ‰¾é€£çºŒçš„æ•¸å­—
                for j in range(i + 1, min(i + 20, len(lines))):
                    candidate = lines[j].strip()
                    if re.match(r'^\d+(?:\.\d+)?[KMB]?$', candidate):
                        numbers.append(candidate)
                    elif candidate and not re.match(r'^\d+(?:\.\d+)?[KMB]?$', candidate) and candidate != "Pinned":
                        # é‡åˆ°éæ•¸å­—è¡Œï¼ˆä½†è·³éPinnedï¼‰ï¼Œåœæ­¢æ”¶é›†
                        break
                
                # å¦‚æœæ‰¾åˆ°äº†æ•¸å­—åºåˆ—ï¼Œè¿”å›
                if len(numbers) >= 3:
                    return numbers
        
        # ç­–ç•¥2: æŸ¥æ‰¾ä»»ä½•ä¸Šä¸‹æ–‡ä¸­çš„é€£çºŒæ•¸å­—
        all_numbers = []
        for i, line in enumerate(lines):
            stripped = line.strip()
            if re.match(r'^\d+(?:\.\d+)?[KMB]?$', stripped):
                # æª¢æŸ¥ä¸Šä¸‹æ–‡æ˜¯å¦åˆç†ï¼ˆé™„è¿‘æœ‰åœ–ç‰‡æˆ–Translateç­‰æ¨™è­˜ï¼‰
                context_valid = False
                for k in range(max(0, i-10), i):
                    if "![Image" in lines[k] or "Translate" in lines[k]:
                        context_valid = True
                        break
                
                if context_valid:
                    all_numbers.append(stripped)
        
        # è¿”å›æ‰¾åˆ°çš„æ•¸å­—ï¼ˆé€šå¸¸å‰4å€‹æ˜¯æŒ‰è®šã€ç•™è¨€ã€è½‰ç™¼ã€åˆ†äº«ï¼‰
        if len(all_numbers) >= 4:
            return all_numbers[:4]
        elif len(all_numbers) >= 3:
            return all_numbers[:3]
        
        return all_numbers
    
    def extract_likes_count(self, markdown_content: str) -> Optional[str]:
        """æå–æŒ‰è®šæ•¸"""
        numbers = self.extract_engagement_numbers(markdown_content)
        if len(numbers) >= 1:
            return numbers[0]
        
        # å‚™é¸æ–¹æ³•ï¼šå°‹æ‰¾èˆŠæ ¼å¼
        lines = markdown_content.split('\n')
        for line in lines:
            if 'ğŸ‘' in line or 'like' in line.lower():
                match = re.search(r'(\d+(?:,\d+)*)', line)
                if match:
                    return match.group(1)
        return None
    
    def extract_comments_count(self, markdown_content: str) -> Optional[str]:
        """æå–ç•™è¨€æ•¸"""
        numbers = self.extract_engagement_numbers(markdown_content)
        if len(numbers) >= 2:
            return numbers[1]
        
        # å‚™é¸æ–¹æ³•ï¼šå°‹æ‰¾èˆŠæ ¼å¼
        lines = markdown_content.split('\n')
        for line in lines:
            if 'ğŸ’¬' in line or 'comment' in line.lower():
                match = re.search(r'(\d+(?:,\d+)*)', line)
                if match:
                    return match.group(1)
        return None
    
    def extract_reposts_count(self, markdown_content: str) -> Optional[str]:
        """æå–è½‰ç™¼æ•¸"""
        numbers = self.extract_engagement_numbers(markdown_content)
        if len(numbers) >= 3:
            return numbers[2]
        
        # å‚™é¸æ–¹æ³•ï¼šå°‹æ‰¾èˆŠæ ¼å¼
        lines = markdown_content.split('\n')
        for line in lines:
            if 'ğŸ”„' in line or 'repost' in line.lower():
                match = re.search(r'(\d+(?:,\d+)*)', line)
                if match:
                    return match.group(1)
        return None
    
    def extract_shares_count(self, markdown_content: str) -> Optional[str]:
        """æå–åˆ†äº«æ•¸"""
        numbers = self.extract_engagement_numbers(markdown_content)
        if len(numbers) >= 4:
            return numbers[3]
        
        # å‚™é¸æ–¹æ³•ï¼šå°‹æ‰¾èˆŠæ ¼å¼
        lines = markdown_content.split('\n')
        for line in lines:
            if 'ğŸ“¤' in line or 'share' in line.lower():
                match = re.search(r'(\d+(?:,\d+)*)', line)
                if match:
                    return match.group(1)
        return None
    
    def fetch_content_jina_api(self, url: str) -> tuple:
        """ä½¿ç”¨Jina AIå®˜æ–¹APIç²å–å…§å®¹"""
        try:
            api_url = f"https://r.jina.ai/{url}"
            headers = {'X-Return-Format': 'markdown'}
            response = requests.get(api_url, headers=headers, timeout=30)
            
            if response.status_code == 200:
                return True, response.text
            else:
                return False, f"HTTP {response.status_code}"
        except Exception as e:
            return False, str(e)
    
    def fetch_content_local(self, url: str, use_cache: bool = True) -> tuple:
        """ä½¿ç”¨æœ¬åœ°Readerç²å–å…§å®¹"""
        headers = self.local_headers.copy()
        if not use_cache:
            headers['x-no-cache'] = 'true'
        
        try:
            response = requests.get(f"{self.local_reader_url}/{url}", headers=headers, timeout=30)
            if response.status_code == 200:
                return True, response.text
            else:
                return False, f"HTTP {response.status_code}"
        except Exception as e:
            return False, str(e)
    
    def parse_post(self, url: str, content: str, source: str) -> Dict:
        """è§£æè²¼æ–‡ - å®Œæ•´ç‰ˆæœ¬åŒ…å«äº’å‹•æ•¸æ“š"""
        post_id = url.split('/')[-1] if '/' in url else url
        views = self.extract_views_count(content, post_id)
        main_content = self.extract_post_content(content)
        
        # æå–äº’å‹•æ•¸æ“š
        likes = self.extract_likes_count(content)
        comments = self.extract_comments_count(content)
        reposts = self.extract_reposts_count(content)
        shares = self.extract_shares_count(content)
        
        return {
            'post_id': post_id,
            'url': url,
            'views': views,
            'content': main_content,
            'source': source,
            'likes': likes,
            'comments': comments,
            'reposts': reposts,
            'shares': shares,
            'success': views is not None and main_content is not None,
            'has_views': views is not None,
            'has_content': main_content is not None,
            'has_likes': likes is not None,
            'has_comments': comments is not None,
            'has_reposts': reposts is not None,
            'has_shares': shares is not None,
            'content_length': len(content)
        }
    
    def process_api_batch(self, urls: List[str], batch_num: int) -> List[Dict]:
        """è™•ç†APIæ‰¹æ¬¡"""
        print(f"ğŸŒ APIæ‰¹æ¬¡ #{batch_num}: ä¸¦è¡Œè™•ç† {len(urls)} å€‹URL...")
        
        results = []
        with ThreadPoolExecutor(max_workers=4) as executor:
            future_to_url = {
                executor.submit(self.fetch_content_jina_api, url): url 
                for url in urls
            }
            
            for completed, future in enumerate(as_completed(future_to_url), 1):
                url = future_to_url[future]
                
                try:
                    success, content = future.result()
                    if success:
                        result = self.parse_post(url, content, 'API-æ‰¹æ¬¡')
                        if result['has_views']:
                            status = f"âœ… ({result['views']})"
                        else:
                            status = "âŒ ç„¡è§€çœ‹æ•¸"
                        print(f"   ğŸŒ {completed}/{len(urls)}: {status} {result['post_id']}")
                        results.append(result)
                    else:
                        print(f"   ğŸŒ {completed}/{len(urls)}: âŒ APIå¤±æ•— {url.split('/')[-1]} ({content})")
                        print(f"      ğŸ”„ APIå¤±æ•—ç«‹å³å›é€€æœ¬åœ°Reader...")
                        
                        # APIå¤±æ•—æ™‚ç«‹å³å›é€€åˆ°æœ¬åœ°Reader
                        local_success, local_content = self.fetch_content_local(url, use_cache=False)
                        if local_success:
                            local_result = self.parse_post(url, local_content, 'API-å¤±æ•—å›é€€')
                            if local_result['has_views']:
                                print(f"      âœ… æœ¬åœ°æ•‘æ´æˆåŠŸ ({local_result['views']}) {local_result['post_id']}")
                                results.append(local_result)
                            else:
                                print(f"      âŒ æœ¬åœ°æ•‘æ´ç„¡è§€çœ‹æ•¸ {local_result['post_id']}")
                                results.append(local_result)
                        else:
                            print(f"      âŒ æœ¬åœ°æ•‘æ´ä¹Ÿå¤±æ•—: {local_content}")
                            results.append({
                                'post_id': url.split('/')[-1],
                                'url': url,
                                'views': None,
                                'content': None,
                                'likes': None,
                                'comments': None,
                                'reposts': None,
                                'shares': None,
                                'success': False,
                                'source': 'API-æ‰¹æ¬¡',
                                'has_views': False,
                                'has_content': False,
                                'has_likes': False,
                                'has_comments': False,
                                'has_reposts': False,
                                'has_shares': False,
                                'content_length': 0,
                                'api_error': content,
                                'local_error': local_content
                            })
                
                except Exception as e:
                    print(f"   ğŸŒ {completed}/{len(urls)}: âŒ APIç•°å¸¸ {url.split('/')[-1]} ({e})")
                    print(f"      ğŸ”„ APIç•°å¸¸ç«‹å³å›é€€æœ¬åœ°Reader...")
                    
                    # APIç•°å¸¸æ™‚ç«‹å³å›é€€åˆ°æœ¬åœ°Reader
                    try:
                        local_success, local_content = self.fetch_content_local(url, use_cache=False)
                        if local_success:
                            local_result = self.parse_post(url, local_content, 'API-ç•°å¸¸å›é€€')
                            if local_result['has_views']:
                                print(f"      âœ… æœ¬åœ°æ•‘æ´æˆåŠŸ ({local_result['views']}) {local_result['post_id']}")
                                results.append(local_result)
                            else:
                                print(f"      âŒ æœ¬åœ°æ•‘æ´ç„¡è§€çœ‹æ•¸ {local_result['post_id']}")
                                results.append(local_result)
                        else:
                            print(f"      âŒ æœ¬åœ°æ•‘æ´ä¹Ÿå¤±æ•—: {local_content}")
                            results.append({
                                'post_id': url.split('/')[-1],
                                'url': url,
                                'views': None,
                                'content': None,
                                'likes': None,
                                'comments': None,
                                'reposts': None,
                                'shares': None,
                                'success': False,
                                'source': 'API-æ‰¹æ¬¡',
                                'has_views': False,
                                'has_content': False,
                                'has_likes': False,
                                'has_comments': False,
                                'has_reposts': False,
                                'has_shares': False,
                                'content_length': 0,
                                'api_error': str(e),
                                'local_error': local_content
                            })
                    except Exception as local_e:
                        print(f"      âŒ æœ¬åœ°æ•‘æ´ä¹Ÿç•°å¸¸: {local_e}")
                        results.append({
                            'post_id': url.split('/')[-1],
                            'url': url,
                            'views': None,
                            'content': None,
                            'likes': None,
                            'comments': None,
                            'reposts': None,
                            'shares': None,
                            'success': False,
                            'source': 'API-æ‰¹æ¬¡',
                            'has_views': False,
                            'has_content': False,
                            'has_likes': False,
                            'has_comments': False,
                            'has_reposts': False,
                            'has_shares': False,
                            'content_length': 0,
                            'api_error': str(e),
                            'local_error': str(local_e)
                        })
        
        return results
    
    def process_local_batch(self, urls: List[str], batch_num: int) -> List[Dict]:
        """è™•ç†æœ¬åœ°æ‰¹æ¬¡ï¼ˆåŒ…å«APIå›é€€ï¼‰"""
        print(f"âš¡ æœ¬åœ°æ‰¹æ¬¡ #{batch_num}: ä¸¦è¡Œè™•ç† {len(urls)} å€‹URL...")
        
        results = []
        failed_urls_for_api = []
        
        # ç¬¬ä¸€éšæ®µï¼šæœ¬åœ°ä¸¦è¡Œè™•ç†
        with ThreadPoolExecutor(max_workers=4) as executor:
            future_to_url = {
                executor.submit(self.fetch_content_local, url, False): url 
                for url in urls
            }
            
            for completed, future in enumerate(as_completed(future_to_url), 1):
                url = future_to_url[future]
                
                try:
                    success, content = future.result()
                    if success:
                        result = self.parse_post(url, content, 'æœ¬åœ°-æ‰¹æ¬¡')
                        if result['has_views']:
                            print(f"   âš¡ {completed}/{len(urls)}: âœ… ({result['views']}) {result['post_id']}")
                            results.append(result)
                        else:
                            print(f"   âš¡ {completed}/{len(urls)}: âŒ æœ¬åœ°å¤±æ•— {url.split('/')[-1]} â†’ è½‰é€API")
                            failed_urls_for_api.append(url)
                    else:
                        print(f"   âš¡ {completed}/{len(urls)}: âŒ æœ¬åœ°å¤±æ•— {url.split('/')[-1]} â†’ è½‰é€API")
                        failed_urls_for_api.append(url)
                
                except Exception as e:
                    print(f"   âš¡ {completed}/{len(urls)}: âŒ æœ¬åœ°ç•°å¸¸ {url.split('/')[-1]} â†’ è½‰é€API")
                    failed_urls_for_api.append(url)
        
        # ç¬¬äºŒéšæ®µï¼šå¤±æ•—é …ç›®çš„APIå›é€€
        if failed_urls_for_api:
            print(f"   ğŸŒ æœ¬åœ°å¤±æ•—é …ç›®ç«‹å³è½‰API: {len(failed_urls_for_api)} å€‹...")
            
            with ThreadPoolExecutor(max_workers=4) as executor:
                future_to_url = {
                    executor.submit(self.fetch_content_jina_api, url): url 
                    for url in failed_urls_for_api
                }
                
                for api_completed, future in enumerate(as_completed(future_to_url), 1):
                    url = future_to_url[future]
                    
                    try:
                        success, content = future.result()
                        if success:
                            result = self.parse_post(url, content, 'API-å›é€€')
                            if result['has_views']:
                                print(f"      ğŸŒ {api_completed}/{len(failed_urls_for_api)}: âœ… APIæ•‘æ´æˆåŠŸ ({result['views']}) {result['post_id']}")
                            else:
                                print(f"      ğŸŒ {api_completed}/{len(failed_urls_for_api)}: âŒ APIç„¡è§€çœ‹æ•¸ {result['post_id']}")
                            results.append(result)
                        else:
                            print(f"      ğŸŒ {api_completed}/{len(failed_urls_for_api)}: âŒ APIä¹Ÿå¤±æ•— {url.split('/')[-1]}")
                            results.append({
                                'post_id': url.split('/')[-1],
                                'url': url,
                                'success': False,
                                'source': 'API-å›é€€',
                                'error': content
                            })
                    
                    except Exception as e:
                        print(f"      ğŸŒ {api_completed}/{len(failed_urls_for_api)}: âŒ APIå›é€€ç•°å¸¸ {url.split('/')[-1]}")
                        results.append({
                            'post_id': url.split('/')[-1],
                            'url': url,
                            'success': False,
                            'source': 'API-å›é€€',
                            'error': str(e)
                        })
        
        return results
    
    def rotation_pipeline(self, urls: List[str]) -> List[Dict]:
        """è¼ªè¿´ç­–ç•¥ç®¡ç·š"""
        print(f"ğŸ”„ è¼ªè¿´ç­–ç•¥ç®¡ç·šå•Ÿå‹•")
        print(f"ğŸ“Š è™•ç† {len(urls)} å€‹URL")
        print(f"ğŸŒ APIæ‰¹æ¬¡å¤§å°: {self.api_batch_size} | âš¡ æœ¬åœ°æ‰¹æ¬¡å¤§å°: {self.local_batch_size}")
        print("âœ… å·²æ•´åˆæœ€ä½³åŒ–: Headersé…ç½® + NBSPæ­£è¦åŒ– + æ™ºèƒ½å…§å®¹æå–")
        print("=" * 60)
        
        all_results = []
        batch_counter = 1
        processed_count = 0
        total_start_time = time.time()
        
        remaining_urls = urls.copy()
        
        while remaining_urls:
            if len(remaining_urls) <= self.api_batch_size:
                # æœ€å¾Œä¸€æ‰¹ï¼Œç›´æ¥ç”¨APIè™•ç†å®Œ
                batch_results = self.process_api_batch(remaining_urls, batch_counter)
                all_results.extend(batch_results)
                processed_count += len(remaining_urls)
                remaining_urls = []
            
            elif processed_count == 0 or (batch_counter - 1) % 2 == 0:
                # APIæ‰¹æ¬¡ï¼ˆç¬¬1,3,5...æ‰¹æ¬¡ï¼‰
                api_urls = remaining_urls[:self.api_batch_size]
                remaining_urls = remaining_urls[self.api_batch_size:]
                
                batch_results = self.process_api_batch(api_urls, batch_counter)
                all_results.extend(batch_results)
                processed_count += len(api_urls)
            
            else:
                # æœ¬åœ°æ‰¹æ¬¡ï¼ˆç¬¬2,4,6...æ‰¹æ¬¡ï¼‰
                local_urls = remaining_urls[:self.local_batch_size]
                remaining_urls = remaining_urls[self.local_batch_size:]
                
                batch_results = self.process_local_batch(local_urls, batch_counter)
                all_results.extend(batch_results)
                processed_count += len(local_urls)
            
            # æ›´æ–°æ‰¹æ¬¡çµ±è¨ˆ
            batch_key = f"{batch_counter}"
            batch_results_success = [r for r in batch_results if r.get('success', False)]
            self.batch_stats[batch_key] = {
                'total': len(batch_results),
                'success': len(batch_results_success),
                'source': batch_results[0]['source'] if batch_results else 'unknown'
            }
            
            print(f"\nğŸ“Š å·²è™•ç†: {processed_count}/{len(urls)} ({processed_count/len(urls)*100:.1f}%)")
            print(f"ğŸ¯ å‰©é¤˜: {len(remaining_urls)} å€‹URL")
            
            batch_counter += 1
            
            # æ‰¹æ¬¡é–“çŸ­æš«åœé “
            if remaining_urls:
                time.sleep(1)
        
        # æœ€çµ‚çµ±è¨ˆ
        total_end_time = time.time()
        success_results = [r for r in all_results if r.get('success', False)]
        success_count = len(success_results)
        
        api_success_count = len([r for r in success_results if 'API' in r.get('source', '')])
        local_success_count = len([r for r in success_results if 'æœ¬åœ°' in r.get('source', '')])
        
        print(f"\n{'='*80}")
        print(f"âœ… æœ€çµ‚æˆåŠŸ: {success_count}/{len(urls)} ({success_count/len(urls)*100:.1f}%)")
        print(f"ğŸŒ APIæˆåŠŸ: {api_success_count} | âš¡ æœ¬åœ°æˆåŠŸ: {local_success_count}")
        print(f"â±ï¸ ç¸½è€—æ™‚: {total_end_time - total_start_time:.1f}s")
        print(f"ğŸï¸ å¹³å‡é€Ÿåº¦: {len(urls)/(total_end_time - total_start_time):.2f} URL/s")
        
        # å„æ‰¹æ¬¡æˆåŠŸç‡
        print(f"\nğŸ“ˆ å„æ‰¹æ¬¡æˆåŠŸç‡:")
        for batch_key, stats in self.batch_stats.items():
            rate = stats['success'] / stats['total'] * 100 if stats['total'] > 0 else 0
            source = stats['source']
            if 'API' in source:
                print(f"   ğŸŒ APIæ‰¹æ¬¡{batch_key}: {stats['success']}/{stats['total']} ({rate:.1f}%)")
            elif 'æœ¬åœ°' in source:
                print(f"   âš¡ æœ¬åœ°æ‰¹æ¬¡{batch_key}: {stats['success']}/{stats['total']} ({rate:.1f}%)")
            elif 'API-å›é€€' in source:
                print(f"   ğŸš€ APIæ•‘æ´{batch_key}: {stats['success']}/{stats['total']} ({rate:.1f}%)")
        
        return all_results