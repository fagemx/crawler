#!/usr/bin/env python3
"""
ç°¡åŒ–çˆ¬èŸ²æ¸¬è©¦è…³æœ¬

æ¸¬è©¦ Crawler Agent æ˜¯å¦èƒ½æ­£ç¢ºæŠ“å– Threads è²¼æ–‡ URL
"""

import asyncio
import json
import sys
from pathlib import Path

# æ·»åŠ å°ˆæ¡ˆæ ¹ç›®éŒ„åˆ° Python è·¯å¾‘
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from agents.crawler.crawler_logic import CrawlerLogic
from common.settings import get_settings


async def test_crawler():
    """æ¸¬è©¦çˆ¬èŸ²åŠŸèƒ½"""
    print("ğŸ§ª æ¸¬è©¦ç°¡åŒ–ç‰ˆ Crawler Agent")
    print("=" * 50)
    
    # æª¢æŸ¥é…ç½®
    settings = get_settings()
    if not settings.apify.token:
        print("âŒ éŒ¯èª¤ï¼šæœªè¨­ç½® APIFY_TOKEN")
        print("è«‹åœ¨ .env æª”æ¡ˆä¸­è¨­ç½® APIFY_TOKEN=your_token_here")
        return
    
    print(f"âœ… Apify Token: {'å·²è¨­ç½®' if settings.apify.token else 'æœªè¨­ç½®'}")
    print(f"ğŸ“ ä½¿ç”¨ Actor: curious_coder/threads-scraper")
    
    # å‰µå»ºçˆ¬èŸ²å¯¦ä¾‹
    crawler = CrawlerLogic()
    
    # æ¸¬è©¦ç”¨æˆ¶ï¼ˆä½¿ç”¨ç”¨æˆ¶æä¾›çš„ç¯„ä¾‹ï¼‰
    test_username = "09johan24"
    max_posts = 5  # æ¸¬è©¦ç”¨å°‘é‡è²¼æ–‡
    
    print(f"\nğŸ¯ æ¸¬è©¦ç›®æ¨™ï¼š@{test_username}")
    print(f"ğŸ“Š æŠ“å–æ•¸é‡ï¼š{max_posts} å‰‡è²¼æ–‡")
    print("\né–‹å§‹æŠ“å–...")
    
    try:
        async for result in crawler.fetch_threads_post_urls(
            username=test_username,
            max_posts=max_posts,
            task_id="test_task_001"
        ):
            # è™•ç†ä¸åŒé¡å‹çš„å›æ‡‰
            if result.get("response_type") == "status":
                content = result.get("content", {})
                status = content.get("status", "")
                message = content.get("message", "")
                progress = content.get("progress")
                
                if progress is not None:
                    print(f"ğŸ“ˆ é€²åº¦ï¼š{progress:.1%} - {message}")
                else:
                    print(f"ğŸ“‹ ç‹€æ…‹ï¼š{status} - {message}")
                    
            elif result.get("response_type") == "text":
                print(f"ğŸ’¬ è¨Šæ¯ï¼š{result.get('content', '')}")
                
            elif result.get("response_type") == "data" and result.get("is_task_complete"):
                # æœ€çµ‚çµæœ
                content = result.get("content", {})
                post_urls = content.get("post_urls", [])
                
                print(f"\nâœ… æŠ“å–å®Œæˆï¼")
                print(f"ğŸ“Š ç¸½å…±æŠ“å–ï¼š{content.get('total_count', 0)} å€‹ URL")
                print(f"â±ï¸  è™•ç†æ™‚é–“ï¼š{content.get('processing_time', 0):.2f} ç§’")
                print(f"ğŸ‘¤ ç”¨æˆ¶ï¼š{content.get('username', '')}")
                
                print(f"\nğŸ“‹ æŠ“å–åˆ°çš„è²¼æ–‡ URLï¼š")
                for i, post_url in enumerate(post_urls, 1):
                    print(f"  {i}. {post_url.get('url', '')}")
                    print(f"     ID: {post_url.get('post_id', '')}")
                
                # é©—è­‰ URL æ ¼å¼ï¼ˆåŸºæ–¼ç”¨æˆ¶æä¾›çš„ç¯„ä¾‹æ ¼å¼ï¼‰
                print(f"\nğŸ” URL æ ¼å¼é©—è­‰ï¼š")
                valid_urls = 0
                expected_format = "https://www.threads.com/@username/post/code"
                print(f"   é æœŸæ ¼å¼ï¼š{expected_format}")
                
                for post_url in post_urls:
                    url = post_url.get('url', '')
                    if url.startswith('https://www.threads.com/@') and '/post/' in url:
                        valid_urls += 1
                        print(f"   âœ… {url}")
                    else:
                        print(f"   âš ï¸  ç„¡æ•ˆ URL æ ¼å¼: {url}")
                
                print(f"\nâœ… æœ‰æ•ˆ URLï¼š{valid_urls}/{len(post_urls)}")
                
                # é©—è­‰ç¯„ä¾‹è²¼æ–‡æ ¼å¼
                example_url = f"https://www.threads.com/@{test_username}/post/DMaHMSqTdFs"
                print(f"\nğŸ“ ç¯„ä¾‹è²¼æ–‡ URL æ ¼å¼ï¼š")
                print(f"   {example_url}")
                print(f"   ï¼ˆé€™æ‡‰è©²æ˜¯é¡ä¼¼çš„æ ¼å¼ï¼‰")
                
            elif result.get("response_type") == "error":
                print(f"âŒ éŒ¯èª¤ï¼š{result.get('content', {}).get('error', '')}")
                
    except Exception as e:
        print(f"âŒ æ¸¬è©¦å¤±æ•—ï¼š{str(e)}")
        import traceback
        traceback.print_exc()


async def test_health_check():
    """æ¸¬è©¦å¥åº·æª¢æŸ¥"""
    print("\nğŸ¥ æ¸¬è©¦å¥åº·æª¢æŸ¥")
    print("-" * 30)
    
    crawler = CrawlerLogic()
    health_status = await crawler.health_check()
    
    print(f"ç‹€æ…‹ï¼š{health_status.get('status', 'unknown')}")
    if health_status.get('error'):
        print(f"éŒ¯èª¤ï¼š{health_status['error']}")
    else:
        print("âœ… å¥åº·æª¢æŸ¥é€šé")


def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸš€ Crawler Agent ç°¡åŒ–ç‰ˆæ¸¬è©¦")
    print("åŸºæ–¼ apify-threads-scraper.md çš„å¯¦ç¾")
    print("åªæŠ“å–è²¼æ–‡ URLï¼Œä¸è™•ç†å…¶ä»–æ•¸æ“š")
    
    # æª¢æŸ¥æ˜¯å¦åœ¨å°ˆæ¡ˆæ ¹ç›®éŒ„
    if not (project_root / ".env.example").exists():
        print("âŒ è«‹åœ¨å°ˆæ¡ˆæ ¹ç›®éŒ„åŸ·è¡Œæ­¤è…³æœ¬")
        sys.exit(1)
    
    # æª¢æŸ¥ .env æª”æ¡ˆ
    env_file = project_root / ".env"
    if not env_file.exists():
        print("âš ï¸  æœªæ‰¾åˆ° .env æª”æ¡ˆ")
        print("è«‹è¤‡è£½ .env.example ç‚º .env ä¸¦è¨­ç½® APIFY_TOKEN")
        sys.exit(1)
    
    # é‹è¡Œæ¸¬è©¦
    asyncio.run(test_health_check())
    asyncio.run(test_crawler())


if __name__ == "__main__":
    main()