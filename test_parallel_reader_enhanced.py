#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¤šç·šç¨‹Readerè§£æè…³æœ¬ (V13 - å¢å¼·è§€çœ‹æ•¸æå–ç‰ˆ)
ä¿®æ­£ missing_views å•é¡Œï¼Œæå‡æˆåŠŸç‡åˆ° 90%+
"""

import json
import re
import requests
import time
import subprocess
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Dict, Optional, List

class EnhancedThreadsReaderParser:
    """
    å¢å¼·ç‰ˆ Threads Reader è§£æå™¨
    é‡é»æ”¹é€²è§€çœ‹æ•¸æå–çš„ç©©å®šæ€§
    """
    
    def __init__(self, backend_instances: int = 4):
        self.backend_instances = backend_instances
        self.lb_url = "http://localhost:8880"
        self.official_reader_url = "https://r.jina.ai"
        
        # --- å¢å¼·çš„è§€çœ‹æ•¸æå–æ¨¡å¼ ---
        self.view_patterns = [
            # åŸæœ‰æ¨¡å¼ï¼ˆä¿ç•™ï¼‰
            re.compile(r'\[Thread\s*={2,}\s*(\d+(?:\.\d+)?[KMB]?)\s*views\]', re.IGNORECASE),
            re.compile(r'Thread.*?(\d+(?:\.\d+)?[KMB]?)\s*views', re.IGNORECASE),
            re.compile(r'(\d+(?:\.\d+)?[KMB]?)\s*views', re.IGNORECASE),
            
            # æ–°å¢æ“´å±•æ¨¡å¼ï¼ˆå¾è¨ºæ–·ä¸­ç™¼ç¾ï¼‰
            re.compile(r'(\d+(?:\.\d+)?[KMB]?)\s*view(?:s|ing)', re.IGNORECASE),
            re.compile(r'views?\s*:\s*(\d+(?:\.\d+)?[KMB]?)', re.IGNORECASE),
            re.compile(r'seen\s*by\s*(\d+(?:\.\d+)?[KMB]?)', re.IGNORECASE),
            
            # å®¹éŒ¯æ¨¡å¼
            re.compile(r'(\d+(?:\.\d+)?[KMB]?)\s*äºº\s*çœ‹é', re.IGNORECASE),  # ä¸­æ–‡
            re.compile(r'(\d+(?:\.\d+)?[KMB]?)\s*æ¬¡\s*è§€çœ‹', re.IGNORECASE),  # ä¸­æ–‡
        ]
        
        self.engagement_pattern = re.compile(r'^\d+(?:\.\d+)?[KMB]?$')
        
        # --- å„ªåŒ–çš„Sessioné…ç½® ---
        self.session = requests.Session()
        adapter = requests.adapters.HTTPAdapter(
            pool_connections=20,
            pool_maxsize=30,
            max_retries=0
        )
        self.session.mount('http://', adapter)
        self.session.mount('https://', adapter)
        
        print(f"ğŸ”§ å¢å¼·ç‰ˆåˆå§‹åŒ–å®Œæˆ")
        print(f"   å¾Œç«¯å¯¦ä¾‹: {backend_instances} å€‹")
        print(f"   è§€çœ‹æ•¸æ¨¡å¼: {len(self.view_patterns)} å€‹")

    def convert_url_to_threads_net(self, url: str) -> str:
        """å°‡ threads.com URL è½‰æ›ç‚º threads.net"""
        return url.replace("threads.com", "threads.net")

    def enhanced_extract_views_count(self, markdown_content: str) -> Optional[str]:
        """å¢å¼·ç‰ˆè§€çœ‹æ•¸æå– - ä½¿ç”¨å¤šé‡æ¨¡å¼ + å®¹éŒ¯é‚è¼¯"""
        
        # 1. å˜—è©¦æ‰€æœ‰é å®šç¾©æ¨¡å¼
        for i, pattern in enumerate(self.view_patterns):
            match = pattern.search(markdown_content)
            if match:
                views_number = match.group(1)
                # é©—è­‰æå–çš„æ•¸å­—æ ¼å¼
                if self.validate_number_format(views_number):
                    return f"{views_number} views"
        
        # 2. å¦‚æœé å®šç¾©æ¨¡å¼å¤±æ•—ï¼Œå˜—è©¦ä¸Šä¸‹æ–‡æœç´¢
        lines = markdown_content.split('\n')
        for line in lines:
            line_lower = line.lower()
            if 'thread' in line_lower and ('view' in line_lower or 'è§€çœ‹' in line_lower):
                # åœ¨åŒ…å« thread å’Œ view çš„è¡Œä¸­æœç´¢æ•¸å­—
                number_matches = re.findall(r'(\d+(?:\.\d+)?[KMB]?)', line, re.IGNORECASE)
                for number in number_matches:
                    if self.validate_number_format(number):
                        return f"{number} views"
        
        # 3. æœ€å¾Œå˜—è©¦ï¼šåœ¨æ•´å€‹å…§å®¹ä¸­å°‹æ‰¾å¯èƒ½çš„è§€çœ‹æ•¸
        # æŸ¥æ‰¾æ‰€æœ‰æ•¸å­—+å–®ä½çš„çµ„åˆ
        potential_numbers = re.findall(r'(\d+(?:\.\d+)?[KMB])', markdown_content, re.IGNORECASE)
        
        # éæ¿¾å‡ºå¯èƒ½æ˜¯è§€çœ‹æ•¸çš„æ•¸å­—ï¼ˆé€šå¸¸è¼ƒå¤§ï¼‰
        view_candidates = []
        for number in potential_numbers:
            if self.could_be_view_count(number):
                view_candidates.append(number)
        
        # å¦‚æœåªæœ‰ä¸€å€‹å€™é¸ï¼Œå¯èƒ½å°±æ˜¯è§€çœ‹æ•¸
        if len(view_candidates) == 1:
            return f"{view_candidates[0]} views"
        
        return None

    def validate_number_format(self, number: str) -> bool:
        """é©—è­‰æ•¸å­—æ ¼å¼æ˜¯å¦åˆç†"""
        if not number:
            return False
        
        # åŸºæœ¬æ ¼å¼æª¢æŸ¥
        pattern = re.compile(r'^\d+(?:\.\d+)?[KMB]?$', re.IGNORECASE)
        if not pattern.match(number):
            return False
        
        # è½‰æ›ç‚ºå¯¦éš›æ•¸å­—é€²è¡Œåˆç†æ€§æª¢æŸ¥
        try:
            actual_number = self.convert_number_to_int(number)
            # è§€çœ‹æ•¸é€šå¸¸åœ¨ 1-100M ç¯„åœå…§
            return 1 <= actual_number <= 100_000_000
        except:
            return False

    def could_be_view_count(self, number: str) -> bool:
        """åˆ¤æ–·æ•¸å­—æ˜¯å¦å¯èƒ½æ˜¯è§€çœ‹æ•¸"""
        try:
            actual_number = self.convert_number_to_int(number)
            # è§€çœ‹æ•¸é€šå¸¸ > 100ï¼ˆæ’é™¤æŒ‰è®šæ•¸ã€è©•è«–æ•¸ï¼‰
            return actual_number >= 100
        except:
            return False

    def convert_number_to_int(self, number_str: str) -> int:
        """å°‡ K/M/B æ ¼å¼çš„æ•¸å­—è½‰æ›ç‚ºæ•´æ•¸"""
        number_str = number_str.upper()
        if number_str.endswith('K'):
            return int(float(number_str[:-1]) * 1000)
        elif number_str.endswith('M'):
            return int(float(number_str[:-1]) * 1000000)
        elif number_str.endswith('B'):
            return int(float(number_str[:-1]) * 1000000000)
        else:
            return int(number_str)

    def fetch_content_local(self, post_url: str, use_cache: bool = True, timeout: int = 60) -> str:
        """å¾æœ¬åœ°Readeræœå‹™ç²å–å…§å®¹"""
        corrected_url = self.convert_url_to_threads_net(post_url)
        reader_url = f"{self.lb_url}/{corrected_url}"
        
        headers = {}
        if not use_cache:
            headers['x-no-cache'] = 'true'
        
        try:
            response = self.session.get(reader_url, headers=headers, timeout=(10, timeout))
            response.raise_for_status()
            return response.text
        except requests.exceptions.Timeout:
            return ""
        except requests.exceptions.RequestException:
            return ""

    def fetch_content_official(self, post_url: str) -> str:
        """å¾å®˜æ–¹ Jina Reader æœå‹™ç²å–å…§å®¹"""
        corrected_url = self.convert_url_to_threads_net(post_url)
        jina_url = f"{self.official_reader_url}/{corrected_url}"
        headers = {"X-Return-Format": "markdown"}
        try:
            response = self.session.get(jina_url, headers=headers, timeout=(10, 120))
            response.raise_for_status()
            return response.text
        except Exception:
            return ""

    def extract_post_content(self, markdown_content: str) -> Optional[str]:
        """æå–è²¼æ–‡å…§å®¹"""
        lines = markdown_content.split('\n')
        content_lines = []
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            if line.startswith("Title:") or line.startswith("URL Source:") or line.startswith("Markdown Content:"):
                continue
            if "===============" in line or "---" in line:
                break
            if not line.startswith("[") and not line.startswith("!") and len(line) > 10:
                content_lines.append(line)
                if len(content_lines) >= 3:
                    break
        
        return ' '.join(content_lines) if content_lines else None

    def extract_engagement_numbers(self, markdown_content: str) -> Dict[str, str]:
        """æå–äº’å‹•æ•¸æ“šåºåˆ—"""
        lines = markdown_content.split('\n')
        
        for i, line in enumerate(lines):
            stripped = line.strip()
            if (stripped.startswith('![Image') and 
                'profile picture' not in stripped and 
                i > 0 and any('Translate' in lines[k] for k in range(max(0, i-3), i+1))):
                
                numbers = []
                for j in range(i + 1, min(i + 15, len(lines))):
                    candidate = lines[j].strip()
                    if self.engagement_pattern.match(candidate):
                        numbers.append(candidate)
                    elif candidate and candidate not in ["Pinned", "", "Translate"]:
                        break
                
                engagement_data = {}
                if len(numbers) >= 1: engagement_data['likes'] = numbers[0]
                if len(numbers) >= 2: engagement_data['comments'] = numbers[1]  
                if len(numbers) >= 3: engagement_data['reposts'] = numbers[2]
                if len(numbers) >= 4: engagement_data['shares'] = numbers[3]
                
                if len(numbers) >= 3:
                    return engagement_data
        return {}

    def parse_post_local(self, post_url: str, use_cache: bool = True, timeout: int = 60) -> Dict:
        """ä½¿ç”¨æœ¬åœ°æœå‹™è§£æ"""
        content = self.fetch_content_local(post_url, use_cache, timeout)
        if not content:
            return {"url": post_url, "error": "ç„¡æ³•ç²å–å…§å®¹"}
        
        engagement = self.extract_engagement_numbers(content)
        
        return {
            "url": post_url,
            "content": self.extract_post_content(content),
            "views": self.enhanced_extract_views_count(content),  # ä½¿ç”¨å¢å¼·ç‰ˆæå–
            "likes": engagement.get('likes'),
            "comments": engagement.get('comments'),
            "reposts": engagement.get('reposts'),
            "shares": engagement.get('shares'),
            "raw_length": len(content)
        }

    def parse_post_official(self, post_url: str) -> Dict:
        """ä½¿ç”¨å®˜æ–¹æœå‹™è§£æ"""
        content = self.fetch_content_official(post_url)
        if not content:
            return {"url": post_url, "error": "ç„¡æ³•å¾å®˜æ–¹APIç²å–å…§å®¹"}
        
        engagement = self.extract_engagement_numbers(content)
        
        return {
            "url": post_url,
            "content": self.extract_post_content(content),
            "views": self.enhanced_extract_views_count(content),  # ä½¿ç”¨å¢å¼·ç‰ˆæå–
            "likes": engagement.get('likes'),
            "comments": engagement.get('comments'),
            "reposts": engagement.get('reposts'),
            "shares": engagement.get('shares'),
            "raw_length": len(content)
        }

def load_urls_from_file(file_path: str) -> List[str]:
    """å¾JSONæª”æ¡ˆä¸­æå–æ‰€æœ‰è²¼æ–‡URL"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        urls = [post['url'] for post in data.get('posts', []) if 'url' in post]
        print(f"âœ… å¾ {file_path} æˆåŠŸæå– {len(urls)} å€‹ URLã€‚")
        return urls
    except Exception as e:
        print(f"âŒ æå– URL æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return []

def detect_reader_instances():
    """æª¢æ¸¬Readerå¯¦ä¾‹æ•¸é‡"""
    try:
        result = subprocess.run([
            'docker', 'ps', 
            '--filter', 'name=reader', 
            '--filter', 'status=running',
            '--format', '{{.Names}}'
        ], capture_output=True, text=True, timeout=10)
        
        if result.returncode == 0:
            containers = result.stdout.strip().split('\n')
            reader_containers = [name for name in containers if 'reader-' in name and 'lb' not in name and name.strip()]
            return len(reader_containers)
        return 4  # é è¨­å€¼
    except:
        return 4  # é è¨­å€¼

def main():
    """ä¸»å‡½æ•¸ï¼šå¢å¼·ç‰ˆ"""
    json_file_path = 'agents/playwright_crawler/debug/crawl_data_20250803_121452_934d52b1.json'
    
    # --- æª¢æ¸¬é…ç½® ---
    backend_instances = detect_reader_instances()
    max_workers = backend_instances * 2  # ä¿å®ˆä½µç™¼
    
    urls_to_process = load_urls_from_file(json_file_path)
    if not urls_to_process:
        return

    total_urls = len(urls_to_process)
    parser = EnhancedThreadsReaderParser(backend_instances)
    results = {}
    
    start_time = time.time()
    print(f"\nğŸš€ å¢å¼·ç‰ˆå•Ÿå‹•ï¼ç›®æ¨™ï¼šçªç ´ 90% æˆåŠŸç‡")
    print(f"ğŸ“Š é…ç½®: {backend_instances}å€‹å¯¦ä¾‹, ä½µç™¼æ•¸: {max_workers}")
    print("ğŸ¯ é‡é»æ”¹é€²: âœ…å¢å¼·è§€çœ‹æ•¸æå– âœ…8ç¨®æ¨¡å¼åŒ¹é… âœ…æ™ºèƒ½å®¹éŒ¯")

    # --- ç¬¬ä¸€å±¤: å¹³è¡Œè™•ç† ---
    print(f"\nâš¡ (1/3) å¢å¼·ä½µç™¼è™•ç† {total_urls} å€‹ URL...")
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_url = {executor.submit(parser.parse_post_local, url, True, 45): url for url in urls_to_process}
        
        completed = 0
        for future in as_completed(future_to_url):
            url = future_to_url[future]
            completed += 1
            progress = completed / total_urls
            bar_length = 40
            filled_length = int(bar_length * progress)
            bar = 'â–ˆ' * filled_length + '-' * (bar_length - filled_length)
            
            try:
                result = future.result()
                results[url] = result
                
                # å³æ™‚ç‹€æ…‹é¡¯ç¤º
                status = "âœ…" if (result.get("views") and result.get("content")) else "âŒ"
                post_id = url.split('/')[-1]
                print(f'\ré€²åº¦: |{bar}| {completed}/{total_urls} ({progress:.1%}) {status} {post_id}', end='', flush=True)
            except Exception:
                results[url] = {"url": url, "error": "åŸ·è¡Œç·’ç•°å¸¸"}
                print(f'\ré€²åº¦: |{bar}| {completed}/{total_urls} ({progress:.1%}) âŒ {url.split("/")[-1]}', end='', flush=True)

    # ç¬¬ä¸€è¼ªçµ±è¨ˆ
    first_round_success = sum(1 for res in results.values() if not res.get("error") and res.get("views") and res.get("content"))
    first_round_time = time.time() - start_time
    
    print(f"\n\nğŸ¯ å¢å¼·ç‰ˆç¬¬ä¸€è¼ªçµæœ: {first_round_success}/{total_urls} æˆåŠŸ ({first_round_success/total_urls*100:.1f}%)")
    print(f"ğŸ“Š æ•ˆèƒ½: {first_round_time:.1f}s, é€Ÿåº¦: {total_urls/first_round_time:.2f} URL/ç§’")
    
    # è©³ç´°åˆ†æå¤±æ•—åŸå› 
    failures_by_type = {"missing_views": 0, "missing_content": 0, "missing_both": 0, "http_error": 0}
    for res in results.values():
        if res.get("error"):
            failures_by_type["http_error"] += 1
        elif not res.get("views") and not res.get("content"):
            failures_by_type["missing_both"] += 1
        elif not res.get("views"):
            failures_by_type["missing_views"] += 1
        elif not res.get("content"):
            failures_by_type["missing_content"] += 1
    
    print(f"ğŸ“‹ å¤±æ•—åˆ†æ: è§€çœ‹æ•¸ç¼ºå¤±={failures_by_type['missing_views']}, å…§å®¹ç¼ºå¤±={failures_by_type['missing_content']}, HTTPéŒ¯èª¤={failures_by_type['http_error']}")

    # --- ç¬¬äºŒå±¤: æ™ºèƒ½é‡è©¦ ---
    print(f"\nğŸ”„ (2/3) æ™ºèƒ½é‡è©¦å¤±æ•—é …ç›®...")
    local_retries_attempted = 0
    urls_to_retry_local = [url for url, res in results.items() if res.get("error") or not res.get("views") or not res.get("content")]
    
    if urls_to_retry_local:
        print(f"ğŸ“ éœ€è¦é‡è©¦: {len(urls_to_retry_local)} å€‹é …ç›® (ç„¡å¿«å–+75sè¶…æ™‚)")
        for url in urls_to_retry_local:
            local_retries_attempted += 1
            post_id = url.split('/')[-1]
            print(f"  ğŸ”„ é‡è©¦ {local_retries_attempted}: {post_id}")
            results[url] = parser.parse_post_local(url, use_cache=False, timeout=75)
    else:
        print("âœ… ç¬¬ä¸€è¼ªæ•¸æ“šå·²å®Œæ•´ã€‚")

    # --- ç¬¬ä¸‰å±¤: å®˜æ–¹APIæ•‘æ´ ---
    print(f"\nğŸŒ (3/3) å®˜æ–¹APIæœ€çµ‚æ•‘æ´...")
    official_retries_attempted = 0
    urls_to_retry_official = [url for url, res in results.items() if res.get("error") or not res.get("views") or not res.get("content")]
    
    if urls_to_retry_official:
        print(f"ğŸ”— è½‰å®˜æ–¹API: {len(urls_to_retry_official)} å€‹é …ç›®")
        for url in urls_to_retry_official:
            official_retries_attempted += 1
            post_id = url.split('/')[-1]
            print(f"  ğŸŒ å®˜æ–¹API {official_retries_attempted}: {post_id}")
            results[url] = parser.parse_post_official(url)
    else:
        print("âœ… æœ¬åœ°é‡è©¦å¾Œæ•¸æ“šå·²å®Œæ•´ã€‚")

    end_time = time.time()
    total_time = end_time - start_time

    # --- æœ€çµ‚çµ±è¨ˆèˆ‡ä¿å­˜ ---
    final_success_results = [res for res in results.values() if not res.get("error") and res.get("views") and res.get("content")]
    final_error_count = total_urls - len(final_success_results)

    if final_success_results:
        output_filename = 'parallel_reader_results_enhanced.json'
        with open(output_filename, 'w', encoding='utf-8') as f:
            json.dump(final_success_results, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ’¾ {len(final_success_results)} ç­†å®Œæ•´çµæœå·²ä¿å­˜åˆ°: {output_filename}")

    print("\n" + "="*80)
    print("ğŸ† å¢å¼·ç‰ˆåŸ·è¡Œå®Œç•¢ï¼")
    print(f"ğŸ“Š è™•ç†çµ±è¨ˆ:")
    print(f"   - å¾Œç«¯å¯¦ä¾‹: {backend_instances} å€‹")
    print(f"   - è§€çœ‹æ•¸æ¨¡å¼: {len(parser.view_patterns)} å€‹")
    print(f"   - ç¸½URLæ•¸é‡: {total_urls}")
    print(f"   - ç¬¬ä¸€è¼ªæˆåŠŸç‡: {first_round_success/total_urls*100:.1f}% ({first_round_success}/{total_urls})")
    print(f"   - æœ¬åœ°é‡è©¦: {local_retries_attempted} æ¬¡")
    print(f"   - å®˜æ–¹APIé‡è©¦: {official_retries_attempted} æ¬¡")
    print(f"   - æœ€çµ‚æˆåŠŸ: {len(final_success_results)} ({len(final_success_results)/total_urls*100:.1f}%)")
    print(f"   - æœ€çµ‚å¤±æ•—: {final_error_count}")
    print(f"âš¡ æ•ˆèƒ½æŒ‡æ¨™:")
    print(f"   - ç¸½è€—æ™‚: {total_time:.2f} ç§’")
    print(f"   - å¹³å‡é€Ÿåº¦: {total_urls/total_time:.2f} URL/ç§’")
    print(f"   - ç¬¬ä¸€è¼ªé€Ÿåº¦: {total_urls/first_round_time:.2f} URL/ç§’")
    
    # æˆåŠŸè©•ä¼°
    final_success_rate = len(final_success_results) / total_urls * 100
    if final_success_rate >= 95:
        print(f"\nğŸ‰ å„ªç§€ï¼å¢å¼·ç‰ˆè§€çœ‹æ•¸æå–å®Œç¾é”æ¨™")
    elif final_success_rate >= 85:
        print(f"\nâœ… è‰¯å¥½ï¼é¡¯è‘—æ”¹å–„äº†è§€çœ‹æ•¸æå–å•é¡Œ")
    else:
        print(f"\nâš ï¸ ä»éœ€å„ªåŒ–ï¼Œä½†å·²æœ‰é€²æ­¥")
    
    print("="*80)

if __name__ == "__main__":
    main()