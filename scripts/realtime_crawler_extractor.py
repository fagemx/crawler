#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¯¦æ™‚çˆ¬èŸ²+æå–å™¨ - æ™ºèƒ½æ»¾å‹•æ”¶é›†URLsä¸¦ç«‹å³é€Jina APIæå–
ç­–ç•¥: çˆ¬å–ä¸€å€‹URL â†’ ç«‹å³é€Jina API â†’ çˆ¬å–ä¸‹ä¸€å€‹URL
å› ç‚ºæ»¾å‹•é€Ÿåº¦æ…¢ï¼Œè‡ªç„¶é¿å…APIé »ç‡é™åˆ¶
"""

import asyncio
import json
import requests
import time
import re
import random
import sys
from datetime import datetime
from typing import Dict, Optional, List, AsyncGenerator
import httpx
from pathlib import Path
from common.config import get_auth_file_path

def safe_print(msg, fallback_msg=None):
    """å®‰å…¨çš„æ‰“å°å‡½æ•¸ï¼Œé¿å…Unicodeç·¨ç¢¼éŒ¯èª¤"""
    try:
        print(msg)
    except UnicodeEncodeError:
        if fallback_msg:
            print(fallback_msg)
        else:
            # ç§»é™¤æ‰€æœ‰éASCIIå­—ç¬¦çš„å®‰å…¨ç‰ˆæœ¬
            ascii_msg = msg.encode('ascii', 'ignore').decode('ascii')
            print(ascii_msg if ascii_msg.strip() else "[ç·¨ç¢¼éŒ¯èª¤ - è¨Šæ¯ç„¡æ³•é¡¯ç¤º]")

class RealtimeCrawlerExtractor:
    """
    å¯¦æ™‚çˆ¬èŸ²+æå–å™¨
    æ™ºèƒ½æ»¾å‹•æ”¶é›†URLsï¼Œæ”¶é›†åˆ°ç«‹å³é€Jina APIæå–
    """
    
    def __init__(self, target_username: str, max_posts: int = 20):
        self.target_username = target_username
        self.max_posts = max_posts
        
        # çˆ¬èŸ²Agentè¨­å®š
        self.agent_url = "http://localhost:8006/v1/playwright/crawl"
        self.auth_file_path = get_auth_file_path(from_project_root=True)
        
        # Jina APIè¨­å®š
        self.official_reader_url = "https://r.jina.ai"
        self.official_headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36',
            'X-Return-Format': 'markdown'
        }
        
        # æœ¬åœ°Readeré…ç½®
        self.local_reader_url = "http://localhost:8880"
        self.local_headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36',
            'x-wait-for-selector': 'article',
            'x-timeout': '25'
        }
        
        # NBSPå­—ç¬¦å’Œæå–æ¨¡å¼
        self.NBSP = "\u00A0"
        self.view_patterns = [
            re.compile(rf'\[Thread[\s{self.NBSP}=]*?(\d+(?:[\.,]\d+)?[KMB]?)\s*views\]', re.IGNORECASE),
            re.compile(rf'Thread[\s{self.NBSP}=]*?(\d+(?:[\.,]\d+)?[KMB]?)[\s{self.NBSP}]*views', re.IGNORECASE | re.MULTILINE),
            re.compile(r'(\d+(?:[\.,]\d+)?[KMB]?)\s*views?', re.IGNORECASE),
            re.compile(r'(\d+(?:[\.,]\d+)?[KMB]?)\s*view(?:s|ing)', re.IGNORECASE),
            re.compile(r'views?\s*[:\-]\s*(\d+(?:[\.,]\d+)?[KMB]?)', re.IGNORECASE),
        ]
        
        # çµæœçµ±è¨ˆ
        self.results = []
        self.start_time = None
        self.api_success_count = 0
        self.api_failure_count = 0
        self.local_success_count = 0
        self.local_failure_count = 0
    
    def normalize_content(self, text: str) -> str:
        """å…§å®¹æ¨™æº–åŒ–"""
        text = text.replace(self.NBSP, " ").replace("\u2002", " ").replace("\u2003", " ")
        text = text.replace("\u2009", " ").replace("\u200A", " ").replace("\u3000", " ").replace("\t", " ")
        text = text.replace("\r\n", "\n").replace("\r", "\n")
        text = re.sub(r"[ \t]{2,}", " ", text)
        return text

    def extract_views_count(self, markdown_content: str, post_id: str = "") -> Optional[str]:
        """è§€çœ‹æ•¸æå–"""
        normalized_content = self.normalize_content(markdown_content)
        
        for pattern in self.view_patterns:
            match = pattern.search(normalized_content)
            if match:
                views_number = match.group(1)
                if self.validate_views_format(views_number):
                    return views_number
        
        for pattern in self.view_patterns:
            match = pattern.search(markdown_content)
            if match:
                views_number = match.group(1)
                if self.validate_views_format(views_number):
                    return views_number
        return None

    def validate_views_format(self, views: str) -> bool:
        """é©—è­‰è§€çœ‹æ•¸æ ¼å¼"""
        if not views: return False
        pattern = re.compile(r'^\d+(?:\.\d+)?[KMB]?$', re.IGNORECASE)
        if not pattern.match(views): return False
        try:
            actual_number = self.convert_to_number(views)
            return 1 <= actual_number <= 100_000_000
        except: return False
    
    def convert_to_number(self, number_str: str) -> int:
        """K/M/Bè½‰æ•¸å­—"""
        number_str = number_str.upper()
        if number_str.endswith('K'): return int(float(number_str[:-1]) * 1000)
        elif number_str.endswith('M'): return int(float(number_str[:-1]) * 1000000)
        elif number_str.endswith('B'): return int(float(number_str[:-1]) * 1000000000)
        else: return int(number_str)

    def extract_post_content(self, content: str) -> Optional[str]:
        """æ™ºèƒ½æå–ä¸»è²¼æ–‡å…§å®¹ - å€åˆ†ä¸»è²¼æ–‡å’Œå›è¦†"""
        lines = content.split('\n')
        
        # ç­–ç•¥1: æŸ¥æ‰¾ä¸»è²¼æ–‡ï¼ˆç¬¬ä¸€å€‹å‡ºç¾çš„å¯¦è³ªå…§å®¹ï¼‰
        main_post_content = self._extract_main_post_from_structure(lines)
        if main_post_content:
            return main_post_content
        
        # ç­–ç•¥2: å›åˆ°åŸå§‹æ–¹æ³•ä½œç‚ºå‚™é¸
        return self._extract_content_fallback(lines)
    
    def _extract_main_post_from_structure(self, lines: List[str]) -> Optional[str]:
        """å¾çµæ§‹åŒ–å…§å®¹ä¸­æå–ä¸»è²¼æ–‡"""
        # æŸ¥æ‰¾æ¨¡å¼ï¼šç”¨æˆ¶å â†’ æ™‚é–“ â†’ ä¸»è²¼æ–‡å…§å®¹ â†’ Translate â†’ æ•¸å­—ï¼ˆäº’å‹•æ•¸æ“šï¼‰
        main_content_candidates = []
        
        for i, line in enumerate(lines):
            stripped = line.strip()
            
            # è·³éæ˜é¡¯çš„å›è¦†æ¨™è­˜
            if stripped.startswith('>>>') or stripped.startswith('å›è¦†') or stripped.startswith('Â·Author'):
                continue
            
            # å°‹æ‰¾ä¸»è²¼æ–‡å…§å®¹çš„æ¨¡å¼
            if (stripped and 
                not stripped.startswith('[') and  # è·³éé€£çµ
                not stripped.startswith('![') and  # è·³éåœ–ç‰‡
                not stripped.startswith('http') and  # è·³éURL
                not stripped.startswith('Log in') and  # è·³éç™»å…¥æç¤º
                not stripped.startswith('Thread') and  # è·³éThreadæ¨™é¡Œ
                not stripped.startswith('gvmonthly') and  # è·³éç”¨æˆ¶å
                not stripped.isdigit() and  # è·³éç´”æ•¸å­—
                not re.match(r'^\d+[dhm]$', stripped) and  # è·³éæ™‚é–“æ ¼å¼
                not stripped in ['Translate', 'views'] and  # è·³éç‰¹æ®Šè©
                len(stripped) > 8):  # å…§å®¹è¦æœ‰ä¸€å®šé•·åº¦
                
                # æª¢æŸ¥é€™æ˜¯å¦å¯èƒ½æ˜¯ä¸»è²¼æ–‡å…§å®¹
                if self._is_likely_main_post_content(stripped, lines, i):
                    main_content_candidates.append(stripped)
        
        # è¿”å›ç¬¬ä¸€å€‹åˆç†çš„ä¸»è²¼æ–‡å€™é¸
        if main_content_candidates:
            return main_content_candidates[0]
        
        return None
    
    def _is_likely_main_post_content(self, content: str, lines: List[str], index: int) -> bool:
        """åˆ¤æ–·å…§å®¹æ˜¯å¦å¯èƒ½æ˜¯ä¸»è²¼æ–‡"""
        # æª¢æŸ¥å¾ŒçºŒæ˜¯å¦æœ‰ "Translate" æ¨™è­˜ï¼ˆä¸»è²¼æ–‡çš„å…¸å‹çµæ§‹ï¼‰
        for j in range(index + 1, min(index + 3, len(lines))):
            if 'Translate' in lines[j]:
                return True
        
        # æª¢æŸ¥æ˜¯å¦åŒ…å«å¸¸è¦‹çš„ä¸»è²¼æ–‡ç‰¹å¾µ
        if (len(content) > 15 and  # æœ‰ä¸€å®šé•·åº¦
            not content.startswith('>>>') and  # ä¸æ˜¯å›è¦†
            not content.startswith('Â·') and  # ä¸æ˜¯å…ƒæ•¸æ“š
            '!' in content or '?' in content or 'ã€‚' in content or 'ï¼Œ' in content):  # åŒ…å«æ¨™é»ç¬¦è™Ÿ
            return True
        
        return False
    
    def _extract_content_fallback(self, lines: List[str]) -> Optional[str]:
        """å‚™é¸å…§å®¹æå–æ–¹æ³•"""
        content_start = -1
        for i, line in enumerate(lines):
            if 'Markdown Content:' in line:
                content_start = i + 1
                break
        
        if content_start == -1:
            return None
        
        content_lines = []
        for i in range(content_start, min(content_start + 15, len(lines))):
            line = lines[i].strip()
            if (line and 
                not line.startswith('[![Image') and 
                not line.startswith('[Image') and
                not line.startswith('>>>')):  # æ’é™¤å›è¦†
                content_lines.append(line)
                
                # å¦‚æœæ‰¾åˆ°äº†åˆç†çš„å…§å®¹å°±åœæ­¢
                if len(content_lines) >= 2 and len(line) > 10:
                    break
        
        return '\n'.join(content_lines) if content_lines else None

    def extract_engagement_numbers(self, markdown_content: str) -> List[str]:
        """æå–æ‰€æœ‰çµ±è¨ˆæ•¸å­—åºåˆ—ï¼ˆæŒ‰è®šã€ç•™è¨€ã€è½‰ç™¼ã€åˆ†äº«ï¼‰"""
        lines = markdown_content.split('\n')
        
        # ç­–ç•¥1: æŸ¥æ‰¾è²¼æ–‡å…§å®¹å¾Œçš„ç¬¬ä¸€å€‹åœ–ç‰‡ï¼Œç„¶å¾Œæå–å¾ŒçºŒæ•¸å­—
        for i, line in enumerate(lines):
            stripped = line.strip()
            # æ‰¾åˆ°è²¼æ–‡åœ–ç‰‡ï¼ˆé€šå¸¸åœ¨Translateä¹‹å¾Œï¼‰
            if stripped.startswith('![Image') and not 'profile picture' in stripped:
                numbers = []
                # åœ¨é€™å€‹åœ–ç‰‡å¾ŒæŸ¥æ‰¾é€£çºŒçš„æ•¸å­—
                for j in range(i + 1, min(i + 20, len(lines))):
                    candidate = lines[j].strip()
                    if re.match(r'^\d+(?:\.\d+)?[KMB]?$', candidate):
                        numbers.append(candidate)
                    elif candidate and not re.match(r'^\d+(?:\.\d+)?[KMB]?$', candidate) and candidate != "Pinned":
                        # é‡åˆ°éæ•¸å­—è¡Œï¼ˆä½†è·³éPinnedï¼‰ï¼Œåœæ­¢æ”¶é›†
                        break
                
                # å¦‚æœæ‰¾åˆ°äº†æ•¸å­—åºåˆ—ï¼Œè¿”å›
                if len(numbers) >= 3:
                    return numbers
        
        # ç­–ç•¥2: å¦‚æœç­–ç•¥1å¤±æ•—ï¼ŒæŸ¥æ‰¾ä»»ä½•é€£çºŒçš„æ•¸å­—åºåˆ—
        all_numbers = []
        for i, line in enumerate(lines):
            stripped = line.strip()
            if re.match(r'^\d+(?:\.\d+)?[KMB]?$', stripped):
                # æª¢æŸ¥å‰å¾Œæ–‡ï¼Œç¢ºä¿é€™æ˜¯çµ±è¨ˆæ•¸å­—
                context_valid = False
                # æª¢æŸ¥å‰é¢10è¡Œæ˜¯å¦æœ‰åœ–ç‰‡æˆ–è²¼æ–‡å…§å®¹
                for k in range(max(0, i-10), i):
                    if "![Image" in lines[k] or "Translate" in lines[k]:
                        context_valid = True
                        break
                
                if context_valid:
                    all_numbers.append(stripped)
        
        # å¦‚æœæ‰¾åˆ°4å€‹æˆ–æ›´å¤šæ•¸å­—ï¼Œå–å‰4å€‹
        if len(all_numbers) >= 4:
            return all_numbers[:4]
        elif len(all_numbers) >= 3:
            return all_numbers[:3]
        
        return all_numbers

    def extract_likes_count(self, markdown_content: str) -> Optional[str]:
        """æå–æŒ‰è®šæ•¸"""
        lines = markdown_content.split('\n')
        
        # æ–¹æ³•1: æŸ¥æ‰¾"Like"æ¨™ç±¤å¾Œçš„æ•¸å­—ï¼ˆèˆŠæ ¼å¼ï¼‰
        for i, line in enumerate(lines):
            if line.strip() == "Like" and i + 2 < len(lines):
                next_line = lines[i + 2].strip()
                if re.match(r'^\d+(?:\.\d+)?[KMB]?$', next_line):
                    return next_line
        
        # æ–¹æ³•2: æ–°æ ¼å¼ - å¾æ•¸å­—åºåˆ—ä¸­å–ç¬¬ä¸€å€‹
        numbers = self.extract_engagement_numbers(markdown_content)
        if len(numbers) >= 1:
            return numbers[0]
        
        return None

    def extract_comments_count(self, markdown_content: str) -> Optional[str]:
        """æå–ç•™è¨€æ•¸"""
        lines = markdown_content.split('\n')
        
        # æ–¹æ³•1: èˆŠæ ¼å¼ - æŸ¥æ‰¾Commentæ¨™ç±¤
        for i, line in enumerate(lines):
            if line.strip() == "Comment" and i + 2 < len(lines):
                next_line = lines[i + 2].strip()
                if re.match(r'^\d+(?:\.\d+)?[KMB]?$', next_line):
                    return next_line
        
        # æ–¹æ³•2: æ–°æ ¼å¼ - å¾æ•¸å­—åºåˆ—ä¸­å–ç¬¬äºŒå€‹
        numbers = self.extract_engagement_numbers(markdown_content)
        if len(numbers) >= 2:
            return numbers[1]
        
        return None

    def extract_reposts_count(self, markdown_content: str) -> Optional[str]:
        """æå–è½‰ç™¼æ•¸"""
        lines = markdown_content.split('\n')
        
        # æ–¹æ³•1: èˆŠæ ¼å¼ - æŸ¥æ‰¾Repostæ¨™ç±¤
        for i, line in enumerate(lines):
            if line.strip() == "Repost" and i + 2 < len(lines):
                next_line = lines[i + 2].strip()
                if re.match(r'^\d+(?:\.\d+)?[KMB]?$', next_line):
                    return next_line
        
        # æ–¹æ³•2: æ–°æ ¼å¼ - å¾æ•¸å­—åºåˆ—ä¸­å–ç¬¬ä¸‰å€‹
        numbers = self.extract_engagement_numbers(markdown_content)
        if len(numbers) >= 3:
            return numbers[2]
        
        return None

    def extract_shares_count(self, markdown_content: str) -> Optional[str]:
        """æå–åˆ†äº«æ•¸"""
        lines = markdown_content.split('\n')
        
        # æ–¹æ³•1: èˆŠæ ¼å¼ - æŸ¥æ‰¾Shareæ¨™ç±¤
        for i, line in enumerate(lines):
            if line.strip() == "Share" and i + 2 < len(lines):
                next_line = lines[i + 2].strip()
                if re.match(r'^\d+(?:\.\d+)?[KMB]?$', next_line):
                    return next_line
        
        # æ–¹æ³•2: æ–°æ ¼å¼ - å¾æ•¸å­—åºåˆ—ä¸­å–ç¬¬å››å€‹
        numbers = self.extract_engagement_numbers(markdown_content)
        if len(numbers) >= 4:
            return numbers[3]
        
        return None

    def fetch_content_jina_api(self, url: str) -> tuple:
        """å¾Jina APIç²å–å…§å®¹"""
        try:
            response = requests.get(f"{self.official_reader_url}/{url}", headers=self.official_headers, timeout=60)
            if response.status_code == 200:
                return True, response.text
            else:
                return False, f"HTTP {response.status_code}"
        except Exception as e:
            return False, str(e)
    
    def fetch_content_local(self, url: str, use_cache: bool = True, max_retries: int = 2) -> tuple:
        """ä½¿ç”¨æœ¬åœ°Readerç²å–å…§å®¹ - å¿«é€Ÿé‡è©¦æ©Ÿåˆ¶"""
        headers = self.local_headers.copy()
        if not use_cache: 
            headers['x-no-cache'] = 'true'
        
        for attempt in range(max_retries + 1):
            try:
                # é™ä½timeoutï¼Œå¿«é€Ÿå¤±æ•—
                timeout = 15 if attempt == 0 else 10  # ç¬¬ä¸€æ¬¡15sï¼Œé‡è©¦10s
                response = requests.get(f"{self.local_reader_url}/{url}", headers=headers, timeout=timeout)
                if response.status_code == 200:
                    return True, response.text
                else:
                    if attempt < max_retries:
                        continue  # é‡è©¦
                    return False, f"HTTP {response.status_code}"
            except Exception as e:
                if attempt < max_retries:
                    # çŸ­æš«ç­‰å¾…å¾Œé‡è©¦
                    import time
                    time.sleep(0.5)
                    continue
                return False, f"æœ€çµ‚å¤±æ•—: {str(e)}"
        
        return False, "é‡è©¦è€—ç›¡"
    
    def fetch_content_local_fast(self, url: str) -> tuple:
        """å¿«é€Ÿæœ¬åœ°Reader - å°ˆé–€ç‚ºå›é€€è¨­è¨ˆ"""
        import concurrent.futures
        
        def try_single_request(instance_id):
            """å˜—è©¦å–®å€‹Readerå¯¦ä¾‹"""
            headers = self.local_headers.copy()
            headers['x-no-cache'] = 'true'  # å¼·åˆ¶ç„¡å¿«å–
            try:
                # è¶…çŸ­timeoutï¼Œå¿«é€Ÿå¤±æ•—
                response = requests.get(f"{self.local_reader_url}/{url}", headers=headers, timeout=8)
                return (True, response.text, instance_id) if response.status_code == 200 else (False, f"HTTP {response.status_code}", instance_id)
            except Exception as e:
                return (False, str(e), instance_id)
        
        # å¹³è¡Œå˜—è©¦å¤šå€‹å¯¦ä¾‹ï¼ˆæ¨¡æ“¬è² è¼‰å‡è¡¡ï¼‰
        with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
            # æäº¤2å€‹å¹³è¡Œè«‹æ±‚ï¼ˆæ¨¡æ“¬é‡è©¦ï¼‰
            futures = [executor.submit(try_single_request, i) for i in range(2)]
            
            # ä½¿ç”¨as_completedï¼Œèª°å…ˆå®Œæˆç”¨èª°
            for future in concurrent.futures.as_completed(futures, timeout=12):
                try:
                    success, content, instance_id = future.result()
                    if success:
                        # å–æ¶ˆå…¶ä»–æ­£åœ¨åŸ·è¡Œçš„è«‹æ±‚
                        for f in futures:
                            f.cancel()
                        return True, content
                except Exception:
                    continue
        
        return False, "æ‰€æœ‰å¹³è¡Œè«‹æ±‚å¤±æ•—"
    
    def parse_post(self, url: str, content: str) -> Dict:
        """è§£æè²¼æ–‡ - å®Œæ•´ç‰ˆæœ¬åŒ…å«äº’å‹•æ•¸æ“š"""
        post_id = url.split('/')[-1] if '/' in url else url
        views = self.extract_views_count(content, post_id)
        main_content = self.extract_post_content(content)
        
        # æå–äº’å‹•æ•¸æ“š
        likes = self.extract_likes_count(content)
        comments = self.extract_comments_count(content)
        reposts = self.extract_reposts_count(content)
        shares = self.extract_shares_count(content)
        
        return {
            'post_id': post_id,
            'url': url,
            'views': views,
            'content': main_content,
            'likes': likes,
            'comments': comments,
            'reposts': reposts,
            'shares': shares,
            'source': 'jina_api',
            'has_views': views is not None,
            'has_content': main_content is not None,
            'has_likes': likes is not None,
            'has_comments': comments is not None,
            'has_reposts': reposts is not None,
            'has_shares': shares is not None,
            'content_length': len(content),
            'extracted_at': datetime.now().isoformat()
        }

    async def collect_urls_only(self) -> List[str]:
        """ç›´æ¥ä½¿ç”¨Playwrighté€²è¡Œç´”URLæ”¶é›†ï¼Œä¸ç¶“éAgent API"""
        from playwright.async_api import async_playwright
        
        # æª¢æŸ¥èªè­‰æª”æ¡ˆ
        if not self.auth_file_path.exists():
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°èªè­‰æª”æ¡ˆ '{self.auth_file_path}'")
        
        # è®€å–èªè­‰å…§å®¹
        with open(self.auth_file_path, "r", encoding="utf-8") as f:
            auth_content = json.load(f)
        
        print(f"ğŸ”§ é–‹å§‹ç›´æ¥Playwrightæ»¾å‹•æ”¶é›†URLs @{self.target_username}")
        print(f"ğŸ¯ ç›®æ¨™æ•¸é‡: {self.max_posts} å€‹URLs")
        print("ğŸ“‹ æ¨¡å¼: ç´”URLæ”¶é›†ï¼Œè·³éæ‰€æœ‰è©³ç´°è™•ç†")
        
        urls = []
        
        async with async_playwright() as p:
            try:
                # å•Ÿå‹•ç€è¦½å™¨
                browser = await p.chromium.launch(headless=True)
                context = await browser.new_context()
                
                # æ³¨å…¥èªè­‰ç‹€æ…‹
                await context.add_cookies(auth_content.get('cookies', []))
                local_storage = auth_content.get('localStorage', [])
                session_storage = auth_content.get('sessionStorage', [])
                
                # å‰µå»ºé é¢ä¸¦å‰å¾€ç›®æ¨™
                page = await context.new_page()
                await page.goto(f"https://www.threads.com/@{self.target_username}")
                await asyncio.sleep(3)  # ç­‰å¾…é é¢è¼‰å…¥
                
                # æ³¨å…¥å„²å­˜ç‹€æ…‹
                if local_storage:
                    for item in local_storage:
                        await page.evaluate(f"localStorage.setItem('{item['name']}', '{item['value']}')")
                if session_storage:
                    for item in session_storage:
                        await page.evaluate(f"sessionStorage.setItem('{item['name']}', '{item['value']}')")
                
                await page.reload()
                await asyncio.sleep(2)
                
                print("ğŸ”„ é–‹å§‹æ™ºèƒ½æ»¾å‹•æ”¶é›†URLs...")
                
                # å¢å¼·çš„æ»¾å‹•æ”¶é›†é‚è¼¯
                collected_count = 0
                scroll_rounds = 0
                max_scroll_rounds = 80  # å¤§å¹…å¢åŠ æœ€å¤§æ»¾å‹•æ¬¡æ•¸
                no_new_content_rounds = 0  # é€£çºŒç„¡æ–°å…§å®¹çš„è¼ªæ¬¡
                max_no_new_rounds = 15  # å¢åŠ é€£çºŒç„¡æ–°å…§å®¹çš„æœ€å¤§å®¹å¿è¼ªæ¬¡ï¼ˆæ›´å¤šè€å¿ƒï¼‰
                last_urls_count = 0
                
                while collected_count < self.max_posts and scroll_rounds < max_scroll_rounds:
                    # æå–ç•¶å‰é é¢çš„URLsï¼ˆéæ¿¾ç„¡æ•ˆURLsï¼‰
                    current_urls = await page.evaluate("""
                        () => {
                            const links = Array.from(document.querySelectorAll('a[href*="/post/"]'));
                            return [...new Set(links.map(link => link.href)
                                .filter(url => url.includes('/post/'))
                                .filter(url => {
                                    const postId = url.split('/post/')[1];
                                    // éæ¿¾æ‰ mediaã€ç„¡æ•ˆIDç­‰
                                    return postId && 
                                           postId !== 'media' && 
                                           postId.length > 5 && 
                                           /^[A-Za-z0-9_-]+$/.test(postId);
                                }))];
                        }
                    """)
                    
                    before_count = len(urls)
                    
                    # å»é‡ä¸¦æ·»åŠ æ–°URLs
                    for url in current_urls:
                        if url not in urls and len(urls) < self.max_posts:
                            urls.append(url)
                            collected_count = len(urls)
                            print(f"   ğŸ“ [{collected_count}] ç™¼ç¾: {url.split('/')[-1]}")
                    
                    # æª¢æŸ¥æ˜¯å¦æœ‰æ–°å…§å®¹
                    new_urls_found = len(urls) - before_count
                    
                    if new_urls_found == 0:
                        no_new_content_rounds += 1
                        print(f"   â³ ç¬¬{scroll_rounds+1}è¼ªæœªç™¼ç¾æ–°URL ({no_new_content_rounds}/{max_no_new_rounds})")
                        
                        # éå¢ç­‰å¾…æ™‚é–“ï¼ˆåŠ å…¥éš¨æ©Ÿæ€§ï¼Œé™åˆ¶æœ€å¤§3.5ç§’ï¼‰
                        base_wait = min(1.2 + (no_new_content_rounds - 1) * 0.3, 3.5)  # 1.2s -> 3.5s
                        random_factor = random.uniform(0.8, 1.2)  # Â±20%éš¨æ©Ÿè®ŠåŒ–
                        progressive_wait = base_wait * random_factor
                        print(f"   â²ï¸ éå¢ç­‰å¾… {progressive_wait:.1f}s...")
                        await asyncio.sleep(progressive_wait)
                        
                        if no_new_content_rounds >= max_no_new_rounds:
                            print(f"   ğŸ›‘ é€£çºŒ{max_no_new_rounds}è¼ªç„¡æ–°å…§å®¹ï¼Œå¯èƒ½å·²åˆ°é”åº•éƒ¨")
                            
                            # æœ€å¾Œå˜—è©¦ï¼šå¤šé‡æ¿€é€²æ»¾å‹•æ¿€ç™¼è¼‰å…¥
                            print("   ğŸš€ æœ€å¾Œå˜—è©¦ï¼šå¤šé‡æ¿€é€²æ»¾å‹•æ¿€ç™¼æ–°å…§å®¹...")
                            
                            # ç¬¬ä¸€æ¬¡ï¼šå¤§å¹…å‘ä¸‹
                            await page.evaluate("window.scrollBy(0, 2500)")
                            await asyncio.sleep(2)
                            
                            # ç¬¬äºŒæ¬¡ï¼šå‘ä¸Šå†å‘ä¸‹ï¼ˆæ¿€ç™¼è¼‰å…¥ï¼‰
                            await page.evaluate("window.scrollBy(0, -500)")
                            await asyncio.sleep(1)
                            await page.evaluate("window.scrollBy(0, 3000)")
                            await asyncio.sleep(3)
                            
                            # ç¬¬ä¸‰æ¬¡ï¼šæ»¾å‹•åˆ°æ›´åº•éƒ¨
                            await page.evaluate("window.scrollBy(0, 2000)")
                            await asyncio.sleep(2)
                            
                            print("   â³ ç­‰å¾…æ‰€æœ‰å…§å®¹è¼‰å…¥å®Œæˆ...")
                            await asyncio.sleep(3)
                            
                            # å†æ¬¡æª¢æŸ¥ï¼ˆéæ¿¾ç„¡æ•ˆURLsï¼‰
                            final_urls = await page.evaluate("""
                                () => {
                                    const links = Array.from(document.querySelectorAll('a[href*="/post/"]'));
                                    return [...new Set(links.map(link => link.href)
                                        .filter(url => url.includes('/post/'))
                                        .filter(url => {
                                            const postId = url.split('/post/')[1];
                                            // éæ¿¾æ‰ mediaã€ç„¡æ•ˆIDç­‰
                                            return postId && 
                                                   postId !== 'media' && 
                                                   postId.length > 5 && 
                                                   /^[A-Za-z0-9_-]+$/.test(postId);
                                        }))];
                                }
                            """)
                            
                            final_new_count = 0
                            for url in final_urls:
                                if url not in urls and len(urls) < self.max_posts:
                                    urls.append(url)
                                    collected_count = len(urls)
                                    final_new_count += 1
                                    print(f"   ğŸ“ [{collected_count}] æœ€å¾Œç™¼ç¾: {url.split('/')[-1]}")
                            
                            if final_new_count == 0:
                                print("   âœ… ç¢ºèªå·²åˆ°é”é é¢åº•éƒ¨")
                                break
                            else:
                                print(f"   ğŸ¯ æœ€å¾Œå˜—è©¦ç™¼ç¾{final_new_count}å€‹æ–°URLï¼Œç¹¼çºŒ...")
                                no_new_content_rounds = 0
                    else:
                        no_new_content_rounds = 0  # é‡ç½®è¨ˆæ•¸å™¨
                        print(f"   âœ… ç¬¬{scroll_rounds+1}è¼ªç™¼ç¾{new_urls_found}å€‹æ–°URL")
                    
                    if collected_count >= self.max_posts:
                        print(f"   ğŸ¯ å·²é”åˆ°ç›®æ¨™æ•¸é‡ {self.max_posts}")
                        break
                    
                    # å„ªåŒ–çš„äººæ€§åŒ–æ»¾å‹•ç­–ç•¥
                    if scroll_rounds % 6 == 5:  # æ¯6è¼ªé€²è¡Œä¸€æ¬¡æ¿€é€²æ»¾å‹•ï¼ˆæé«˜é »ç‡ï¼‰
                        print("   ğŸš€ åŸ·è¡Œæ¿€é€²æ»¾å‹•æ¿€ç™¼è¼‰å…¥...")
                        # æ¨¡æ“¬ç”¨æˆ¶å¿«é€Ÿæ»¾å‹•è¡Œç‚º
                        await page.evaluate("window.scrollBy(0, 1600)")
                        await asyncio.sleep(1.2)
                        # ç¨å¾®å›æ»¾ï¼ˆåƒç”¨æˆ¶æ»¾éé ­äº†ï¼‰
                        await page.evaluate("window.scrollBy(0, -250)")
                        await asyncio.sleep(0.8)
                        # å†ç¹¼çºŒå‘ä¸‹
                        await page.evaluate("window.scrollBy(0, 1400)")
                        await asyncio.sleep(3.5)
                        
                    elif scroll_rounds % 3 == 2:  # æ¯3è¼ªé€²è¡Œä¸€æ¬¡ä¸­åº¦æ»¾å‹•
                        print("   ğŸ”„ åŸ·è¡Œä¸­åº¦æ»¾å‹•...")
                        # åˆ†æ®µæ»¾å‹•ï¼Œæ›´åƒäººé¡è¡Œç‚º
                        await page.evaluate("window.scrollBy(0, 800)")
                        await asyncio.sleep(1)
                        await page.evaluate("window.scrollBy(0, 600)")
                        await asyncio.sleep(2.8)
                        
                    else:
                        # æ­£å¸¸æ»¾å‹•ï¼ŒåŠ å…¥éš¨æ©Ÿæ€§å’Œäººæ€§åŒ–
                        scroll_distance = 900 + (scroll_rounds % 3) * 100  # 900-1100pxéš¨æ©Ÿ
                        await page.evaluate(f"window.scrollBy(0, {scroll_distance})")
                        
                        # çŸ­æš«æš«åœï¼ˆæ¨¡æ“¬ç”¨æˆ¶é–±è®€ï¼‰
                        await asyncio.sleep(1.8 + (scroll_rounds % 2) * 0.4)  # 1.8-2.2ç§’éš¨æ©Ÿ
                    
                    # çµ±ä¸€çš„è¼‰å…¥æª¢æ¸¬ï¼ˆæ‰€æœ‰æ»¾å‹•å¾Œéƒ½æª¢æŸ¥ï¼‰
                    has_loading = await page.evaluate("""
                        () => {
                            const indicators = document.querySelectorAll('[role="progressbar"], .loading, [aria-label*="loading"], [aria-label*="Loading"]');
                            return indicators.length > 0;
                        }
                    """)
                    
                    if has_loading:
                        print("   â³ æª¢æ¸¬åˆ°è¼‰å…¥æŒ‡ç¤ºå™¨ï¼Œé¡å¤–ç­‰å¾…...")
                        # éš¨æ©Ÿç­‰å¾…2-3.5ç§’
                        loading_wait = random.uniform(2.0, 3.5)
                        await asyncio.sleep(loading_wait)
                    
                    scroll_rounds += 1
                    
                    # æ¯5è¼ªé¡¯ç¤ºé€²åº¦
                    if scroll_rounds % 5 == 0:
                        print(f"   ğŸ“Š æ»¾å‹•é€²åº¦: ç¬¬{scroll_rounds}è¼ªï¼Œå·²æ”¶é›†{collected_count}å€‹URL")
                
                if scroll_rounds >= max_scroll_rounds:
                    print(f"   âš ï¸ é”åˆ°æœ€å¤§æ»¾å‹•è¼ªæ¬¡ ({max_scroll_rounds})ï¼Œåœæ­¢æ»¾å‹•")
                
                await browser.close()
                
                print(f"âœ… URLæ”¶é›†å®Œæˆï¼Œå…±æ”¶é›†åˆ° {len(urls)} å€‹URL")
                return urls[:self.max_posts]  # ç¢ºä¿ä¸è¶…éç›®æ¨™æ•¸é‡
                
            except Exception as e:
                print(f"âŒ Playwright URLæ”¶é›†éŒ¯èª¤: {e}")
                if 'browser' in locals():
                    await browser.close()
                return []

    async def process_url_realtime(self, url: str, index: int, total: int) -> Optional[Dict]:
        """å¯¦æ™‚è™•ç†å–®å€‹URL"""
        post_id = url.split('/')[-1]
        
        print(f"ğŸŒ [{index}/{total}] é€Jina API: {post_id}...", end=" ")
        
        # é€Jina APIæå–
        success, content = self.fetch_content_jina_api(url)
        
        if success:
            result = self.parse_post(url, content)
            if result['has_views']:
                self.api_success_count += 1
                print(f"âœ… ({result['views']})")
                return result
            else:
                self.api_failure_count += 1
                print(f"âŒ ç„¡è§€çœ‹æ•¸")
                return result
        else:
            # APIå¤±æ•—ï¼Œå¿«é€Ÿå›é€€åˆ°æœ¬åœ°Reader
            print(f"âŒ APIå¤±æ•—: {content}")
            print(f"   ğŸ”„ å¿«é€Ÿå›é€€æœ¬åœ°Reader: {post_id}...", end=" ")
            
            # ä½¿ç”¨å¿«é€Ÿé‡è©¦çš„æœ¬åœ°Reader
            local_success, local_content = self.fetch_content_local_fast(url)
            
            if local_success:
                local_result = self.parse_post(url, local_content)
                local_result['source'] = 'local_reader_fallback'
                if local_result['has_views']:
                    self.local_success_count += 1
                    print(f"âœ… æœ¬åœ°æˆåŠŸ ({local_result['views']})")
                    return local_result
                else:
                    self.local_failure_count += 1
                    print(f"âŒ æœ¬åœ°ç„¡è§€çœ‹æ•¸")
                    return local_result
            else:
                self.local_failure_count += 1
                print(f"âŒ æœ¬åœ°ä¹Ÿå¤±æ•—: {local_content}")
                self.api_failure_count += 1  # ä»ç„¶è¨ˆå…¥APIå¤±æ•—
                return {
                    'post_id': post_id,
                    'url': url,
                    'api_error': content,
                    'local_error': local_content,
                    'source': 'both_failed',
                    'has_views': False,
                    'extracted_at': datetime.now().isoformat()
                }

    async def run_realtime_extraction(self):
        """åŸ·è¡Œå¯¦æ™‚çˆ¬å–+æå–"""
        self.start_time = time.time()
        self.url_collection_time = 0  # åˆå§‹åŒ–URLæ”¶é›†æ™‚é–“
        
        safe_print("ğŸš€ å¯¦æ™‚çˆ¬èŸ²+æå–å™¨å•Ÿå‹•", "[å¯¦æ™‚çˆ¬èŸ²+æå–å™¨å•Ÿå‹•]")
        safe_print("ç­–ç•¥: æ™ºèƒ½æ»¾å‹•æ”¶é›†URLs â†’ æŒ‰é †åºé€Jina API", "ç­–ç•¥: æ™ºèƒ½æ»¾å‹•æ”¶é›†URLs -> æŒ‰é †åºé€Jina API")
        print("=" * 80)
        
        # ç¬¬ä¸€éšæ®µï¼šæ”¶é›†æ‰€æœ‰URLsï¼ˆå¿«é€Ÿï¼‰
        url_collection_start = time.time()
        urls = await self.collect_urls_only()
        url_collection_time = time.time() - url_collection_start
        
        if not urls:
            safe_print("âŒ æ²’æœ‰æ”¶é›†åˆ°ä»»ä½•URL", "[X] æ²’æœ‰æ”¶é›†åˆ°ä»»ä½•URL")
            return

        safe_print(f"âœ… URLæ”¶é›†å®Œæˆï¼æ”¶é›†åˆ° {len(urls)} å€‹URL", f"[OK] URLæ”¶é›†å®Œæˆï¼æ”¶é›†åˆ° {len(urls)} å€‹URL")
        safe_print(f"â±ï¸ URLæ”¶é›†è€—æ™‚: {url_collection_time:.1f}s", f"[æ™‚é–“] URLæ”¶é›†è€—æ™‚: {url_collection_time:.1f}s")
        safe_print(f"ğŸï¸ æ”¶é›†é€Ÿåº¦: {len(urls)/url_collection_time:.2f} URL/s", f"[é€Ÿåº¦] æ”¶é›†é€Ÿåº¦: {len(urls)/url_collection_time:.2f} URL/s")
        
        # ä¿å­˜URLæ”¶é›†æ™‚é–“
        self.url_collection_time = url_collection_time
        
        safe_print(f"\nğŸ”„ ç¬¬äºŒéšæ®µï¼šä½¿ç”¨è¼ªè¿´ç­–ç•¥å¿«é€Ÿæå– {len(urls)} å€‹URL...", f"\n[è™•ç†] ç¬¬äºŒéšæ®µï¼šä½¿ç”¨è¼ªè¿´ç­–ç•¥å¿«é€Ÿæå– {len(urls)} å€‹URL...")
        print("ç­–ç•¥: 10å€‹API â†’ 20å€‹æœ¬åœ° â†’ è¼ªè¿´ï¼Œé¿å…APIé˜»æ“‹")
        print("=" * 60)
        
        # å°å…¥rotationç­–ç•¥
        try:
            from test_reader_rotation import RotationPipelineReader
            
            # å‰µå»ºrotationå¯¦ä¾‹
            rotation_reader = RotationPipelineReader()
            
            # æº–å‚™URLsï¼ˆç¢ºä¿æ ¼å¼æ­£ç¢ºï¼‰
            formatted_urls = []
            for url in urls:
                if url.startswith('http'):
                    formatted_urls.append(url)
                else:
                    formatted_urls.append(f"https://www.threads.net/t/{url}")
            
            print(f"ğŸ”„ é–‹å§‹è¼ªè¿´ç­–ç•¥è™•ç†...")
            
            # åŸ·è¡Œrotationç­–ç•¥
            rotation_results = rotation_reader.rotation_pipeline(formatted_urls)
            
            # rotation_results æ˜¯ä¸€å€‹listï¼Œç›´æ¥ä½¿ç”¨ä¸¦ä¿®æ­£æ ¼å¼
            if isinstance(rotation_results, list):
                # ä¿®æ­£çµæœæ ¼å¼ï¼Œç¢ºä¿åŒ…å«extracted_atå’Œæ­£ç¢ºçš„sourceæ ¼å¼
                fixed_results = []
                for r in rotation_results:
                    fixed_result = r.copy()
                    
                    # çµ±ä¸€sourceæ ¼å¼
                    if 'API-æ‰¹æ¬¡' in r.get('source', ''):
                        fixed_result['source'] = 'jina_api'
                    elif 'æœ¬åœ°-æ‰¹æ¬¡' in r.get('source', ''):
                        fixed_result['source'] = 'local_reader'
                    elif 'API-å›é€€' in r.get('source', ''):
                        fixed_result['source'] = 'local_reader_fallback'
                    
                    # ç¢ºä¿æœ‰extracted_at
                    if 'extracted_at' not in fixed_result:
                        fixed_result['extracted_at'] = datetime.now().isoformat()
                    
                    # å¦‚æœcontentç‚ºNoneä½†æœ‰content_lengthï¼Œå˜—è©¦é‡æ–°æå–ï¼ˆè‡¨æ™‚æ–¹æ¡ˆï¼‰
                    if not fixed_result.get('content') and fixed_result.get('content_length', 0) > 0:
                        print(f"   âš ï¸ {fixed_result['post_id']}: æ–‡å­—å…§å®¹éºå¤±ï¼Œæ¨™è¨˜ç‚ºç„¡å…§å®¹")
                        fixed_result['has_content'] = False
                    
                    fixed_results.append(fixed_result)
                
                self.results = fixed_results
                
                # çµ±è¨ˆrotationçµæœ
                self.api_success_count = len([r for r in fixed_results if r.get('source') == 'jina_api' and r.get('has_views')])
                self.local_success_count = len([r for r in fixed_results if r.get('source') in ['local_reader', 'local_reader_fallback'] and r.get('has_views')])
                self.api_failure_count = len([r for r in fixed_results if r.get('source') == 'jina_api' and not r.get('has_views')])
                self.local_failure_count = len([r for r in fixed_results if r.get('source') in ['local_reader', 'local_reader_fallback'] and not r.get('has_views')])
            else:
                # å¦‚æœæ˜¯dictæ ¼å¼ï¼ˆèˆŠç‰ˆæœ¬ï¼‰
                self.results = rotation_results.get('results', [])
                summary = rotation_results.get('summary', {})
                self.api_success_count = summary.get('api_batch_success', 0)
                self.local_success_count = summary.get('local_batch_success', 0) + summary.get('api_fallback_success', 0)
                self.api_failure_count = summary.get('api_batch_failure', 0)
                self.local_failure_count = summary.get('local_batch_failure', 0)
            
            print("âœ… è¼ªè¿´ç­–ç•¥æå–å®Œæˆï¼")
            
        except ImportError as e:
            print(f"âŒ ç„¡æ³•å°å…¥rotationç­–ç•¥: {e}")
            print("ğŸ”„ æ”¹ç”¨åŸå§‹é€ä¸€è™•ç†æ–¹å¼...")
            
            # å›é€€åˆ°åŸå§‹æ–¹å¼
            for i, url in enumerate(urls):
                result = await self.process_url_realtime(url, i+1, len(urls))
                if result:
                    self.results.append(result)
                
                elapsed = time.time() - self.start_time
                success_rate = self.api_success_count / (i+1) * 100
                
                print(f"   ğŸ“Š é€²åº¦: {i+1}/{len(urls)} | æˆåŠŸç‡: {success_rate:.1f}% | è€—æ™‚: {elapsed:.1f}s")
                
                if i < len(urls) - 1:
                    sleep_time = random.choice([1.0, 1.5, 2.0])
                    print(f"   â¸ï¸ ç­‰å¾… {sleep_time}s å¾Œè™•ç†ä¸‹ä¸€å€‹...")
                    await asyncio.sleep(sleep_time)
        
        # æœ€çµ‚çµ±è¨ˆ
        self.show_final_statistics()

    def show_final_statistics(self):
        """é¡¯ç¤ºæœ€çµ‚çµ±è¨ˆ"""
        total_time = time.time() - self.start_time
        total_processed = len(self.results)
        
        print("\n" + "=" * 80)
        print("ğŸ å¯¦æ™‚çˆ¬å–+æå–å®Œæˆ")
        print("=" * 80)
        
        print(f"ğŸ“Š è™•ç†çµ±è¨ˆ:")
        print(f"   - ç¸½è™•ç†æ•¸: {total_processed}")
        print(f"   - ğŸŒ APIæˆåŠŸ: {self.api_success_count} | APIå¤±æ•—: {self.api_failure_count}")
        print(f"   - âš¡ æœ¬åœ°æˆåŠŸ: {self.local_success_count} | æœ¬åœ°å¤±æ•—: {self.local_failure_count}")
        total_success = self.api_success_count + self.local_success_count
        print(f"   - ğŸ“ˆ æ•´é«”æˆåŠŸç‡: {total_success/total_processed*100:.1f}%" if total_processed > 0 else "   - æ•´é«”æˆåŠŸç‡: N/A")
        
        print(f"â±ï¸ æ™‚é–“çµ±è¨ˆ:")
        extraction_time = total_time - getattr(self, 'url_collection_time', 0)
        print(f"   - ğŸ“¡ URLæ”¶é›†: {getattr(self, 'url_collection_time', 0):.1f}s")
        print(f"   - ğŸ”„ å…§å®¹æå–: {extraction_time:.1f}s") 
        print(f"   - ğŸ ç¸½è€—æ™‚: {total_time:.1f}s")
        print(f"   - ğŸ“Š æ”¶é›†é€Ÿåº¦: {total_processed/getattr(self, 'url_collection_time', 1):.2f} URL/s (æ”¶é›†)")
        print(f"   - ğŸš€ æå–é€Ÿåº¦: {total_processed/extraction_time:.2f} URL/s (æå–)" if extraction_time > 0 else "   - æå–é€Ÿåº¦: N/A")
        print(f"   - ğŸ“ˆ æ•´é«”é€Ÿåº¦: {total_processed/total_time:.2f} URL/s (ç¸½è¨ˆ)" if total_time > 0 else "   - æ•´é«”é€Ÿåº¦: N/A")
        
        # é¡¯ç¤ºæˆåŠŸæ¡ˆä¾‹çµ±è¨ˆ
        successful_views = [r for r in self.results if r.get('has_views')]
        successful_content = [r for r in self.results if r.get('has_content')]
        successful_likes = [r for r in self.results if r.get('has_likes')]
        successful_comments = [r for r in self.results if r.get('has_comments')]
        successful_reposts = [r for r in self.results if r.get('has_reposts')]
        successful_shares = [r for r in self.results if r.get('has_shares')]
        
        print(f"\nğŸ“ˆ æå–çµ±è¨ˆ:")
        print(f"   - ğŸ“Š è§€çœ‹æ•¸æˆåŠŸ: {len(successful_views)}/{total_processed} ({len(successful_views)/total_processed*100:.1f}%)" if total_processed > 0 else "   - è§€çœ‹æ•¸æˆåŠŸ: N/A")
        print(f"   - ğŸ“ æ–‡å­—å…§å®¹æˆåŠŸ: {len(successful_content)}/{total_processed} ({len(successful_content)/total_processed*100:.1f}%)" if total_processed > 0 else "   - æ–‡å­—å…§å®¹æˆåŠŸ: N/A")
        print(f"   - ğŸ‘ æŒ‰è®šæ•¸æˆåŠŸ: {len(successful_likes)}/{total_processed} ({len(successful_likes)/total_processed*100:.1f}%)" if total_processed > 0 else "   - æŒ‰è®šæ•¸æˆåŠŸ: N/A")
        print(f"   - ğŸ’¬ ç•™è¨€æ•¸æˆåŠŸ: {len(successful_comments)}/{total_processed} ({len(successful_comments)/total_processed*100:.1f}%)" if total_processed > 0 else "   - ç•™è¨€æ•¸æˆåŠŸ: N/A")
        print(f"   - ğŸ”„ è½‰ç™¼æ•¸æˆåŠŸ: {len(successful_reposts)}/{total_processed} ({len(successful_reposts)/total_processed*100:.1f}%)" if total_processed > 0 else "   - è½‰ç™¼æ•¸æˆåŠŸ: N/A")
        print(f"   - ğŸ“¤ åˆ†äº«æ•¸æˆåŠŸ: {len(successful_shares)}/{total_processed} ({len(successful_shares)/total_processed*100:.1f}%)" if total_processed > 0 else "   - åˆ†äº«æ•¸æˆåŠŸ: N/A")
        
        if successful_views:
            print(f"\nğŸ¯ æˆåŠŸæå–çš„è²¼æ–‡ (å‰5ç­†è©³ç´°):")
            for i, r in enumerate(successful_views[:5]):
                content_preview = r.get('content', '')[:40] + "..." if r.get('content') and len(r.get('content', '')) > 40 else r.get('content', 'ç„¡å…§å®¹')
                print(f"   âœ… {r['post_id']}:")
                print(f"      ğŸ“Š è§€çœ‹: {r.get('views', 'N/A')} | ğŸ‘ è®š: {r.get('likes', 'N/A')} | ğŸ’¬ ç•™è¨€: {r.get('comments', 'N/A')} | ğŸ”„ è½‰ç™¼: {r.get('reposts', 'N/A')} | ğŸ“¤ åˆ†äº«: {r.get('shares', 'N/A')}")
                print(f"      ğŸ“ å…§å®¹: {content_preview}")
        
        # ä¿å­˜çµæœ
        self.save_results()

    def save_results(self):
        """ä¿å­˜çµæœåˆ°JSONæ–‡ä»¶"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"realtime_extraction_results_{timestamp}.json"
        
        total_time = time.time() - self.start_time
        extraction_time = total_time - getattr(self, 'url_collection_time', 0)
        
        output_data = {
            'timestamp': datetime.now().isoformat(),
            'target_username': self.target_username,
            'max_posts': self.max_posts,
            'total_processed': len(self.results),
            'api_success_count': self.api_success_count,
            'api_failure_count': self.api_failure_count,
            'local_success_count': self.local_success_count,
            'local_failure_count': self.local_failure_count,
            'overall_success_rate': (self.api_success_count + self.local_success_count) / len(self.results) * 100 if self.results else 0,
            'timing': {
                'url_collection_time': getattr(self, 'url_collection_time', 0),
                'content_extraction_time': extraction_time,
                'total_time': total_time,
                'url_collection_speed': len(self.results) / getattr(self, 'url_collection_time', 1) if getattr(self, 'url_collection_time', 0) > 0 else 0,
                'content_extraction_speed': len(self.results) / extraction_time if extraction_time > 0 else 0,
                'overall_speed': len(self.results) / total_time if total_time > 0 else 0
            },
            'views_extraction_count': len([r for r in self.results if r.get('has_views')]),
            'content_extraction_count': len([r for r in self.results if r.get('has_content')]),
            'likes_extraction_count': len([r for r in self.results if r.get('has_likes')]),
            'comments_extraction_count': len([r for r in self.results if r.get('has_comments')]),
            'reposts_extraction_count': len([r for r in self.results if r.get('has_reposts')]),
            'shares_extraction_count': len([r for r in self.results if r.get('has_shares')]),
            'views_extraction_rate': len([r for r in self.results if r.get('has_views')]) / len(self.results) * 100 if self.results else 0,
            'content_extraction_rate': len([r for r in self.results if r.get('has_content')]) / len(self.results) * 100 if self.results else 0,
            'likes_extraction_rate': len([r for r in self.results if r.get('has_likes')]) / len(self.results) * 100 if self.results else 0,
            'comments_extraction_rate': len([r for r in self.results if r.get('has_comments')]) / len(self.results) * 100 if self.results else 0,
            'reposts_extraction_rate': len([r for r in self.results if r.get('has_reposts')]) / len(self.results) * 100 if self.results else 0,
            'shares_extraction_rate': len([r for r in self.results if r.get('has_shares')]) / len(self.results) * 100 if self.results else 0,
            'results': self.results
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        safe_print(f"ğŸ’¾ çµæœå·²ä¿å­˜åˆ°: {filename}", f"[ä¿å­˜] çµæœå·²ä¿å­˜åˆ°: {filename}")

async def main():
    """ä¸»å‡½æ•¸"""
    import argparse
    import sys
    import os
    
    # Windowsç·¨ç¢¼ä¿®æ­£
    if sys.platform == 'win32':
        try:
            # è¨­ç½®æ§åˆ¶å°ç·¨ç¢¼ç‚ºUTF-8
            os.system('chcp 65001 >nul 2>&1')
            # é‡æ–°é…ç½®stdoutç·¨ç¢¼
            if hasattr(sys.stdout, 'reconfigure'):
                sys.stdout.reconfigure(encoding='utf-8')
            if hasattr(sys.stderr, 'reconfigure'):
                sys.stderr.reconfigure(encoding='utf-8')
        except:
            pass
    
    # è¨­å®šå‘½ä»¤è¡Œåƒæ•¸
    parser = argparse.ArgumentParser(description='å¯¦æ™‚çˆ¬èŸ²+æå–å™¨')
    parser.add_argument('--username', default='gvmonthly', help='ç›®æ¨™å¸³è™Ÿç”¨æˆ¶å')
    parser.add_argument('--max_posts', type=int, default=100, help='è¦çˆ¬å–çš„è²¼æ–‡æ•¸é‡')
    
    args = parser.parse_args()
    
    # å‰µå»ºä¸¦åŸ·è¡Œå¯¦æ™‚æå–å™¨
    extractor = RealtimeCrawlerExtractor(args.username, args.max_posts)
    await extractor.run_realtime_extraction()

if __name__ == "__main__":
    asyncio.run(main())