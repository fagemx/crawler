#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¯¦æ™‚çˆ¬èŸ²æ ¸å¿ƒ
ä¸»è¦å”èª¿é‚è¼¯å’Œæµç¨‹æ§åˆ¶
"""

import asyncio
import json
import time
import threading
import concurrent.futures
from datetime import datetime
from typing import Dict, Optional, List
from pathlib import Path

from playwright.async_api import async_playwright

from common.config import get_auth_file_path
from common.incremental_crawl_manager import IncrementalCrawlManager
from ..processors import PostProcessor
from ..crawlers import UrlCollector
from ..utils.helpers import safe_print

class RealtimeCrawler:
    """å¯¦æ™‚çˆ¬èŸ²æ ¸å¿ƒé¡"""
    
    def __init__(self, target_username: str, max_posts: int = 20, incremental: bool = True):
        self.target_username = target_username
        self.max_posts = max_posts
        self.incremental = incremental
        
        # å¢é‡çˆ¬å–ç®¡ç†å™¨
        self.crawl_manager = IncrementalCrawlManager() if incremental else None
        
        # çˆ¬èŸ²Agentè¨­å®š
        self.agent_url = "http://localhost:8006/v1/playwright/crawl"
        self.auth_file_path = get_auth_file_path(from_project_root=True)
        
        # è™•ç†å™¨
        self.post_processor = PostProcessor(target_username)
        self.url_collector = UrlCollector(target_username, max_posts, self.auth_file_path)
        
        # çµæœçµ±è¨ˆ
        self.results = []
        self.start_time = None
        self.api_success_count = 0
        self.api_failure_count = 0
        self.local_success_count = 0
        self.local_failure_count = 0
    
    async def collect_urls_only(self) -> List[str]:
        """æ”¶é›†URLs"""
        # å¢é‡æ¨¡å¼ï¼šç²å–å·²å­˜åœ¨çš„post_ids
        existing_post_ids = set()
        if self.incremental and self.crawl_manager:
            safe_print("ğŸ” æ­£åœ¨æª¢æŸ¥è³‡æ–™åº«é€£æ¥...")
            try:
                existing_post_ids = await self.crawl_manager.get_existing_post_ids(self.target_username)
                checkpoint = await self.crawl_manager.get_crawl_checkpoint(self.target_username)
                safe_print(f"âœ… è³‡æ–™åº«é€£æ¥æˆåŠŸ")
                safe_print(f"ğŸ” å¢é‡æ¨¡å¼: å·²çˆ¬å– {len(existing_post_ids)} å€‹è²¼æ–‡")
                if len(existing_post_ids) > 0:
                    safe_print(f"ğŸ“‹ å·²å­˜åœ¨è²¼æ–‡IDç¯„ä¾‹: {list(existing_post_ids)[:3]}...")
                if checkpoint:
                    safe_print(f"ğŸ“Š ä¸Šæ¬¡æª¢æŸ¥é»: {checkpoint.latest_post_id} (ç¸½è¨ˆ: {checkpoint.total_crawled})")
                else:
                    safe_print("ğŸ“Š æœªæ‰¾åˆ°æª¢æŸ¥é»è¨˜éŒ„ (é¦–æ¬¡çˆ¬å–)")
            except Exception as e:
                safe_print(f"âŒ è³‡æ–™åº«é€£æ¥å¤±æ•—: {e}")
                safe_print("ğŸ”„ å›é€€åˆ°å…¨é‡æ¨¡å¼")
                self.incremental = False
                self.crawl_manager = None
        else:
            safe_print("ğŸ“‹ å…¨é‡æ¨¡å¼: çˆ¬å–æ‰€æœ‰æ‰¾åˆ°çš„è²¼æ–‡")
        
        # ä½¿ç”¨URLæ”¶é›†å™¨
        return await self.url_collector.collect_urls(existing_post_ids, self.incremental)
    
    async def run_realtime_extraction(self):
        """ä¸»è¦åŸ·è¡Œæµç¨‹"""
        self.start_time = time.time()
        safe_print(f"ğŸš€ é–‹å§‹å¯¦æ™‚çˆ¬å–: @{self.target_username}")
        safe_print(f"ğŸ“Š æ¨¡å¼: {'å¢é‡' if self.incremental else 'å…¨é‡'}, ç›®æ¨™: {self.max_posts} å€‹è²¼æ–‡")
        
        # URLæ”¶é›†éšæ®µ
        safe_print("=" * 60)
        safe_print("ğŸ“ éšæ®µ1: æ™ºèƒ½æ»¾å‹•æ”¶é›†URLs")
        url_start_time = time.time()
        
        urls = await self.collect_urls_only()
        
        url_end_time = time.time()
        url_collection_time = url_end_time - url_start_time
        
        if not urls:
            safe_print("âŒ æœªæ”¶é›†åˆ°ä»»ä½•URL")
            return
        
        safe_print(f"âœ… URLæ”¶é›†å®Œæˆ: {len(urls)} å€‹URL, è€—æ™‚ {url_collection_time:.2f}s")
        safe_print(f"âš¡ URLæ”¶é›†é€Ÿåº¦: {len(urls)/url_collection_time:.2f} URLs/ç§’")
        
        # å…§å®¹æå–éšæ®µ
        safe_print("=" * 60)
        safe_print("ğŸ“ éšæ®µ2: å¯¦æ™‚å…§å®¹æå–")
        extraction_start_time = time.time()
        
        for i, url in enumerate(urls):
            result = await self.post_processor.process_url_realtime(url, i, len(urls))
            if result:
                self.results.append(result)
                # çµ±è¨ˆAPIä½¿ç”¨æƒ…æ³
                if result.get('source') == 'jina_api':
                    self.api_success_count += 1
                elif result.get('source') == 'local_reader':
                    self.local_success_count += 1
                else:
                    self.api_failure_count += 1
        
        extraction_end_time = time.time()
        extraction_time = extraction_end_time - extraction_start_time
        
        # é¡¯ç¤ºæœ€çµ‚çµ±è¨ˆ
        safe_print("=" * 60)
        safe_print("ğŸ“Š æœ€çµ‚çµ±è¨ˆ")
        self.show_final_statistics()
        
        # ä¿å­˜çµæœ
        safe_print("=" * 60)
        safe_print("ğŸ’¾ ä¿å­˜çµæœ")
        results_file = self.save_results()
        
        safe_print("=" * 60)
        safe_print("ğŸ‰ çˆ¬å–å®Œæˆï¼")
        safe_print(f"ğŸ“ çµæœæ–‡ä»¶: {results_file}")
        
        return results_file
    
    def show_final_statistics(self):
        """é¡¯ç¤ºæœ€çµ‚çµ±è¨ˆ"""
        total_time = time.time() - self.start_time if self.start_time else 0
        total_processed = len(self.results)
        
        # åŸºæœ¬çµ±è¨ˆ
        safe_print(f"ğŸ“Š ç¸½è™•ç†æ•¸: {total_processed}")
        safe_print(f"â±ï¸  ç¸½è€—æ™‚: {total_time:.2f}ç§’")
        if total_processed > 0:
            safe_print(f"âš¡ å¹³å‡é€Ÿåº¦: {total_processed/total_time:.2f} è²¼æ–‡/ç§’")
        
        # APIä½¿ç”¨çµ±è¨ˆ
        safe_print(f"ğŸ“¡ Jina API æˆåŠŸ: {self.api_success_count}")
        safe_print(f"ğŸ  æœ¬åœ°Reader æˆåŠŸ: {self.local_success_count}")
        safe_print(f"âŒ å¤±æ•—: {self.api_failure_count}")
        
        total_attempts = self.api_success_count + self.local_success_count + self.api_failure_count
        if total_attempts > 0:
            safe_print(f"âœ… æ•´é«”æˆåŠŸç‡: {((self.api_success_count + self.local_success_count) / total_attempts * 100):.1f}%")
        
        # æå–æˆåŠŸç‡çµ±è¨ˆ
        if self.results:
            views_count = len([r for r in self.results if r.get('has_views')])
            content_count = len([r for r in self.results if r.get('has_content')])
            likes_count = len([r for r in self.results if r.get('has_likes')])
            comments_count = len([r for r in self.results if r.get('has_comments')])
            reposts_count = len([r for r in self.results if r.get('has_reposts')])
            shares_count = len([r for r in self.results if r.get('has_shares')])
            
            safe_print(f"ğŸ‘ï¸  è§€çœ‹æ•¸æå–: {views_count}/{total_processed} ({views_count/total_processed*100:.1f}%)")
            safe_print(f"ğŸ“ å…§å®¹æå–: {content_count}/{total_processed} ({content_count/total_processed*100:.1f}%)")
            safe_print(f"ğŸ‘ æŒ‰è®šæ•¸æå–: {likes_count}/{total_processed} ({likes_count/total_processed*100:.1f}%)")
            safe_print(f"ğŸ’¬ ç•™è¨€æ•¸æå–: {comments_count}/{total_processed} ({comments_count/total_processed*100:.1f}%)")
            safe_print(f"ğŸ”„ è½‰ç™¼æ•¸æå–: {reposts_count}/{total_processed} ({reposts_count/total_processed*100:.1f}%)")
            safe_print(f"ğŸ“¤ åˆ†äº«æ•¸æå–: {shares_count}/{total_processed} ({shares_count/total_processed*100:.1f}%)")
    
    def save_results(self):
        """ä¿å­˜çµæœåˆ°æ–‡ä»¶å’Œè³‡æ–™åº«"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # ç¢ºä¿ç›®éŒ„å­˜åœ¨
        results_dir = Path('extraction_results')
        results_dir.mkdir(exist_ok=True)
        
        filename = f"realtime_extraction_results_{timestamp}.json"
        filepath = results_dir / filename
        
        # è¨ˆç®—çµ±è¨ˆä¿¡æ¯
        total_time = time.time() - self.start_time if self.start_time else 0
        total_processed = len(self.results)
        
        # æ§‹å»ºå®Œæ•´çµæœ
        full_result = {
            'timestamp': datetime.now().isoformat(),
            'target_username': self.target_username,
            'max_posts': self.max_posts,
            'total_processed': total_processed,
            'api_success_count': self.api_success_count,
            'api_failure_count': self.api_failure_count,
            'local_success_count': self.local_success_count,
            'local_failure_count': self.local_failure_count,
            'overall_success_rate': ((self.api_success_count + self.local_success_count) / max(total_processed, 1)) * 100,
            'timing': {
                'total_time': total_time,
                'overall_speed': total_processed / max(total_time, 1)
            },
            'results': self.results
        }
        
        # æ·»åŠ æå–çµ±è¨ˆ
        if self.results:
            views_count = len([r for r in self.results if r.get('has_views')])
            content_count = len([r for r in self.results if r.get('has_content')])
            likes_count = len([r for r in self.results if r.get('has_likes')])
            comments_count = len([r for r in self.results if r.get('has_comments')])
            reposts_count = len([r for r in self.results if r.get('has_reposts')])
            shares_count = len([r for r in self.results if r.get('has_shares')])
            
            full_result.update({
                'views_extraction_count': views_count,
                'content_extraction_count': content_count,
                'likes_extraction_count': likes_count,
                'comments_extraction_count': comments_count,
                'reposts_extraction_count': reposts_count,
                'shares_extraction_count': shares_count,
                'views_extraction_rate': (views_count / total_processed * 100) if total_processed > 0 else 0,
                'content_extraction_rate': (content_count / total_processed * 100) if total_processed > 0 else 0,
                'likes_extraction_rate': (likes_count / total_processed * 100) if total_processed > 0 else 0,
                'comments_extraction_rate': (comments_count / total_processed * 100) if total_processed > 0 else 0,
                'reposts_extraction_rate': (reposts_count / total_processed * 100) if total_processed > 0 else 0,
                'shares_extraction_rate': (shares_count / total_processed * 100) if total_processed > 0 else 0,
            })
        
        # ä¿å­˜åˆ°JSONæ–‡ä»¶
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(full_result, f, ensure_ascii=False, indent=2)
            safe_print(f"âœ… çµæœå·²ä¿å­˜åˆ°: {filepath}")
        except Exception as e:
            safe_print(f"âŒ ä¿å­˜JSONæ–‡ä»¶å¤±æ•—: {e}")
            return None
        
        # ä¿å­˜åˆ°è³‡æ–™åº«ï¼ˆå¢é‡æ¨¡å¼ï¼‰
        if self.incremental and self.crawl_manager and self.results:
            safe_print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜ {len(self.results)} å€‹çµæœåˆ°è³‡æ–™åº«...")
            try:
                import asyncio
                try:
                    current_loop = asyncio.get_running_loop()
                    safe_print("ğŸ” æª¢æ¸¬åˆ°é‹è¡Œä¸­çš„äº‹ä»¶å¾ªç’°ï¼Œä½¿ç”¨åŒæ­¥æ¨¡å¼...")
                    self._save_to_database_sync()
                except RuntimeError:
                    safe_print("ğŸ“¤ æ­£åœ¨å¯«å…¥è³‡æ–™åº«...")
                    loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(loop)
                    saved_count = loop.run_until_complete(
                        self.crawl_manager.save_quick_crawl_results(self.results, self.target_username)
                    )
                    safe_print(f"âœ… æˆåŠŸä¿å­˜ {saved_count} å€‹è²¼æ–‡åˆ°è³‡æ–™åº«")
                    
                    # æ›´æ–°æª¢æŸ¥é»
                    if self.results and saved_count > 0:
                        latest_post_id = self.results[0].get('post_id')
                        if latest_post_id:
                            safe_print(f"ğŸ“Š æ›´æ–°æª¢æŸ¥é»: {latest_post_id}")
                            loop.run_until_complete(
                                self.crawl_manager.update_crawl_checkpoint(
                                    self.target_username, 
                                    latest_post_id, 
                                    saved_count
                                )
                            )
                            safe_print(f"âœ… æª¢æŸ¥é»æ›´æ–°æˆåŠŸ")
                    
                    loop.close()
                    safe_print(f"ğŸ’¾ è³‡æ–™åº«æ“ä½œå®Œæˆ: ä¿å­˜ {saved_count} å€‹æ–°è²¼æ–‡")
            except Exception as e:
                safe_print(f"âŒ è³‡æ–™åº«ä¿å­˜å¤±æ•—: {e}")
                safe_print(f"ğŸ” éŒ¯èª¤è©³æƒ…: {type(e).__name__}: {str(e)}")
                safe_print("ğŸ“ å°‡åªä¿å­˜åˆ°JSONæ–‡ä»¶")
        
        return str(filepath)
    
    def _save_to_database_sync(self):
        """åŒæ­¥ç‰ˆæœ¬çš„è³‡æ–™åº«ä¿å­˜"""
        def async_save_worker():
            import asyncio
            from common.incremental_crawl_manager import IncrementalCrawlManager
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            new_crawl_manager = IncrementalCrawlManager()
            try:
                safe_print("ğŸ“¤ æ­£åœ¨å¯«å…¥è³‡æ–™åº«...")
                saved_count = loop.run_until_complete(
                    new_crawl_manager.save_quick_crawl_results(self.results, self.target_username)
                )
                safe_print(f"âœ… æˆåŠŸä¿å­˜ {saved_count} å€‹è²¼æ–‡åˆ°è³‡æ–™åº«")
                if self.results and saved_count > 0:
                    latest_post_id = self.results[0].get('post_id')
                    if latest_post_id:
                        safe_print(f"ğŸ“Š æ›´æ–°æª¢æŸ¥é»: {latest_post_id}")
                        loop.run_until_complete(
                            new_crawl_manager.update_crawl_checkpoint(
                                self.target_username, 
                                latest_post_id, 
                                saved_count
                            )
                        )
                        safe_print(f"âœ… æª¢æŸ¥é»æ›´æ–°æˆåŠŸ")
                safe_print(f"ğŸ’¾ è³‡æ–™åº«æ“ä½œå®Œæˆ: ä¿å­˜ {saved_count} å€‹æ–°è²¼æ–‡")
                return saved_count
            finally:
                try:
                    loop.run_until_complete(new_crawl_manager.db.close_pool())
                except:
                    pass
                loop.close()
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
            future = executor.submit(async_save_worker)
            try:
                result = future.result(timeout=60)
                safe_print(f"ğŸ¯ ç·šç¨‹åŸ·è¡Œå®Œæˆï¼Œçµæœ: {result}")
                return result
            except concurrent.futures.TimeoutError:
                safe_print("âŒ è³‡æ–™åº«ä¿å­˜è¶…æ™‚")
                raise
            except Exception as e:
                safe_print(f"âŒ è³‡æ–™åº«ä¿å­˜ç·šç¨‹åŸ·è¡Œå¤±æ•—: {e}")
                safe_print(f"ğŸ” éŒ¯èª¤é¡å‹: {type(e).__name__}")
                raise