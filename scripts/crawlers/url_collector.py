#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
URLæ”¶é›†å™¨
è™•ç†Playwrightæ»¾å‹•å’ŒURLæ”¶é›†
"""

import asyncio
import json
import random
from typing import List, Set
from pathlib import Path
from playwright.async_api import async_playwright

from ..utils.helpers import safe_print

class UrlCollector:
    """URLæ”¶é›†å™¨"""
    
    def __init__(self, target_username: str, max_posts: int, auth_file_path: Path):
        self.target_username = target_username
        self.max_posts = max_posts
        self.auth_file_path = auth_file_path
    
    async def collect_urls(self, existing_post_ids: Set[str] = None, incremental: bool = False) -> List[str]:
        """æ”¶é›†URLs"""
        existing_post_ids = existing_post_ids or set()
        
        # æª¢æŸ¥èªè­‰æª”æ¡ˆ
        if not self.auth_file_path.exists():
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°èªè­‰æª”æ¡ˆ '{self.auth_file_path}'")
        
        # è®€å–èªè­‰å…§å®¹
        with open(self.auth_file_path, "r", encoding="utf-8") as f:
            auth_content = json.load(f)
        
        safe_print(f"ğŸ”§ é–‹å§‹ç›´æ¥Playwrightæ»¾å‹•æ”¶é›†URLs @{self.target_username}")
        safe_print(f"ğŸ¯ ç›®æ¨™æ•¸é‡: {self.max_posts} å€‹URLs")
        safe_print(f"ğŸ“‹ æ¨¡å¼: {'å¢é‡æ”¶é›†' if incremental else 'å…¨é‡æ”¶é›†'}")
        
        urls = []
        
        async with async_playwright() as p:
            try:
                # å•Ÿå‹•ç€è¦½å™¨
                browser = await p.chromium.launch(headless=True)
                context = await browser.new_context()
                
                # æ³¨å…¥èªè­‰ç‹€æ…‹
                await context.add_cookies(auth_content.get('cookies', []))
                local_storage = auth_content.get('localStorage', [])
                session_storage = auth_content.get('sessionStorage', [])
                
                # å‰µå»ºé é¢ä¸¦å‰å¾€ç›®æ¨™
                page = await context.new_page()
                await page.goto(f"https://www.threads.com/@{self.target_username}")
                await asyncio.sleep(3)  # ç­‰å¾…é é¢è¼‰å…¥
                
                # æ³¨å…¥å„²å­˜ç‹€æ…‹
                if local_storage:
                    for item in local_storage:
                        await page.evaluate(f"localStorage.setItem('{item['name']}', '{item['value']}')")
                if session_storage:
                    for item in session_storage:
                        await page.evaluate(f"sessionStorage.setItem('{item['name']}', '{item['value']}')")
                
                await page.reload()
                await asyncio.sleep(2)
                
                safe_print("ğŸ”„ é–‹å§‹æ™ºèƒ½æ»¾å‹•æ”¶é›†URLs...")
                
                # æ»¾å‹•æ”¶é›†é‚è¼¯
                urls = await self._scroll_and_collect(page, existing_post_ids, incremental)
                
                await browser.close()
                
                safe_print(f"âœ… URLæ”¶é›†å®Œæˆï¼Œå…±æ”¶é›†åˆ° {len(urls)} å€‹URL")
                return urls[:self.max_posts]  # ç¢ºä¿ä¸è¶…éç›®æ¨™æ•¸é‡
                
            except Exception as e:
                safe_print(f"âŒ Playwright URLæ”¶é›†éŒ¯èª¤: {e}")
                if 'browser' in locals():
                    await browser.close()
                return []
    
    async def _scroll_and_collect(self, page, existing_post_ids: Set[str], incremental: bool) -> List[str]:
        """åŸ·è¡Œæ»¾å‹•å’Œæ”¶é›†é‚è¼¯"""
        urls = []
        
        # å¢å¼·çš„æ»¾å‹•æ”¶é›†é‚è¼¯
        scroll_rounds = 0
        max_scroll_rounds = 80  # å¤§å¹…å¢åŠ æœ€å¤§æ»¾å‹•æ¬¡æ•¸
        no_new_content_rounds = 0  # é€£çºŒç„¡æ–°å…§å®¹çš„è¼ªæ¬¡
        max_no_new_rounds = 15  # å¢åŠ é€£çºŒç„¡æ–°å…§å®¹çš„æœ€å¤§å®¹å¿è¼ªæ¬¡ï¼ˆæ›´å¤šè€å¿ƒï¼‰
        consecutive_existing_rounds = 0  # å¢é‡æ¨¡å¼ï¼šé€£çºŒç™¼ç¾å·²å­˜åœ¨è²¼æ–‡çš„è¼ªæ¬¡
        max_consecutive_existing = 15  # å¢é‡æ¨¡å¼ï¼šå…è¨±çš„æœ€å¤§é€£çºŒå·²å­˜åœ¨è¼ªæ¬¡ï¼ˆæ”¾å¯¬é™åˆ¶ï¼‰
        
        while len(urls) < self.max_posts and scroll_rounds < max_scroll_rounds:
            # æå–ç•¶å‰é é¢çš„URLsï¼ˆéæ¿¾ç„¡æ•ˆURLså’Œéç›®æ¨™ç”¨æˆ¶ï¼‰
            current_urls = await page.evaluate(f"""
                () => {{
                    const targetUsername = '{self.target_username}';
                    const links = Array.from(document.querySelectorAll('a[href*="/post/"]'));
                    return [...new Set(links.map(link => link.href)
                        .filter(url => url.includes('/post/'))
                        .filter(url => {{
                            // æª¢æŸ¥URLæ˜¯å¦å±¬æ–¼ç›®æ¨™ç”¨æˆ¶
                            const usernamePart = url.split('/@')[1];
                            if (!usernamePart) return false;
                            const extractedUsername = usernamePart.split('/')[0];
                            
                            const postId = url.split('/post/')[1];
                            // éæ¿¾æ‰ mediaã€ç„¡æ•ˆIDç­‰ï¼Œä¸¦ç¢ºä¿æ˜¯ç›®æ¨™ç”¨æˆ¶çš„è²¼æ–‡
                            return postId && 
                                   postId !== 'media' && 
                                   postId.length > 5 && 
                                   /^[A-Za-z0-9_-]+$/.test(postId) &&
                                   extractedUsername === targetUsername;
                        }}))];
                }}
            """)
            
            before_count = len(urls)
            
            # å»é‡ä¸¦æ·»åŠ æ–°URLsï¼ˆæ”¯æŒå¢é‡æª¢æ¸¬ï¼‰
            new_urls_this_round = 0
            found_existing_this_round = False
            existing_skipped_this_round = 0
            
            for url in current_urls:
                post_id = url.split('/')[-1] if url else None
                
                # è·³éå·²æ”¶é›†çš„URL
                if url in urls:
                    continue
                    
                # å¢é‡æ¨¡å¼ï¼šæª¢æŸ¥æ˜¯å¦å·²å­˜åœ¨æ–¼è³‡æ–™åº«
                if incremental and post_id in existing_post_ids:
                    safe_print(f"   ğŸ” [{len(urls)+1}] ç™¼ç¾å·²çˆ¬å–è²¼æ–‡: {post_id} - è·³é (å·²åœ¨è³‡æ–™åº«)")
                    found_existing_this_round = True
                    existing_skipped_this_round += 1
                    continue
                
                # æª¢æŸ¥æ˜¯å¦å·²é”åˆ°ç›®æ¨™æ•¸é‡
                if len(urls) >= self.max_posts:
                    break
                    
                urls.append(url)
                new_urls_this_round += 1
                
                status_icon = "ğŸ†•" if incremental else "ğŸ“"
                safe_print(f"   {status_icon} [{len(urls)}] ç™¼ç¾: {post_id}")
            
            # å¢é‡æ¨¡å¼ï¼šæ™ºèƒ½åœæ­¢æ¢ä»¶
            if incremental:
                if found_existing_this_round:
                    consecutive_existing_rounds += 1
                    if len(urls) >= self.max_posts:
                        safe_print(f"   âœ… å¢é‡æª¢æ¸¬: å·²æ”¶é›†è¶³å¤ æ–°è²¼æ–‡ ({len(urls)} å€‹)")
                        break
                    elif consecutive_existing_rounds >= max_consecutive_existing:
                        safe_print(f"   â¹ï¸ å¢é‡æª¢æ¸¬: é€£çºŒ {consecutive_existing_rounds} è¼ªç™¼ç¾å·²å­˜åœ¨è²¼æ–‡ï¼Œåœæ­¢æ”¶é›†")
                        safe_print(f"   ğŸ“Š æœ€çµ‚æ”¶é›†: {len(urls)} å€‹æ–°è²¼æ–‡ (ç›®æ¨™: {self.max_posts})")
                        break
                    else:
                        safe_print(f"   ğŸ” å¢é‡æª¢æ¸¬: ç™¼ç¾å·²å­˜åœ¨è²¼æ–‡ä½†æ•¸é‡ä¸è¶³ ({len(urls)}/{self.max_posts})ï¼Œç¹¼çºŒæ»¾å‹•... (é€£çºŒç™¼ç¾: {consecutive_existing_rounds}/{max_consecutive_existing})")
                else:
                    # é€™è¼ªæ²’æœ‰ç™¼ç¾å·²å­˜åœ¨è²¼æ–‡ï¼Œé‡ç½®è¨ˆæ•¸å™¨
                    consecutive_existing_rounds = 0
            
            # æª¢æŸ¥æ˜¯å¦æœ‰æ–°å…§å®¹
            new_urls_found = len(urls) - before_count
            
            if new_urls_found == 0:
                no_new_content_rounds += 1
                safe_print(f"   â³ ç¬¬{scroll_rounds+1}è¼ªæœªç™¼ç¾æ–°URL ({no_new_content_rounds}/{max_no_new_rounds})")
                
                # éå¢ç­‰å¾…æ™‚é–“ï¼ˆåŠ å…¥éš¨æ©Ÿæ€§ï¼Œé™åˆ¶æœ€å¤§3.5ç§’ï¼‰
                base_wait = min(1.2 + (no_new_content_rounds - 1) * 0.3, 3.5)  # 1.2s -> 3.5s
                random_factor = random.uniform(0.8, 1.2)  # Â±20%éš¨æ©Ÿè®ŠåŒ–
                progressive_wait = base_wait * random_factor
                safe_print(f"   â²ï¸ éå¢ç­‰å¾… {progressive_wait:.1f}s...")
                await asyncio.sleep(progressive_wait)
                
                if no_new_content_rounds >= max_no_new_rounds:
                    safe_print(f"   ğŸ›‘ é€£çºŒ{max_no_new_rounds}è¼ªç„¡æ–°å…§å®¹ï¼Œå¯èƒ½å·²åˆ°é”åº•éƒ¨")
                    
                    # æœ€å¾Œå˜—è©¦
                    final_count = await self._final_attempt(page, urls)
                    if final_count == 0:
                        safe_print("   âœ… ç¢ºèªå·²åˆ°é”é é¢åº•éƒ¨")
                        break
                    else:
                        safe_print(f"   ğŸ¯ æœ€å¾Œå˜—è©¦ç™¼ç¾{final_count}å€‹æ–°URLï¼Œç¹¼çºŒ...")
                        no_new_content_rounds = 0
            else:
                no_new_content_rounds = 0  # é‡ç½®è¨ˆæ•¸å™¨
                safe_print(f"   âœ… ç¬¬{scroll_rounds+1}è¼ªç™¼ç¾{new_urls_found}å€‹æ–°URL")
            
            if len(urls) >= self.max_posts:
                safe_print(f"   ğŸ¯ å·²é”åˆ°ç›®æ¨™æ•¸é‡ {self.max_posts}")
                break
            
            # åŸ·è¡Œæ»¾å‹•
            await self._perform_scroll(page, scroll_rounds)
            
            scroll_rounds += 1
            
            # æ¯5è¼ªé¡¯ç¤ºé€²åº¦
            if scroll_rounds % 5 == 0:
                safe_print(f"   ğŸ“Š æ»¾å‹•é€²åº¦: ç¬¬{scroll_rounds}è¼ªï¼Œå·²æ”¶é›†{len(urls)}å€‹URL")
        
        if scroll_rounds >= max_scroll_rounds:
            safe_print(f"   âš ï¸ é”åˆ°æœ€å¤§æ»¾å‹•è¼ªæ¬¡ ({max_scroll_rounds})ï¼Œåœæ­¢æ»¾å‹•")
        
        return urls
    
    async def _final_attempt(self, page, urls: List[str]) -> int:
        """æœ€å¾Œå˜—è©¦æ»¾å‹•æ¿€ç™¼æ–°å…§å®¹"""
        safe_print("   ğŸš€ æœ€å¾Œå˜—è©¦ï¼šå¤šé‡æ¿€é€²æ»¾å‹•æ¿€ç™¼æ–°å…§å®¹...")
        
        # ç¬¬ä¸€æ¬¡ï¼šå¤§å¹…å‘ä¸‹
        await page.evaluate("window.scrollBy(0, 2500)")
        await asyncio.sleep(2)
        
        # ç¬¬äºŒæ¬¡ï¼šå‘ä¸Šå†å‘ä¸‹ï¼ˆæ¿€ç™¼è¼‰å…¥ï¼‰
        await page.evaluate("window.scrollBy(0, -500)")
        await asyncio.sleep(1)
        await page.evaluate("window.scrollBy(0, 3000)")
        await asyncio.sleep(3)
        
        # ç¬¬ä¸‰æ¬¡ï¼šæ»¾å‹•åˆ°æ›´åº•éƒ¨
        await page.evaluate("window.scrollBy(0, 2000)")
        await asyncio.sleep(2)
        
        safe_print("   â³ ç­‰å¾…æ‰€æœ‰å…§å®¹è¼‰å…¥å®Œæˆ...")
        await asyncio.sleep(3)
        
        # å†æ¬¡æª¢æŸ¥ï¼ˆéæ¿¾ç„¡æ•ˆURLsï¼‰
        final_urls = await page.evaluate(f"""
            () => {{
                const targetUsername = '{self.target_username}';
                const links = Array.from(document.querySelectorAll('a[href*="/post/"]'));
                return [...new Set(links.map(link => link.href)
                    .filter(url => url.includes('/post/'))
                    .filter(url => {{
                        const usernamePart = url.split('/@')[1];
                        if (!usernamePart) return false;
                        const extractedUsername = usernamePart.split('/')[0];
                        
                        const postId = url.split('/post/')[1];
                        // éæ¿¾æ‰ mediaã€ç„¡æ•ˆIDç­‰ï¼Œä¸¦ç¢ºä¿æ˜¯ç›®æ¨™ç”¨æˆ¶çš„è²¼æ–‡
                        return postId && 
                               postId !== 'media' && 
                               postId.length > 5 && 
                               /^[A-Za-z0-9_-]+$/.test(postId) &&
                               extractedUsername === targetUsername;
                    }}))];
            }}
        """)
        
        final_new_count = 0
        for url in final_urls:
            if url not in urls and len(urls) < self.max_posts:
                urls.append(url)
                final_new_count += 1
                safe_print(f"   ğŸ“ [{len(urls)}] æœ€å¾Œç™¼ç¾: {url.split('/')[-1]}")
        
        return final_new_count
    
    async def _perform_scroll(self, page, scroll_rounds: int):
        """åŸ·è¡Œæ»¾å‹•æ“ä½œ"""
        # å„ªåŒ–çš„äººæ€§åŒ–æ»¾å‹•ç­–ç•¥
        if scroll_rounds % 6 == 5:  # æ¯6è¼ªé€²è¡Œä¸€æ¬¡æ¿€é€²æ»¾å‹•ï¼ˆæé«˜é »ç‡ï¼‰
            safe_print("   ğŸš€ åŸ·è¡Œæ¿€é€²æ»¾å‹•æ¿€ç™¼è¼‰å…¥...")
            # æ¨¡æ“¬ç”¨æˆ¶å¿«é€Ÿæ»¾å‹•è¡Œç‚º
            await page.evaluate("window.scrollBy(0, 1600)")
            await asyncio.sleep(1.2)
            # ç¨å¾®å›æ»¾ï¼ˆåƒç”¨æˆ¶æ»¾éé ­äº†ï¼‰
            await page.evaluate("window.scrollBy(0, -250)")
            await asyncio.sleep(0.8)
            # å†ç¹¼çºŒå‘ä¸‹
            await page.evaluate("window.scrollBy(0, 1400)")
            await asyncio.sleep(3.5)
            
        elif scroll_rounds % 3 == 2:  # æ¯3è¼ªé€²è¡Œä¸€æ¬¡ä¸­åº¦æ»¾å‹•
            safe_print("   ğŸ”„ åŸ·è¡Œä¸­åº¦æ»¾å‹•...")
            # åˆ†æ®µæ»¾å‹•ï¼Œæ›´åƒäººé¡è¡Œç‚º
            await page.evaluate("window.scrollBy(0, 800)")
            await asyncio.sleep(1)
            await page.evaluate("window.scrollBy(0, 600)")
            await asyncio.sleep(2.8)
            
        else:
            # æ­£å¸¸æ»¾å‹•ï¼ŒåŠ å…¥éš¨æ©Ÿæ€§å’Œäººæ€§åŒ–
            scroll_distance = 900 + (scroll_rounds % 3) * 100  # 900-1100pxéš¨æ©Ÿ
            await page.evaluate(f"window.scrollBy(0, {scroll_distance})")
            
            # çŸ­æš«æš«åœï¼ˆæ¨¡æ“¬ç”¨æˆ¶é–±è®€ï¼‰
            await asyncio.sleep(1.8 + (scroll_rounds % 2) * 0.4)  # 1.8-2.2ç§’éš¨æ©Ÿ
        
        # çµ±ä¸€çš„è¼‰å…¥æª¢æ¸¬ï¼ˆæ‰€æœ‰æ»¾å‹•å¾Œéƒ½æª¢æŸ¥ï¼‰
        has_loading = await page.evaluate("""
            () => {
                const indicators = document.querySelectorAll('[role="progressbar"], .loading, [aria-label*="loading"], [aria-label*="Loading"]');
                return indicators.length > 0;
            }
        """)
        
        if has_loading:
            safe_print("   â³ æª¢æ¸¬åˆ°è¼‰å…¥æŒ‡ç¤ºå™¨ï¼Œé¡å¤–ç­‰å¾…...")
            # éš¨æ©Ÿç­‰å¾…2-3.5ç§’
            loading_wait = random.uniform(2.0, 3.5)
            await asyncio.sleep(loading_wait)