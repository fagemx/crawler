#!/usr/bin/env python3
"""
å°ˆç”¨è³‡æ–™æŠ“å–è…³æœ¬ - ç‚º Plan E åˆ†æéšæ®µæº–å‚™ç´ æ

åŠŸèƒ½ï¼š
1. æŠ“å–æŒ‡å®š Threads ç”¨æˆ¶çš„æœ€æ–° N ç­†è²¼æ–‡ URLã€‚
2. ä½¿ç”¨ JinaMarkdownAgent è™•ç†é€™äº› URLï¼Œå°‡ Markdown å…§å®¹å’ŒæŒ‡æ¨™åˆ†åˆ¥å¯«å…¥
   PostgreSQL (Tier-1) å’Œ Redis (Tier-0)ã€‚

å¦‚ä½•ä½¿ç”¨ï¼š
python scripts/fetch_posts.py --username <threads_username> --count <post_count>
"""

import asyncio
import argparse
import sys
from pathlib import Path

# Add project root to Python path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from agents.crawler.crawler_logic import CrawlerLogic
from agents.jina_markdown.jina_markdown_logic import JinaMarkdownAgent
from agents.post_analyzer.analyzer_logic import PostAnalyzerAgent
from common.db_client import get_db_client
from common.settings import get_settings

# Load environment variables from .env file
settings = get_settings()
print("âœ… å·²è¼‰å…¥ .env æª”æ¡ˆ")

async def fetch_and_store_posts(author_id: str, max_posts: int = 50):
    """
    å®Œæ•´çš„ä¸‰éšæ®µè™•ç†æµç¨‹ï¼š
    1. CrawlerLogic - çˆ¬å– URL
    2. JinaMarkdownAgent - è§£æ markdown å’ŒåŸºç¤æ•¸å€¼
    3. PostAnalyzerAgent - æ™ºèƒ½åˆ†æï¼ˆåŒ…æ‹¬äº”æ¬„ä½æª¢æŸ¥ã€vision åˆ¤æ–·ç­‰ï¼‰
    """
    print(f"--- Starting complete post fetching and analysis for author: {author_id} ---")
    
    # åˆå§‹åŒ–æ‰€æœ‰ä¸‰å€‹ Agent
    crawler = CrawlerLogic()
    markdown_agent = JinaMarkdownAgent()
    analyzer_agent = PostAnalyzerAgent()
    db_client = await get_db_client()

    try:
        # === ç¬¬ä¸€éšæ®µï¼šçˆ¬å– URL ===
        print(f"Stage 1: Crawling up to {max_posts} posts for {author_id}...")
        
        crawled_urls = []
        async for result in crawler.fetch_threads_post_urls(username=author_id.lstrip('@'), max_posts=max_posts):
            if result.get("response_type") == "data" and result.get("is_task_complete"):
                posts_data = result.get("content", {}).get("post_urls", [])
                crawled_urls = [post['url'] for post in posts_data]
                break
            elif result.get("response_type") == "error":
                print(f"Crawler Error: {result.get('content', {}).get('error')}")
                return

        if not crawled_urls:
            print("No posts found or an error occurred during crawling.")
            return
        print(f"âœ… Stage 1 complete. Found {len(crawled_urls)} post URLs.")

        # === ç¬¬äºŒéšæ®µï¼šMarkdown è§£æ ===
        print(f"\nStage 2: Processing markdown and extracting basic metrics...")
        
        batch_size = 3  # ä¿æŒæ‰¹æ¬¡è™•ç†ä»¥é¿å… API é™æµ
        stage2_success = 0
        stage2_failed = 0
        
        for i in range(0, len(crawled_urls), batch_size):
            batch = crawled_urls[i:i + batch_size]
            batch_num = (i // batch_size) + 1
            total_batches = (len(crawled_urls) + batch_size - 1) // batch_size
            
            print(f"  Processing batch {batch_num}/{total_batches} ({len(batch)} URLs)...")
            
            # ç¬¬äºŒéšæ®µï¼šJinaMarkdownAgent æ‰¹æ¬¡è™•ç†
            tasks = []
            for url in batch:
                task = asyncio.create_task(
                    markdown_agent.process_single_post_with_storage(
                        post_url=url,
                        author=author_id
                    )
                )
                tasks.append(task)

            batch_results = await asyncio.gather(*tasks, return_exceptions=True)

            # çµ±è¨ˆç¬¬äºŒéšæ®µçµæœ
            for j, result in enumerate(batch_results):
                if isinstance(result, Exception):
                    stage2_failed += 1
                    print(f"    -> Failed: {batch[j]} - {result}")
                else:
                    stage2_success += 1
                    # é¡¯ç¤ºç¬¬äºŒéšæ®µçš„çµæœæ‘˜è¦
                    missing_fields = result.get('missing_fields', [])
                    needs_vision = result.get('needs_vision', False)
                    print(f"    -> Processed: {batch[j]} - Missing: {len(missing_fields)} fields, Needs vision: {needs_vision}")
            
            # æ‰¹æ¬¡é–“å»¶é²
            if i + batch_size < len(crawled_urls):
                await asyncio.sleep(1.0)
        
        print(f"âœ… Stage 2 complete. Success: {stage2_success}, Failed: {stage2_failed}.")

        # === ç¬¬ä¸‰éšæ®µï¼šæ™ºèƒ½åˆ†æ ===
        print(f"\nStage 3: Intelligent analysis and processing...")
        
        # ç²å–æ‰€æœ‰æˆåŠŸè™•ç†çš„ URL é€²è¡Œç¬¬ä¸‰éšæ®µåˆ†æ
        successful_urls = []
        for i in range(0, len(crawled_urls), batch_size):
            batch = crawled_urls[i:i + batch_size]
            tasks = []
            for url in batch:
                task = asyncio.create_task(
                    db_client.get_post(url)  # ä½¿ç”¨ get_post æª¢æŸ¥
                )
                tasks.append(task)
            
            batch_results = await asyncio.gather(*tasks, return_exceptions=True)
            for j, post_data in enumerate(batch_results):
                # å¦‚æœ get_post å›å‚³é None å€¼ï¼Œä»£è¡¨è²¼æ–‡å­˜åœ¨
                if not isinstance(post_data, Exception) and post_data:
                    successful_urls.append(batch[j])
        
        print(f"  Found {len(successful_urls)} posts ready for Stage 3 analysis...")
        
        # ç¬¬ä¸‰éšæ®µï¼šæ™ºèƒ½åˆ†æï¼ˆåŒ…å«äº”æ¬„ä½æª¢æŸ¥ã€vision åˆ¤æ–·ã€LLM åˆ†æç­‰ï¼‰
        stage3_success = 0
        if successful_urls:
            try:
                print("  Starting intelligent analysis with PostAnalyzerAgent...")
                
                # ä½¿ç”¨ PostAnalyzerAgent é€²è¡Œæ™ºèƒ½åˆ†æ
                # é è¨­ä½¿ç”¨ mode 1 é€²è¡Œåˆ†æï¼Œæ‚¨å¯ä»¥æ ¹æ“šéœ€è¦èª¿æ•´
                analysis_result = await analyzer_agent.analyze_posts(
                    post_urls=successful_urls, 
                    analysis_mode=1
                )
                
                if analysis_result.get("status") == "success":
                    stage3_success = len(successful_urls)
                    print(f"  âœ… Stage 3 intelligent analysis completed successfully.")
                    print(f"     Analysis mode: {analysis_result.get('analysis_mode')}")
                    print(f"     Result: {analysis_result.get('message')}")
                else:
                    print(f"  âŒ Stage 3 analysis failed: {analysis_result.get('message')}")
                    
            except Exception as e:
                print(f"  âŒ Stage 3 analysis failed: {e}")
        
        print(f"\nğŸ‰ Complete three-stage processing finished!")
        print(f"   Stage 1 (Crawling): {len(crawled_urls)} URLs found")
        print(f"   Stage 2 (Markdown): {stage2_success} success, {stage2_failed} failed")
        print(f"   Stage 3 (Analysis): {stage3_success} posts analyzed")

    except Exception as e:
        print(f"An unexpected error occurred in the main process: {e}")
    finally:
        await db_client.close_pool()
        print("\n--- Complete processing script finished. ---")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Fetch and store Threads posts for a specific author.")
    parser.add_argument(
        "--author-id",
        type=str,
        required=True,
        help="The Threads author ID to fetch posts from (e.g., '@wanyu_npp')."
    )
    parser.add_argument(
        "--max-posts",
        type=int,
        default=50,
        help="The maximum number of posts to fetch."
    )
    args = parser.parse_args()

    author_id_clean = args.author_id
    if not author_id_clean.startswith('@'):
        author_id_clean = '@' + author_id_clean

    asyncio.run(fetch_and_store_posts(author_id=author_id_clean, max_posts=args.max_posts)) 