#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
API-æœ¬åœ°è¼ªè¿´ç­–ç•¥ Threads Reader è§£æè…³æœ¬
ç­–ç•¥: 10å€‹API â†’ 20å€‹æœ¬åœ° â†’ 10å€‹API â†’ 20å€‹æœ¬åœ° (è¼ªè¿´)
é¿å…APIè¢«æŒçºŒé˜»æ“‹ï¼Œè®“APIæœ‰æ™‚é–“å†·å»
"""

import json
import re
import requests
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
from typing import Dict, Optional, List
import random

class RotationPipelineReader:
    """
    è¼ªè¿´ç­–ç•¥ Reader è§£æå™¨
    10å€‹API â†’ 20å€‹æœ¬åœ° â†’ 10å€‹API â†’ 20å€‹æœ¬åœ° è¼ªè¿´
    """
    
    def __init__(self):
        self.local_reader_url = "http://localhost:8880"
        self.official_reader_url = "https://r.jina.ai"
        
        # è¼ªè¿´ç­–ç•¥é…ç½®
        self.api_batch_size = 10    # æ¯æ¬¡APIæ‰¹æ¬¡å¤§å°
        self.local_batch_size = 20  # æ¯æ¬¡æœ¬åœ°æ‰¹æ¬¡å¤§å°
        # ä¸éœ€è¦APIå†·å»æ™‚é–“ï¼Œæœ¬åœ°æ‰¹æ¬¡å•Ÿå‹•æ™‚é–“å·²è¶³å¤ 
        
        self.NBSP = "\u00A0"
        
        # å®˜æ–¹API headers
        self.official_headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36',
            'X-Return-Format': 'markdown'
        }
        
        # ä¾†è‡ª complete.py çš„å·²é©—è­‰æœ‰æ•ˆçš„æœ¬åœ° headers é…ç½®
        self.local_headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36',
            'x-wait-for-selector': 'article',
            'x-timeout': '25'
        }
        
        # è§€çœ‹æ•¸æå–æ¨¡å¼
        self.view_patterns = [
            re.compile(rf'\[Thread[\s{self.NBSP}=]*?(\d+(?:[\.,]\d+)?[KMB]?)\s*views\]', re.IGNORECASE),
            re.compile(rf'Thread[\s{self.NBSP}=]*?(\d+(?:[\.,]\d+)?[KMB]?)[\s{self.NBSP}]*views', re.IGNORECASE | re.MULTILINE),
            re.compile(r'(\d+(?:[\.,]\d+)?[KMB]?)\s*views?', re.IGNORECASE),
            re.compile(r'(\d+(?:[\.,]\d+)?[KMB]?)\s*view(?:s|ing)', re.IGNORECASE),
            re.compile(r'views?\s*[:\-]\s*(\d+(?:[\.,]\d+)?[KMB]?)', re.IGNORECASE),
        ]
    
    def normalize_content(self, text: str) -> str:
        """ä¾†è‡ª final.py çš„æœ€ robust çš„å…§å®¹æ¨™æº–åŒ– - å®Œæ•´ç‰ˆæœ¬"""
        # â‘  å°‡å„ç¨®ç©ºç™½å­—ç¬¦çµ±ä¸€æ›¿æ›ç‚ºæ¨™æº–ç©ºæ ¼
        text = text.replace(self.NBSP, " ")  # NBSP (U+00A0) 
        text = text.replace("\u2002", " ")   # En Space
        text = text.replace("\u2003", " ")   # Em Space
        text = text.replace("\u2009", " ")   # Thin Space
        text = text.replace("\u200A", " ")   # Hair Space
        text = text.replace("\u3000", " ")   # Ideographic Space
        text = text.replace("\t", " ")       # Tab æ›¿æ›ç‚ºç©ºæ ¼
        
        # â‘¡ æ¨™æº–åŒ–è¡ŒçµæŸç¬¦
        text = text.replace("\r\n", "\n")
        text = text.replace("\r", "\n")
        
        # â‘¢ å£“ç¸®å¤šå€‹é€£çºŒç©ºæ ¼ï¼ˆä½†ä¿ç•™å–®å€‹ç©ºæ ¼ï¼‰
        text = re.sub(r"[ \t]{2,}", " ", text)
        
        return text

    def extract_views_count(self, markdown_content: str, post_id: str = "") -> Optional[str]:
        """ä¾†è‡ª final.py çš„æœ€ robust çš„è§€çœ‹æ•¸æå– - å®Œæ•´ç‰ˆæœ¬"""
        
        # æ¨™æº–åŒ–å…§å®¹
        normalized_content = self.normalize_content(markdown_content)
        
        # 1. å˜—è©¦æ‰€æœ‰æ¨¡å¼åœ¨æ¨™æº–åŒ–å¾Œçš„å…§å®¹ä¸Š
        for i, pattern in enumerate(self.view_patterns):
            match = pattern.search(normalized_content)
            if match:
                views_number = match.group(1)
                if self.validate_views_format(views_number):
                    return views_number
        
        # 2. å¦‚æœé‚„æ˜¯å¤±æ•—ï¼Œå˜—è©¦åœ¨åŸå§‹å…§å®¹ä¸Šæœç´¢ï¼ˆé˜²æ­¢æ¨™æº–åŒ–éåº¦ï¼‰
        for i, pattern in enumerate(self.view_patterns):
            match = pattern.search(markdown_content)
            if match:
                views_number = match.group(1)
                if self.validate_views_format(views_number):
                    return views_number
        
        return None

    def validate_views_format(self, views: str) -> bool:
        """é©—è­‰è§€çœ‹æ•¸æ ¼å¼æ˜¯å¦åˆç† - å®Œæ•´ç‰ˆæœ¬"""
        if not views:
            return False
        
        # åŸºæœ¬æ ¼å¼æª¢æŸ¥
        pattern = re.compile(r'^\d+(?:\.\d+)?[KMB]?$', re.IGNORECASE)
        if not pattern.match(views):
            return False
        
        # æ•¸å­—åˆç†æ€§æª¢æŸ¥
        try:
            actual_number = self.convert_to_number(views)
            # è§€çœ‹æ•¸é€šå¸¸åœ¨ 1-100M ç¯„åœå…§
            return 1 <= actual_number <= 100_000_000
        except:
            return False
    
    def convert_to_number(self, number_str: str) -> int:
        """K/M/Bè½‰æ•¸å­—"""
        number_str = number_str.upper()
        if number_str.endswith('K'): return int(float(number_str[:-1]) * 1000)
        elif number_str.endswith('M'): return int(float(number_str[:-1]) * 1000000)
        elif number_str.endswith('B'): return int(float(number_str[:-1]) * 1000000000)
        else: return int(number_str)

    def extract_post_content(self, content: str) -> Optional[str]:
        """æå–è²¼æ–‡ä¸»è¦å…§å®¹ - å¢å¼·ç‰ˆæœ¬"""
        lines = content.split('\n')
        
        # å¤šç¨®ç­–ç•¥å°‹æ‰¾å…§å®¹
        content_start = -1
        
        # ç­–ç•¥1: å°‹æ‰¾ 'Markdown Content:'
        for i, line in enumerate(lines):
            if 'Markdown Content:' in line:
                content_start = i + 1
                break
        
        # ç­–ç•¥2: å¦‚æœæ²’æ‰¾åˆ°ï¼Œå°‹æ‰¾ç¬¬ä¸€å€‹éç©ºçš„å¯¦è³ªå…§å®¹è¡Œ
        if content_start == -1:
            for i, line in enumerate(lines):
                stripped = line.strip()
                if (stripped and 
                    not stripped.startswith('#') and 
                    not stripped.startswith('[') and
                    not stripped.startswith('http') and
                    not stripped.startswith('!') and
                    len(stripped) > 20):  # è‡³å°‘20å­—ç¬¦æ‰ç®—å¯¦è³ªå…§å®¹
                    content_start = i
                    break
        
        if content_start == -1:
            return None
        
        # æå–å‰å¹¾è¡Œä½œç‚ºä¸»è¦å…§å®¹
        content_lines = []
        for i in range(content_start, min(content_start + 15, len(lines))):
            if i >= len(lines):
                break
            line = lines[i].strip()
            if (line and 
                not line.startswith('[![Image') and 
                not line.startswith('[Image') and
                not line.startswith('http') and
                not line.startswith('![')):
                content_lines.append(line)
                # å¦‚æœå·²ç¶“æœ‰3è¡Œæœ‰æ•ˆå…§å®¹ï¼Œå°±è¶³å¤ äº†
                if len(content_lines) >= 3:
                    break
        
        result = '\n'.join(content_lines) if content_lines else None
        return result

    def extract_engagement_numbers(self, markdown_content: str) -> List[str]:
        """æå–æ‰€æœ‰çµ±è¨ˆæ•¸å­—åºåˆ—ï¼ˆæŒ‰è®šã€ç•™è¨€ã€è½‰ç™¼ã€åˆ†äº«ï¼‰"""
        lines = markdown_content.split('\n')
        
        # ç­–ç•¥1: æŸ¥æ‰¾è²¼æ–‡å…§å®¹å¾Œçš„ç¬¬ä¸€å€‹åœ–ç‰‡ï¼Œç„¶å¾Œæå–å¾ŒçºŒæ•¸å­—
        for i, line in enumerate(lines):
            stripped = line.strip()
            # æ‰¾åˆ°è²¼æ–‡åœ–ç‰‡ï¼ˆé€šå¸¸åœ¨Translateä¹‹å¾Œï¼‰
            if stripped.startswith('![Image') and not 'profile picture' in stripped:
                numbers = []
                # åœ¨é€™å€‹åœ–ç‰‡å¾ŒæŸ¥æ‰¾é€£çºŒçš„æ•¸å­—
                for j in range(i + 1, min(i + 20, len(lines))):
                    candidate = lines[j].strip()
                    if re.match(r'^\d+(?:\.\d+)?[KMB]?$', candidate):
                        numbers.append(candidate)
                    elif candidate and not re.match(r'^\d+(?:\.\d+)?[KMB]?$', candidate) and candidate != "Pinned":
                        # é‡åˆ°éæ•¸å­—è¡Œï¼ˆä½†è·³éPinnedï¼‰ï¼Œåœæ­¢æ”¶é›†
                        break
                
                # å¦‚æœæ‰¾åˆ°äº†æ•¸å­—åºåˆ—ï¼Œè¿”å›
                if len(numbers) >= 3:
                    return numbers
        
        # ç­–ç•¥2: å¦‚æœç­–ç•¥1å¤±æ•—ï¼ŒæŸ¥æ‰¾ä»»ä½•é€£çºŒçš„æ•¸å­—åºåˆ—
        all_numbers = []
        for i, line in enumerate(lines):
            stripped = line.strip()
            if re.match(r'^\d+(?:\.\d+)?[KMB]?$', stripped):
                # æª¢æŸ¥å‰å¾Œæ–‡ï¼Œç¢ºä¿é€™æ˜¯çµ±è¨ˆæ•¸å­—
                context_valid = False
                # æª¢æŸ¥å‰é¢10è¡Œæ˜¯å¦æœ‰åœ–ç‰‡æˆ–è²¼æ–‡å…§å®¹
                for k in range(max(0, i-10), i):
                    if "![Image" in lines[k] or "Translate" in lines[k]:
                        context_valid = True
                        break
                
                if context_valid:
                    all_numbers.append(stripped)
        
        # å¦‚æœæ‰¾åˆ°4å€‹æˆ–æ›´å¤šæ•¸å­—ï¼Œå–å‰4å€‹
        if len(all_numbers) >= 4:
            return all_numbers[:4]
        elif len(all_numbers) >= 3:
            return all_numbers[:3]
        
        return all_numbers

    def extract_likes_count(self, markdown_content: str) -> Optional[str]:
        """æå–æŒ‰è®šæ•¸"""
        lines = markdown_content.split('\n')
        
        # æ–¹æ³•1: æŸ¥æ‰¾"Like"æ¨™ç±¤å¾Œçš„æ•¸å­—ï¼ˆèˆŠæ ¼å¼ï¼‰
        for i, line in enumerate(lines):
            if line.strip() == "Like" and i + 2 < len(lines):
                next_line = lines[i + 2].strip()
                if re.match(r'^\d+(?:\.\d+)?[KMB]?$', next_line):
                    return next_line
        
        # æ–¹æ³•2: æ–°æ ¼å¼ - å¾æ•¸å­—åºåˆ—ä¸­å–ç¬¬ä¸€å€‹
        numbers = self.extract_engagement_numbers(markdown_content)
        if len(numbers) >= 1:
            return numbers[0]
        
        return None

    def extract_comments_count(self, markdown_content: str) -> Optional[str]:
        """æå–ç•™è¨€æ•¸"""
        lines = markdown_content.split('\n')
        
        # æ–¹æ³•1: èˆŠæ ¼å¼ - æŸ¥æ‰¾Commentæ¨™ç±¤
        for i, line in enumerate(lines):
            if line.strip() == "Comment" and i + 2 < len(lines):
                next_line = lines[i + 2].strip()
                if re.match(r'^\d+(?:\.\d+)?[KMB]?$', next_line):
                    return next_line
        
        # æ–¹æ³•2: æ–°æ ¼å¼ - å¾æ•¸å­—åºåˆ—ä¸­å–ç¬¬äºŒå€‹
        numbers = self.extract_engagement_numbers(markdown_content)
        if len(numbers) >= 2:
            return numbers[1]
        
        return None

    def extract_reposts_count(self, markdown_content: str) -> Optional[str]:
        """æå–è½‰ç™¼æ•¸"""
        lines = markdown_content.split('\n')
        
        # æ–¹æ³•1: èˆŠæ ¼å¼ - æŸ¥æ‰¾Repostæ¨™ç±¤
        for i, line in enumerate(lines):
            if line.strip() == "Repost" and i + 2 < len(lines):
                next_line = lines[i + 2].strip()
                if re.match(r'^\d+(?:\.\d+)?[KMB]?$', next_line):
                    return next_line
        
        # æ–¹æ³•2: æ–°æ ¼å¼ - å¾æ•¸å­—åºåˆ—ä¸­å–ç¬¬ä¸‰å€‹
        numbers = self.extract_engagement_numbers(markdown_content)
        if len(numbers) >= 3:
            return numbers[2]
        
        return None

    def extract_shares_count(self, markdown_content: str) -> Optional[str]:
        """æå–åˆ†äº«æ•¸"""
        lines = markdown_content.split('\n')
        
        # æ–¹æ³•1: èˆŠæ ¼å¼ - æŸ¥æ‰¾Shareæ¨™ç±¤
        for i, line in enumerate(lines):
            if line.strip() == "Share" and i + 2 < len(lines):
                next_line = lines[i + 2].strip()
                if re.match(r'^\d+(?:\.\d+)?[KMB]?$', next_line):
                    return next_line
        
        # æ–¹æ³•2: æ–°æ ¼å¼ - å¾æ•¸å­—åºåˆ—ä¸­å–ç¬¬å››å€‹
        numbers = self.extract_engagement_numbers(markdown_content)
        if len(numbers) >= 4:
            return numbers[3]
        
        return None

    def fetch_content_official(self, url: str) -> tuple:
        """å®˜æ–¹APIè«‹æ±‚"""
        try:
            response = requests.get(f"{self.official_reader_url}/{url}", headers=self.official_headers, timeout=60)
            if response.status_code == 200: return True, response.text
            elif response.status_code == 429: return False, "RATE_LIMITED"
            elif response.status_code == 403: return False, "BLOCKED"
            else: return False, f"HTTP {response.status_code}"
        except requests.exceptions.Timeout: return False, "TIMEOUT"
        except Exception as e: return False, str(e)

    def fetch_content_local(self, url: str, use_cache: bool = True) -> tuple:
        """æœ¬åœ°Readerè«‹æ±‚"""
        headers = self.local_headers.copy()
        if not use_cache: headers['x-no-cache'] = 'true'
        try:
            response = requests.get(f"{self.local_reader_url}/{url}", headers=headers, timeout=30)
            return (True, response.text) if response.status_code == 200 else (False, f"HTTP {response.status_code}")
        except Exception as e: return False, str(e)
    
    def parse_post(self, url: str, content: str, source: str) -> Dict:
        """è§£æè²¼æ–‡ - å®Œæ•´ç‰ˆæœ¬åŒ…å«äº’å‹•æ•¸æ“š"""
        post_id = url.split('/')[-1] if '/' in url else url
        views = self.extract_views_count(content, post_id)
        main_content = self.extract_post_content(content)
        
        # æå–äº’å‹•æ•¸æ“š
        likes = self.extract_likes_count(content)
        comments = self.extract_comments_count(content)
        reposts = self.extract_reposts_count(content)
        shares = self.extract_shares_count(content)
        
        return {
            'post_id': post_id, 'url': url, 'views': views, 'content': main_content, 'source': source,
            'likes': likes, 'comments': comments, 'reposts': reposts, 'shares': shares,
            'success': views is not None and main_content is not None,
            'has_views': views is not None, 'has_content': main_content is not None,
            'has_likes': likes is not None, 'has_comments': comments is not None,
            'has_reposts': reposts is not None, 'has_shares': shares is not None,
            'content_length': len(content)
        }

    def process_api_batch(self, urls: List[str], batch_num: int) -> Dict[str, Dict]:
        """è™•ç†APIæ‰¹æ¬¡ (ä¸¦è¡Œ)"""
        print(f"ğŸŒ APIæ‰¹æ¬¡ #{batch_num}: ä¸¦è¡Œè™•ç† {len(urls)} å€‹URL...")
        
        batch_results = {}
        max_workers = min(4, len(urls))
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_url = {executor.submit(self.fetch_content_official, url): url for url in urls}
            
            completed = 0
            for future in as_completed(future_to_url):
                url = future_to_url[future]
                completed += 1
                
                success, content = future.result()
                if success:
                    result = self.parse_post(url, content, f"API-æ‰¹æ¬¡{batch_num}")
                    batch_results[url] = result
                    status = f"âœ… ({result['views']})" if result['has_views'] else "âŒ ç„¡è§€çœ‹æ•¸"
                    print(f"   ğŸŒ {completed}/{len(urls)}: {status} {result['post_id']}")
                else:
                    batch_results[url] = {'url': url, 'success': False, 'source': f"API-æ‰¹æ¬¡{batch_num}", 'error': content}
                    print(f"   ğŸŒ {completed}/{len(urls)}: âŒ APIå¤±æ•— {url.split('/')[-1]} ({content})")
        
        return batch_results

    def process_local_batch(self, urls: List[str], batch_num: int) -> Dict[str, Dict]:
        """è™•ç†æœ¬åœ°æ‰¹æ¬¡ (ä¸¦è¡Œ + å¤±æ•—ç«‹å³è½‰API)"""
        print(f"âš¡ æœ¬åœ°æ‰¹æ¬¡ #{batch_num}: ä¸¦è¡Œè™•ç† {len(urls)} å€‹URL...")
        
        batch_results = {}
        failed_urls_for_api = []
        max_workers = min(4, len(urls))
        
        # === ç¬¬ä¸€éšæ®µï¼šæœ¬åœ°ä¸¦è¡Œè™•ç† ===
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_url = {executor.submit(self.fetch_content_local, url): url for url in urls}
            
            completed = 0
            for future in as_completed(future_to_url):
                url = future_to_url[future]
                completed += 1
                
                success, content = future.result()
                if success:
                    result = self.parse_post(url, content, f"æœ¬åœ°-æ‰¹æ¬¡{batch_num}")
                    if result['has_views']:
                        batch_results[url] = result
                        print(f"   âš¡ {completed}/{len(urls)}: âœ… ({result['views']}) {result['post_id']}")
                        continue
                
                # æœ¬åœ°å¤±æ•—ï¼ŒåŠ å…¥APIå¾…è™•ç†åˆ—è¡¨
                failed_urls_for_api.append(url)
                print(f"   âš¡ {completed}/{len(urls)}: âŒ æœ¬åœ°å¤±æ•— {url.split('/')[-1]} â†’ è½‰é€API")
        
        # === ç¬¬äºŒéšæ®µï¼šå¤±æ•—çš„URLç«‹å³å¹³è¡Œè½‰é€API ===
        if failed_urls_for_api:
            print(f"   ğŸŒ æœ¬åœ°å¤±æ•—é …ç›®ç«‹å³è½‰API: {len(failed_urls_for_api)} å€‹...")
            
            with ThreadPoolExecutor(max_workers=max_workers) as api_executor:
                api_future_to_url = {api_executor.submit(self.fetch_content_official, url): url for url in failed_urls_for_api}
                
                api_completed = 0
                for api_future in as_completed(api_future_to_url):
                    url = api_future_to_url[api_future]
                    api_completed += 1
                    
                    api_success, api_content = api_future.result()
                    if api_success:
                        result = self.parse_post(url, api_content, f"API-å›é€€{batch_num}")
                        batch_results[url] = result
                        if result['has_views']:
                            print(f"      ğŸŒ {api_completed}/{len(failed_urls_for_api)}: âœ… APIæ•‘æ´æˆåŠŸ ({result['views']}) {result['post_id']}")
                        else:
                            print(f"      ğŸŒ {api_completed}/{len(failed_urls_for_api)}: âŒ APIç„¡è§€çœ‹æ•¸ {result['post_id']}")
                    else:
                        batch_results[url] = {'url': url, 'success': False, 'source': f"API-å›é€€{batch_num}", 'error': api_content}
                        print(f"      ğŸŒ {api_completed}/{len(failed_urls_for_api)}: âŒ APIä¹Ÿå¤±æ•— {url.split('/')[-1]}")
        
        return batch_results

    def rotation_pipeline(self, urls: List[str]):
        """è¼ªè¿´ç­–ç•¥ç®¡ç·šè™•ç†"""
        total_start_time = time.time()
        
        print(f"ğŸ”„ è¼ªè¿´ç­–ç•¥ç®¡ç·šå•Ÿå‹•")
        print(f"ğŸ“Š è™•ç† {len(urls)} å€‹URL")
        print(f"ğŸŒ APIæ‰¹æ¬¡å¤§å°: {self.api_batch_size} | âš¡ æœ¬åœ°æ‰¹æ¬¡å¤§å°: {self.local_batch_size}")
        print(f"ğŸ•’ æœ¬åœ°æ‰¹æ¬¡å•Ÿå‹•æ™‚é–“è‡ªå‹•æä¾›APIå†·å»")
        print("âœ… å·²æ•´åˆæœ€ä½³åŒ–: Headersé…ç½® + NBSPæ­£è¦åŒ– + é›™é‡æå–é‚è¼¯")
        print("=" * 80)
        
        all_results = {}
        remaining_urls = urls.copy()
        batch_counter = 1
        
        while remaining_urls:
            current_batch_size = len(remaining_urls)
            
            # === APIæ‰¹æ¬¡è™•ç† ===
            if current_batch_size > 0:
                api_batch_size = min(self.api_batch_size, current_batch_size)
                api_batch_urls = remaining_urls[:api_batch_size]
                remaining_urls = remaining_urls[api_batch_size:]
                
                api_results = self.process_api_batch(api_batch_urls, batch_counter)
                all_results.update(api_results)
                # æœ¬åœ°æ‰¹æ¬¡å•Ÿå‹•æ™‚é–“å·²æä¾›è¶³å¤ çš„APIå†·å»
            
            # === æœ¬åœ°æ‰¹æ¬¡è™•ç† ===
            if remaining_urls:
                local_batch_size = min(self.local_batch_size, len(remaining_urls))
                local_batch_urls = remaining_urls[:local_batch_size]
                remaining_urls = remaining_urls[local_batch_size:]
                
                local_results = self.process_local_batch(local_batch_urls, batch_counter)
                all_results.update(local_results)
            
            batch_counter += 1
            
            # é¡¯ç¤ºé€²åº¦
            processed_count = len(all_results)
            print(f"\nğŸ“Š å·²è™•ç†: {processed_count}/{len(urls)} ({processed_count/len(urls)*100:.1f}%)")
            print(f"ğŸ¯ å‰©é¤˜: {len(remaining_urls)} å€‹URL")
            
            if remaining_urls:
                print("-" * 60)
        
        # === æœ€çµ‚çµ±è¨ˆ ===
        total_end_time = time.time()
        final_results = [all_results.get(url, {'url': url, 'success': False}) for url in urls]
        
        print("\n" + "=" * 80)
        success_count = len([res for res in final_results if res.get('has_views')])
        api_success_count = len([res for res in final_results if res.get('has_views') and 'API' in res.get('source', '')])
        local_success_count = success_count - api_success_count
        
        # çµ±è¨ˆå„æ‰¹æ¬¡çš„æˆåŠŸç‡
        api_batches = {}
        local_batches = {}
        api_rescue_batches = {}
        
        for res in final_results:
            if res.get('source'):
                source = res['source']
                if 'API-æ‰¹æ¬¡' in source:
                    batch_key = source.split('æ‰¹æ¬¡')[1]
                    if batch_key not in api_batches:
                        api_batches[batch_key] = {'total': 0, 'success': 0}
                    api_batches[batch_key]['total'] += 1
                    if res.get('has_views'):
                        api_batches[batch_key]['success'] += 1
                elif 'æœ¬åœ°-æ‰¹æ¬¡' in source:
                    batch_key = source.split('æ‰¹æ¬¡')[1]
                    if batch_key not in local_batches:
                        local_batches[batch_key] = {'total': 0, 'success': 0}
                    local_batches[batch_key]['total'] += 1
                    if res.get('has_views'):
                        local_batches[batch_key]['success'] += 1
                elif 'API-å›é€€' in source:
                    batch_key = source.split('å›é€€')[1]
                    if batch_key not in api_rescue_batches:
                        api_rescue_batches[batch_key] = {'total': 0, 'success': 0}
                    api_rescue_batches[batch_key]['total'] += 1
                    if res.get('has_views'):
                        api_rescue_batches[batch_key]['success'] += 1
        
        print(f"âœ… æœ€çµ‚æˆåŠŸ: {success_count}/{len(urls)} ({success_count/len(urls)*100:.1f}%)")
        print(f"ğŸŒ APIæˆåŠŸ: {api_success_count} | âš¡ æœ¬åœ°æˆåŠŸ: {local_success_count}")
        print(f"â±ï¸ ç¸½è€—æ™‚: {total_end_time - total_start_time:.1f}s")
        print(f"ğŸï¸ å¹³å‡é€Ÿåº¦: {len(urls)/(total_end_time - total_start_time):.2f} URL/s")
        
        print(f"\nğŸ“ˆ å„æ‰¹æ¬¡æˆåŠŸç‡:")
        for batch_key in sorted(api_batches.keys()):
            stats = api_batches[batch_key]
            rate = stats['success'] / stats['total'] * 100 if stats['total'] > 0 else 0
            print(f"   ğŸŒ APIæ‰¹æ¬¡{batch_key}: {stats['success']}/{stats['total']} ({rate:.1f}%)")
        for batch_key in sorted(local_batches.keys()):
            stats = local_batches[batch_key]
            rate = stats['success'] / stats['total'] * 100 if stats['total'] > 0 else 0
            print(f"   âš¡ æœ¬åœ°æ‰¹æ¬¡{batch_key}: {stats['success']}/{stats['total']} ({rate:.1f}%)")
        for batch_key in sorted(api_rescue_batches.keys()):
            stats = api_rescue_batches[batch_key]
            rate = stats['success'] / stats['total'] * 100 if stats['total'] > 0 else 0
            print(f"   ğŸš€ APIæ•‘æ´{batch_key}: {stats['success']}/{stats['total']} ({rate:.1f}%)")
        
        return final_results

def generate_test_urls(original_urls: List[str], target_count: int = 100) -> List[str]:
    """å¾åŸå§‹URLç”Ÿæˆæ¸¬è©¦ç”¨çš„URLåˆ—è¡¨"""
    test_urls = []
    test_urls.extend(original_urls)
    
    while len(test_urls) < target_count:
        remaining = target_count - len(test_urls)
        if remaining >= len(original_urls):
            shuffled_urls = original_urls.copy()
            random.shuffle(shuffled_urls)
            test_urls.extend(shuffled_urls)
        else:
            selected_urls = random.sample(original_urls, remaining)
            test_urls.extend(selected_urls)
    
    return test_urls[:target_count]

def load_urls_from_json(file_path: str) -> List[str]:
    try:
        with open(file_path, 'r', encoding='utf-8') as f: data = json.load(f)
        urls = [post['url'] for post in data.get('posts', []) if 'url' in post]
        print(f"âœ… å¾ {file_path} æˆåŠŸæå– {len(urls)} å€‹åŸå§‹ URLã€‚")
        return urls
    except Exception as e:
        print(f"âŒ æå– URL æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return []

def main():
    # è¼‰å…¥åŸå§‹URL
    original_urls = load_urls_from_json("agents/playwright_crawler/debug/crawl_data_20250803_121452_934d52b1.json")
    if not original_urls: return
    
    # ç”Ÿæˆ100å€‹æ¸¬è©¦URL
    test_urls = generate_test_urls(original_urls, 100)
    print(f"ğŸ¯ ç”Ÿæˆ {len(test_urls)} å€‹æ¸¬è©¦URL (å¾ {len(original_urls)} å€‹åŸå§‹URLæ“´å±•)")
    
    pipeline = RotationPipelineReader()
    results = pipeline.rotation_pipeline(test_urls)
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    filename = f"rotation_pipeline_results_{timestamp}.json"
    with open(filename, 'w', encoding='utf-8') as f: 
        json.dump({
            'total_urls': len(results),
            'successful_extractions': len([r for r in results if r.get('has_views')]),
            'api_batch_size': 10,
            'local_batch_size': 20,
            'original_url_count': len(original_urls),
            'results': results
        }, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ çµæœå·²ä¿å­˜åˆ°: {filename}")

if __name__ == '__main__':
    main()