#!/usr/bin/env python3
"""
Simple Header Fix Test - åŸºæ–¼æ¸¬è©¦çµæœçš„æœ€å°ä¿®æ­£
"""
import requests
import json
import time
import re
import uuid
from datetime import datetime

class SimpleHeadersFix:
    def __init__(self):
        self.base_url = "http://localhost:8880"
        
    def normalize_content(self, content):
        """æ¨™æº–åŒ–å…§å®¹è™•ç† NBSP"""
        content = content.replace('\u00a0', ' ')  # NBSP
        content = content.replace('\u2002', ' ')  # En space
        content = content.replace('\u2003', ' ')  # Em space
        content = re.sub(r' +', ' ', content)
        return content
    
    def extract_views(self, content):
        """æå–è§€çœ‹æ•¸"""
        content = self.normalize_content(content)
        
        patterns = [
            r'Thread\s*={6}\s*([0-9,\.]+[KMB]?)\s*views?',
            r'Thread\s*={6}\s*([0-9,\.]+[KMB]?)',
            r'(\d+(?:\.\d+)?[KMB]?)\s*views?',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, content, re.IGNORECASE)
            if match:
                return match.group(1)
        return None
    
    def is_aggregated_page(self, content, url):
        """æª¢æŸ¥æ˜¯å¦ç‚ºèšåˆé é¢"""
        if 'Related threads' in content:
            return True
        
        # æå–åŸå§‹ post_id
        post_id_match = re.search(r'/post/([A-Za-z0-9_-]+)', url)
        if not post_id_match:
            return False
        
        original_post_id = post_id_match.group(1)
        other_post_ids = re.findall(r'/post/([A-Za-z0-9_-]+)', content)
        other_unique_ids = set(other_post_ids) - {original_post_id}
        
        return len(other_unique_ids) > 1
    
    def fetch_with_optimal_headers(self, url):
        """ä½¿ç”¨æœ€ä½³åŒ–çš„ headers ç²å–å…§å®¹"""
        headers = {
            "User-Agent": (
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                "AppleWebKit/537.36 (KHTML, like Gecko) "
                "Chrome/126.0.0.0 Safari/537.36"
            ),
            "x-wait-for-selector": "article",
            "x-timeout": "25"
        }
        
        try:
            response = requests.get(f"{self.base_url}/{url}", headers=headers, timeout=60)
            
            if response.status_code != 200:
                return None, f"HTTP {response.status_code}"
            
            content = response.text
            
            # æª¢æŸ¥æ˜¯å¦ç‚ºèšåˆé é¢
            if self.is_aggregated_page(content, url):
                return None, "èšåˆé é¢"
            
            # æå–è§€çœ‹æ•¸
            views = self.extract_views(content)
            
            if views:
                return {
                    'views': views,
                    'content': content[:500] + '...' if len(content) > 500 else content,
                    'content_length': len(content)
                }, "æˆåŠŸ"
            else:
                return None, "æœªæ‰¾åˆ°è§€çœ‹æ•¸"
                
        except Exception as e:
            return None, f"è«‹æ±‚éŒ¯èª¤: {str(e)}"
    
    def test_urls_list(self, urls):
        """æ¸¬è©¦ URL åˆ—è¡¨"""
        print("ğŸš€ ç°¡åŒ–ç‰ˆ Header ä¿®æ­£æ¸¬è©¦")
        print("=" * 60)
        
        results = []
        
        for i, url in enumerate(urls, 1):
            print(f"\nğŸ“„ æ¸¬è©¦ {i}/{len(urls)}: {url.split('/')[-1]}")
            
            result, reason = self.fetch_with_optimal_headers(url)
            
            if result:
                print(f"âœ… æˆåŠŸæå–è§€çœ‹æ•¸: {result['views']}")
                print(f"   å…§å®¹é•·åº¦: {result['content_length']:,} å­—ç¬¦")
                success = True
            else:
                print(f"âŒ å¤±æ•—: {reason}")
                success = False
            
            results.append({
                'url': url,
                'success': success,
                'views': result['views'] if result else None,
                'reason': reason,
                'timestamp': datetime.now().isoformat()
            })
            
            # é–“éš”é¿å…éæ–¼é »ç¹
            if i < len(urls):
                time.sleep(2)
        
        # çµ±è¨ˆçµæœ
        success_count = sum(1 for r in results if r['success'])
        total_count = len(results)
        success_rate = success_count / total_count * 100
        
        print("\n" + "=" * 60)
        print("ğŸ“Š æ¸¬è©¦çµæœç¸½çµ")
        print("=" * 60)
        print(f"âœ… æˆåŠŸ: {success_count}/{total_count} ({success_rate:.1f}%)")
        
        # é¡¯ç¤ºæˆåŠŸæ¡ˆä¾‹
        successful = [r for r in results if r['success']]
        if successful:
            print("\nğŸ¯ æˆåŠŸæ¡ˆä¾‹:")
            for r in successful:
                post_id = r['url'].split('/')[-1]
                print(f"   â€¢ {post_id}: {r['views']}")
        
        # é¡¯ç¤ºå¤±æ•—åŸå› 
        failed = [r for r in results if not r['success']]
        if failed:
            print("\nâŒ å¤±æ•—åŸå› çµ±è¨ˆ:")
            reasons = {}
            for r in failed:
                reason = r['reason']
                reasons[reason] = reasons.get(reason, 0) + 1
            
            for reason, count in reasons.items():
                print(f"   â€¢ {reason}: {count} æ¬¡")
        
        # ä¿å­˜çµæœ
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"simple_headers_test_{timestamp}.json"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump({
                'test_name': 'ç°¡åŒ–ç‰ˆHeaderä¿®æ­£æ¸¬è©¦',
                'timestamp': datetime.now().isoformat(),
                'success_rate': success_rate,
                'results': results
            }, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ è©³ç´°çµæœå·²ä¿å­˜: {filename}")
        
        return success_rate >= 70  # å¦‚æœæˆåŠŸç‡ >= 70% å°±ç®—é€šé

def main():
    # ä½¿ç”¨åŸå§‹æ¸¬è©¦ URL
    test_urls = [
        'https://www.threads.com/@ttshow.tw/post/DMfOVeqSkM5',
        'https://www.threads.com/@ttshow.tw/post/DIfkbgLSjO3',
        'https://www.threads.com/@ttshow.tw/post/DL_vyT-RZQ6',
    ]
    
    tester = SimpleHeadersFix()
    success = tester.test_urls_list(test_urls)
    
    if success:
        print("\nğŸ‰ æ¸¬è©¦é€šéï¼å¯ä»¥å°‡é€™äº› headers æ•´åˆåˆ°ä¸»ç¨‹å¼ä¸­")
    else:
        print("\nâš ï¸ æˆåŠŸç‡ä»éœ€æ”¹å–„ï¼Œå»ºè­°é€²ä¸€æ­¥èª¿æ•´ç­–ç•¥")

if __name__ == '__main__':
    main()