"""
ä¿®å¾©ç‰ˆå…§å®¹æå–å™¨
åŸºæ–¼æ””æˆªåˆ°çš„ BarcelonaProfileThreadsTabRefetchableDirectQuery API
"""

import asyncio
import json
import httpx
import urllib.parse
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Optional, List

import sys
sys.path.append(str(Path(__file__).parent))

from common.config import get_auth_file_path

# ä¿®å¾©å¾Œçš„åƒæ•¸
FIXED_DOC_ID = "24435639366126837"  # å¾æ””æˆªçµæœä¸­ç²å¾—
QUERY_NAME = "BarcelonaProfileThreadsTabRefetchableDirectQuery"

# æ¸¬è©¦ç›®æ¨™
TEST_USERNAME = "threads"  # ä½¿ç”¨threadså®˜æ–¹å¸³è™Ÿé€²è¡Œæ¸¬è©¦

class FixedContentExtractor:
    """ä¿®å¾©ç‰ˆå…§å®¹æå–å™¨ - ä½¿ç”¨æ–°ç™¼ç¾çš„API"""
    
    def __init__(self):
        self.auth_data = None
        self.headers = None
        self.cookies = None
        
    async def load_auth(self):
        """è¼‰å…¥èªè­‰ä¿¡æ¯"""
        auth_file_path = get_auth_file_path()
        if not auth_file_path.exists():
            raise FileNotFoundError(f"èªè­‰æª”æ¡ˆ {auth_file_path} ä¸å­˜åœ¨")
            
        self.auth_data = json.loads(auth_file_path.read_text())
        self.cookies = {cookie['name']: cookie['value'] for cookie in self.auth_data.get('cookies', [])}
        
        # å¾æ””æˆªçµæœä¸­å¾©åˆ¶åŸºæœ¬headers
        self.headers = {
            "accept": "*/*",
            "accept-language": "zh-TW,zh;q=0.9,en;q=0.8",
            "content-type": "application/x-www-form-urlencoded",
            "sec-ch-prefers-color-scheme": "dark",
            "sec-ch-ua": '"Google Chrome";v="131", "Chromium";v="131", "Not_A Brand";v="24"',
            "sec-ch-ua-mobile": "?0",
            "sec-ch-ua-platform": '"Windows"',
            "sec-fetch-dest": "empty",
            "sec-fetch-mode": "cors",
            "sec-fetch-site": "same-origin",
            "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
            "x-asbd-id": "129477",
            "x-fb-friendly-name": QUERY_NAME,
            "x-fb-lsd": "",  # éœ€è¦å¾cookiesä¸­ç²å–
            "x-ig-app-id": "238260118697367"
        }
        
        # å¾cookiesä¸­ç²å–é—œéµèªè­‰ä¿¡æ¯
        self.fb_dtsg = None
        # æŸ¥æ‰¾å„ç¨®å¯èƒ½çš„dtsgæ ¼å¼
        dtsg_candidates = ['fb_dtsg', 'dtsg', 'datr']
        for candidate in dtsg_candidates:
            if candidate in self.cookies:
                self.fb_dtsg = self.cookies[candidate]
                break
        
        # è¨­ç½®LSD - æŸ¥æ‰¾å„ç¨®å¯èƒ½çš„lsdæ ¼å¼  
        lsd_candidates = ['lsd', 'x-fb-lsd', '_js_lsd']
        for candidate in lsd_candidates:
            if candidate in self.cookies:
                self.headers["x-fb-lsd"] = self.cookies[candidate]
                break
        
        # å¦‚æœé‚„æ˜¯æ²’æœ‰æ‰¾åˆ°ï¼Œå˜—è©¦å¾localStorageä¸­ç²å–
        if not self.fb_dtsg:
            local_storage = self.auth_data.get('localStorage', [])
            for item in local_storage:
                if 'dtsg' in item.get('name', '').lower():
                    self.fb_dtsg = item.get('value', '')
                    break
        
        # èª¿è©¦ä¿¡æ¯ï¼šåˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„cookies
        print(f"   ğŸ“‹ å¯ç”¨cookies: {list(self.cookies.keys())}")
        if hasattr(self.auth_data, 'get') and 'localStorage' in self.auth_data:
            local_storage_keys = [item.get('name', '') for item in self.auth_data.get('localStorage', [])]
            print(f"   ğŸ“‹ å¯ç”¨localStorage: {local_storage_keys[:10]}...")  # åªé¡¯ç¤ºå‰10å€‹
        
        print(f"âœ… èªè­‰è¼‰å…¥å®Œæˆ")
        print(f"   ğŸ“Š Cookies: {len(self.cookies)} å€‹")
        print(f"   ğŸ”‘ FB DTSG: {'æ˜¯' if self.fb_dtsg else 'å¦'}")
        print(f"   ğŸ« LSD: {'æ˜¯' if self.headers.get('x-fb-lsd') else 'å¦'}")
    
    async def get_user_posts(self, username: str, limit: int = 10) -> Optional[List[Dict[str, Any]]]:
        """ç²å–ç”¨æˆ¶çš„è²¼æ–‡åˆ—è¡¨"""
        print(f"ğŸ” ç²å– @{username} çš„è²¼æ–‡ (é™åˆ¶: {limit} ç¯‡)...")
        
        # åŸºæ–¼æ””æˆªçµæœæ§‹å»ºvariables
        variables = {
            "after": None,
            "before": None,
            "first": limit,
            "last": None,
            "userID": None,  # éœ€è¦æŸ¥æ‰¾userID
            "__relay_internal__pv__BarcelonaIsLoggedInrelayprovider": True,
            "__relay_internal__pv__BarcelonaHasSelfReplyContextrelayprovider": False,
            "__relay_internal__pv__BarcelonaHasInlineReplyComposerrelayprovider": True,
            "__relay_internal__pv__BarcelonaHasEventBadgerelayprovider": False,
            "__relay_internal__pv__BarcelonaIsSearchDiscoveryEnabledrelayprovider": False,
            "__relay_internal__pv__IsTagIndicatorEnabledrelayprovider": True,
            "__relay_internal__pv__BarcelonaOptionalCookiesEnabledrelayprovider": True,
            "__relay_internal__pv__BarcelonaHasSelfThreadCountrelayprovider": False,
            "__relay_internal__pv__BarcelonaHasSpoilerStylingInforelayprovider": True,
            "__relay_internal__pv__BarcelonaHasDeepDiverelayprovider": False,
            "__relay_internal__pv__BarcelonaQuotedPostUFIEnabledrelayprovider": False,
            "__relay_internal__pv__BarcelonaHasTopicTagsrelayprovider": True,
            "__relay_internal__pv__BarcelonaIsCrawlerrelayprovider": False,
            "__relay_internal__pv__BarcelonaHasDisplayNamesrelayprovider": False,
            "__relay_internal__pv__BarcelonaCanSeeSponsoredContentrelayprovider": True,
            "__relay_internal__pv__BarcelonaShouldShowFediverseM075Featuresrelayprovider": True,
            "__relay_internal__pv__BarcelonaImplicitTrendsGKrelayprovider": False,
            "__relay_internal__pv__BarcelonaIsInternalUserrelayprovider": False
        }
        
        # å¦‚æœæ˜¯threadså®˜æ–¹å¸³è™Ÿï¼Œä½¿ç”¨å·²çŸ¥çš„userID
        if username == "threads":
            variables["userID"] = "63082166531"  # å¾æ””æˆªçµæœä¸­ç²å¾—
        else:
            print(f"âš ï¸ æœªçŸ¥ç”¨æˆ¶çš„userIDï¼Œå¯èƒ½éœ€è¦å…ˆæŸ¥è©¢")
            return None
        
        # æ§‹å»ºPOSTæ•¸æ“š (åŸºæ–¼æ””æˆªçµæœçš„æ ¼å¼)
        post_data = {
            "av": "17841476239996865",
            "__user": "0",
            "__a": "1",
            "__req": "1",
            "__hs": "20306.HYP:barcelona_web_pkg.2.1...0",
            "dpr": "3",
            "__ccg": "EXCELLENT",
            "__rev": "1025565823",
            "__s": "ibysx9:m2yycf:cxltff",
            "__hsi": "7535438263061243030",
            "__comet_req": "29",
            "fb_dtsg": self.fb_dtsg or "",
            "jazoest": "26145",
            "lsd": self.headers.get("x-fb-lsd", ""),
            "__spin_r": "1025565823",
            "__spin_b": "trunk",
            "__spin_t": str(int(datetime.now().timestamp())),
            "fb_api_caller_class": "RelayModern",
            "fb_api_req_friendly_name": QUERY_NAME,
            "variables": json.dumps(variables),
            "server_timestamps": "true",
            "doc_id": FIXED_DOC_ID
        }
        
        # URLç·¨ç¢¼
        encoded_data = urllib.parse.urlencode(post_data)
        
        # ç™¼é€è«‹æ±‚
        async with httpx.AsyncClient(
            headers=self.headers,
            cookies=self.cookies,
            timeout=30.0,
            follow_redirects=True
        ) as client:
            try:
                response = await client.post(
                    "https://www.threads.com/graphql/query",
                    data=encoded_data
                )
                
                print(f"   ğŸ“¡ HTTP {response.status_code}")
                
                if response.status_code == 200:
                    try:
                        result = response.json()
                        
                        if "errors" in result:
                            print(f"   âŒ GraphQL éŒ¯èª¤: {result['errors']}")
                            return None
                        
                        if "data" in result and result["data"] and "mediaData" in result["data"]:
                            media_data = result["data"]["mediaData"]
                            posts = self._parse_posts_from_response(media_data)
                            print(f"   âœ… æˆåŠŸç²å– {len(posts)} ç¯‡è²¼æ–‡")
                            return posts
                        else:
                            print(f"   âŒ æœªé æœŸçš„éŸ¿æ‡‰çµæ§‹: {list(result.get('data', {}).keys()) if result.get('data') else 'No data'}")
                            return None
                    
                    except Exception as e:
                        print(f"   âŒ è§£æéŸ¿æ‡‰å¤±æ•—: {e}")
                        # ä¿å­˜åŸå§‹éŸ¿æ‡‰ç”¨æ–¼èª¿è©¦
                        debug_file = Path(f"debug_response_{datetime.now().strftime('%H%M%S')}.json")
                        with open(debug_file, 'w', encoding='utf-8') as f:
                            f.write(response.text)
                        print(f"   ğŸ“ åŸå§‹éŸ¿æ‡‰å·²ä¿å­˜: {debug_file}")
                        return None
                
                else:
                    print(f"   âŒ HTTP éŒ¯èª¤: {response.status_code}")
                    print(f"   ğŸ“„ éŒ¯èª¤å…§å®¹: {response.text[:500]}...")
                    return None
            
            except Exception as e:
                print(f"   âŒ è«‹æ±‚å¤±æ•—: {e}")
                return None
    
    def _parse_posts_from_response(self, media_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """å¾éŸ¿æ‡‰ä¸­è§£æè²¼æ–‡æ•¸æ“š"""
        posts = []
        
        try:
            edges = media_data.get("edges", [])
            for edge in edges:
                node = edge.get("node", {})
                thread_items = node.get("thread_items", [])
                
                for item in thread_items:
                    post_data = item.get("post", {})
                    if not post_data:
                        continue
                    
                    # æå–åŸºæœ¬ä¿¡æ¯
                    pk = post_data.get("pk", "")
                    user_info = post_data.get("user", {})
                    username = user_info.get("username", "")
                    
                    # æå–è¨ˆæ•¸æ•¸æ“š
                    like_count = post_data.get("like_count", 0)
                    text_info = post_data.get("text_post_app_info", {})
                    direct_reply_count = text_info.get("direct_reply_count", 0)
                    repost_count = text_info.get("repost_count", 0)
                    reshare_count = text_info.get("reshare_count", 0)
                    
                    # æå–å…§å®¹
                    content = ""
                    text_fragments = text_info.get("text_fragments", {})
                    fragments = text_fragments.get("fragments", [])
                    for fragment in fragments:
                        if fragment.get("fragment_type") == "plaintext":
                            content += fragment.get("plaintext", "")
                    
                    # æ§‹å»ºçµæœ
                    post_result = {
                        "pk": pk,
                        "username": username,
                        "content": content.strip(),
                        "like_count": like_count,
                        "comment_count": direct_reply_count,
                        "repost_count": repost_count,
                        "share_count": reshare_count,
                        "url": f"https://www.threads.com/@{username}/post/{pk}" if username and pk else "",
                        "extracted_at": datetime.now().isoformat(),
                        "source": "fixed_extractor",
                        "success": True
                    }
                    
                    posts.append(post_result)
                    
                    print(f"      ğŸ“„ @{username}: {like_count} è®š, {direct_reply_count} ç•™è¨€")
                    if content:
                        print(f"         ğŸ“ å…§å®¹: {content[:50]}...")
        
        except Exception as e:
            print(f"   âŒ è§£æè²¼æ–‡æ•¸æ“šå¤±æ•—: {e}")
        
        return posts

async def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸš€ ä¿®å¾©ç‰ˆå…§å®¹æå–å™¨")
    print(f"ğŸ¯ ä½¿ç”¨æ–°ç™¼ç¾çš„API: {QUERY_NAME}")
    print(f"ğŸ“‹ Doc ID: {FIXED_DOC_ID}")
    
    extractor = FixedContentExtractor()
    
    try:
        # è¼‰å…¥èªè­‰
        await extractor.load_auth()
        
        # æ¸¬è©¦ç²å–è²¼æ–‡
        posts = await extractor.get_user_posts(TEST_USERNAME, limit=5)
        
        if posts:
            print(f"\nğŸ‰ ä¿®å¾©æˆåŠŸï¼ç²å–åˆ° {len(posts)} ç¯‡è²¼æ–‡")
            
            # ä¿å­˜çµæœ
            result_file = Path(f"fixed_extraction_result_{datetime.now().strftime('%H%M%S')}.json")
            with open(result_file, 'w', encoding='utf-8') as f:
                json.dump(posts, f, indent=2, ensure_ascii=False)
            print(f"ğŸ“ çµæœå·²ä¿å­˜: {result_file}")
            
            # é¡¯ç¤ºçµ±è¨ˆ
            total_likes = sum(p.get('like_count', 0) for p in posts)
            total_comments = sum(p.get('comment_count', 0) for p in posts)
            print(f"\nğŸ“Š çµ±è¨ˆ:")
            print(f"   ğŸ‘ ç¸½æŒ‰è®šæ•¸: {total_likes:,}")
            print(f"   ğŸ’¬ ç¸½ç•™è¨€æ•¸: {total_comments:,}")
            
            print(f"\nâœ… APIä¿®å¾©å®Œæˆï¼ç¾åœ¨å¯ä»¥å°‡æ­¤é‚è¼¯æ•´åˆåˆ°ä¸»çˆ¬èŸ²ä¸­ã€‚")
        else:
            print(f"\nğŸ˜ ä¿®å¾©æ¸¬è©¦å¤±æ•—")
    
    except Exception as e:
        print(f"\nâŒ ä¿®å¾©æ¸¬è©¦éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")

if __name__ == "__main__":
    asyncio.run(main())